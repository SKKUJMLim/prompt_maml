{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "00a643d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pytorchcv.model_provider import get_model as ptcv_get_model # model\n",
    "import sys, os\n",
    "import easydict\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline \n",
    "\n",
    "# enable cuda devices\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from meta_neural_network_architectures import VGGReLUNormNetwork, ResNet12\n",
    "from utils.parser_utils import get_args\n",
    "from data import MetaLearningSystemDataLoader\n",
    "from experiment_builder import ExperimentBuilder\n",
    "from utils import basic\n",
    "from few_shot_learning_system import MAMLFewShotClassifier\n",
    "import prompters\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "abfd3787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화 하기 위한 라이브러리\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e0281c",
   "metadata": {},
   "source": [
    "# 1. Dataset 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1418f5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choices=['padding', 'random_patch', 'fixed_patch'],\n",
    "method = 'padding'\n",
    "\n",
    "dataset = \"mini_imagenet_full_size\"\n",
    "# dataset = \"tiered_imagenet\" \n",
    "dataset = \"CIFAR_FS\" \n",
    "# dataset = \"CUB\" \n",
    "os.environ['TEST_DATASET'] = dataset\n",
    "\n",
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1855befa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.environ['DATASET_DIR'] ===  C:/Users/JM/PycharmProjects/MAML/datasets\n"
     ]
    }
   ],
   "source": [
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "print(\"os.environ['DATASET_DIR'] === \", os.environ['DATASET_DIR'])\n",
    "\n",
    "args = easydict.EasyDict(\n",
    "{\n",
    "  \"batch_size\":2,\n",
    "  \"image_height\":84,\n",
    "  \"image_width\":84,\n",
    "  \"image_channels\":3,\n",
    "  \"gpu_to_use\":0,\n",
    "  \"num_dataprovider_workers\":4,\n",
    "  \"max_models_to_save\":5,\n",
    "  \"dataset_name\":\"mini_imagenet_full_size\",\n",
    "  \"dataset_path\":\"mini_imagenet_full_size\",\n",
    "  \"reset_stored_paths\":False,\n",
    "  \"experiment_name\":\"../ANIL_5way_5shot_filter64_miniImagenet\",\n",
    "  \"train_seed\": 0, \"val_seed\": 0,\n",
    "  \"indexes_of_folders_indicating_class\": [-3, -2],\n",
    "  \"sets_are_pre_split\": True,\n",
    "  \"train_val_test_split\": [0.64, 0.16, 0.20],\n",
    "  \"evaluate_on_test_set_only\": False,\n",
    "\n",
    "  \"total_epochs\": 100,\n",
    "  \"total_iter_per_epoch\":500, \"continue_from_epoch\": -2,\n",
    "  \"num_evaluation_tasks\":600,\n",
    "  \"multi_step_loss_num_epochs\": 15,\n",
    "  \"minimum_per_task_contribution\": 0.01,\n",
    "  \"learnable_per_layer_per_step_inner_loop_learning_rate\": False,\n",
    "  \"enable_inner_loop_optimizable_bn_params\": False,\n",
    "  \"evalute_on_test_set_only\": False,\n",
    "\n",
    "  \"max_pooling\": True,\n",
    "  \"per_step_bn_statistics\": False,\n",
    "  \"learnable_batch_norm_momentum\": False,\n",
    "  \"load_into_memory\": False,\n",
    "  \"init_inner_loop_learning_rate\": 0.01,\n",
    "  \"init_inner_loop_weight_decay\": 0.0005,\n",
    "  \"learnable_bn_gamma\": True,\n",
    "  \"learnable_bn_beta\": True,\n",
    "\n",
    "  \"dropout_rate_value\":0.0,\n",
    "  \"min_learning_rate\":0.001,\n",
    "  \"meta_learning_rate\":0.001,   \"total_epochs_before_pause\": 100,\n",
    "  \"first_order_to_second_order_epoch\":-1,\n",
    "  \"weight_decay\": 0.0,\n",
    "\n",
    "  \"norm_layer\":\"batch_norm\",\n",
    "  \"cnn_num_filters\":48,\n",
    "  \"num_stages\":4,\n",
    "  \"conv_padding\": True,\n",
    "  \"number_of_training_steps_per_iter\":5,\n",
    "  \"number_of_evaluation_steps_per_iter\":5,\n",
    "  \"cnn_blocks_per_stage\":1,\n",
    "  \"num_classes_per_set\":5,\n",
    "  \"num_samples_per_class\":5,\n",
    "  \"num_target_samples\": 15,\n",
    "  \"samples_per_iter\" : 1,\n",
    "\n",
    "  \"second_order\": True,\n",
    "  \"backbone\" : \"CONV\",\n",
    "  \"use_multi_step_loss_optimization\":False,\n",
    "  \"prompter\": False,\n",
    "  \"prompt_engineering\": \"not\",\n",
    "  \"prompt_size\" : 0,\n",
    "  \"image_size\" : 0,\n",
    "  \"ANIL\": False,\n",
    "  \"BOIL\": False\n",
    "}\n",
    ")\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "args.im_shape = (2, 3, args.image_height, args.image_width)\n",
    "\n",
    "args.use_cuda = torch.cuda.is_available()\n",
    "args.seed = 104\n",
    "args.reverse_channels=False\n",
    "args.labels_as_int=False\n",
    "args.reset_stored_filepaths=False\n",
    "args.num_of_gpus=1\n",
    "\n",
    "args.continue_from_epoch='latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc47251",
   "metadata": {},
   "source": [
    "# 2. 모델 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "197487d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using max pooling\n",
      "meta network params\n",
      "Inner Loop parameters\n",
      "Outer Loop parameters\n",
      "classifier.layer_dict.conv0.conv.weight torch.Size([48, 3, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv0.conv.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv0.norm_layer.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv0.norm_layer.weight torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv1.conv.weight torch.Size([48, 48, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv1.conv.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv1.norm_layer.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv1.norm_layer.weight torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv2.conv.weight torch.Size([48, 48, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv2.conv.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv2.norm_layer.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv2.norm_layer.weight torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv3.conv.weight torch.Size([48, 48, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv3.conv.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv3.norm_layer.bias torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.conv3.norm_layer.weight torch.Size([48]) cuda:0 True\n",
      "classifier.layer_dict.linear.weights torch.Size([5, 1200]) cuda:0 True\n",
      "classifier.layer_dict.linear.bias torch.Size([5]) cuda:0 True\n",
      "1\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\ANIL_5way_5shot_filter64_miniImagenet\n",
      "attempting to find existing checkpoint\n",
      "data {'test': 12000, 'train': 38400, 'val': 9600}\n",
      "train_seed 985773, val_seed: 985773, at start time\n",
      "50000 50000\n"
     ]
    }
   ],
   "source": [
    "# 모델을 구성한다\n",
    "model = MAMLFewShotClassifier(args=args, device=device,\n",
    "                              im_shape=(2, 3,\n",
    "                                        args.image_height, args.image_width))\n",
    "\n",
    "data = MetaLearningSystemDataLoader\n",
    "\n",
    "maml_system = ExperimentBuilder(model=model, data=data, args=args, device=device)\n",
    "# maml_system.state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd3d9cc",
   "metadata": {},
   "source": [
    "# 3. 가우시안 커널 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3439444e",
   "metadata": {},
   "source": [
    "## 3-1 test tasks에 대한 centroid를 저장한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bcc22e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Extract test task feature vector:\n",
      "count ==  1\n",
      "count ==  2\n",
      "count ==  3\n",
      "count ==  4\n",
      "count ==  5\n",
      "count ==  6\n",
      "count ==  7\n",
      "count ==  8\n",
      "count ==  9\n",
      "count ==  10\n",
      "count ==  11\n",
      "count ==  12\n",
      "count ==  13\n",
      "count ==  14\n",
      "count ==  15\n",
      "count ==  16\n",
      "count ==  17\n",
      "count ==  18\n",
      "count ==  19\n",
      "count ==  20\n",
      "count ==  21\n",
      "count ==  22\n",
      "count ==  23\n",
      "count ==  24\n",
      "count ==  25\n",
      "count ==  26\n",
      "count ==  27\n",
      "count ==  28\n",
      "count ==  29\n",
      "count ==  30\n",
      "count ==  31\n",
      "count ==  32\n",
      "count ==  33\n",
      "count ==  34\n",
      "count ==  35\n",
      "count ==  36\n",
      "count ==  37\n",
      "count ==  38\n",
      "count ==  39\n",
      "count ==  40\n",
      "count ==  41\n",
      "count ==  42\n",
      "count ==  43\n",
      "count ==  44\n",
      "count ==  45\n",
      "count ==  46\n",
      "count ==  47\n",
      "count ==  48\n",
      "count ==  49\n",
      "count ==  50\n",
      "count ==  51\n",
      "count ==  52\n",
      "count ==  53\n",
      "count ==  54\n",
      "count ==  55\n",
      "count ==  56\n",
      "count ==  57\n",
      "count ==  58\n",
      "count ==  59\n",
      "count ==  60\n",
      "count ==  61\n",
      "count ==  62\n",
      "count ==  63\n",
      "count ==  64\n",
      "count ==  65\n",
      "count ==  66\n",
      "count ==  67\n",
      "count ==  68\n",
      "count ==  69\n",
      "count ==  70\n",
      "count ==  71\n",
      "count ==  72\n",
      "count ==  73\n",
      "count ==  74\n",
      "count ==  75\n",
      "count ==  76\n",
      "count ==  77\n",
      "count ==  78\n",
      "count ==  79\n",
      "count ==  80\n",
      "count ==  81\n",
      "count ==  82\n",
      "count ==  83\n",
      "count ==  84\n",
      "count ==  85\n",
      "count ==  86\n",
      "count ==  87\n",
      "count ==  88\n",
      "count ==  89\n",
      "count ==  90\n",
      "count ==  91\n",
      "count ==  92\n",
      "count ==  93\n",
      "count ==  94\n",
      "count ==  95\n",
      "count ==  96\n",
      "count ==  97\n",
      "count ==  98\n",
      "count ==  99\n",
      "count ==  100\n",
      "count ==  101\n",
      "count ==  102\n",
      "count ==  103\n",
      "count ==  104\n",
      "count ==  105\n",
      "count ==  106\n",
      "count ==  107\n",
      "count ==  108\n",
      "count ==  109\n",
      "count ==  110\n",
      "count ==  111\n",
      "count ==  112\n",
      "count ==  113\n",
      "count ==  114\n",
      "count ==  115\n",
      "count ==  116\n",
      "count ==  117\n",
      "count ==  118\n",
      "count ==  119\n",
      "count ==  120\n",
      "count ==  121\n",
      "count ==  122\n",
      "count ==  123\n",
      "count ==  124\n",
      "count ==  125\n",
      "count ==  126\n",
      "count ==  127\n",
      "count ==  128\n",
      "count ==  129\n",
      "count ==  130\n",
      "count ==  131\n",
      "count ==  132\n",
      "count ==  133\n",
      "count ==  134\n",
      "count ==  135\n",
      "count ==  136\n",
      "count ==  137\n",
      "count ==  138\n",
      "count ==  139\n",
      "count ==  140\n",
      "count ==  141\n",
      "count ==  142\n",
      "count ==  143\n",
      "count ==  144\n",
      "count ==  145\n",
      "count ==  146\n",
      "count ==  147\n",
      "count ==  148\n",
      "count ==  149\n",
      "count ==  150\n",
      "count ==  151\n",
      "count ==  152\n",
      "count ==  153\n",
      "count ==  154\n",
      "count ==  155\n",
      "count ==  156\n",
      "count ==  157\n",
      "count ==  158\n",
      "count ==  159\n",
      "count ==  160\n",
      "count ==  161\n",
      "count ==  162\n",
      "count ==  163\n",
      "count ==  164\n",
      "count ==  165\n",
      "count ==  166\n",
      "count ==  167\n",
      "count ==  168\n",
      "count ==  169\n",
      "count ==  170\n",
      "count ==  171\n",
      "count ==  172\n",
      "count ==  173\n",
      "count ==  174\n",
      "count ==  175\n",
      "count ==  176\n",
      "count ==  177\n",
      "count ==  178\n",
      "count ==  179\n",
      "count ==  180\n",
      "count ==  181\n",
      "count ==  182\n",
      "count ==  183\n",
      "count ==  184\n",
      "count ==  185\n",
      "count ==  186\n",
      "count ==  187\n",
      "count ==  188\n",
      "count ==  189\n",
      "count ==  190\n",
      "count ==  191\n",
      "count ==  192\n",
      "count ==  193\n",
      "count ==  194\n",
      "count ==  195\n",
      "count ==  196\n",
      "count ==  197\n",
      "count ==  198\n",
      "count ==  199\n",
      "count ==  200\n",
      "...DONE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = maml_system.data.get_test_batches(total_batches=20, augment_images=False)\n",
    "\n",
    "print(\"==> Extract test task feature vector:\")\n",
    "\n",
    "count = 0\n",
    "test_vector = torch.FloatTensor().cuda()\n",
    "\n",
    "for sample_idx, train_sample in enumerate(test_data):\n",
    "    \n",
    "    x_support_set, x_target_set, y_support_set, y_target_set, seed = train_sample\n",
    "    \n",
    "    x_support_set = torch.Tensor(x_support_set).float().to(device=maml_system.model.device)\n",
    "    x_target_set = torch.Tensor(x_target_set).float().to(device=maml_system.model.device)\n",
    "    y_support_set = torch.Tensor(y_support_set).long().to(device=maml_system.model.device)\n",
    "    y_target_set = torch.Tensor(y_target_set).long().to(device=maml_system.model.device)\n",
    "    \n",
    "    for task_id, (x_support_set_task, y_support_set_task, x_target_set_task, y_target_set_task) in enumerate(zip(x_support_set,\n",
    "                              y_support_set,\n",
    "                              x_target_set,\n",
    "                              y_target_set)):\n",
    "        \n",
    "        \n",
    "        names_weights_copy = maml_system.model.get_inner_loop_parameter_dict(maml_system.model.classifier.named_parameters())\n",
    "\n",
    "        num_devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "\n",
    "        names_weights_copy = {\n",
    "                name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
    "                    [num_devices] + [1 for i in range(len(value.shape))]) for\n",
    "                name, value in names_weights_copy.items()}\n",
    "\n",
    "        prompted_weights_copy = {}\n",
    "        if args.prompter:\n",
    "            prompted_weights_copy = {key: value for key, value in names_weights_copy.items() if 'prompt' in key}\n",
    "\n",
    "        names_weights_copy = {key: value for key, value in names_weights_copy.items() if 'layer_dict' in key}\n",
    "        \n",
    "        \n",
    "        n, s, c, h, w = x_target_set_task.shape\n",
    "\n",
    "        x_support_set_task = x_support_set_task.view(-1, c, h, w)\n",
    "        y_support_set_task = y_support_set_task.view(-1)\n",
    "        x_target_set_task = x_target_set_task.view(-1, c, h, w)\n",
    "        y_target_set_task = y_target_set_task.view(-1)\n",
    "        \n",
    "        \n",
    "        for num_step in range(5):\n",
    "\n",
    "            support_loss, support_preds, _ = maml_system.model.net_forward_feature_extractor(\n",
    "              x=x_support_set_task,\n",
    "              y=y_support_set_task,\n",
    "              weights=names_weights_copy,\n",
    "              prompted_weights=prompted_weights_copy,\n",
    "              backup_running_statistics=num_step == 0,\n",
    "              training=True,\n",
    "              num_step=num_step,\n",
    "              training_phase=False,\n",
    "              epoch=0)\n",
    "            \n",
    "            names_weights_copy, prompted_weights_copy = maml_system.model.apply_inner_loop_update(\n",
    "             loss=support_loss,\n",
    "             names_weights_copy=names_weights_copy,\n",
    "             prompted_weights_copy=prompted_weights_copy,\n",
    "             use_second_order=True,\n",
    "             current_step_idx=num_step,\n",
    "             current_iter='test',\n",
    "             training_phase=False)\n",
    "            \n",
    "            if num_step == 4:\n",
    "                \n",
    "                count = count + 1\n",
    "                print(\"count == \", count)\n",
    "                                \n",
    "                target_loss, target_preds, feature_map_list = maml_system.model.net_forward_feature_extractor(\n",
    "                    x=x_target_set_task,\n",
    "                    y=y_target_set_task,\n",
    "                    weights=names_weights_copy,\n",
    "                    prompted_weights=prompted_weights_copy,\n",
    "                    backup_running_statistics=False, training=True,\n",
    "                    num_step=num_step,\n",
    "                    training_phase=False,\n",
    "                    epoch=0)\n",
    "\n",
    "                query_feature_map = feature_map_list[3].detach()\n",
    "                task_feature = torch.mean(query_feature_map, dim=0).unsqueeze(dim=0)\n",
    "                test_vector = torch.cat((test_vector, task_feature), dim=0)\n",
    "\n",
    "print(\"...DONE\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a32c6a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 48, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "print(test_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b2ffdc",
   "metadata": {},
   "source": [
    "## 3.2 normalized_test_vector를 npy형태로 저장한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e3e04ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "# Step 1: GAP → (B, 64, 1, 1) → view → (B, 64)\n",
    "batch_size = test_vector.shape[0]\n",
    "test_vector_gap = gap(test_vector)              # shape: (B, 64, 1, 1)\n",
    "test_vector_flat = test_vector_gap.view(batch_size, 48)  # shape: (B, 64)\n",
    "\n",
    "# Step 2: Normalize test vector\n",
    "normalized_test_vector = torch.nn.functional.normalize(test_vector_flat)\n",
    "\n",
    "normalized_test_vector_np = normalized_test_vector.detach().cpu().numpy()\n",
    "np.save(\"MAML_\" + dataset +  \"_test_vector.npy\", normalized_test_vector_np)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
