{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00a643d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pytorchcv.model_provider import get_model as ptcv_get_model # model\n",
    "import sys, os\n",
    "import easydict\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline \n",
    "\n",
    "# enable cuda devices\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from meta_neural_network_architectures import VGGReLUNormNetwork, ResNet12\n",
    "from utils.parser_utils import get_args\n",
    "from data import MetaLearningSystemDataLoader\n",
    "from experiment_builder import ExperimentBuilder\n",
    "from utils import basic\n",
    "from few_shot_learning_system import MAMLFewShotClassifier\n",
    "import prompters\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccab7ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화 하기 위한 라이브러리\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e0281c",
   "metadata": {},
   "source": [
    "# 1. Dataset 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1418f5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choices=['padding', 'random_patch', 'fixed_patch'],\n",
    "method = 'padding'\n",
    "\n",
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "# os.environ['TEST_DATASET'] = \"tiered_imagenet\" \n",
    "# os.environ['TEST_DATASET'] = \"CIFAR_FS\" \n",
    "# os.environ['TEST_DATASET'] = \"CUB\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1855befa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.environ['DATASET_DIR'] ===  C:/Users/JM/PycharmProjects/MAML/datasets\n"
     ]
    }
   ],
   "source": [
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "print(\"os.environ['DATASET_DIR'] === \", os.environ['DATASET_DIR'])\n",
    "\n",
    "args = easydict.EasyDict(\n",
    "{\n",
    "  \"batch_size\":2,\n",
    "  \"image_height\":84,\n",
    "  \"image_width\":84,\n",
    "  \"image_channels\":3,\n",
    "  \"gpu_to_use\":0,\n",
    "  \"num_dataprovider_workers\":4,\n",
    "  \"max_models_to_save\":5,\n",
    "  \"dataset_name\":\"mini_imagenet_full_size\",\n",
    "  \"dataset_path\":\"mini_imagenet_full_size\",\n",
    "  \"reset_stored_paths\":False,\n",
    "  \"experiment_name\":\"../MAML_Prompt_padding_5way_5shot_filter128_miniImagenet\",\n",
    "  \"train_seed\": 0, \"val_seed\": 0,\n",
    "  \"indexes_of_folders_indicating_class\": [-3, -2],\n",
    "  \"sets_are_pre_split\": True,\n",
    "  \"train_val_test_split\": [0.64, 0.16, 0.20],\n",
    "  \"evaluate_on_test_set_only\": False,\n",
    "\n",
    "  \"total_epochs\": 100,\n",
    "  \"total_iter_per_epoch\":500, \"continue_from_epoch\": -2,\n",
    "  \"num_evaluation_tasks\":600,\n",
    "  \"multi_step_loss_num_epochs\": 15,\n",
    "  \"minimum_per_task_contribution\": 0.01,\n",
    "  \"learnable_per_layer_per_step_inner_loop_learning_rate\": True,\n",
    "  \"enable_inner_loop_optimizable_bn_params\": False,\n",
    "  \"evalute_on_test_set_only\": False,\n",
    "\n",
    "  \"max_pooling\": True,\n",
    "  \"per_step_bn_statistics\": False,\n",
    "  \"learnable_batch_norm_momentum\": False,\n",
    "  \"load_into_memory\": False,\n",
    "  \"init_inner_loop_learning_rate\": 0.01,\n",
    "  \"init_inner_loop_weight_decay\": 0.0005,\n",
    "  \"learnable_bn_gamma\": True,\n",
    "  \"learnable_bn_beta\": True,\n",
    "\n",
    "  \"dropout_rate_value\":0.0,\n",
    "  \"min_learning_rate\":0.001,\n",
    "  \"meta_learning_rate\":0.001,   \"total_epochs_before_pause\": 100,\n",
    "  \"first_order_to_second_order_epoch\":-1,\n",
    "  \"weight_decay\": 0.0,\n",
    "\n",
    "  \"norm_layer\":\"batch_norm\",\n",
    "  \"cnn_num_filters\":128,\n",
    "  \"num_stages\":4,\n",
    "  \"conv_padding\": True,\n",
    "  \"number_of_training_steps_per_iter\":5,\n",
    "  \"number_of_evaluation_steps_per_iter\":5,\n",
    "  \"cnn_blocks_per_stage\":1,\n",
    "  \"num_classes_per_set\":5,\n",
    "  \"num_samples_per_class\":5,\n",
    "  \"num_target_samples\": 15,\n",
    "  \"samples_per_iter\" : 1,\n",
    "\n",
    "  \"second_order\": True,\n",
    "  \"use_multi_step_loss_optimization\":False,\n",
    "  \"backbone\": \"4-CONV\",\n",
    "  \"arbiter\": False,\n",
    "  \"use_bias\": True,\n",
    "  \"prompter\": True,\n",
    "  \"prompt_engineering\": method,\n",
    "  \"prompt_size\" : 5,\n",
    "  \"image_size\" : 84,\n",
    "  \"prompt_random_init\": False,\n",
    "  \"outer_prompt_learning_rate\": 0.001,\n",
    "  \"inner_prompt_learning_rate\": 0.01,\n",
    "  \"data_aug\" : \"random\"\n",
    "}\n",
    ")\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "args.im_shape = (2, 3, args.image_height, args.image_width)\n",
    "\n",
    "args.use_cuda = torch.cuda.is_available()\n",
    "args.seed = 104\n",
    "args.reverse_channels=False\n",
    "args.labels_as_int=False\n",
    "args.reset_stored_filepaths=False\n",
    "args.num_of_gpus=1\n",
    "\n",
    "args.continue_from_epoch='latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc47251",
   "metadata": {},
   "source": [
    "# 2. 모델 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "197487d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using max pooling\n",
      "meta network params\n",
      "0.01\n",
      "Inner Loop parameters\n",
      "prompt_learning_rates_dict.prompt_weight_learning_rate torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv0-conv-weight torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv0-conv-bias torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv1-conv-weight torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv1-conv-bias torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv2-conv-weight torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv2-conv-bias torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv3-conv-weight torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv3-conv-bias torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-linear-weights torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-linear-bias torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv0-conv-weight torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv0-conv-bias torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv1-conv-weight torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv1-conv-bias torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv2-conv-weight torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv2-conv-bias torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv3-conv-weight torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv3-conv-bias torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-linear-weights torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-linear-bias torch.Size([6])\n",
      "Outer Loop parameters\n",
      "classifier.layer_dict.conv0.conv.weight torch.Size([128, 3, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv0.conv.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv0.norm_layer.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv0.norm_layer.weight torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv1.conv.weight torch.Size([128, 128, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv1.conv.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv1.norm_layer.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv1.norm_layer.weight torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv2.conv.weight torch.Size([128, 128, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv2.conv.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv2.norm_layer.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv2.norm_layer.weight torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv3.conv.weight torch.Size([128, 128, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv3.conv.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv3.norm_layer.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv3.norm_layer.weight torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.linear.weights torch.Size([5, 3200]) cuda:0 True\n",
      "classifier.layer_dict.linear.bias torch.Size([5]) cuda:0 True\n",
      "classifier.prompt.prompt_dict.pad_up torch.Size([3, 5, 84]) cuda:0 True\n",
      "classifier.prompt.prompt_dict.pad_down torch.Size([3, 5, 84]) cuda:0 True\n",
      "classifier.prompt.prompt_dict.pad_left torch.Size([3, 74, 5]) cuda:0 True\n",
      "classifier.prompt.prompt_dict.pad_right torch.Size([3, 74, 5]) cuda:0 True\n",
      "inner_loop_optimizer.prompt_learning_rates_dict.prompt_weight_learning_rate torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv0-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv0-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv1-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv1-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv2-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv2-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv3-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv3-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-weights torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv0-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv0-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv1-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv1-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv2-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv2-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv3-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv3-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-linear-weights torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-linear-bias torch.Size([6]) cuda:0 True\n",
      "1\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\MAML_Prompt_padding_5way_5shot_filter128_miniImagenet\n",
      "attempting to find existing checkpoint\n",
      "data {'test': 12000, 'train': 38400, 'val': 9600}\n",
      "train_seed 985773, val_seed: 985773, at start time\n",
      "75000 50000\n"
     ]
    }
   ],
   "source": [
    "# 모델을 구성한다\n",
    "model = MAMLFewShotClassifier(args=args, device=device,\n",
    "                              im_shape=(2, 3,\n",
    "                                        args.image_height, args.image_width))\n",
    "\n",
    "data = MetaLearningSystemDataLoader\n",
    "\n",
    "maml_system = ExperimentBuilder(model=model, data=data, args=args, device=device)\n",
    "# maml_system.state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd3d9cc",
   "metadata": {},
   "source": [
    "# 3. 가우시안 커널 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b842b0a",
   "metadata": {},
   "source": [
    "## 3-1 먼저 training task에 대한 centroid를 저장한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56e5e812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Extract training task feature vector:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count ==  1\n",
      "count ==  2\n",
      "count ==  3\n",
      "count ==  4\n",
      "count ==  5\n",
      "count ==  6\n",
      "count ==  7\n",
      "count ==  8\n",
      "count ==  9\n",
      "count ==  10\n",
      "count ==  11\n",
      "count ==  12\n",
      "count ==  13\n",
      "count ==  14\n",
      "count ==  15\n",
      "count ==  16\n",
      "count ==  17\n",
      "count ==  18\n",
      "count ==  19\n",
      "count ==  20\n",
      "count ==  21\n",
      "count ==  22\n",
      "count ==  23\n",
      "count ==  24\n",
      "count ==  25\n",
      "count ==  26\n",
      "count ==  27\n",
      "count ==  28\n",
      "count ==  29\n",
      "count ==  30\n",
      "count ==  31\n",
      "count ==  32\n",
      "count ==  33\n",
      "count ==  34\n",
      "count ==  35\n",
      "count ==  36\n",
      "count ==  37\n",
      "count ==  38\n",
      "count ==  39\n",
      "count ==  40\n",
      "count ==  41\n",
      "count ==  42\n",
      "count ==  43\n",
      "count ==  44\n",
      "count ==  45\n",
      "count ==  46\n",
      "count ==  47\n",
      "count ==  48\n",
      "count ==  49\n",
      "count ==  50\n",
      "count ==  51\n",
      "count ==  52\n",
      "count ==  53\n",
      "count ==  54\n",
      "count ==  55\n",
      "count ==  56\n",
      "count ==  57\n",
      "count ==  58\n",
      "count ==  59\n",
      "count ==  60\n",
      "count ==  61\n",
      "count ==  62\n",
      "count ==  63\n",
      "count ==  64\n",
      "count ==  65\n",
      "count ==  66\n",
      "count ==  67\n",
      "count ==  68\n",
      "count ==  69\n",
      "count ==  70\n",
      "count ==  71\n",
      "count ==  72\n",
      "count ==  73\n",
      "count ==  74\n",
      "count ==  75\n",
      "count ==  76\n",
      "count ==  77\n",
      "count ==  78\n",
      "count ==  79\n",
      "count ==  80\n",
      "count ==  81\n",
      "count ==  82\n",
      "count ==  83\n",
      "count ==  84\n",
      "count ==  85\n",
      "count ==  86\n",
      "count ==  87\n",
      "count ==  88\n",
      "count ==  89\n",
      "count ==  90\n",
      "count ==  91\n",
      "count ==  92\n",
      "count ==  93\n",
      "count ==  94\n",
      "count ==  95\n",
      "count ==  96\n",
      "count ==  97\n",
      "count ==  98\n",
      "count ==  99\n",
      "count ==  100\n",
      "count ==  101\n",
      "count ==  102\n",
      "count ==  103\n",
      "count ==  104\n",
      "count ==  105\n",
      "count ==  106\n",
      "count ==  107\n",
      "count ==  108\n",
      "count ==  109\n",
      "count ==  110\n",
      "count ==  111\n",
      "count ==  112\n",
      "count ==  113\n",
      "count ==  114\n",
      "count ==  115\n",
      "count ==  116\n",
      "count ==  117\n",
      "count ==  118\n",
      "count ==  119\n",
      "count ==  120\n",
      "count ==  121\n",
      "count ==  122\n",
      "count ==  123\n",
      "count ==  124\n",
      "count ==  125\n",
      "count ==  126\n",
      "count ==  127\n",
      "count ==  128\n",
      "count ==  129\n",
      "count ==  130\n",
      "count ==  131\n",
      "count ==  132\n",
      "count ==  133\n",
      "count ==  134\n",
      "count ==  135\n",
      "count ==  136\n",
      "count ==  137\n",
      "count ==  138\n",
      "count ==  139\n",
      "count ==  140\n",
      "count ==  141\n",
      "count ==  142\n",
      "count ==  143\n",
      "count ==  144\n",
      "count ==  145\n",
      "count ==  146\n",
      "count ==  147\n",
      "count ==  148\n",
      "count ==  149\n",
      "count ==  150\n",
      "count ==  151\n",
      "count ==  152\n",
      "count ==  153\n",
      "count ==  154\n",
      "count ==  155\n",
      "count ==  156\n",
      "count ==  157\n",
      "count ==  158\n",
      "count ==  159\n",
      "count ==  160\n",
      "count ==  161\n",
      "count ==  162\n",
      "count ==  163\n",
      "count ==  164\n",
      "count ==  165\n",
      "count ==  166\n",
      "count ==  167\n",
      "count ==  168\n",
      "count ==  169\n",
      "count ==  170\n",
      "count ==  171\n",
      "count ==  172\n",
      "count ==  173\n",
      "count ==  174\n",
      "count ==  175\n",
      "count ==  176\n",
      "count ==  177\n",
      "count ==  178\n",
      "count ==  179\n",
      "count ==  180\n",
      "count ==  181\n",
      "count ==  182\n",
      "count ==  183\n",
      "count ==  184\n",
      "count ==  185\n",
      "count ==  186\n",
      "count ==  187\n",
      "count ==  188\n",
      "count ==  189\n",
      "count ==  190\n",
      "count ==  191\n",
      "count ==  192\n",
      "count ==  193\n",
      "count ==  194\n",
      "count ==  195\n",
      "count ==  196\n",
      "count ==  197\n",
      "count ==  198\n",
      "count ==  199\n",
      "count ==  200\n",
      "count ==  201\n",
      "count ==  202\n",
      "count ==  203\n",
      "count ==  204\n",
      "count ==  205\n",
      "count ==  206\n",
      "count ==  207\n",
      "count ==  208\n",
      "count ==  209\n",
      "count ==  210\n",
      "count ==  211\n",
      "count ==  212\n",
      "count ==  213\n",
      "count ==  214\n",
      "count ==  215\n",
      "count ==  216\n",
      "count ==  217\n",
      "count ==  218\n",
      "count ==  219\n",
      "count ==  220\n",
      "count ==  221\n",
      "count ==  222\n",
      "count ==  223\n",
      "count ==  224\n",
      "count ==  225\n",
      "count ==  226\n",
      "count ==  227\n",
      "count ==  228\n",
      "count ==  229\n",
      "count ==  230\n",
      "count ==  231\n",
      "count ==  232\n",
      "count ==  233\n",
      "count ==  234\n",
      "count ==  235\n",
      "count ==  236\n",
      "count ==  237\n",
      "count ==  238\n",
      "count ==  239\n",
      "count ==  240\n",
      "count ==  241\n",
      "count ==  242\n",
      "count ==  243\n",
      "count ==  244\n",
      "count ==  245\n",
      "count ==  246\n",
      "count ==  247\n",
      "count ==  248\n",
      "count ==  249\n",
      "count ==  250\n",
      "count ==  251\n",
      "count ==  252\n",
      "count ==  253\n",
      "count ==  254\n",
      "count ==  255\n",
      "count ==  256\n",
      "count ==  257\n",
      "count ==  258\n",
      "count ==  259\n",
      "count ==  260\n",
      "count ==  261\n",
      "count ==  262\n",
      "count ==  263\n",
      "count ==  264\n",
      "count ==  265\n",
      "count ==  266\n",
      "count ==  267\n",
      "count ==  268\n",
      "count ==  269\n",
      "count ==  270\n",
      "count ==  271\n",
      "count ==  272\n",
      "count ==  273\n",
      "count ==  274\n",
      "count ==  275\n",
      "count ==  276\n",
      "count ==  277\n",
      "count ==  278\n",
      "count ==  279\n",
      "count ==  280\n",
      "count ==  281\n",
      "count ==  282\n",
      "count ==  283\n",
      "count ==  284\n",
      "count ==  285\n",
      "count ==  286\n",
      "count ==  287\n",
      "count ==  288\n",
      "count ==  289\n",
      "count ==  290\n",
      "count ==  291\n",
      "count ==  292\n",
      "count ==  293\n",
      "count ==  294\n",
      "count ==  295\n",
      "count ==  296\n",
      "count ==  297\n",
      "count ==  298\n",
      "count ==  299\n",
      "count ==  300\n",
      "count ==  301\n",
      "count ==  302\n",
      "count ==  303\n",
      "count ==  304\n",
      "count ==  305\n",
      "count ==  306\n",
      "count ==  307\n",
      "count ==  308\n",
      "count ==  309\n",
      "count ==  310\n",
      "count ==  311\n",
      "count ==  312\n",
      "count ==  313\n",
      "count ==  314\n",
      "count ==  315\n",
      "count ==  316\n",
      "count ==  317\n",
      "count ==  318\n",
      "count ==  319\n",
      "count ==  320\n",
      "count ==  321\n",
      "count ==  322\n",
      "count ==  323\n",
      "count ==  324\n",
      "count ==  325\n",
      "count ==  326\n",
      "count ==  327\n",
      "count ==  328\n",
      "count ==  329\n",
      "count ==  330\n",
      "count ==  331\n",
      "count ==  332\n",
      "count ==  333\n",
      "count ==  334\n",
      "count ==  335\n",
      "count ==  336\n",
      "count ==  337\n",
      "count ==  338\n",
      "count ==  339\n",
      "count ==  340\n",
      "count ==  341\n",
      "count ==  342\n",
      "count ==  343\n",
      "count ==  344\n",
      "count ==  345\n",
      "count ==  346\n",
      "count ==  347\n",
      "count ==  348\n",
      "count ==  349\n",
      "count ==  350\n",
      "count ==  351\n",
      "count ==  352\n",
      "count ==  353\n",
      "count ==  354\n",
      "count ==  355\n",
      "count ==  356\n",
      "count ==  357\n",
      "count ==  358\n",
      "count ==  359\n",
      "count ==  360\n",
      "count ==  361\n",
      "count ==  362\n",
      "count ==  363\n",
      "count ==  364\n",
      "count ==  365\n",
      "count ==  366\n",
      "count ==  367\n",
      "count ==  368\n",
      "count ==  369\n",
      "count ==  370\n",
      "count ==  371\n",
      "count ==  372\n",
      "count ==  373\n",
      "count ==  374\n",
      "count ==  375\n",
      "count ==  376\n",
      "count ==  377\n",
      "count ==  378\n",
      "count ==  379\n",
      "count ==  380\n",
      "count ==  381\n",
      "count ==  382\n",
      "count ==  383\n",
      "count ==  384\n",
      "count ==  385\n",
      "count ==  386\n",
      "count ==  387\n",
      "count ==  388\n",
      "count ==  389\n",
      "count ==  390\n",
      "count ==  391\n",
      "count ==  392\n",
      "count ==  393\n",
      "count ==  394\n",
      "count ==  395\n",
      "count ==  396\n",
      "count ==  397\n",
      "count ==  398\n",
      "count ==  399\n",
      "count ==  400\n",
      "count ==  401\n",
      "count ==  402\n",
      "count ==  403\n",
      "count ==  404\n",
      "count ==  405\n",
      "count ==  406\n",
      "count ==  407\n",
      "count ==  408\n",
      "count ==  409\n",
      "count ==  410\n",
      "count ==  411\n",
      "count ==  412\n",
      "count ==  413\n",
      "count ==  414\n",
      "count ==  415\n",
      "count ==  416\n",
      "count ==  417\n",
      "count ==  418\n",
      "count ==  419\n",
      "count ==  420\n",
      "count ==  421\n",
      "count ==  422\n",
      "count ==  423\n",
      "count ==  424\n",
      "count ==  425\n",
      "count ==  426\n",
      "count ==  427\n",
      "count ==  428\n",
      "count ==  429\n",
      "count ==  430\n",
      "count ==  431\n",
      "count ==  432\n",
      "count ==  433\n",
      "count ==  434\n",
      "count ==  435\n",
      "count ==  436\n",
      "count ==  437\n",
      "count ==  438\n",
      "count ==  439\n",
      "count ==  440\n",
      "count ==  441\n",
      "count ==  442\n",
      "count ==  443\n",
      "count ==  444\n",
      "count ==  445\n",
      "count ==  446\n",
      "count ==  447\n",
      "count ==  448\n",
      "count ==  449\n",
      "count ==  450\n",
      "count ==  451\n",
      "count ==  452\n",
      "count ==  453\n",
      "count ==  454\n",
      "count ==  455\n",
      "count ==  456\n",
      "count ==  457\n",
      "count ==  458\n",
      "count ==  459\n",
      "count ==  460\n",
      "count ==  461\n",
      "count ==  462\n",
      "count ==  463\n",
      "count ==  464\n",
      "count ==  465\n",
      "count ==  466\n",
      "count ==  467\n",
      "count ==  468\n",
      "count ==  469\n",
      "count ==  470\n",
      "count ==  471\n",
      "count ==  472\n",
      "count ==  473\n",
      "count ==  474\n",
      "count ==  475\n",
      "count ==  476\n",
      "count ==  477\n",
      "count ==  478\n",
      "count ==  479\n",
      "count ==  480\n",
      "count ==  481\n",
      "count ==  482\n",
      "count ==  483\n",
      "count ==  484\n",
      "count ==  485\n",
      "count ==  486\n",
      "count ==  487\n",
      "count ==  488\n",
      "count ==  489\n",
      "count ==  490\n",
      "count ==  491\n",
      "count ==  492\n",
      "count ==  493\n",
      "count ==  494\n",
      "count ==  495\n",
      "count ==  496\n",
      "count ==  497\n",
      "count ==  498\n",
      "count ==  499\n",
      "count ==  500\n",
      "count ==  501\n",
      "count ==  502\n",
      "count ==  503\n",
      "count ==  504\n",
      "count ==  505\n",
      "count ==  506\n",
      "count ==  507\n",
      "count ==  508\n",
      "count ==  509\n",
      "count ==  510\n",
      "count ==  511\n",
      "count ==  512\n",
      "count ==  513\n",
      "count ==  514\n",
      "count ==  515\n",
      "count ==  516\n",
      "count ==  517\n",
      "count ==  518\n",
      "count ==  519\n",
      "count ==  520\n",
      "count ==  521\n",
      "count ==  522\n",
      "count ==  523\n",
      "count ==  524\n",
      "count ==  525\n",
      "count ==  526\n",
      "count ==  527\n",
      "count ==  528\n",
      "count ==  529\n",
      "count ==  530\n",
      "count ==  531\n",
      "count ==  532\n",
      "count ==  533\n",
      "count ==  534\n",
      "count ==  535\n",
      "count ==  536\n",
      "count ==  537\n",
      "count ==  538\n",
      "count ==  539\n",
      "count ==  540\n",
      "count ==  541\n",
      "count ==  542\n",
      "count ==  543\n",
      "count ==  544\n",
      "count ==  545\n",
      "count ==  546\n",
      "count ==  547\n",
      "count ==  548\n",
      "count ==  549\n",
      "count ==  550\n",
      "count ==  551\n",
      "count ==  552\n",
      "count ==  553\n",
      "count ==  554\n",
      "count ==  555\n",
      "count ==  556\n",
      "count ==  557\n",
      "count ==  558\n",
      "count ==  559\n",
      "count ==  560\n",
      "count ==  561\n",
      "count ==  562\n",
      "count ==  563\n",
      "count ==  564\n",
      "count ==  565\n",
      "count ==  566\n",
      "count ==  567\n",
      "count ==  568\n",
      "count ==  569\n",
      "count ==  570\n",
      "count ==  571\n",
      "count ==  572\n",
      "count ==  573\n",
      "count ==  574\n",
      "count ==  575\n",
      "count ==  576\n",
      "count ==  577\n",
      "count ==  578\n",
      "count ==  579\n",
      "count ==  580\n",
      "count ==  581\n",
      "count ==  582\n",
      "count ==  583\n",
      "count ==  584\n",
      "count ==  585\n",
      "count ==  586\n",
      "count ==  587\n",
      "count ==  588\n",
      "count ==  589\n",
      "count ==  590\n",
      "count ==  591\n",
      "count ==  592\n",
      "count ==  593\n",
      "count ==  594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count ==  595\n",
      "count ==  596\n",
      "count ==  597\n",
      "count ==  598\n",
      "count ==  599\n",
      "count ==  600\n",
      "count ==  601\n",
      "count ==  602\n",
      "count ==  603\n",
      "count ==  604\n",
      "count ==  605\n",
      "count ==  606\n",
      "count ==  607\n",
      "count ==  608\n",
      "count ==  609\n",
      "count ==  610\n",
      "count ==  611\n",
      "count ==  612\n",
      "count ==  613\n",
      "count ==  614\n",
      "count ==  615\n",
      "count ==  616\n",
      "count ==  617\n",
      "count ==  618\n",
      "count ==  619\n",
      "count ==  620\n",
      "count ==  621\n",
      "count ==  622\n",
      "count ==  623\n",
      "count ==  624\n",
      "count ==  625\n",
      "count ==  626\n",
      "count ==  627\n",
      "count ==  628\n",
      "count ==  629\n",
      "count ==  630\n",
      "count ==  631\n",
      "count ==  632\n",
      "count ==  633\n",
      "count ==  634\n",
      "count ==  635\n",
      "count ==  636\n",
      "count ==  637\n",
      "count ==  638\n",
      "count ==  639\n",
      "count ==  640\n",
      "count ==  641\n",
      "count ==  642\n",
      "count ==  643\n",
      "count ==  644\n",
      "count ==  645\n",
      "count ==  646\n",
      "count ==  647\n",
      "count ==  648\n",
      "count ==  649\n",
      "count ==  650\n",
      "count ==  651\n",
      "count ==  652\n",
      "count ==  653\n",
      "count ==  654\n",
      "count ==  655\n",
      "count ==  656\n",
      "count ==  657\n",
      "count ==  658\n",
      "count ==  659\n",
      "count ==  660\n",
      "count ==  661\n",
      "count ==  662\n",
      "count ==  663\n",
      "count ==  664\n",
      "count ==  665\n",
      "count ==  666\n",
      "count ==  667\n",
      "count ==  668\n",
      "count ==  669\n",
      "count ==  670\n",
      "count ==  671\n",
      "count ==  672\n",
      "count ==  673\n",
      "count ==  674\n",
      "count ==  675\n",
      "count ==  676\n",
      "count ==  677\n",
      "count ==  678\n",
      "count ==  679\n",
      "count ==  680\n",
      "count ==  681\n",
      "count ==  682\n",
      "count ==  683\n",
      "count ==  684\n",
      "count ==  685\n",
      "count ==  686\n",
      "count ==  687\n",
      "count ==  688\n",
      "count ==  689\n",
      "count ==  690\n",
      "count ==  691\n",
      "count ==  692\n",
      "count ==  693\n",
      "count ==  694\n",
      "count ==  695\n",
      "count ==  696\n",
      "count ==  697\n",
      "count ==  698\n",
      "count ==  699\n",
      "count ==  700\n",
      "count ==  701\n",
      "count ==  702\n",
      "count ==  703\n",
      "count ==  704\n",
      "count ==  705\n",
      "count ==  706\n",
      "count ==  707\n",
      "count ==  708\n",
      "count ==  709\n",
      "count ==  710\n",
      "count ==  711\n",
      "count ==  712\n",
      "count ==  713\n",
      "count ==  714\n",
      "count ==  715\n",
      "count ==  716\n",
      "count ==  717\n",
      "count ==  718\n",
      "count ==  719\n",
      "count ==  720\n",
      "count ==  721\n",
      "count ==  722\n",
      "count ==  723\n",
      "count ==  724\n",
      "count ==  725\n",
      "count ==  726\n",
      "count ==  727\n",
      "count ==  728\n",
      "count ==  729\n",
      "count ==  730\n",
      "count ==  731\n",
      "count ==  732\n",
      "count ==  733\n",
      "count ==  734\n",
      "count ==  735\n",
      "count ==  736\n",
      "count ==  737\n",
      "count ==  738\n",
      "count ==  739\n",
      "count ==  740\n",
      "count ==  741\n",
      "count ==  742\n",
      "count ==  743\n",
      "count ==  744\n",
      "count ==  745\n",
      "count ==  746\n",
      "count ==  747\n",
      "count ==  748\n",
      "count ==  749\n",
      "count ==  750\n",
      "count ==  751\n",
      "count ==  752\n",
      "count ==  753\n",
      "count ==  754\n",
      "count ==  755\n",
      "count ==  756\n",
      "count ==  757\n",
      "count ==  758\n",
      "count ==  759\n",
      "count ==  760\n",
      "count ==  761\n",
      "count ==  762\n",
      "count ==  763\n",
      "count ==  764\n",
      "count ==  765\n",
      "count ==  766\n",
      "count ==  767\n",
      "count ==  768\n",
      "count ==  769\n",
      "count ==  770\n",
      "count ==  771\n",
      "count ==  772\n",
      "count ==  773\n",
      "count ==  774\n",
      "count ==  775\n",
      "count ==  776\n",
      "count ==  777\n",
      "count ==  778\n",
      "count ==  779\n",
      "count ==  780\n",
      "count ==  781\n",
      "count ==  782\n",
      "count ==  783\n",
      "count ==  784\n",
      "count ==  785\n",
      "count ==  786\n",
      "count ==  787\n",
      "count ==  788\n",
      "count ==  789\n",
      "count ==  790\n",
      "count ==  791\n",
      "count ==  792\n",
      "count ==  793\n",
      "count ==  794\n",
      "count ==  795\n",
      "count ==  796\n",
      "count ==  797\n",
      "count ==  798\n",
      "count ==  799\n",
      "count ==  800\n",
      "count ==  801\n",
      "count ==  802\n",
      "count ==  803\n",
      "count ==  804\n",
      "count ==  805\n",
      "count ==  806\n",
      "count ==  807\n",
      "count ==  808\n",
      "count ==  809\n",
      "count ==  810\n",
      "count ==  811\n",
      "count ==  812\n",
      "count ==  813\n",
      "count ==  814\n",
      "count ==  815\n",
      "count ==  816\n",
      "count ==  817\n",
      "count ==  818\n",
      "count ==  819\n",
      "count ==  820\n",
      "count ==  821\n",
      "count ==  822\n",
      "count ==  823\n",
      "count ==  824\n",
      "count ==  825\n",
      "count ==  826\n",
      "count ==  827\n",
      "count ==  828\n",
      "count ==  829\n",
      "count ==  830\n",
      "count ==  831\n",
      "count ==  832\n",
      "count ==  833\n",
      "count ==  834\n",
      "count ==  835\n",
      "count ==  836\n",
      "count ==  837\n",
      "count ==  838\n",
      "count ==  839\n",
      "count ==  840\n",
      "count ==  841\n",
      "count ==  842\n",
      "count ==  843\n",
      "count ==  844\n",
      "count ==  845\n",
      "count ==  846\n",
      "count ==  847\n",
      "count ==  848\n",
      "count ==  849\n",
      "count ==  850\n",
      "count ==  851\n",
      "count ==  852\n",
      "count ==  853\n",
      "count ==  854\n",
      "count ==  855\n",
      "count ==  856\n",
      "count ==  857\n",
      "count ==  858\n",
      "count ==  859\n",
      "count ==  860\n",
      "count ==  861\n",
      "count ==  862\n",
      "count ==  863\n",
      "count ==  864\n",
      "count ==  865\n",
      "count ==  866\n",
      "count ==  867\n",
      "count ==  868\n",
      "count ==  869\n",
      "count ==  870\n",
      "count ==  871\n",
      "count ==  872\n",
      "count ==  873\n",
      "count ==  874\n",
      "count ==  875\n",
      "count ==  876\n",
      "count ==  877\n",
      "count ==  878\n",
      "count ==  879\n",
      "count ==  880\n",
      "count ==  881\n",
      "count ==  882\n",
      "count ==  883\n",
      "count ==  884\n",
      "count ==  885\n",
      "count ==  886\n",
      "count ==  887\n",
      "count ==  888\n",
      "count ==  889\n",
      "count ==  890\n",
      "count ==  891\n",
      "count ==  892\n",
      "count ==  893\n",
      "count ==  894\n",
      "count ==  895\n",
      "count ==  896\n",
      "count ==  897\n",
      "count ==  898\n",
      "count ==  899\n",
      "count ==  900\n",
      "count ==  901\n",
      "count ==  902\n",
      "count ==  903\n",
      "count ==  904\n",
      "count ==  905\n",
      "count ==  906\n",
      "count ==  907\n",
      "count ==  908\n",
      "count ==  909\n",
      "count ==  910\n",
      "count ==  911\n",
      "count ==  912\n",
      "count ==  913\n",
      "count ==  914\n",
      "count ==  915\n",
      "count ==  916\n",
      "count ==  917\n",
      "count ==  918\n",
      "count ==  919\n",
      "count ==  920\n",
      "count ==  921\n",
      "count ==  922\n",
      "count ==  923\n",
      "count ==  924\n",
      "count ==  925\n",
      "count ==  926\n",
      "count ==  927\n",
      "count ==  928\n",
      "count ==  929\n",
      "count ==  930\n",
      "count ==  931\n",
      "count ==  932\n",
      "count ==  933\n",
      "count ==  934\n",
      "count ==  935\n",
      "count ==  936\n",
      "count ==  937\n",
      "count ==  938\n",
      "count ==  939\n",
      "count ==  940\n",
      "count ==  941\n",
      "count ==  942\n",
      "count ==  943\n",
      "count ==  944\n",
      "count ==  945\n",
      "count ==  946\n",
      "count ==  947\n",
      "count ==  948\n",
      "count ==  949\n",
      "count ==  950\n",
      "count ==  951\n",
      "count ==  952\n",
      "count ==  953\n",
      "count ==  954\n",
      "count ==  955\n",
      "count ==  956\n",
      "count ==  957\n",
      "count ==  958\n",
      "count ==  959\n",
      "count ==  960\n",
      "count ==  961\n",
      "count ==  962\n",
      "count ==  963\n",
      "count ==  964\n",
      "count ==  965\n",
      "count ==  966\n",
      "count ==  967\n",
      "count ==  968\n",
      "count ==  969\n",
      "count ==  970\n",
      "count ==  971\n",
      "count ==  972\n",
      "count ==  973\n",
      "count ==  974\n",
      "count ==  975\n",
      "count ==  976\n",
      "count ==  977\n",
      "count ==  978\n",
      "count ==  979\n",
      "count ==  980\n",
      "count ==  981\n",
      "count ==  982\n",
      "count ==  983\n",
      "count ==  984\n",
      "count ==  985\n",
      "count ==  986\n",
      "count ==  987\n",
      "count ==  988\n",
      "count ==  989\n",
      "count ==  990\n",
      "count ==  991\n",
      "count ==  992\n",
      "count ==  993\n",
      "count ==  994\n",
      "count ==  995\n",
      "count ==  996\n",
      "count ==  997\n",
      "count ==  998\n",
      "count ==  999\n",
      "count ==  1000\n",
      "count ==  1001\n",
      "count ==  1002\n",
      "count ==  1003\n",
      "count ==  1004\n",
      "count ==  1005\n",
      "count ==  1006\n",
      "count ==  1007\n",
      "count ==  1008\n",
      "count ==  1009\n",
      "count ==  1010\n",
      "count ==  1011\n",
      "count ==  1012\n",
      "count ==  1013\n",
      "count ==  1014\n",
      "count ==  1015\n",
      "count ==  1016\n",
      "count ==  1017\n",
      "count ==  1018\n",
      "count ==  1019\n",
      "count ==  1020\n",
      "count ==  1021\n",
      "count ==  1022\n",
      "count ==  1023\n",
      "count ==  1024\n",
      "count ==  1025\n",
      "count ==  1026\n",
      "count ==  1027\n",
      "count ==  1028\n",
      "count ==  1029\n",
      "count ==  1030\n",
      "count ==  1031\n",
      "count ==  1032\n",
      "count ==  1033\n",
      "count ==  1034\n",
      "count ==  1035\n",
      "count ==  1036\n",
      "count ==  1037\n",
      "count ==  1038\n",
      "count ==  1039\n",
      "count ==  1040\n",
      "count ==  1041\n",
      "count ==  1042\n",
      "count ==  1043\n",
      "count ==  1044\n",
      "count ==  1045\n",
      "count ==  1046\n",
      "count ==  1047\n",
      "count ==  1048\n",
      "count ==  1049\n",
      "count ==  1050\n",
      "count ==  1051\n",
      "count ==  1052\n",
      "count ==  1053\n",
      "count ==  1054\n",
      "count ==  1055\n",
      "count ==  1056\n",
      "count ==  1057\n",
      "count ==  1058\n",
      "count ==  1059\n",
      "count ==  1060\n",
      "count ==  1061\n",
      "count ==  1062\n",
      "count ==  1063\n",
      "count ==  1064\n",
      "count ==  1065\n",
      "count ==  1066\n",
      "count ==  1067\n",
      "count ==  1068\n",
      "count ==  1069\n",
      "count ==  1070\n",
      "count ==  1071\n",
      "count ==  1072\n",
      "count ==  1073\n",
      "count ==  1074\n",
      "count ==  1075\n",
      "count ==  1076\n",
      "count ==  1077\n",
      "count ==  1078\n",
      "count ==  1079\n",
      "count ==  1080\n",
      "count ==  1081\n",
      "count ==  1082\n",
      "count ==  1083\n",
      "count ==  1084\n",
      "count ==  1085\n",
      "count ==  1086\n",
      "count ==  1087\n",
      "count ==  1088\n",
      "count ==  1089\n",
      "count ==  1090\n",
      "count ==  1091\n",
      "count ==  1092\n",
      "count ==  1093\n",
      "count ==  1094\n",
      "count ==  1095\n",
      "count ==  1096\n",
      "count ==  1097\n",
      "count ==  1098\n",
      "count ==  1099\n",
      "count ==  1100\n",
      "count ==  1101\n",
      "count ==  1102\n",
      "count ==  1103\n",
      "count ==  1104\n",
      "count ==  1105\n",
      "count ==  1106\n",
      "count ==  1107\n",
      "count ==  1108\n",
      "count ==  1109\n",
      "count ==  1110\n",
      "count ==  1111\n",
      "count ==  1112\n",
      "count ==  1113\n",
      "count ==  1114\n",
      "count ==  1115\n",
      "count ==  1116\n",
      "count ==  1117\n",
      "count ==  1118\n",
      "count ==  1119\n",
      "count ==  1120\n",
      "count ==  1121\n",
      "count ==  1122\n",
      "count ==  1123\n",
      "count ==  1124\n",
      "count ==  1125\n",
      "count ==  1126\n",
      "count ==  1127\n",
      "count ==  1128\n",
      "count ==  1129\n",
      "count ==  1130\n",
      "count ==  1131\n",
      "count ==  1132\n",
      "count ==  1133\n",
      "count ==  1134\n",
      "count ==  1135\n",
      "count ==  1136\n",
      "count ==  1137\n",
      "count ==  1138\n",
      "count ==  1139\n",
      "count ==  1140\n",
      "count ==  1141\n",
      "count ==  1142\n",
      "count ==  1143\n",
      "count ==  1144\n",
      "count ==  1145\n",
      "count ==  1146\n",
      "count ==  1147\n",
      "count ==  1148\n",
      "count ==  1149\n",
      "count ==  1150\n",
      "count ==  1151\n",
      "count ==  1152\n",
      "count ==  1153\n",
      "count ==  1154\n",
      "count ==  1155\n",
      "count ==  1156\n",
      "count ==  1157\n",
      "count ==  1158\n",
      "count ==  1159\n",
      "count ==  1160\n",
      "count ==  1161\n",
      "count ==  1162\n",
      "count ==  1163\n",
      "count ==  1164\n",
      "count ==  1165\n",
      "count ==  1166\n",
      "count ==  1167\n",
      "count ==  1168\n",
      "count ==  1169\n",
      "count ==  1170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count ==  1171\n",
      "count ==  1172\n",
      "count ==  1173\n",
      "count ==  1174\n",
      "count ==  1175\n",
      "count ==  1176\n",
      "count ==  1177\n",
      "count ==  1178\n",
      "count ==  1179\n",
      "count ==  1180\n",
      "count ==  1181\n",
      "count ==  1182\n",
      "count ==  1183\n",
      "count ==  1184\n",
      "count ==  1185\n",
      "count ==  1186\n",
      "count ==  1187\n",
      "count ==  1188\n",
      "count ==  1189\n",
      "count ==  1190\n",
      "count ==  1191\n",
      "count ==  1192\n",
      "count ==  1193\n",
      "count ==  1194\n",
      "count ==  1195\n",
      "count ==  1196\n",
      "count ==  1197\n",
      "count ==  1198\n",
      "count ==  1199\n",
      "count ==  1200\n",
      "...DONE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train_data = maml_system.data.get_train_batches(total_batches=int(600/1), augment_images=False)\n",
    "train_data = maml_system.data.get_train_batches(total_batches=2, augment_images=False)\n",
    "\n",
    "print(\"==> Extract training task feature vector:\")\n",
    "\n",
    "count = 0\n",
    "trained_vector = torch.FloatTensor().cuda()\n",
    "\n",
    "for sample_idx, train_sample in enumerate(train_data):\n",
    "    \n",
    "    x_support_set, x_target_set, y_support_set, y_target_set, seed = train_sample\n",
    "    \n",
    "    x_support_set = torch.Tensor(x_support_set).float().to(device=maml_system.model.device)\n",
    "    x_target_set = torch.Tensor(x_target_set).float().to(device=maml_system.model.device)\n",
    "    y_support_set = torch.Tensor(y_support_set).long().to(device=maml_system.model.device)\n",
    "    y_target_set = torch.Tensor(y_target_set).long().to(device=maml_system.model.device)\n",
    "    \n",
    "    for task_id, (x_support_set_task, y_support_set_task, x_target_set_task, y_target_set_task) in enumerate(zip(x_support_set,\n",
    "                              y_support_set,\n",
    "                              x_target_set,\n",
    "                              y_target_set)):\n",
    "        \n",
    "        \n",
    "        names_weights_copy = maml_system.model.get_inner_loop_parameter_dict(maml_system.model.classifier.named_parameters())\n",
    "\n",
    "        num_devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "\n",
    "        names_weights_copy = {\n",
    "                name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
    "                    [num_devices] + [1 for i in range(len(value.shape))]) for\n",
    "                name, value in names_weights_copy.items()}\n",
    "\n",
    "        prompted_weights_copy = {}\n",
    "        if args.prompter:\n",
    "            prompted_weights_copy = {key: value for key, value in names_weights_copy.items() if 'prompt' in key}\n",
    "\n",
    "        names_weights_copy = {key: value for key, value in names_weights_copy.items() if 'layer_dict' in key}\n",
    "        \n",
    "        \n",
    "        n, s, c, h, w = x_target_set_task.shape\n",
    "\n",
    "        x_support_set_task = x_support_set_task.view(-1, c, h, w)\n",
    "        y_support_set_task = y_support_set_task.view(-1)\n",
    "        x_target_set_task = x_target_set_task.view(-1, c, h, w)\n",
    "        y_target_set_task = y_target_set_task.view(-1)\n",
    "        \n",
    "        \n",
    "        for num_step in range(5):\n",
    "\n",
    "            support_loss, support_preds, _ = maml_system.model.net_forward_feature_extractor(\n",
    "              x=x_support_set_task,\n",
    "              y=y_support_set_task,\n",
    "              weights=names_weights_copy,\n",
    "              prompted_weights=prompted_weights_copy,\n",
    "              backup_running_statistics=num_step == 0,\n",
    "              training=True,\n",
    "              num_step=num_step,\n",
    "              training_phase=False,\n",
    "              epoch=0)\n",
    "            \n",
    "            names_weights_copy, prompted_weights_copy = maml_system.model.apply_inner_loop_update(\n",
    "             loss=support_loss,\n",
    "             names_weights_copy=names_weights_copy,\n",
    "             prompted_weights_copy=prompted_weights_copy,\n",
    "             use_second_order=True,\n",
    "             current_step_idx=num_step,\n",
    "             current_iter='test',\n",
    "             training_phase=False)\n",
    "            \n",
    "            if num_step == 4:\n",
    "                count = count + 1\n",
    "                print(\"count == \", count)\n",
    "                target_loss, target_preds, feature_map_list = maml_system.model.net_forward_feature_extractor(\n",
    "                    x=x_target_set_task,\n",
    "                    y=y_target_set_task,\n",
    "                    weights=names_weights_copy,\n",
    "                    prompted_weights=prompted_weights_copy,\n",
    "                    backup_running_statistics=False, training=True,\n",
    "                    num_step=num_step,\n",
    "                    training_phase=False,\n",
    "                    epoch=0)\n",
    "\n",
    "                query_feature_map = feature_map_list[3].detach()\n",
    "                task_feature = torch.mean(query_feature_map, dim=0).unsqueeze(dim=0)\n",
    "                trained_vector = torch.cat((trained_vector, task_feature), dim=0)\n",
    "\n",
    "print(\"...DONE\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50fd8588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1200, 128, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "print(trained_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa417b7",
   "metadata": {},
   "source": [
    "## 3-2 test tasks에 대한 centroid를 저장한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66c764b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Extract test task feature vector:\n",
      "count ==  1\n",
      "count ==  2\n",
      "count ==  3\n",
      "count ==  4\n",
      "count ==  5\n",
      "count ==  6\n",
      "count ==  7\n",
      "count ==  8\n",
      "count ==  9\n",
      "count ==  10\n",
      "count ==  11\n",
      "count ==  12\n",
      "count ==  13\n",
      "count ==  14\n",
      "count ==  15\n",
      "count ==  16\n",
      "count ==  17\n",
      "count ==  18\n",
      "count ==  19\n",
      "count ==  20\n",
      "count ==  21\n",
      "count ==  22\n",
      "count ==  23\n",
      "count ==  24\n",
      "count ==  25\n",
      "count ==  26\n",
      "count ==  27\n",
      "count ==  28\n",
      "count ==  29\n",
      "count ==  30\n",
      "count ==  31\n",
      "count ==  32\n",
      "count ==  33\n",
      "count ==  34\n",
      "count ==  35\n",
      "count ==  36\n",
      "count ==  37\n",
      "count ==  38\n",
      "count ==  39\n",
      "count ==  40\n",
      "...DONE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = maml_system.data.get_test_batches(total_batches=20, augment_images=False)\n",
    "\n",
    "print(\"==> Extract test task feature vector:\")\n",
    "\n",
    "count = 0\n",
    "test_vector = torch.FloatTensor().cuda()\n",
    "\n",
    "for sample_idx, train_sample in enumerate(test_data):\n",
    "    \n",
    "    x_support_set, x_target_set, y_support_set, y_target_set, seed = train_sample\n",
    "    \n",
    "    x_support_set = torch.Tensor(x_support_set).float().to(device=maml_system.model.device)\n",
    "    x_target_set = torch.Tensor(x_target_set).float().to(device=maml_system.model.device)\n",
    "    y_support_set = torch.Tensor(y_support_set).long().to(device=maml_system.model.device)\n",
    "    y_target_set = torch.Tensor(y_target_set).long().to(device=maml_system.model.device)\n",
    "    \n",
    "    for task_id, (x_support_set_task, y_support_set_task, x_target_set_task, y_target_set_task) in enumerate(zip(x_support_set,\n",
    "                              y_support_set,\n",
    "                              x_target_set,\n",
    "                              y_target_set)):\n",
    "        \n",
    "        \n",
    "        names_weights_copy = maml_system.model.get_inner_loop_parameter_dict(maml_system.model.classifier.named_parameters())\n",
    "\n",
    "        num_devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "\n",
    "        names_weights_copy = {\n",
    "                name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
    "                    [num_devices] + [1 for i in range(len(value.shape))]) for\n",
    "                name, value in names_weights_copy.items()}\n",
    "\n",
    "        prompted_weights_copy = {}\n",
    "        if args.prompter:\n",
    "            prompted_weights_copy = {key: value for key, value in names_weights_copy.items() if 'prompt' in key}\n",
    "\n",
    "        names_weights_copy = {key: value for key, value in names_weights_copy.items() if 'layer_dict' in key}\n",
    "        \n",
    "        \n",
    "        n, s, c, h, w = x_target_set_task.shape\n",
    "\n",
    "        x_support_set_task = x_support_set_task.view(-1, c, h, w)\n",
    "        y_support_set_task = y_support_set_task.view(-1)\n",
    "        x_target_set_task = x_target_set_task.view(-1, c, h, w)\n",
    "        y_target_set_task = y_target_set_task.view(-1)\n",
    "        \n",
    "        \n",
    "        for num_step in range(5):\n",
    "\n",
    "            support_loss, support_preds, _ = maml_system.model.net_forward_feature_extractor(\n",
    "              x=x_support_set_task,\n",
    "              y=y_support_set_task,\n",
    "              weights=names_weights_copy,\n",
    "              prompted_weights=prompted_weights_copy,\n",
    "              backup_running_statistics=num_step == 0,\n",
    "              training=True,\n",
    "              num_step=num_step,\n",
    "              training_phase=False,\n",
    "              epoch=0)\n",
    "            \n",
    "            names_weights_copy, prompted_weights_copy = maml_system.model.apply_inner_loop_update(\n",
    "             loss=support_loss,\n",
    "             names_weights_copy=names_weights_copy,\n",
    "             prompted_weights_copy=prompted_weights_copy,\n",
    "             use_second_order=True,\n",
    "             current_step_idx=num_step,\n",
    "             current_iter='test',\n",
    "             training_phase=False)\n",
    "            \n",
    "            if num_step == 4:\n",
    "                \n",
    "                count = count + 1\n",
    "                print(\"count == \", count)\n",
    "                                \n",
    "                target_loss, target_preds, feature_map_list = maml_system.model.net_forward_feature_extractor(\n",
    "                    x=x_target_set_task,\n",
    "                    y=y_target_set_task,\n",
    "                    weights=names_weights_copy,\n",
    "                    prompted_weights=prompted_weights_copy,\n",
    "                    backup_running_statistics=False, training=True,\n",
    "                    num_step=num_step,\n",
    "                    training_phase=False,\n",
    "                    epoch=0)\n",
    "\n",
    "                query_feature_map = feature_map_list[3].detach()\n",
    "                task_feature = torch.mean(query_feature_map, dim=0).unsqueeze(dim=0)\n",
    "                test_vector = torch.cat((test_vector, task_feature), dim=0)\n",
    "\n",
    "print(\"...DONE\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f9a1411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 128, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "print(test_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e487ea74",
   "metadata": {},
   "source": [
    "# 4. Computing Euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95e7df4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "\n",
    "gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "batch_size = trained_vector.shape[0]\n",
    "trained_vector_gap = gap(trained_vector)              \n",
    "trained_vectorr_flat = trained_vector_gap.view(batch_size, 128)  \n",
    "\n",
    "# Step 2: Normalize test vector\n",
    "normalized_trained_vector = torch.nn.functional.normalize(trained_vectorr_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c71a441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "batch_size = test_vector.shape[0]\n",
    "test_vector_gap = gap(test_vector)              \n",
    "test_vector_flat = test_vector_gap.view(batch_size, 128) \n",
    "\n",
    "# Step 2: Normalize test vector\n",
    "normalized_test_vector = torch.nn.functional.normalize(test_vector_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b1b9813",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 40 is out of bounds for dimension 0 with size 40",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_50044\\1452937860.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     dist = torch.cdist(\n\u001b[0;32m      5\u001b[0m         \u001b[0mnormalized_trained_vector\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# shape: (1, 64)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mnormalized_test_vector\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m      \u001b[1;31m# shape: (1, 64)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     )  # result: shape (1, 1)\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 40 is out of bounds for dimension 0 with size 40"
     ]
    }
   ],
   "source": [
    "distance = torch.empty(0).cuda()\n",
    "\n",
    "for i in range(len(normalized_trained_vector)):\n",
    "    dist = torch.cdist(\n",
    "        normalized_trained_vector[i].unsqueeze(0),  # shape: (1, 64)\n",
    "        normalized_test_vector[i].unsqueeze(0)     \n",
    "    )  # result: shape (1, 1)\n",
    "    \n",
    "    distance = torch.cat((distance, dist.view(1)), dim=0)  # flatten & accumulate\n",
    "\n",
    "print(distance.shape)  # (B,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d0d105",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_np = distance.detach().cpu().numpy()\n",
    "np.save(\"metapro_distance.npy\", distance_np)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.kdeplot(distance_np, shade=True, color=\"tomato\", label=\"Trained-Test Distance\")\n",
    "plt.xlabel(\"Distance\", fontsize=14)\n",
    "plt.ylabel(\"Density\", fontsize=14)\n",
    "plt.title(\"KDE of Feature Distance\", fontsize=16)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2b213c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
