{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd87b33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pytorchcv.model_provider import get_model as ptcv_get_model # model\n",
    "import sys, os\n",
    "import easydict\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline \n",
    "\n",
    "# enable cuda devices\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from meta_neural_network_architectures import VGGReLUNormNetwork, ResNet12\n",
    "from utils.parser_utils import get_args\n",
    "from data import MetaLearningSystemDataLoader\n",
    "from experiment_builder import ExperimentBuilder\n",
    "from utils.basic import show_batch\n",
    "from few_shot_learning_system import MAMLFewShotClassifier\n",
    "import prompters\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from data_augmentation import mixup_data, cutmix_data, random_flip, random_flip_like_torchvision\n",
    "from utils.grad_cam import visualize_and_save_grad_cam, make_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c638561",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = \"mini_imagenet\"\n",
    "# datasets = \"tiered_imagenet\"\n",
    "# datasets = \"CIFAR_FS\"\n",
    "# datasets = \"CUB\"\n",
    "\n",
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "# os.environ['TEST_DATASET'] = \"tiered_imagenet\" # https://mtl.yyliu.net/download/Lmzjm9tX.html\n",
    "# os.environ['TEST_DATASET'] = \"CIFAR_FS\" # https://drive.google.com/file/d/1pTsCCMDj45kzFYgrnO67BWVbKs48Q3NI/view\n",
    "# os.environ['TEST_DATASET'] = \"CUB\" # https://data.caltech.edu/records/65de6-vp158"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8daf114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.environ['DATASET_DIR'] ===  C:/Users/JM/PycharmProjects/MAML/datasets\n"
     ]
    }
   ],
   "source": [
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "print(\"os.environ['DATASET_DIR'] === \", os.environ['DATASET_DIR'])\n",
    "\n",
    "args = easydict.EasyDict(\n",
    "{\n",
    "  \"batch_size\":2,\n",
    "  \"image_height\":84,\n",
    "  \"image_width\":84,\n",
    "  \"image_channels\":3,\n",
    "  \"gpu_to_use\":0,\n",
    "  \"num_dataprovider_workers\":4,\n",
    "  \"max_models_to_save\":5,\n",
    "  \"dataset_name\":\"mini_imagenet_full_size\",\n",
    "  \"dataset_path\":\"mini_imagenet_full_size\",\n",
    "  \"reset_stored_paths\":False,\n",
    "  \"experiment_name\":\"../MAML_5way_5shot_filter128_miniImagenet\",\n",
    "  \"train_seed\": 0, \"val_seed\": 0,\n",
    "  \"indexes_of_folders_indicating_class\": [-3, -2],\n",
    "  \"sets_are_pre_split\": True,\n",
    "  \"train_val_test_split\": [0.64, 0.16, 0.20],\n",
    "  \"evaluate_on_test_set_only\": False,\n",
    "\n",
    "  \"total_epochs\": 100,\n",
    "  \"total_iter_per_epoch\":500, \"continue_from_epoch\": -2,\n",
    "  \"num_evaluation_tasks\":600,\n",
    "  \"multi_step_loss_num_epochs\": 15,\n",
    "  \"minimum_per_task_contribution\": 0.01,\n",
    "  \"learnable_per_layer_per_step_inner_loop_learning_rate\": False,\n",
    "  \"enable_inner_loop_optimizable_bn_params\": False,\n",
    "  \"evalute_on_test_set_only\": False,\n",
    "\n",
    "  \"max_pooling\": True,\n",
    "  \"per_step_bn_statistics\": False,\n",
    "  \"learnable_batch_norm_momentum\": False,\n",
    "  \"load_into_memory\": False,\n",
    "  \"init_inner_loop_learning_rate\": 0.01,\n",
    "  \"init_inner_loop_weight_decay\": 0.0005,\n",
    "  \"learnable_bn_gamma\": True,\n",
    "  \"learnable_bn_beta\": True,\n",
    "\n",
    "  \"dropout_rate_value\":0.0,\n",
    "  \"min_learning_rate\":0.001,\n",
    "  \"meta_learning_rate\":0.001,   \"total_epochs_before_pause\": 100,\n",
    "  \"first_order_to_second_order_epoch\":-1,\n",
    "  \"weight_decay\": 0.0,\n",
    "\n",
    "  \"norm_layer\":\"batch_norm\",\n",
    "  \"cnn_num_filters\":128,\n",
    "  \"num_stages\":4,\n",
    "  \"conv_padding\": True,\n",
    "  \"number_of_training_steps_per_iter\":5,\n",
    "  \"number_of_evaluation_steps_per_iter\":5,\n",
    "  \"cnn_blocks_per_stage\":1,\n",
    "  \"num_classes_per_set\":5,\n",
    "  \"num_samples_per_class\":5,\n",
    "  \"num_target_samples\": 15,\n",
    "  \"samples_per_iter\" : 1,\n",
    "\n",
    "  \"second_order\": True,\n",
    "  \"use_multi_step_loss_optimization\":False,\n",
    "  \"backbone\": \"4-CONV\",\n",
    "  \"use_bias\": True,\n",
    "  \"prompter\": False,\n",
    "  \"prompt_engineering\": 'not',\n",
    "  \"prompt_size\" : 5,\n",
    "  \"image_size\" : 84,\n",
    "  \"prompt_random_init\": False,\n",
    "  \"outer_prompt_learning_rate\": 0.001,\n",
    "  \"inner_prompt_learning_rate\": 0.01,\n",
    "  \"ANIL\": False,\n",
    "  \"BOIL\": False,\n",
    "  \"data_aug\" : \"none\",\n",
    "  \"ablation_record\": False,\n",
    "  \"random_prompt_init\": False\n",
    "}\n",
    ")\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "args.im_shape = (2, 3, args.image_height, args.image_width)\n",
    "\n",
    "args.use_cuda = torch.cuda.is_available()\n",
    "args.seed = 104\n",
    "args.reverse_channels=False\n",
    "args.labels_as_int=False\n",
    "args.reset_stored_filepaths=False\n",
    "args.num_of_gpus=1\n",
    "\n",
    "args.continue_from_epoch='latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dba8492c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using max pooling\n",
      "meta network params\n",
      "Inner Loop parameters\n",
      "Outer Loop parameters\n",
      "classifier.layer_dict.conv0.conv.weight torch.Size([128, 3, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv0.conv.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv0.norm_layer.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv0.norm_layer.weight torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv1.conv.weight torch.Size([128, 128, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv1.conv.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv1.norm_layer.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv1.norm_layer.weight torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv2.conv.weight torch.Size([128, 128, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv2.conv.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv2.norm_layer.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv2.norm_layer.weight torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv3.conv.weight torch.Size([128, 128, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv3.conv.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv3.norm_layer.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv3.norm_layer.weight torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.linear.weights torch.Size([5, 3200]) cuda:0 True\n",
      "classifier.layer_dict.linear.bias torch.Size([5]) cuda:0 True\n",
      "1\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\MAML_5way_5shot_filter128_miniImagenet\n",
      "attempting to find existing checkpoint\n",
      "data {'test': 12000, 'train': 38400, 'val': 9600}\n",
      "train_seed 985773, val_seed: 985773, at start time\n",
      "50000 50000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_val_acc': 0.6124888875087102,\n",
       " 'best_val_iter': 49000,\n",
       " 'current_iter': 50000,\n",
       " 'best_epoch': 98,\n",
       " 'train_loss_mean': 0.7120679829120636,\n",
       " 'train_loss_std': 0.14076044237870394,\n",
       " 'train_accuracy_mean': 0.7360666686296463,\n",
       " 'train_accuracy_std': 0.06549975402460093,\n",
       " 'train_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_0_std': 0.0,\n",
       " 'train_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_1_std': 0.0,\n",
       " 'train_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_2_std': 0.0,\n",
       " 'train_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_3_std': 0.0,\n",
       " 'train_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'train_loss_importance_vector_4_std': 0.0,\n",
       " 'train_learning_rate_mean': 0.0010000000000000005,\n",
       " 'train_learning_rate_std': 4.336808689942018e-19,\n",
       " 'val_loss_mean': 1.0103712218999863,\n",
       " 'val_loss_std': 0.14254508195556043,\n",
       " 'val_accuracy_mean': 0.6018888874848684,\n",
       " 'val_accuracy_std': 0.06325599603173361,\n",
       " 'val_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_0_std': 0.0,\n",
       " 'val_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_1_std': 0.0,\n",
       " 'val_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_2_std': 0.0,\n",
       " 'val_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_3_std': 0.0,\n",
       " 'val_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'val_loss_importance_vector_4_std': 0.0,\n",
       " 'network': OrderedDict([('classifier.layer_dict.conv0.conv.weight',\n",
       "               tensor([[[[ 1.9280e-01, -4.3790e-01,  1.9152e-01],\n",
       "                         [-6.5922e-02,  1.1912e-03,  1.7843e-03],\n",
       "                         [-1.6238e-01,  3.9789e-01, -1.6815e-01]],\n",
       "               \n",
       "                        [[ 2.1157e-01, -4.2421e-01,  2.3436e-01],\n",
       "                         [-8.7411e-02,  2.6903e-02,  5.9444e-02],\n",
       "                         [-2.2375e-01,  3.0355e-01, -1.7231e-01]],\n",
       "               \n",
       "                        [[ 2.3858e-01, -3.1557e-01,  1.3068e-01],\n",
       "                         [ 8.0670e-03,  7.8086e-02, -7.0633e-02],\n",
       "                         [-1.7405e-01,  2.7057e-01, -1.8861e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[-7.0620e-05, -8.2205e-06,  2.6136e-05],\n",
       "                         [-8.7238e-05, -2.1879e-05,  1.3032e-05],\n",
       "                         [-7.1553e-05, -2.8386e-05, -4.3008e-06]],\n",
       "               \n",
       "                        [[-5.9485e-04, -5.3694e-04, -4.8319e-04],\n",
       "                         [-6.3765e-04, -5.6882e-04, -5.5036e-04],\n",
       "                         [-6.1299e-04, -6.0580e-04, -5.3954e-04]],\n",
       "               \n",
       "                        [[-4.9697e-04, -4.6914e-04, -4.6201e-04],\n",
       "                         [-5.1963e-04, -4.9912e-04, -4.8904e-04],\n",
       "                         [-5.1239e-04, -5.0165e-04, -4.8583e-04]]],\n",
       "               \n",
       "               \n",
       "                       [[[-2.0115e-05, -2.1234e-05, -2.4298e-05],\n",
       "                         [-2.2605e-05, -2.2636e-05, -2.4083e-05],\n",
       "                         [-2.1091e-05, -1.9453e-05, -2.1206e-05]],\n",
       "               \n",
       "                        [[-4.4698e-05, -4.6108e-05, -4.4551e-05],\n",
       "                         [-4.4194e-05, -4.5868e-05, -4.4358e-05],\n",
       "                         [-3.9435e-05, -4.0482e-05, -3.9965e-05]],\n",
       "               \n",
       "                        [[-1.0740e-05, -9.3691e-06, -6.9972e-06],\n",
       "                         [-1.1458e-05, -1.0037e-05, -7.7999e-06],\n",
       "                         [-7.5839e-06, -6.2566e-06, -5.1486e-06]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-4.9202e-06, -4.6687e-06, -5.0839e-06],\n",
       "                         [-4.4180e-06, -4.2686e-06, -5.4262e-06],\n",
       "                         [-4.2905e-06, -4.1260e-06, -4.7666e-06]],\n",
       "               \n",
       "                        [[-8.7833e-06, -8.1300e-06, -8.6526e-06],\n",
       "                         [-8.3399e-06, -7.2779e-06, -9.2245e-06],\n",
       "                         [-7.9753e-06, -7.4950e-06, -7.5235e-06]],\n",
       "               \n",
       "                        [[-8.7469e-06, -8.8710e-06, -8.6904e-06],\n",
       "                         [-8.4479e-06, -8.4006e-06, -8.7219e-06],\n",
       "                         [-8.2647e-06, -8.1701e-06, -8.4768e-06]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 2.2547e-05,  2.0962e-05,  2.0187e-05],\n",
       "                         [ 1.9970e-05,  1.6926e-05,  1.7004e-05],\n",
       "                         [ 2.0628e-05,  1.6703e-05,  1.7402e-05]],\n",
       "               \n",
       "                        [[-7.9465e-06, -4.0863e-05, -5.3787e-05],\n",
       "                         [-5.2562e-05, -8.0573e-05, -8.7200e-05],\n",
       "                         [-5.5343e-05, -5.5339e-05, -5.6871e-05]],\n",
       "               \n",
       "                        [[-2.6506e-05, -4.2413e-05, -3.5386e-05],\n",
       "                         [-4.9446e-05, -5.9901e-05, -4.7677e-05],\n",
       "                         [-6.1406e-05, -6.8441e-05, -5.2134e-05]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.4805e-03, -1.9604e-03, -1.6109e-03],\n",
       "                         [-1.6623e-03, -2.2106e-03, -1.7465e-03],\n",
       "                         [-1.1675e-03, -1.5785e-03, -1.1540e-03]],\n",
       "               \n",
       "                        [[-5.3214e-04, -8.9222e-04, -5.3660e-04],\n",
       "                         [-5.4592e-04, -1.0055e-03, -5.7528e-04],\n",
       "                         [ 3.4384e-05, -3.2995e-04,  9.2586e-05]],\n",
       "               \n",
       "                        [[-1.8811e-04, -5.5706e-04, -3.6363e-04],\n",
       "                         [-3.5050e-04, -7.7277e-04, -4.6043e-04],\n",
       "                         [ 7.8910e-05, -2.3774e-04,  1.1306e-04]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.conv.bias',\n",
       "               tensor([ 1.9924e-05, -1.0459e-05,  8.8542e-10, -3.7238e-08, -1.6649e-05,\n",
       "                        4.9035e-08, -1.3620e-14, -2.2902e-05, -7.5720e-07, -5.1962e-06,\n",
       "                       -3.1257e-08,  7.3433e-08, -2.0846e-05, -6.0071e-06, -3.4515e-05,\n",
       "                       -2.3894e-05, -6.6552e-12,  1.8293e-06,  2.6198e-13, -1.2016e-12,\n",
       "                        1.7226e-13, -6.3585e-14, -4.6319e-09, -1.7165e-05, -4.3471e-06,\n",
       "                       -6.2456e-09,  4.9483e-06,  9.4787e-07,  1.2061e-07, -2.2758e-05,\n",
       "                        5.6330e-05,  6.4718e-09, -1.6194e-05,  4.9077e-05, -6.3712e-06,\n",
       "                       -2.9390e-05, -3.2277e-06,  7.9065e-05,  1.9385e-05, -8.4401e-07,\n",
       "                       -4.8057e-08,  7.0701e-05, -4.1233e-05, -1.0828e-05,  3.0057e-05,\n",
       "                        2.4914e-05, -3.9026e-08,  2.0632e-07,  7.9858e-16, -1.9853e-06,\n",
       "                        8.8536e-13, -4.1784e-07, -1.1039e-04, -1.9879e-08, -6.7593e-10,\n",
       "                       -2.5542e-09, -1.5002e-07,  1.0581e-05, -3.0748e-09, -3.3157e-09,\n",
       "                       -6.4115e-14, -7.7585e-08, -7.4239e-07, -2.2794e-07, -2.9752e-07,\n",
       "                       -8.6416e-07,  3.5432e-05,  7.7230e-06, -2.4498e-08,  4.7322e-05,\n",
       "                       -1.3563e-07, -1.2694e-08,  7.4521e-06,  2.0596e-05, -6.4155e-08,\n",
       "                        4.3269e-08, -1.1718e-06, -4.3509e-07,  2.2859e-10, -3.4772e-12,\n",
       "                        1.1295e-12,  3.8278e-05,  6.4234e-09,  4.6697e-06, -3.2185e-05,\n",
       "                       -5.4889e-05, -3.8230e-06,  6.8019e-05,  1.9937e-11,  1.8124e-06,\n",
       "                        6.7002e-06, -1.4733e-07,  1.8495e-07,  6.7032e-05, -7.2066e-09,\n",
       "                       -8.3670e-07, -2.7113e-05, -2.7875e-08,  8.9182e-08, -3.8117e-08,\n",
       "                       -6.6436e-12, -1.2460e-04,  1.9808e-08,  7.3627e-09, -2.9729e-06,\n",
       "                        1.5614e-07, -1.0659e-05,  2.7780e-09,  1.0052e-04, -5.8965e-10,\n",
       "                       -7.0760e-05,  4.0617e-06, -4.8929e-10, -3.7253e-06, -4.1863e-13,\n",
       "                        4.9513e-09, -6.0602e-06, -3.9608e-08,  3.8746e-05,  3.9676e-05,\n",
       "                       -6.9208e-07, -1.7428e-05,  5.2189e-06, -2.0982e-08, -6.6100e-08,\n",
       "                       -1.6050e-05, -1.7712e-08,  4.0746e-06], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.bias',\n",
       "               tensor([ 5.8875e-02, -1.6507e-02, -5.5632e-03, -3.2579e-02, -2.5903e-01,\n",
       "                       -3.1091e-02, -1.7938e-10,  1.0507e-01, -4.1229e-02, -4.1087e-05,\n",
       "                       -2.6563e-02, -1.4379e-02, -1.2836e-03, -4.8512e-01,  1.1094e-01,\n",
       "                       -5.3164e-02, -5.4222e-08, -2.8535e-04, -3.0020e-10, -6.2241e-10,\n",
       "                       -3.2172e-13, -2.3991e-09, -3.1204e-02, -7.0961e-04, -5.5829e-03,\n",
       "                       -1.7764e-08, -3.6690e-01,  1.4929e-01, -4.1558e-02, -1.6044e-01,\n",
       "                        3.2861e-01, -1.0627e-02, -5.9325e-02, -3.1993e-01, -3.3567e-02,\n",
       "                       -6.1848e-02, -4.8050e-02,  1.9956e-01, -3.7749e-02, -3.5481e-01,\n",
       "                       -9.4190e-09,  4.6122e-02,  3.1968e-01,  1.2120e-02, -2.9340e-03,\n",
       "                       -2.9909e-01, -1.5349e-02, -1.6058e-02, -5.0766e-08, -1.2728e-02,\n",
       "                       -3.5614e-10, -2.9107e-02,  3.2736e-02, -1.1427e-02, -6.9402e-04,\n",
       "                       -1.4424e-07, -5.2031e-01,  4.2317e-02, -3.1742e-02, -1.2503e-09,\n",
       "                       -8.2197e-09, -1.2892e-02, -1.1950e-03, -2.2422e-02, -2.4193e-04,\n",
       "                       -2.8786e-02,  6.5221e-03, -3.4590e-03, -6.9081e-08, -1.5704e-01,\n",
       "                       -2.7188e-02, -3.7463e-02, -4.0883e-01, -4.0661e-01, -2.1873e-03,\n",
       "                       -8.5496e-07, -2.0373e-02, -8.1102e-04, -9.2724e-08, -1.9095e-06,\n",
       "                       -1.9449e-06, -1.4061e-01, -1.8938e-02, -1.6230e-02,  1.7541e-03,\n",
       "                        4.3162e-01,  7.2506e-02,  1.1899e-02, -4.4395e-07, -8.0588e-03,\n",
       "                       -2.8841e-06, -5.1763e-02, -3.3110e-02, -3.7981e-03, -3.4457e-02,\n",
       "                       -4.5250e-06, -7.9839e-03, -3.2400e-03, -2.9872e-04, -2.9507e-02,\n",
       "                       -4.4084e-06, -9.0507e-02, -1.3810e-02, -2.6987e-02,  1.5051e+00,\n",
       "                       -2.3176e-02, -7.8117e-04, -3.8410e-03, -1.8036e-01, -3.2485e-04,\n",
       "                        2.5858e-01,  2.3084e-01, -1.0850e-09, -2.5405e-05, -4.5761e-10,\n",
       "                       -1.8750e-02, -3.6644e-01, -3.3475e-05, -1.7724e-01,  1.7695e-02,\n",
       "                       -3.7874e-02, -3.6177e-01, -2.2040e-02, -3.6412e-03, -1.2955e-06,\n",
       "                       -2.9565e-04, -6.5529e-03, -3.4861e-02], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.weight',\n",
       "               tensor([ 3.5256e-01,  4.5398e-03,  2.6845e-04,  3.0078e-03,  1.5962e-01,\n",
       "                       -6.6458e-03, -7.6049e-20,  1.1900e-01, -4.7208e-03,  1.0172e-04,\n",
       "                       -3.5963e-03,  4.2365e-03,  3.5623e-01,  2.6186e-01,  3.9544e-01,\n",
       "                        5.4347e-01,  1.0270e-06, -1.1157e-04,  3.3736e-08,  8.5558e-09,\n",
       "                        1.6795e-07, -8.7807e-05,  8.4113e-03,  9.5707e-05,  4.4606e-04,\n",
       "                        6.3641e-17,  3.1820e-01,  2.4728e-01,  1.3751e-02,  3.7538e-01,\n",
       "                        3.3552e-01,  1.0095e-03,  6.4059e-01,  2.3765e-01,  9.2894e-02,\n",
       "                        4.9484e-01, -1.6546e-02,  3.6623e-01, -1.1182e-02,  2.0961e-01,\n",
       "                       -1.0071e-04,  2.8318e-01,  4.2412e-01,  6.8211e-01,  3.0807e-01,\n",
       "                        1.4962e-01,  5.2144e-03,  7.5074e-04,  1.1369e-21, -5.9218e-04,\n",
       "                       -2.6292e-17, -3.9562e-03,  5.1322e-01,  7.2713e-05,  3.8695e-04,\n",
       "                       -1.0072e-06,  4.6624e-01,  5.6281e-01, -4.9686e-03,  6.0579e-11,\n",
       "                       -2.0300e-10,  3.1792e-06, -1.0363e-04, -2.1424e-03,  2.5959e-04,\n",
       "                        5.7541e-03,  3.0508e-01,  9.8782e-05,  1.1452e-04,  4.5039e-01,\n",
       "                        8.6143e-04, -1.2736e-03,  2.9750e-01,  3.4057e-01, -8.4181e-04,\n",
       "                        1.4124e-07,  5.4905e-03, -8.9509e-06, -6.1155e-07, -1.4527e-12,\n",
       "                        7.5905e-07,  3.9020e-01,  1.1763e-03,  2.3310e-03,  2.6705e-01,\n",
       "                        3.9585e-01,  3.1681e-01,  3.6561e-01,  1.8483e-07,  2.2871e-03,\n",
       "                       -6.1005e-05,  9.4555e-03, -4.8394e-03,  2.1536e-01,  6.6461e-04,\n",
       "                        9.1718e-06, -3.4866e-04, -1.1992e-03,  2.5918e-04, -9.6956e-03,\n",
       "                       -2.2165e-14,  2.7859e-01,  1.1398e-03,  1.2738e-05, -7.9364e-03,\n",
       "                        5.9986e-03,  1.7318e-04,  8.7608e-04,  3.7191e-01, -4.2371e-05,\n",
       "                        3.7213e-01,  2.3429e-01,  1.9676e-07,  1.2573e-04,  6.7429e-11,\n",
       "                        6.5774e-03,  2.3760e-01,  3.5525e-06,  2.6147e-01,  6.1072e-01,\n",
       "                       -9.0689e-03,  4.0115e-01, -2.5560e-03, -1.5187e-03, -2.8847e-06,\n",
       "                       -1.2760e-04, -4.0000e-04, -1.0465e-02], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.weight',\n",
       "               tensor([[[[-3.6319e-03, -2.4029e-03, -2.9941e-03],\n",
       "                         [-3.7478e-03, -3.5717e-03, -3.2160e-03],\n",
       "                         [-4.4030e-03, -5.0988e-03, -5.0992e-03]],\n",
       "               \n",
       "                        [[-6.0083e-07, -6.4784e-07, -6.4837e-07],\n",
       "                         [-5.8849e-07, -6.1763e-07, -6.2497e-07],\n",
       "                         [-4.6389e-07, -4.6099e-07, -4.7544e-07]],\n",
       "               \n",
       "                        [[-6.5591e-08, -1.6902e-07, -3.0110e-08],\n",
       "                         [-3.8240e-08, -2.2291e-08, -1.9450e-08],\n",
       "                         [-9.7124e-08, -2.6355e-08, -8.2254e-08]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.5691e-08, -6.2661e-09, -5.6003e-09],\n",
       "                         [-1.1575e-08, -2.0409e-09, -5.1226e-10],\n",
       "                         [-1.5853e-08, -6.3271e-09, -2.5929e-09]],\n",
       "               \n",
       "                        [[-4.4751e-08,  2.4202e-08, -2.2466e-08],\n",
       "                         [ 1.4180e-08, -3.3342e-08,  2.6222e-08],\n",
       "                         [-1.2136e-07, -9.3364e-08, -6.1480e-08]],\n",
       "               \n",
       "                        [[ 9.2501e-07,  4.4943e-07,  2.1767e-07],\n",
       "                         [ 3.9802e-07,  1.5660e-07,  1.3197e-07],\n",
       "                         [ 7.9802e-07,  4.6077e-07,  1.1520e-07]]],\n",
       "               \n",
       "               \n",
       "                       [[[-4.4757e-02, -1.7065e-02, -4.7850e-02],\n",
       "                         [ 1.4945e-02, -5.9358e-03, -2.6825e-03],\n",
       "                         [-2.3155e-02, -4.0659e-03, -3.2097e-02]],\n",
       "               \n",
       "                        [[ 1.2731e-05, -2.2570e-05, -2.9567e-05],\n",
       "                         [ 2.1660e-05, -9.4165e-06, -2.2859e-05],\n",
       "                         [ 1.3591e-05, -8.1483e-06, -2.4980e-05]],\n",
       "               \n",
       "                        [[-1.9419e-05, -7.9506e-06, -2.4352e-05],\n",
       "                         [-1.7625e-05, -6.1509e-06, -2.1212e-05],\n",
       "                         [-1.0439e-05,  2.3977e-06, -1.6663e-05]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-3.9899e-06, -3.3739e-06, -4.1323e-06],\n",
       "                         [-8.0649e-07, -1.3309e-06, -1.7243e-06],\n",
       "                         [-2.3244e-07, -8.3678e-07, -2.0660e-06]],\n",
       "               \n",
       "                        [[-1.4989e-05, -4.7650e-06, -1.3635e-05],\n",
       "                         [-1.5479e-05, -7.0523e-06, -1.6260e-05],\n",
       "                         [-7.7871e-06,  1.6265e-06, -7.0778e-06]],\n",
       "               \n",
       "                        [[ 2.0818e-04,  1.7182e-04,  2.8575e-04],\n",
       "                         [ 1.1209e-04,  5.5394e-05,  1.3279e-04],\n",
       "                         [ 1.5004e-04,  1.2398e-04,  2.7530e-04]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 5.0943e-02,  3.4830e-02, -3.4788e-03],\n",
       "                         [-3.9771e-02,  4.5893e-02, -3.2045e-02],\n",
       "                         [ 4.5478e-03, -1.4779e-02,  2.0475e-02]],\n",
       "               \n",
       "                        [[ 8.0918e-05,  1.4310e-04,  1.2333e-04],\n",
       "                         [ 7.7939e-05,  1.6308e-04,  1.5572e-04],\n",
       "                         [ 4.7141e-05,  1.1664e-04,  1.2561e-04]],\n",
       "               \n",
       "                        [[ 1.5314e-05,  3.4003e-05,  1.1451e-05],\n",
       "                         [ 1.7662e-06,  1.9379e-05, -3.2651e-06],\n",
       "                         [ 3.0583e-05,  4.6992e-05,  2.7428e-05]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.5220e-06,  1.9458e-06,  3.3541e-06],\n",
       "                         [ 1.1352e-07, -2.2257e-06,  3.8241e-07],\n",
       "                         [ 9.3011e-06,  7.2981e-06,  8.5882e-06]],\n",
       "               \n",
       "                        [[ 3.2462e-05,  2.7762e-05,  7.7657e-06],\n",
       "                         [ 2.7584e-05,  2.5719e-05,  4.1392e-06],\n",
       "                         [ 5.9097e-05,  6.2332e-05,  4.0349e-05]],\n",
       "               \n",
       "                        [[-1.4692e-03, -1.3385e-03, -1.1955e-03],\n",
       "                         [-1.6537e-03, -1.4342e-03, -1.2311e-03],\n",
       "                         [-1.4783e-03, -1.2883e-03, -1.1577e-03]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 6.0390e-02,  3.7160e-02,  1.9511e-02],\n",
       "                         [ 4.7778e-02,  3.9960e-02,  2.4796e-02],\n",
       "                         [-1.2573e-03, -3.9521e-02,  1.2405e-02]],\n",
       "               \n",
       "                        [[-3.1003e-06,  2.1067e-05,  2.3154e-05],\n",
       "                         [-6.3560e-05, -3.8287e-05, -3.8302e-05],\n",
       "                         [-7.6998e-05, -7.0532e-05, -6.3432e-05]],\n",
       "               \n",
       "                        [[ 6.8480e-06, -3.3661e-07, -1.3470e-05],\n",
       "                         [-3.8919e-06, -1.1970e-05, -2.4683e-05],\n",
       "                         [-2.6073e-05, -3.4784e-05, -5.1807e-05]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.2197e-05,  8.4621e-06,  8.5162e-06],\n",
       "                         [ 1.0556e-05,  7.0314e-06,  8.0000e-06],\n",
       "                         [-1.7280e-06, -5.4469e-06, -4.8228e-06]],\n",
       "               \n",
       "                        [[ 4.0718e-05,  2.5003e-05,  1.3424e-05],\n",
       "                         [ 3.1414e-05,  1.6538e-05,  3.2728e-06],\n",
       "                         [ 1.6283e-05,  4.1536e-06, -1.2184e-05]],\n",
       "               \n",
       "                        [[ 9.7963e-04,  9.8013e-04,  9.0022e-04],\n",
       "                         [ 8.3971e-04,  8.2202e-04,  8.2474e-04],\n",
       "                         [ 8.5672e-04,  8.6708e-04,  8.5943e-04]]],\n",
       "               \n",
       "               \n",
       "                       [[[-3.8312e-05,  1.0251e-06,  1.0318e-05],\n",
       "                         [-1.0409e-04, -2.4805e-05, -1.1405e-04],\n",
       "                         [-8.0779e-05, -2.2906e-05, -6.8712e-05]],\n",
       "               \n",
       "                        [[-7.1199e-07, -3.6842e-07, -4.3430e-07],\n",
       "                         [-7.4192e-07, -3.8498e-07, -4.5176e-07],\n",
       "                         [ 1.1457e-06,  2.9272e-08,  2.8550e-07]],\n",
       "               \n",
       "                        [[-1.2098e-07,  7.7095e-08,  4.3486e-08],\n",
       "                         [-9.6199e-08,  8.8844e-09,  2.2543e-08],\n",
       "                         [ 9.6028e-08,  1.1829e-07, -1.8539e-08]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.0844e-08,  7.4109e-09, -7.7607e-09],\n",
       "                         [ 4.7967e-09, -3.5201e-09, -6.1481e-09],\n",
       "                         [-1.8283e-09,  7.4973e-09,  8.9766e-10]],\n",
       "               \n",
       "                        [[ 1.2422e-07,  4.7094e-08,  2.3744e-07],\n",
       "                         [-1.7350e-07,  6.0298e-08,  1.3922e-07],\n",
       "                         [ 8.8284e-08,  2.8056e-08,  1.2179e-07]],\n",
       "               \n",
       "                        [[-1.7240e-06, -2.2433e-06, -1.8211e-06],\n",
       "                         [-2.1118e-06, -2.1155e-06, -2.3288e-06],\n",
       "                         [-1.7395e-06, -1.8751e-06, -1.9184e-06]]],\n",
       "               \n",
       "               \n",
       "                       [[[-2.6110e-02, -2.2529e-02, -2.5122e-02],\n",
       "                         [-2.7151e-02, -1.7962e-02, -1.4958e-02],\n",
       "                         [-3.1427e-03,  2.7597e-03, -2.1675e-02]],\n",
       "               \n",
       "                        [[-2.6333e-04, -2.8387e-04, -2.9862e-04],\n",
       "                         [-2.9564e-04, -3.0040e-04, -2.9397e-04],\n",
       "                         [-3.0231e-04, -2.9341e-04, -2.9939e-04]],\n",
       "               \n",
       "                        [[-4.2939e-06, -9.9942e-07, -2.0946e-05],\n",
       "                         [-2.5559e-07,  2.3444e-06, -1.7537e-05],\n",
       "                         [-4.0459e-06, -1.6869e-06, -2.1539e-05]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.5276e-08,  9.6605e-07, -5.2397e-07],\n",
       "                         [ 9.7625e-07,  1.7293e-06,  2.4947e-07],\n",
       "                         [ 1.6325e-06,  1.9559e-06,  7.3510e-07]],\n",
       "               \n",
       "                        [[ 2.3391e-07,  6.8103e-06, -1.8778e-05],\n",
       "                         [ 6.2078e-06,  9.3873e-06, -1.5353e-05],\n",
       "                         [ 2.6139e-06,  4.8347e-06, -1.9370e-05]],\n",
       "               \n",
       "                        [[-4.1799e-05, -1.9525e-04, -3.8717e-04],\n",
       "                         [-4.2534e-05, -1.6565e-04, -2.9608e-04],\n",
       "                         [-1.2816e-04, -1.8896e-04, -2.6848e-04]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.bias',\n",
       "               tensor([ 2.6900e-05, -6.5595e-06, -8.7568e-05, -4.3382e-07,  8.7394e-06,\n",
       "                        2.0289e-08, -3.0442e-12, -2.2150e-07, -1.5177e-06, -1.7663e-05,\n",
       "                       -2.0117e-05,  4.1438e-13, -3.4517e-05, -3.2384e-07,  3.6857e-05,\n",
       "                       -2.6251e-07,  8.3486e-07, -2.8589e-05,  3.1626e-09, -2.2998e-08,\n",
       "                       -5.3691e-06,  2.6952e-05,  1.5281e-05,  1.6274e-05,  4.8061e-05,\n",
       "                        2.8057e-07, -3.1891e-13, -5.7183e-09, -7.8425e-05,  1.6048e-05,\n",
       "                        3.1350e-05, -8.0524e-09,  1.0216e-06, -1.6684e-05, -6.7499e-05,\n",
       "                       -1.4223e-05,  6.9706e-08,  3.5480e-06, -2.7778e-05,  4.6265e-05,\n",
       "                       -7.2724e-05,  5.4853e-09,  5.2728e-14,  2.0158e-08, -5.7724e-05,\n",
       "                        7.4220e-06, -9.6476e-06,  2.2786e-08, -1.0026e-05, -3.9781e-06,\n",
       "                        7.9659e-05,  2.4166e-15,  1.1635e-05, -2.9546e-05, -1.2303e-05,\n",
       "                       -9.2554e-06,  3.6869e-05,  1.1826e-08, -4.2767e-08, -2.6572e-05,\n",
       "                       -1.9988e-05,  2.3989e-06,  2.5861e-12, -1.3938e-05, -1.6100e-07,\n",
       "                        2.3228e-06, -4.2757e-06,  4.2337e-05,  7.1519e-06, -1.0348e-11,\n",
       "                        3.6085e-05,  3.3840e-05, -1.5303e-05, -1.3210e-05,  1.3334e-05,\n",
       "                        4.1375e-05, -5.5250e-12, -2.8712e-05,  8.5811e-05, -4.2067e-05,\n",
       "                       -3.6366e-05, -1.2349e-09, -3.9637e-07, -8.1313e-07, -6.7153e-09,\n",
       "                       -2.9715e-08, -2.9927e-06, -5.1825e-05,  4.0887e-07, -2.9760e-08,\n",
       "                       -6.5241e-07, -1.2906e-05, -2.1030e-09, -2.3387e-07,  3.3135e-05,\n",
       "                        4.6546e-08, -8.1595e-08,  4.8264e-05, -2.3707e-05, -7.0209e-06,\n",
       "                        2.2178e-07, -1.9282e-07,  4.5077e-05,  4.9777e-06, -1.1040e-05,\n",
       "                       -4.6582e-05, -1.0479e-06,  1.0106e-05,  6.0236e-06, -3.0419e-05,\n",
       "                        8.5684e-06,  1.0165e-07,  1.8141e-05,  3.0308e-08, -4.4154e-05,\n",
       "                       -4.7657e-06,  2.6109e-05, -6.7611e-08, -7.7525e-07, -1.5557e-08,\n",
       "                        3.3917e-13,  1.6812e-05,  2.0108e-05,  3.3877e-10, -3.2936e-05,\n",
       "                       -1.6064e-05,  1.3179e-06, -2.1865e-06], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.bias',\n",
       "               tensor([-6.5894e-02, -4.1680e-01, -1.9986e-01, -1.1177e-01, -3.9570e-01,\n",
       "                       -9.6203e-03, -1.6180e-06, -9.7843e-03, -1.4586e-01, -3.1857e-01,\n",
       "                       -2.8052e-01, -8.5220e-10, -3.5090e-01, -7.4466e-02, -1.2458e-01,\n",
       "                       -6.1644e-03, -6.1493e-02, -4.9841e-02, -1.6776e-05, -1.2254e-02,\n",
       "                       -2.4496e-01, -2.7650e-01, -1.5880e-01, -2.6694e-01, -2.2381e-01,\n",
       "                       -1.3687e-01, -3.5366e-08, -1.0252e-06, -2.8601e-01, -3.5874e-02,\n",
       "                       -2.0093e-01, -5.5769e-09, -2.0361e-02, -2.8031e-01, -7.3831e-01,\n",
       "                       -8.8902e-02, -3.6346e-02, -2.5472e-01, -4.2278e-01, -4.2137e-01,\n",
       "                       -1.1647e-01, -4.0180e-02, -6.9121e-11, -5.3825e-02, -3.0069e-01,\n",
       "                       -2.0984e-01, -8.4217e-05, -4.5293e-02, -1.7622e-01, -2.2553e-01,\n",
       "                       -2.8262e-01, -1.4293e-08, -1.9587e-01, -2.2473e-02, -2.3963e-01,\n",
       "                       -3.2992e-01, -3.0638e-01, -8.2058e-03, -5.3624e-10, -1.8553e-01,\n",
       "                       -9.7904e-02, -5.1164e-02, -3.2063e-08, -2.3313e-01, -1.4495e-02,\n",
       "                       -2.8352e-02, -9.4010e-03, -1.9367e-01, -3.1832e-02, -2.6360e-05,\n",
       "                       -2.4571e-01, -6.2383e-02, -3.3948e-01, -3.2323e-01, -3.9085e-01,\n",
       "                       -2.5592e-01, -1.8423e-07, -4.5853e-09, -7.4085e-01, -3.5497e-01,\n",
       "                       -2.0161e-01, -2.8744e-11, -2.1863e-02, -2.9371e-02, -1.4371e-03,\n",
       "                       -1.9039e-03, -2.1477e-02,  2.9696e-02, -3.9358e-02, -3.0938e-02,\n",
       "                       -3.3905e-02, -1.7676e-01, -2.6967e-02, -5.1794e-02, -4.2056e-01,\n",
       "                       -4.5000e-02, -2.0467e-03, -1.2845e-01, -1.7540e-01, -4.9008e-01,\n",
       "                       -3.2852e-02, -5.2650e-02, -2.0596e-01, -2.5445e-06, -2.7965e-01,\n",
       "                       -2.4044e-01, -2.2723e-10, -1.9977e-01, -3.1761e-01, -1.4142e-01,\n",
       "                       -1.3067e-01, -2.7056e-09, -3.4200e-01, -7.7128e-03, -1.9892e-01,\n",
       "                       -2.6984e-01, -1.2467e-01, -9.0398e-03, -2.9621e-01, -1.2779e-03,\n",
       "                       -2.2005e-09, -2.6208e-02, -1.8151e-01, -3.3465e-06, -3.6962e-01,\n",
       "                       -1.7610e-01, -1.9106e-02, -2.5738e-01], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.weight',\n",
       "               tensor([ 1.4776e-03,  1.7577e-01,  3.4986e-01,  3.9883e-02,  2.5503e-01,\n",
       "                        8.0670e-04, -2.8331e-05,  1.4019e-03,  2.7739e-01,  3.8661e-01,\n",
       "                        4.0365e-01,  2.0880e-05,  6.2287e-01,  6.4222e-03,  2.7962e-01,\n",
       "                        1.3977e-03, -2.9075e-02,  2.4703e-01, -6.5268e-05, -1.9559e-03,\n",
       "                        3.5083e-01,  4.1200e-01,  3.0045e-01,  2.9823e-01,  3.6435e-01,\n",
       "                        2.8969e-01, -6.0188e-05,  3.0660e-05,  5.0924e-01, -3.1072e-03,\n",
       "                        2.6527e-01,  1.2914e-13, -2.7317e-03,  4.8735e-01,  8.9920e-01,\n",
       "                        2.7169e-01, -4.8176e-03,  3.7313e-01,  3.4028e-01,  6.8782e-01,\n",
       "                        3.3941e-01, -4.2073e-04,  9.9610e-06,  2.4144e-03,  3.6867e-01,\n",
       "                        4.5565e-01, -1.8673e-04,  1.3430e-03,  4.0117e-01,  2.0252e-01,\n",
       "                        4.5331e-01,  3.7316e-10,  3.2241e-01,  4.6881e-01,  4.4550e-01,\n",
       "                        5.3071e-01,  3.8138e-01, -1.4329e-03,  3.4925e-08,  4.0700e-01,\n",
       "                        2.5328e-01,  8.2359e-03,  1.8866e-05,  2.3877e-01,  1.0863e-03,\n",
       "                       -2.3883e-03,  1.3396e-03,  4.6400e-01, -4.6217e-03,  2.2957e-11,\n",
       "                        3.6182e-01,  2.2680e-01,  2.8429e-01,  3.9848e-01,  3.8920e-01,\n",
       "                        2.8574e-01, -3.2401e-05, -8.4436e-14,  7.0433e-01,  3.9335e-01,\n",
       "                        2.9960e-01,  3.9948e-10, -2.7935e-03, -4.3542e-03, -5.8002e-04,\n",
       "                       -6.6287e-04,  2.9302e-03,  3.8288e-01,  6.4711e-03, -4.2360e-03,\n",
       "                        3.2460e-03,  3.1026e-01,  4.4484e-03,  7.9029e-03,  5.2831e-01,\n",
       "                       -6.6318e-04,  4.7576e-04,  1.9685e-01,  3.7861e-01,  3.7198e-01,\n",
       "                        5.6704e-03,  1.8654e-02,  2.9070e-01, -6.5794e-05,  3.4116e-01,\n",
       "                        4.0385e-01, -8.4406e-10,  2.8588e-01,  4.0388e-01,  2.0668e-01,\n",
       "                        4.9685e-02, -6.5412e-07,  5.3500e-01, -1.0895e-03,  3.6675e-01,\n",
       "                        3.1254e-01,  4.0570e-01,  1.2260e-03,  5.1393e-01, -7.2392e-05,\n",
       "                       -1.1608e-05, -9.4721e-04,  3.0207e-01,  1.5205e-04,  2.9936e-01,\n",
       "                        3.4729e-01,  2.3841e-03,  3.3763e-01], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.weight',\n",
       "               tensor([[[[-6.8672e-06, -2.9169e-06, -8.2194e-06],\n",
       "                         [-5.1529e-06, -1.2574e-06, -6.8999e-06],\n",
       "                         [-8.8348e-06, -5.1660e-06, -1.1119e-05]],\n",
       "               \n",
       "                        [[ 2.9410e-04,  5.8138e-04,  2.7135e-04],\n",
       "                         [ 4.7891e-04,  9.6214e-04,  5.3539e-04],\n",
       "                         [ 3.8489e-04,  7.4056e-04,  4.9740e-04]],\n",
       "               \n",
       "                        [[ 6.4209e-04,  3.4441e-03,  1.5158e-03],\n",
       "                         [ 1.8526e-03,  2.4644e-03,  3.1586e-04],\n",
       "                         [ 2.4591e-03,  1.5145e-03,  4.3444e-04]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 3.0472e-03,  2.9620e-03,  4.1208e-03],\n",
       "                         [ 2.1971e-03,  2.1632e-03,  2.7610e-03],\n",
       "                         [-4.0041e-05,  1.1827e-03,  1.4291e-03]],\n",
       "               \n",
       "                        [[ 1.2966e-06,  9.7443e-07,  4.7913e-06],\n",
       "                         [-1.3285e-06,  4.0382e-06,  1.2783e-06],\n",
       "                         [-1.8839e-07,  3.3109e-06,  4.5608e-06]],\n",
       "               \n",
       "                        [[ 8.5852e-04,  2.0822e-03,  6.5570e-04],\n",
       "                         [ 1.9355e-03,  1.5864e-03,  1.6391e-03],\n",
       "                         [ 8.3072e-04,  1.6096e-03,  2.5315e-05]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 3.0451e-05,  6.7104e-05,  8.0611e-05],\n",
       "                         [-3.2286e-05,  4.1417e-06,  3.8286e-05],\n",
       "                         [-2.8237e-05,  1.1654e-05,  6.1791e-05]],\n",
       "               \n",
       "                        [[-3.9974e-03, -4.1785e-02, -8.9643e-03],\n",
       "                         [-8.3617e-03, -3.8878e-02, -1.0992e-02],\n",
       "                         [-1.9297e-03, -2.8271e-02, -1.3313e-02]],\n",
       "               \n",
       "                        [[-1.8920e-02,  1.6615e-03, -8.9880e-03],\n",
       "                         [-8.1319e-03,  7.5580e-03,  1.0962e-02],\n",
       "                         [-1.3648e-02,  1.5768e-02,  1.3728e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-7.9629e-02, -9.0064e-02, -9.6926e-02],\n",
       "                         [-4.5335e-02, -7.0465e-02, -5.2149e-02],\n",
       "                         [-6.8765e-02, -6.3180e-02, -3.2950e-02]],\n",
       "               \n",
       "                        [[-1.9450e-04, -2.1696e-04, -5.2379e-05],\n",
       "                         [ 4.3969e-05, -1.9784e-05,  2.6398e-04],\n",
       "                         [ 2.5545e-04,  2.3284e-04,  3.5308e-04]],\n",
       "               \n",
       "                        [[-2.3973e-02, -6.0555e-02, -1.7821e-02],\n",
       "                         [-5.6975e-02, -8.0944e-02, -2.3566e-02],\n",
       "                         [-3.8529e-02, -7.3318e-02, -2.6316e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.0040e-06, -6.8575e-07, -5.4517e-07],\n",
       "                         [-1.3445e-06,  1.0945e-07,  1.9683e-08],\n",
       "                         [-1.2558e-06,  1.0106e-07,  3.2454e-08]],\n",
       "               \n",
       "                        [[-2.0580e-06,  1.9756e-05,  1.9641e-05],\n",
       "                         [ 6.0015e-06,  1.5114e-05,  1.9394e-05],\n",
       "                         [-1.8161e-06,  9.2914e-06,  7.4785e-06]],\n",
       "               \n",
       "                        [[-7.5780e-05, -2.8921e-05, -3.2657e-05],\n",
       "                         [-4.7154e-05, -6.3266e-06, -1.0670e-05],\n",
       "                         [-7.5620e-05, -5.9465e-05, -4.3638e-05]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.1817e-05,  3.1018e-05,  3.3254e-05],\n",
       "                         [ 1.0249e-05,  6.1196e-05,  5.2028e-05],\n",
       "                         [ 9.3710e-06,  4.7757e-05,  1.1478e-05]],\n",
       "               \n",
       "                        [[ 1.3005e-08,  6.5891e-08, -1.8392e-08],\n",
       "                         [ 1.9123e-09,  1.5908e-07,  7.8537e-08],\n",
       "                         [ 1.6560e-08,  7.0118e-08,  4.2910e-08]],\n",
       "               \n",
       "                        [[-1.2483e-04, -1.0533e-04, -4.9037e-05],\n",
       "                         [-1.3506e-04, -6.4488e-05, -3.5841e-05],\n",
       "                         [-1.0661e-04, -4.5592e-05, -4.2855e-05]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-1.1610e-04, -1.6460e-04, -2.0218e-04],\n",
       "                         [-6.0168e-05, -1.1961e-04, -1.8068e-04],\n",
       "                         [-1.0660e-04, -1.2589e-04, -2.1726e-04]],\n",
       "               \n",
       "                        [[-3.7134e-03, -3.1289e-02, -1.8402e-02],\n",
       "                         [ 8.4536e-03, -1.0620e-02,  1.3048e-03],\n",
       "                         [ 1.9129e-02,  1.7596e-03,  1.0516e-02]],\n",
       "               \n",
       "                        [[-4.5031e-02, -9.2480e-02, -2.1342e-02],\n",
       "                         [-4.0951e-02, -8.2628e-02, -3.6945e-02],\n",
       "                         [-5.5429e-02, -9.3086e-02, -2.8754e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 3.7379e-02,  2.7309e-02,  4.0735e-02],\n",
       "                         [-2.3583e-02, -1.1655e-02, -1.4489e-02],\n",
       "                         [ 3.7759e-02,  1.1979e-02,  5.1427e-02]],\n",
       "               \n",
       "                        [[ 2.6452e-04,  8.2040e-05,  1.4454e-04],\n",
       "                         [ 2.6428e-04,  1.7568e-04,  2.0918e-04],\n",
       "                         [ 1.4597e-04,  1.2492e-04,  9.9553e-05]],\n",
       "               \n",
       "                        [[-9.6914e-02, -5.7667e-02, -2.1430e-02],\n",
       "                         [-7.4969e-02, -3.9933e-02, -3.1507e-02],\n",
       "                         [-5.1446e-02, -2.4259e-02, -1.8895e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 2.5022e-04, -4.4218e-04, -9.0695e-04],\n",
       "                         [ 7.9486e-04,  2.7777e-04,  4.4047e-04],\n",
       "                         [ 1.6921e-04,  5.0632e-06, -1.4959e-04]],\n",
       "               \n",
       "                        [[-4.8373e-02, -1.8955e-02, -3.7945e-02],\n",
       "                         [ 7.6282e-03, -2.6252e-03, -6.1565e-03],\n",
       "                         [ 1.1433e-02, -3.4937e-03, -2.2909e-02]],\n",
       "               \n",
       "                        [[-3.7636e-02, -3.9278e-02, -7.0374e-02],\n",
       "                         [-9.0985e-02, -1.0313e-01, -7.5680e-02],\n",
       "                         [-7.8908e-02, -4.1610e-02, -2.0231e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.5288e-03, -1.4759e-01, -1.0915e-01],\n",
       "                         [ 4.6189e-02, -1.3344e-02, -2.9445e-02],\n",
       "                         [ 1.0056e-01,  2.2916e-01,  2.1675e-01]],\n",
       "               \n",
       "                        [[ 6.5898e-04,  6.6343e-04,  6.5274e-04],\n",
       "                         [ 4.0791e-04,  3.3772e-04,  4.0065e-04],\n",
       "                         [ 4.1352e-04,  4.4215e-04,  4.3419e-04]],\n",
       "               \n",
       "                        [[-4.3819e-02, -8.2117e-02, -2.6754e-02],\n",
       "                         [ 7.8815e-03, -1.1616e-02,  2.9545e-02],\n",
       "                         [-1.3340e-02,  1.1500e-02,  3.7260e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.1916e-03, -1.9241e-03, -2.0255e-04],\n",
       "                         [-2.9255e-03, -4.5503e-03, -3.8254e-04],\n",
       "                         [-3.0500e-03, -2.3076e-03, -3.0902e-05]],\n",
       "               \n",
       "                        [[-2.4515e-02,  1.0860e-02, -5.3342e-02],\n",
       "                         [-7.9512e-03,  1.3322e-03, -2.8704e-02],\n",
       "                         [ 1.3080e-02, -2.3675e-02, -3.6541e-02]],\n",
       "               \n",
       "                        [[-7.6196e-03, -6.7835e-02, -1.4912e-01],\n",
       "                         [ 6.4926e-03, -3.9544e-02, -1.5755e-01],\n",
       "                         [ 2.4306e-02, -3.4117e-02, -1.1837e-01]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.6682e-02, -2.6836e-02, -3.6881e-02],\n",
       "                         [-3.0934e-02, -7.8056e-02, -8.0833e-02],\n",
       "                         [ 3.1869e-02,  1.3968e-02, -4.7496e-02]],\n",
       "               \n",
       "                        [[-1.5228e-04,  1.3461e-04,  3.0296e-04],\n",
       "                         [-1.9235e-04,  1.2276e-04,  3.1070e-04],\n",
       "                         [-1.3345e-04,  1.8122e-04,  3.7382e-04]],\n",
       "               \n",
       "                        [[ 3.9520e-02, -2.1105e-02, -6.8913e-02],\n",
       "                         [-4.5269e-02, -6.9267e-02, -3.2724e-02],\n",
       "                         [-2.0126e-02, -9.3996e-03, -1.9952e-03]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.bias',\n",
       "               tensor([-1.9599e-09,  1.9654e-05, -7.8099e-08,  2.5908e-06,  1.8220e-05,\n",
       "                       -2.0199e-08,  2.3164e-05,  3.5488e-05, -7.0819e-05, -2.1736e-09,\n",
       "                        7.3224e-08, -8.4634e-07, -5.8984e-05, -3.8238e-06,  4.0884e-05,\n",
       "                       -1.2828e-05, -2.6508e-06,  1.9264e-05,  2.0786e-05, -5.5656e-07,\n",
       "                        8.3039e-05,  7.5530e-05,  1.4751e-05,  2.2571e-05, -1.9870e-05,\n",
       "                        1.8190e-05,  1.6173e-05,  3.1449e-05,  4.3093e-06, -2.0070e-06,\n",
       "                       -1.2397e-05,  2.1020e-05,  6.0270e-05,  4.0851e-06, -1.3128e-06,\n",
       "                        3.4171e-11,  9.0193e-09,  1.1828e-05,  1.8698e-05, -6.0175e-05,\n",
       "                        2.4434e-05,  3.9698e-08, -1.5985e-08,  3.0793e-05, -1.5584e-05,\n",
       "                        1.6977e-09,  1.4415e-05,  1.3310e-07,  2.3757e-05, -1.3269e-05,\n",
       "                       -1.5336e-05,  3.6398e-05, -1.0814e-04,  1.4929e-05, -2.4642e-05,\n",
       "                        3.8758e-06, -1.8094e-06, -2.3887e-05, -1.0761e-07, -7.1853e-06,\n",
       "                       -5.6711e-06,  1.2003e-08, -6.6861e-06, -1.7087e-05, -6.5205e-07,\n",
       "                        4.2140e-07,  1.8579e-05,  4.5895e-05,  3.9058e-06, -1.2510e-09,\n",
       "                        2.3102e-05,  1.1495e-08, -4.3086e-05, -1.5485e-08, -6.0918e-06,\n",
       "                        2.0077e-05,  3.2462e-07, -1.1452e-05,  5.7940e-06, -1.5744e-05,\n",
       "                       -7.8761e-06,  4.3584e-05, -2.6489e-05, -5.2743e-07,  2.3048e-05,\n",
       "                        1.5094e-05, -6.4111e-05,  9.3565e-10,  3.2712e-05, -6.4166e-06,\n",
       "                        2.2135e-05, -3.7162e-16, -1.2453e-05,  2.2398e-05,  2.6598e-06,\n",
       "                       -2.9687e-05,  5.6285e-05,  1.6876e-05,  6.7740e-05,  3.1873e-06,\n",
       "                        5.2483e-05,  3.1314e-07, -9.1065e-06, -1.1194e-05, -4.3759e-05,\n",
       "                       -2.9988e-06,  2.0034e-05,  1.7558e-05,  7.3987e-05,  2.2340e-05,\n",
       "                        1.1248e-05, -1.6942e-05,  9.5347e-07,  5.5198e-05,  1.4529e-05,\n",
       "                        8.1642e-09,  2.8436e-05,  1.5248e-05,  3.9910e-05, -1.8272e-05,\n",
       "                        2.0692e-06, -2.5543e-05, -2.8784e-05, -8.6508e-06,  1.0820e-04,\n",
       "                        5.2961e-06, -5.1769e-05,  2.5568e-05], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.bias',\n",
       "               tensor([-5.8863e-02, -2.7271e-01, -1.2923e-02, -3.6429e-01, -4.2500e-01,\n",
       "                       -1.8255e-03, -4.0533e-01, -4.6672e-01, -2.5798e-01, -6.8581e-08,\n",
       "                       -6.0119e-03, -1.1208e-01, -3.3841e-01, -2.4926e-01, -6.2420e-01,\n",
       "                       -4.3773e-01, -2.5978e-01, -3.2436e-01, -3.7992e-01, -2.3172e-01,\n",
       "                       -3.9909e-01, -5.6985e-01, -2.8010e-01, -2.7561e-01, -3.9532e-01,\n",
       "                       -2.6204e-01, -7.5720e-04, -2.3220e-01, -1.7822e-01, -3.1918e-01,\n",
       "                       -4.9414e-01, -3.5185e-01, -2.6213e-01, -3.7918e-01, -5.9130e-01,\n",
       "                       -9.2343e-07, -5.4865e-05, -2.9592e-01, -4.9600e-01, -3.2370e-01,\n",
       "                       -3.9939e-02, -2.8656e-03, -1.5376e-02, -4.6058e-01, -4.5736e-01,\n",
       "                       -4.4371e-06, -3.7823e-01, -1.4247e-04, -2.7710e-01, -3.8966e-01,\n",
       "                       -4.3980e-01, -5.1808e-01, -4.1148e-01, -2.6134e-01, -2.1917e-01,\n",
       "                       -4.6193e-02, -8.9427e-02, -6.2116e-01, -5.2052e-03, -2.6476e-01,\n",
       "                       -5.1890e-02, -9.0577e-04, -3.8315e-01, -2.8713e-01, -2.7741e-02,\n",
       "                       -7.8634e-09, -2.3271e-06, -2.9527e-01, -3.8531e-01, -1.7338e-05,\n",
       "                       -4.2433e-01, -7.2882e-06, -3.3336e-01, -3.5224e-03, -2.9893e-01,\n",
       "                       -2.2262e-01, -2.3432e-01, -3.2890e-01, -2.7609e-01, -6.7401e-01,\n",
       "                       -5.9531e-01, -3.3217e-01, -5.4949e-01, -3.9693e-06, -1.6807e-01,\n",
       "                       -7.1265e-01, -2.3703e-01, -2.4301e-06, -3.4231e-01, -3.5280e-01,\n",
       "                       -3.5228e-01, -2.4360e-06, -3.5980e-01, -1.9319e-01, -3.6922e-01,\n",
       "                       -1.0395e-01, -1.7770e-01, -2.2495e-01, -3.4926e-01, -1.0176e-02,\n",
       "                       -2.0861e-01, -2.4558e-01, -2.1847e-01, -1.9803e-01, -2.8916e-01,\n",
       "                       -3.1519e-01, -4.1611e-01, -2.9888e-01, -5.8433e-02, -3.1034e-01,\n",
       "                       -3.0383e-01, -3.6118e-03, -1.3297e-03, -7.1576e-01, -2.2139e-01,\n",
       "                       -1.0888e-08, -1.8471e-02, -4.9200e-01, -3.7252e-01, -3.8182e-02,\n",
       "                       -2.4937e-01, -2.0329e-01, -3.1795e-01, -3.0571e-01, -3.1332e-01,\n",
       "                       -2.9134e-01, -1.3150e-01, -2.5140e-01], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.weight',\n",
       "               tensor([-8.2706e-03,  1.9896e-01, -1.9210e-03,  3.3728e-01,  3.2817e-01,\n",
       "                       -5.7242e-04,  2.6652e-01,  3.1978e-01,  2.0816e-01,  1.3554e-06,\n",
       "                       -1.0940e-03,  1.4203e-01,  3.3767e-01,  2.7242e-01,  4.1624e-01,\n",
       "                        4.1707e-01,  2.6832e-01,  2.9872e-01,  2.6950e-01,  3.0517e-01,\n",
       "                        3.6775e-01,  5.4328e-01,  2.4616e-01,  1.9587e-01,  4.2849e-01,\n",
       "                        2.5437e-01, -4.3058e-06,  2.4728e-01,  1.4499e-01,  7.6752e-02,\n",
       "                        3.7730e-01,  1.8832e-01,  2.3391e-01,  2.3562e-01,  2.6264e-01,\n",
       "                        8.5797e-05, -9.7988e-08,  2.0589e-01,  3.3056e-01,  2.7006e-01,\n",
       "                       -3.9391e-03,  1.1260e-03,  2.8087e-04,  4.7935e-01,  3.3669e-01,\n",
       "                       -5.0146e-05,  1.8119e-01,  2.5592e-05,  2.0378e-01,  3.0841e-01,\n",
       "                        1.6395e-01,  3.5014e-01,  4.6291e-01,  2.3778e-01,  2.3749e-01,\n",
       "                        5.3196e-03,  1.5704e-02,  6.8204e-01,  1.1343e-03,  2.6876e-01,\n",
       "                        9.5578e-03, -4.3172e-04,  2.9703e-01,  1.7313e-01, -8.1630e-04,\n",
       "                        1.9859e-10,  1.4646e-06,  1.7411e-01,  3.2567e-01,  8.0589e-05,\n",
       "                        3.7196e-01,  7.4002e-05,  3.6500e-01,  8.0556e-04,  3.4106e-01,\n",
       "                        2.2122e-01,  6.7540e-02,  2.5713e-01,  2.0304e-01,  3.2130e-01,\n",
       "                        5.1764e-01,  2.5169e-01,  4.4900e-01,  5.1771e-05,  1.7645e-01,\n",
       "                        2.9488e-01,  2.0789e-01, -7.6787e-12,  2.8325e-01,  3.2906e-01,\n",
       "                        3.2638e-01,  6.4497e-12,  3.2234e-01,  1.3635e-01,  1.6348e-01,\n",
       "                        1.8313e-01,  2.3404e-01,  2.3974e-01,  3.5284e-01, -2.9568e-03,\n",
       "                        2.1223e-01,  2.8558e-01,  2.6943e-01,  3.1552e-01,  3.7824e-01,\n",
       "                        2.5577e-01,  4.0683e-01,  1.6942e-01,  1.7448e-01,  2.8443e-01,\n",
       "                        2.1024e-01,  1.6858e-03,  6.8733e-04,  4.9120e-01,  1.8046e-01,\n",
       "                       -3.9189e-06,  7.6110e-04,  3.4936e-01,  2.9943e-01,  6.0030e-03,\n",
       "                        5.3747e-02,  2.2632e-01,  3.5393e-01,  3.4199e-01,  1.9215e-01,\n",
       "                        1.9759e-01,  1.7432e-01,  3.5813e-01], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.weight',\n",
       "               tensor([[[[-1.3626e-05, -5.2096e-06,  4.9667e-09],\n",
       "                         [ 5.7807e-07, -1.0431e-08, -5.0756e-06],\n",
       "                         [-2.8279e-07, -3.3490e-06,  4.3756e-08]],\n",
       "               \n",
       "                        [[-3.6652e-12,  2.8022e-12, -5.6369e-12],\n",
       "                         [-1.7122e-12,  2.2655e-12, -4.3743e-12],\n",
       "                         [ 9.1133e-13,  5.8190e-13, -3.5275e-12]],\n",
       "               \n",
       "                        [[-1.0520e-13,  4.4409e-14,  6.1958e-14],\n",
       "                         [-6.4600e-14,  2.4436e-14,  5.9635e-14],\n",
       "                         [-1.8250e-13, -7.7655e-14, -9.0419e-14]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 3.0849e-11,  3.1384e-11,  3.0166e-11],\n",
       "                         [ 1.4248e-11,  1.2587e-11,  1.1987e-11],\n",
       "                         [ 2.6784e-12, -7.0145e-12,  1.8924e-12]],\n",
       "               \n",
       "                        [[ 1.6275e-11,  5.0429e-12,  4.9756e-13],\n",
       "                         [-5.1840e-12, -1.8071e-12, -6.4535e-12],\n",
       "                         [ 8.5453e-12,  6.2189e-12,  1.2713e-11]],\n",
       "               \n",
       "                        [[ 6.0361e-12,  9.4172e-12, -1.0923e-12],\n",
       "                         [ 5.6644e-12,  1.1414e-11,  2.3657e-12],\n",
       "                         [ 7.8168e-12,  9.5976e-12,  2.3874e-12]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.0916e-11, -3.7129e-10, -3.2185e-05],\n",
       "                         [ 9.3230e-06,  1.5678e-10, -4.6521e-15],\n",
       "                         [-1.3126e-05,  9.5732e-09, -1.6743e-16]],\n",
       "               \n",
       "                        [[-1.2949e-28, -3.2544e-28, -9.1626e-29],\n",
       "                         [-2.5882e-29, -1.7116e-28, -1.8815e-28],\n",
       "                         [-1.6051e-28, -3.7357e-28, -2.2038e-28]],\n",
       "               \n",
       "                        [[ 9.2972e-16,  2.0244e-07,  5.4461e-12],\n",
       "                         [-4.2864e-08, -2.4441e-16,  5.9996e-14],\n",
       "                         [ 3.0839e-15, -2.4778e-10,  1.8726e-12]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 3.0629e-29, -4.8819e-29, -1.9428e-29],\n",
       "                         [-7.6296e-29, -9.8632e-29,  3.8476e-28],\n",
       "                         [ 1.2817e-29,  9.2909e-29,  9.2353e-29]],\n",
       "               \n",
       "                        [[ 1.1052e-28,  9.7853e-30, -5.4202e-29],\n",
       "                         [-5.7447e-29,  1.2871e-28, -1.1140e-28],\n",
       "                         [ 6.3755e-29,  1.2060e-28, -1.8315e-28]],\n",
       "               \n",
       "                        [[-6.2643e-29, -2.4604e-29, -4.4143e-29],\n",
       "                         [ 1.3372e-29,  3.7152e-30,  1.4904e-30],\n",
       "                         [-4.4476e-30,  2.9843e-29, -1.7923e-28]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 2.4589e-06, -3.7781e-05, -1.4431e-11],\n",
       "                         [ 3.0181e-13,  5.9135e-17,  3.6496e-13],\n",
       "                         [ 2.3285e-05,  4.1227e-16, -5.5824e-05]],\n",
       "               \n",
       "                        [[-7.6787e-41,  2.0858e-41, -1.2313e-41],\n",
       "                         [ 7.3750e-42,  2.8113e-41,  6.1036e-41],\n",
       "                         [-1.0722e-40,  7.0942e-41, -2.9650e-41]],\n",
       "               \n",
       "                        [[-3.1446e-16,  4.1140e-12, -2.0967e-09],\n",
       "                         [ 4.2442e-16, -3.0194e-15,  4.1479e-15],\n",
       "                         [-1.5424e-10,  4.2189e-11, -3.0296e-16]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 6.4767e-41, -2.6728e-41,  2.4269e-41],\n",
       "                         [ 2.7537e-41, -2.8236e-41,  2.1017e-41],\n",
       "                         [-1.0072e-40,  1.3032e-42, -4.7134e-41]],\n",
       "               \n",
       "                        [[ 4.4494e-41,  5.9498e-41,  5.9859e-41],\n",
       "                         [-6.7558e-41, -2.1972e-41, -3.9517e-43],\n",
       "                         [-2.6817e-41, -4.1165e-41, -9.9742e-41]],\n",
       "               \n",
       "                        [[-5.6683e-41, -5.1351e-41, -7.2741e-41],\n",
       "                         [-3.8436e-41,  7.1571e-41, -5.7916e-41],\n",
       "                         [-5.0116e-41, -3.3436e-41, -7.7469e-41]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-1.7597e-02, -1.4021e-02, -1.8673e-02],\n",
       "                         [-1.6914e-02, -1.4102e-02, -1.5663e-02],\n",
       "                         [-1.3241e-02, -9.1461e-03, -1.2855e-02]],\n",
       "               \n",
       "                        [[ 2.9576e-02,  2.7999e-03,  2.6861e-02],\n",
       "                         [ 5.0766e-02,  3.5488e-02,  7.0416e-02],\n",
       "                         [ 3.2077e-02,  1.3892e-02,  5.6088e-02]],\n",
       "               \n",
       "                        [[-1.6464e-04, -5.3925e-05,  2.3582e-05],\n",
       "                         [-1.1019e-04,  7.0539e-06,  9.2628e-05],\n",
       "                         [-2.9694e-04, -1.1107e-04, -1.9229e-05]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-4.4457e-02, -2.5298e-02, -3.7246e-02],\n",
       "                         [-1.2950e-02, -5.4157e-02, -3.7648e-02],\n",
       "                         [ 5.1571e-02,  5.5319e-02,  2.5344e-02]],\n",
       "               \n",
       "                        [[ 7.2343e-02,  3.7968e-04,  3.2945e-02],\n",
       "                         [ 1.6696e-02,  1.2282e-03, -4.4035e-02],\n",
       "                         [ 3.5446e-02,  4.7256e-02,  2.9527e-02]],\n",
       "               \n",
       "                        [[ 1.3742e-01, -4.8703e-02,  5.6480e-02],\n",
       "                         [ 1.0890e-01, -2.4996e-02,  5.7883e-02],\n",
       "                         [ 1.9145e-01,  1.5196e-03,  4.2889e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-5.8733e-10, -9.5260e-10,  6.8561e-09],\n",
       "                         [ 2.9631e-10,  2.7234e-09,  2.5274e-09],\n",
       "                         [-2.2114e-09,  4.2165e-09, -4.9285e-09]],\n",
       "               \n",
       "                        [[-6.0573e-09,  3.7926e-08,  2.8662e-08],\n",
       "                         [ 8.2116e-09,  3.8500e-08,  2.1555e-08],\n",
       "                         [ 1.7811e-08,  3.5848e-08,  3.1282e-08]],\n",
       "               \n",
       "                        [[ 2.5922e-10,  1.8378e-10, -9.7768e-11],\n",
       "                         [-3.0448e-10, -3.2663e-11, -1.9733e-10],\n",
       "                         [-9.4388e-12,  4.7521e-10,  1.9791e-10]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.3978e-08, -1.1384e-08, -8.6547e-09],\n",
       "                         [ 1.6476e-10, -3.4109e-10, -2.4939e-08],\n",
       "                         [ 1.2021e-08,  1.3850e-08, -2.4821e-09]],\n",
       "               \n",
       "                        [[-3.3069e-08, -6.9515e-08, -4.8079e-08],\n",
       "                         [ 2.8832e-08,  8.4224e-09,  3.0018e-08],\n",
       "                         [-3.4410e-08,  4.5180e-08,  7.4753e-09]],\n",
       "               \n",
       "                        [[-6.6550e-09,  3.1807e-08, -1.8695e-08],\n",
       "                         [ 6.9603e-09,  1.6127e-08,  1.7215e-09],\n",
       "                         [-7.9749e-09, -2.2133e-09, -6.4502e-09]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.0388e-02, -3.5333e-03, -1.0982e-02],\n",
       "                         [-4.5617e-03,  3.1857e-03,  5.7775e-04],\n",
       "                         [-1.0973e-02, -3.6279e-03, -5.6255e-03]],\n",
       "               \n",
       "                        [[-7.0742e-02, -8.3304e-02, -5.6016e-02],\n",
       "                         [-7.1947e-02, -4.5737e-02, -3.0302e-02],\n",
       "                         [-6.4014e-02, -2.6834e-02, -5.4325e-02]],\n",
       "               \n",
       "                        [[-4.0518e-04, -3.6946e-04, -3.9112e-04],\n",
       "                         [-1.5472e-04, -2.3909e-05, -1.3379e-04],\n",
       "                         [-2.4176e-04, -2.4088e-04, -1.9972e-04]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.5234e-02,  1.2735e-02, -3.4739e-02],\n",
       "                         [-6.3534e-02, -4.5612e-02, -4.4632e-02],\n",
       "                         [-5.5161e-02, -2.6618e-02, -5.7026e-02]],\n",
       "               \n",
       "                        [[-5.1360e-02, -4.4446e-02, -3.2588e-02],\n",
       "                         [-1.3432e-02, -4.5558e-02, -3.0515e-02],\n",
       "                         [-2.8431e-03, -2.9357e-02, -6.7847e-02]],\n",
       "               \n",
       "                        [[ 9.8042e-02, -8.1130e-04,  5.2942e-02],\n",
       "                         [ 7.6951e-02,  3.5607e-02,  3.5402e-02],\n",
       "                         [ 9.8434e-02,  6.6473e-02,  7.7795e-02]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.bias',\n",
       "               tensor([ 1.2474e-07,  8.3529e-10, -3.5954e-09, -1.1243e-04, -8.0715e-11,\n",
       "                       -7.9404e-12,  1.1989e-07, -7.2538e-11, -5.8850e-05, -2.2647e-04,\n",
       "                       -5.5247e-12, -1.5081e-07,  1.9057e-16, -1.5592e-12, -3.9959e-13,\n",
       "                        5.7390e-06, -1.1272e-06, -3.5743e-15, -1.3412e-05,  8.5533e-05,\n",
       "                       -4.2110e-09, -5.9328e-06, -1.0945e-04,  1.2528e-11, -1.8881e-17,\n",
       "                       -3.6147e-07,  4.0298e-05, -1.4696e-10,  3.2507e-11, -3.2964e-08,\n",
       "                        6.1595e-05, -5.4239e-06,  1.7324e-07, -7.6415e-08,  6.1682e-08,\n",
       "                        2.9372e-06,  7.3479e-08, -2.1423e-05, -2.1327e-05,  2.4301e-15,\n",
       "                       -9.4411e-05,  3.1364e-11, -5.1348e-11,  1.3483e-08, -2.4696e-05,\n",
       "                        2.5209e-15, -5.3528e-05,  5.0681e-06, -7.0226e-05,  4.4547e-05,\n",
       "                       -5.8086e-07,  4.7412e-05,  1.1524e-12, -5.0147e-05, -2.3253e-09,\n",
       "                       -5.1393e-14,  1.6361e-06,  6.7064e-10,  4.5234e-05,  4.6349e-05,\n",
       "                       -5.4447e-10,  1.9009e-05,  1.3423e-07, -5.6338e-09,  8.9344e-07,\n",
       "                        1.0365e-13,  8.9351e-11,  1.0696e-06,  3.7029e-09,  4.2673e-08,\n",
       "                       -1.1591e-05, -2.1314e-11, -1.0674e-05, -2.9134e-16,  5.6780e-08,\n",
       "                        6.3848e-08, -5.6436e-08,  3.9196e-14,  8.4092e-06,  5.9941e-08,\n",
       "                       -1.6619e-09,  6.8056e-10, -2.8813e-11,  1.6209e-14,  5.2137e-10,\n",
       "                        6.1546e-07, -9.1840e-07, -2.0963e-08, -3.4811e-05,  4.3940e-12,\n",
       "                       -3.3408e-08,  1.1787e-05,  1.5888e-05,  1.1920e-07, -6.2443e-15,\n",
       "                       -2.8411e-05, -1.5123e-05, -1.2792e-08, -6.4233e-10,  1.4263e-08,\n",
       "                       -4.0949e-11, -8.3376e-06,  1.2987e-09,  2.9154e-09,  7.1303e-07,\n",
       "                        2.3263e-15, -1.1953e-06, -9.5665e-11,  5.2765e-08, -1.6255e-08,\n",
       "                        2.0749e-08,  1.6260e-05,  1.0709e-04,  2.8085e-08,  4.9082e-13,\n",
       "                        1.2540e-05,  2.7439e-11, -2.0910e-04, -9.0532e-09,  1.5273e-07,\n",
       "                        2.5381e-07,  2.5483e-05, -2.4105e-07,  1.7459e-07, -2.3457e-13,\n",
       "                       -5.4282e-05, -8.8874e-09, -9.2977e-06], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.bias',\n",
       "               tensor([-7.1620e-08, -6.0826e-12, -2.7865e-32, -3.2023e-01, -6.8529e-06,\n",
       "                       -3.1271e-06, -6.1388e-04, -1.3992e-04, -8.5151e-02, -1.5301e-01,\n",
       "                        1.8483e-23, -4.9942e-03,  1.1227e-16, -1.0756e-11, -4.4863e-18,\n",
       "                        3.6869e-03, -1.0111e-08, -2.8382e-10, -2.3811e-04, -2.1905e-01,\n",
       "                       -1.2856e-05, -1.3898e-24, -4.2978e-01, -1.5727e-05, -7.3300e-26,\n",
       "                       -7.2522e-03, -7.1993e-02, -5.3896e-14, -1.9753e-10, -1.0996e-02,\n",
       "                        6.4270e-02, -1.4181e-03, -8.0525e-18, -8.3340e-05, -1.5914e-04,\n",
       "                       -4.1732e-03, -1.8452e-10, -2.8199e-03, -8.2090e-01, -5.2094e-20,\n",
       "                       -2.9427e-01, -3.5205e-11, -3.6202e-06, -2.5351e-02, -8.1109e-15,\n",
       "                       -4.3392e-15,  2.9023e-02, -1.5157e-02,  5.4082e-02, -3.1775e-01,\n",
       "                       -1.8445e-04, -1.8412e-01, -1.1496e-18, -7.2471e-01, -1.1610e-03,\n",
       "                       -8.2993e-11, -3.8304e-11, -1.0031e-16, -8.7215e-02, -4.5989e-01,\n",
       "                       -3.7096e-21, -1.8729e-09, -4.1193e-21, -4.2024e-06, -3.1148e-02,\n",
       "                       -3.5310e-12, -5.8718e-25, -1.8372e-02, -1.0535e-20, -6.3321e-02,\n",
       "                       -8.9527e-03, -1.3584e-05, -4.0536e-02, -4.6767e-25, -1.2126e-02,\n",
       "                       -5.6092e-04, -2.9739e-02, -4.9602e-14, -1.3850e-10, -8.1659e-22,\n",
       "                       -6.9822e-06,  2.5313e-31, -1.0229e-11, -6.0036e-10, -2.0415e-07,\n",
       "                       -1.8210e-01, -1.2115e-02, -1.7664e-26,  2.3043e-02, -2.0851e-20,\n",
       "                       -1.5769e-02, -3.0931e-01, -5.3834e-01, -1.0558e-02, -2.9381e-13,\n",
       "                       -1.3225e-04, -1.7740e-01, -3.1059e-06, -9.2071e-09, -2.2956e-09,\n",
       "                       -3.0312e-27, -2.5429e-02, -8.3470e-03, -6.2538e-03, -3.4932e-27,\n",
       "                       -2.2883e-12, -9.6938e-03, -1.8642e-18, -1.7826e-24, -3.6652e-26,\n",
       "                       -1.0050e-02,  4.2925e-03, -7.5282e-02, -3.0185e-03,  3.1709e-15,\n",
       "                       -1.7519e-02, -8.0316e-08, -2.8944e-02, -2.3526e-18, -3.4382e-05,\n",
       "                       -4.6061e-10, -6.3197e-03, -3.3616e-02, -2.7239e-02, -3.5685e-07,\n",
       "                       -9.9894e-02, -1.4954e-03, -1.8286e-01], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.weight',\n",
       "               tensor([-3.1487e-05, -6.8584e-19,  8.1354e-36,  2.0999e+00, -1.9710e-05,\n",
       "                        6.7900e-05, -5.6583e-04, -1.5968e-04,  2.0462e+00,  1.9488e+00,\n",
       "                        4.1756e-28,  9.6540e-04,  6.2126e-20,  2.6990e-07,  1.5773e-25,\n",
       "                       -1.1460e-03,  7.2322e-05, -3.3473e-16, -7.6543e-04,  1.2895e+00,\n",
       "                        1.1310e-03,  6.0339e-32,  3.0664e+00, -1.1426e-05,  9.0468e-21,\n",
       "                       -1.9757e-03,  2.3985e+00, -1.0829e-20, -1.0081e-08,  2.4231e-03,\n",
       "                        2.2152e+00,  2.4695e+00, -1.1895e-14, -1.7169e-04, -2.4454e-04,\n",
       "                        2.6661e+00, -6.3320e-07, -8.2696e-04,  5.2541e-01,  6.8408e-16,\n",
       "                        2.0169e+00,  9.3954e-16,  1.6282e-05, -4.2077e-03, -8.3496e-11,\n",
       "                       -1.4702e-11,  2.5448e+00, -2.3547e-03,  2.7171e+00,  2.5717e+00,\n",
       "                       -2.6721e-05,  2.3676e+00,  1.2904e-13,  8.3232e-01,  3.1225e-04,\n",
       "                       -5.2466e-08, -2.4874e-06, -3.7405e-12,  2.0870e+00,  2.3723e+00,\n",
       "                        6.7220e-20, -6.4266e-06, -2.5965e-16, -5.3359e-05, -5.7208e-03,\n",
       "                       -7.8571e-08,  2.7381e-30, -4.3091e-03, -4.1451e-17,  2.6921e-02,\n",
       "                        4.8118e-04, -5.6978e-05, -2.6167e-02,  7.7980e-25,  1.0503e-03,\n",
       "                       -1.2961e-03, -1.9416e-03,  3.1878e-09,  6.9528e-08, -8.6010e-17,\n",
       "                        3.5177e-06, -1.5287e-37,  5.0229e-08,  1.8745e-13, -1.3746e-06,\n",
       "                        1.0408e-01,  6.3177e-04,  1.7992e-28,  2.0180e+00,  1.3208e-23,\n",
       "                        4.3199e-03,  2.2067e+00,  1.9879e+00,  3.3688e-05,  8.5628e-09,\n",
       "                       -1.2707e-04,  2.5976e+00,  2.3458e-05, -3.4713e-06,  2.9476e-07,\n",
       "                        1.8118e-22,  1.0643e-03, -5.1305e-05, -7.4374e-04, -4.2192e-33,\n",
       "                       -2.6668e-08,  3.1340e-03,  1.9462e-24, -7.0890e-22, -2.3593e-31,\n",
       "                        7.3384e-04,  2.1035e+00,  2.5438e+00, -4.0767e-04,  9.3998e-11,\n",
       "                       -2.7851e-03, -3.7550e-06,  2.2698e+00, -5.4479e-24,  7.9170e-05,\n",
       "                       -7.2895e-07, -4.0732e-03,  4.4775e-03,  2.4946e-03,  4.1060e-06,\n",
       "                        2.6848e+00,  2.1215e-04,  2.7493e+00], device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.weights',\n",
       "               tensor([[-1.2620e-09, -1.1494e-09, -1.0408e-09,  ..., -1.8324e-02,\n",
       "                         9.2089e-04, -1.6748e-02],\n",
       "                       [-4.0167e-09, -4.8029e-09, -4.3050e-09,  ...,  6.4022e-03,\n",
       "                         1.6302e-02, -6.4443e-03],\n",
       "                       [ 2.4739e-09,  2.5134e-09,  2.4981e-09,  ...,  1.8054e-02,\n",
       "                         6.9249e-03,  2.2034e-02],\n",
       "                       [ 3.3332e-09,  3.1781e-09,  3.0361e-09,  ..., -1.3501e-02,\n",
       "                        -7.5778e-04, -8.7850e-03],\n",
       "                       [ 6.8908e-10,  4.7838e-10,  4.8875e-10,  ..., -9.6571e-03,\n",
       "                         2.3779e-03, -2.2437e-02]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.bias',\n",
       "               tensor([-0.1256,  0.1947,  0.2516, -0.1696, -0.1356], device='cuda:0'))]),\n",
       " 'per_epoch_statistics': {'train_loss_mean': [1.4148391263484954,\n",
       "   1.2850838079452516,\n",
       "   1.234405325651169,\n",
       "   1.1901023918390274,\n",
       "   1.1555011553764343,\n",
       "   1.1420887540578841,\n",
       "   1.126902024269104,\n",
       "   1.094039020061493,\n",
       "   1.0918079406023025,\n",
       "   1.069633360505104,\n",
       "   1.033455386519432,\n",
       "   1.0312737307548523,\n",
       "   1.0195280549526216,\n",
       "   0.9831074362993241,\n",
       "   0.9820001821517944,\n",
       "   0.967357803106308,\n",
       "   0.9426143865585327,\n",
       "   0.9462028987407685,\n",
       "   0.9351131874322891,\n",
       "   0.919215365409851,\n",
       "   0.9147093745470047,\n",
       "   0.8874881510734558,\n",
       "   0.8892993838787079,\n",
       "   0.8814472304582596,\n",
       "   0.87502427059412,\n",
       "   0.8634148318767547,\n",
       "   0.8625385798811912,\n",
       "   0.852057049036026,\n",
       "   0.847868034362793,\n",
       "   0.8541822862029076,\n",
       "   0.8438476754426956,\n",
       "   0.8351223621368408,\n",
       "   0.8269131827354431,\n",
       "   0.8266038798689842,\n",
       "   0.8316236357092858,\n",
       "   0.8114956017136574,\n",
       "   0.8322962763309478,\n",
       "   0.828861712872982,\n",
       "   0.8179938963651657,\n",
       "   0.800685377061367,\n",
       "   0.8182524373531341,\n",
       "   0.7944688241481781,\n",
       "   0.801009521663189,\n",
       "   0.805276904284954,\n",
       "   0.7881326129436493,\n",
       "   0.788897706925869,\n",
       "   0.7855682786107063,\n",
       "   0.7878517743349075,\n",
       "   0.7792132802605629,\n",
       "   0.7792390701174736,\n",
       "   0.7833095765709877,\n",
       "   0.7664081097841263,\n",
       "   0.7849734002947807,\n",
       "   0.7675189506411553,\n",
       "   0.786938060760498,\n",
       "   0.7756104214787484,\n",
       "   0.7715072603225708,\n",
       "   0.7588127337694168,\n",
       "   0.7746694704890251,\n",
       "   0.7635702539384365,\n",
       "   0.7753667259216308,\n",
       "   0.7655872809290886,\n",
       "   0.755951149046421,\n",
       "   0.7596817498207092,\n",
       "   0.7533216822743416,\n",
       "   0.7616983464956284,\n",
       "   0.7650411180853843,\n",
       "   0.7498987961411476,\n",
       "   0.7519565269351005,\n",
       "   0.762845426440239,\n",
       "   0.748459632575512,\n",
       "   0.757353158056736,\n",
       "   0.7593442623615265,\n",
       "   0.7535930503010749,\n",
       "   0.7610507143735885,\n",
       "   0.7414995050430297,\n",
       "   0.7505370728373527,\n",
       "   0.7532634260654449,\n",
       "   0.7523385016918183,\n",
       "   0.7517774405479432,\n",
       "   0.7453418579697609,\n",
       "   0.7435467994213104,\n",
       "   0.7470131074786186,\n",
       "   0.7467320259809495,\n",
       "   0.7361779639720917,\n",
       "   0.7378786770701409,\n",
       "   0.7362030243873596,\n",
       "   0.7399881846904754,\n",
       "   0.7335205945372582,\n",
       "   0.7287819336652755,\n",
       "   0.7373892474770546,\n",
       "   0.7340173846483231,\n",
       "   0.7219217199087143,\n",
       "   0.7379472731351853,\n",
       "   0.7350863538384438,\n",
       "   0.7393584677577019,\n",
       "   0.7336323546767235,\n",
       "   0.7370106744170188,\n",
       "   0.7175015631318092],\n",
       "  'train_loss_std': [0.1826681135761118,\n",
       "   0.1288377491300771,\n",
       "   0.13773733745539654,\n",
       "   0.13108647596767542,\n",
       "   0.13067571864052607,\n",
       "   0.1383504588395245,\n",
       "   0.1438557548117241,\n",
       "   0.1400917940835232,\n",
       "   0.14589112787734426,\n",
       "   0.13534779948677175,\n",
       "   0.14401140157182035,\n",
       "   0.1443286266706901,\n",
       "   0.14491294498143495,\n",
       "   0.14488493719365797,\n",
       "   0.14242255779372126,\n",
       "   0.14271958579011623,\n",
       "   0.13892433839861018,\n",
       "   0.14740218998145443,\n",
       "   0.14631484371847067,\n",
       "   0.14327812861699413,\n",
       "   0.14714697006005104,\n",
       "   0.14017082761065305,\n",
       "   0.14792603739073468,\n",
       "   0.15560832383621817,\n",
       "   0.1431631766148746,\n",
       "   0.14209645844281377,\n",
       "   0.15110682037659673,\n",
       "   0.14580091235132095,\n",
       "   0.14422986975680474,\n",
       "   0.15302126583095932,\n",
       "   0.14900670488500495,\n",
       "   0.14776191708499037,\n",
       "   0.14473676193078863,\n",
       "   0.16010264642203345,\n",
       "   0.14646350418103854,\n",
       "   0.14936936481829524,\n",
       "   0.14591519952298704,\n",
       "   0.15122003943083534,\n",
       "   0.1513991002383813,\n",
       "   0.15359697480395537,\n",
       "   0.14623592463355273,\n",
       "   0.1382586031990714,\n",
       "   0.15448382282212342,\n",
       "   0.15119872254135566,\n",
       "   0.1486679096039874,\n",
       "   0.14416223223257577,\n",
       "   0.151034227269142,\n",
       "   0.15269478786689544,\n",
       "   0.14685961509403117,\n",
       "   0.15222028129390672,\n",
       "   0.1430349566089284,\n",
       "   0.14787277625294729,\n",
       "   0.14810804521535947,\n",
       "   0.15112841537078067,\n",
       "   0.15727366281885202,\n",
       "   0.15107450711417494,\n",
       "   0.14187554864970453,\n",
       "   0.14764969160003102,\n",
       "   0.14497946748854088,\n",
       "   0.14381697130812207,\n",
       "   0.14958868460147026,\n",
       "   0.15066876127987489,\n",
       "   0.1532638111182471,\n",
       "   0.1489007850123692,\n",
       "   0.14178559272494667,\n",
       "   0.14405070436673945,\n",
       "   0.15538686826824996,\n",
       "   0.1443242867753082,\n",
       "   0.14730620252787088,\n",
       "   0.15460051327224672,\n",
       "   0.14288586670854733,\n",
       "   0.1485165455160528,\n",
       "   0.14980704903800496,\n",
       "   0.14365167328768788,\n",
       "   0.14912290769205147,\n",
       "   0.14571576177808995,\n",
       "   0.1526107761783275,\n",
       "   0.14473865818343498,\n",
       "   0.14709971779588613,\n",
       "   0.15760438636671528,\n",
       "   0.14991426235754676,\n",
       "   0.14707933649896887,\n",
       "   0.15213190189894366,\n",
       "   0.14801477382863204,\n",
       "   0.14632618062059047,\n",
       "   0.13741298690832104,\n",
       "   0.15215697823283328,\n",
       "   0.14643444260641747,\n",
       "   0.14970986797928496,\n",
       "   0.14117438440202978,\n",
       "   0.14288647613477334,\n",
       "   0.15100353539595399,\n",
       "   0.14091825381323306,\n",
       "   0.15166535883802648,\n",
       "   0.14803273381551596,\n",
       "   0.15225173815089932,\n",
       "   0.1384365303019416,\n",
       "   0.15159785145106944,\n",
       "   0.14659036833598182],\n",
       "  'train_accuracy_mean': [0.42573333406448366,\n",
       "   0.4750000002384186,\n",
       "   0.5014400001764298,\n",
       "   0.5210933329463006,\n",
       "   0.5414666671156884,\n",
       "   0.5461466665267944,\n",
       "   0.5543333331346512,\n",
       "   0.5659733322262764,\n",
       "   0.5675466651320458,\n",
       "   0.5787599987983704,\n",
       "   0.5948533331751823,\n",
       "   0.5938400005698204,\n",
       "   0.5992666658759117,\n",
       "   0.6183066642880439,\n",
       "   0.6185066655278206,\n",
       "   0.6262133329510688,\n",
       "   0.6330399994850159,\n",
       "   0.6339599996805191,\n",
       "   0.6379600004553795,\n",
       "   0.6444666659832001,\n",
       "   0.6467200000882148,\n",
       "   0.6588933324813843,\n",
       "   0.6599999994635583,\n",
       "   0.6606933332085609,\n",
       "   0.6642266654372215,\n",
       "   0.6686400000452996,\n",
       "   0.6695866650342941,\n",
       "   0.6736933344006538,\n",
       "   0.6759600004553795,\n",
       "   0.6718133327364921,\n",
       "   0.6796399990320205,\n",
       "   0.6809333336949348,\n",
       "   0.6854133333563804,\n",
       "   0.687013331592083,\n",
       "   0.6838666677474976,\n",
       "   0.694106664776802,\n",
       "   0.6834266652464867,\n",
       "   0.6851999995112419,\n",
       "   0.6891866657137871,\n",
       "   0.6969733338952064,\n",
       "   0.687679999768734,\n",
       "   0.6961999999284745,\n",
       "   0.6967733338475227,\n",
       "   0.6943999989032745,\n",
       "   0.6998000006079674,\n",
       "   0.7004666655063629,\n",
       "   0.7027466668486595,\n",
       "   0.7005200011134147,\n",
       "   0.7052533333301544,\n",
       "   0.7049600003957749,\n",
       "   0.7028133320808411,\n",
       "   0.711013333082199,\n",
       "   0.7025066659450531,\n",
       "   0.7111999990344048,\n",
       "   0.701973333477974,\n",
       "   0.7070133331418037,\n",
       "   0.7088133340477943,\n",
       "   0.7126933336853981,\n",
       "   0.707519998550415,\n",
       "   0.7114666666388512,\n",
       "   0.7065466678142548,\n",
       "   0.7105600010156632,\n",
       "   0.7176000005006791,\n",
       "   0.7144000002741814,\n",
       "   0.7156800001859664,\n",
       "   0.7104799982309341,\n",
       "   0.7119999992847442,\n",
       "   0.7195466661453247,\n",
       "   0.7177600009441376,\n",
       "   0.7122266675233841,\n",
       "   0.7180533339977264,\n",
       "   0.716066665649414,\n",
       "   0.7136799995303154,\n",
       "   0.7158133339881897,\n",
       "   0.7127733327150345,\n",
       "   0.7216266677379608,\n",
       "   0.7178533331155778,\n",
       "   0.7156399990320206,\n",
       "   0.7154666669368744,\n",
       "   0.71605333340168,\n",
       "   0.7210933340787887,\n",
       "   0.7213733332157135,\n",
       "   0.7183733338117599,\n",
       "   0.7192933334112167,\n",
       "   0.7238933333158493,\n",
       "   0.723746667265892,\n",
       "   0.7244266659617424,\n",
       "   0.7230666661262513,\n",
       "   0.7244133324623108,\n",
       "   0.7275999994277954,\n",
       "   0.7217733325958252,\n",
       "   0.7227999997138977,\n",
       "   0.7302799991369248,\n",
       "   0.7200266658067703,\n",
       "   0.7217466663718224,\n",
       "   0.7214133335947991,\n",
       "   0.7249599999785423,\n",
       "   0.7227733337879181,\n",
       "   0.7307066665887832],\n",
       "  'train_accuracy_std': [0.07293601458232205,\n",
       "   0.07212566132281877,\n",
       "   0.07477294373201494,\n",
       "   0.0706343008897512,\n",
       "   0.0713550118961158,\n",
       "   0.07238167820155632,\n",
       "   0.07622058668295484,\n",
       "   0.07257829014039446,\n",
       "   0.07512436313941183,\n",
       "   0.07190839927406019,\n",
       "   0.07401621943371137,\n",
       "   0.07194140268551032,\n",
       "   0.07286453267843895,\n",
       "   0.07405868075781674,\n",
       "   0.07023447281214795,\n",
       "   0.07231270903628825,\n",
       "   0.06951324974493459,\n",
       "   0.0733361107997935,\n",
       "   0.07382392380030589,\n",
       "   0.07075751988201853,\n",
       "   0.07190485537505688,\n",
       "   0.07018022697919614,\n",
       "   0.06934743541604578,\n",
       "   0.07528351873107218,\n",
       "   0.0685714692350026,\n",
       "   0.07043496166199072,\n",
       "   0.07360499930308814,\n",
       "   0.07091875985609566,\n",
       "   0.06755105548411157,\n",
       "   0.071684499047967,\n",
       "   0.06963111916160014,\n",
       "   0.07094799380396892,\n",
       "   0.06900584766222419,\n",
       "   0.0738061419103742,\n",
       "   0.07200604935767589,\n",
       "   0.0715524808617258,\n",
       "   0.07190543237123667,\n",
       "   0.06932935938405553,\n",
       "   0.07199231887920474,\n",
       "   0.07340720197255969,\n",
       "   0.07152836268579693,\n",
       "   0.06560796205129193,\n",
       "   0.07039120583945847,\n",
       "   0.07228412931444143,\n",
       "   0.0691758163248331,\n",
       "   0.0676767147266623,\n",
       "   0.0714765723473027,\n",
       "   0.06819837606212988,\n",
       "   0.06911988043427529,\n",
       "   0.07245487901831295,\n",
       "   0.06689424889006425,\n",
       "   0.0701524989433708,\n",
       "   0.06823183622004701,\n",
       "   0.07214848877058729,\n",
       "   0.07410274594137982,\n",
       "   0.07134432795699223,\n",
       "   0.06725153957430972,\n",
       "   0.07208476067137877,\n",
       "   0.06820740241654706,\n",
       "   0.06677777534169134,\n",
       "   0.07128337101411891,\n",
       "   0.07215383058460415,\n",
       "   0.06875476549763124,\n",
       "   0.07239717611652398,\n",
       "   0.06607339107504838,\n",
       "   0.06819736503053322,\n",
       "   0.07274185795762865,\n",
       "   0.06818646772335195,\n",
       "   0.06869679057074402,\n",
       "   0.07267291690269914,\n",
       "   0.0680387910109378,\n",
       "   0.06981941791239779,\n",
       "   0.06993625418193797,\n",
       "   0.06853567370470534,\n",
       "   0.06956785441632422,\n",
       "   0.06939883547035282,\n",
       "   0.06916463948935755,\n",
       "   0.06780045461231811,\n",
       "   0.06803580668998643,\n",
       "   0.07434978387252135,\n",
       "   0.0702494302407364,\n",
       "   0.06730612018591463,\n",
       "   0.07326677620893525,\n",
       "   0.06735602337837615,\n",
       "   0.06879371457376711,\n",
       "   0.06461859296283667,\n",
       "   0.07012341708159682,\n",
       "   0.06925521363452421,\n",
       "   0.06834804964246351,\n",
       "   0.06546259312717488,\n",
       "   0.06801805140291863,\n",
       "   0.07079974797240698,\n",
       "   0.06437122602133584,\n",
       "   0.06946397555382708,\n",
       "   0.0711009792705055,\n",
       "   0.07157748041833484,\n",
       "   0.06692830786351807,\n",
       "   0.06911727436062,\n",
       "   0.06883499313241226],\n",
       "  'train_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'train_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_learning_rate_mean': [0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005],\n",
       "  'train_learning_rate_std': [4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19],\n",
       "  'val_loss_mean': [1.3745469013849894,\n",
       "   1.3695764211813608,\n",
       "   1.2929714647928874,\n",
       "   1.259792274037997,\n",
       "   1.2438737877209982,\n",
       "   1.2929946307341258,\n",
       "   1.2280837829907736,\n",
       "   1.2198979951937994,\n",
       "   1.1909209815661113,\n",
       "   1.1727346549431483,\n",
       "   1.1784495540459952,\n",
       "   1.1976960388819378,\n",
       "   1.1605947665373484,\n",
       "   1.1465414975086847,\n",
       "   1.1539290394385655,\n",
       "   1.1203218160072963,\n",
       "   1.1540218629439671,\n",
       "   1.1098630841573078,\n",
       "   1.1042029196023941,\n",
       "   1.0779762564102808,\n",
       "   1.0881786062320074,\n",
       "   1.0698428467909495,\n",
       "   1.084576109846433,\n",
       "   1.0696169839302698,\n",
       "   1.0682709028323492,\n",
       "   1.08734093328317,\n",
       "   1.0660387603441874,\n",
       "   1.0548791648944218,\n",
       "   1.0410837330420812,\n",
       "   1.0497131991386413,\n",
       "   1.0477100511391957,\n",
       "   1.0470172490676244,\n",
       "   1.0472956323623657,\n",
       "   1.0430193396409353,\n",
       "   1.0348207489649455,\n",
       "   1.0615056353807448,\n",
       "   1.0745573113361995,\n",
       "   1.0355657289425533,\n",
       "   1.0565921835104624,\n",
       "   1.024602055946986,\n",
       "   1.0536076726516088,\n",
       "   1.0672129287322363,\n",
       "   1.0655620195468267,\n",
       "   1.028231447339058,\n",
       "   1.0386091715097427,\n",
       "   1.0527648494640987,\n",
       "   1.0398179858922958,\n",
       "   1.0549954732259115,\n",
       "   1.0181299243370692,\n",
       "   1.0369174383083979,\n",
       "   1.0568806233008703,\n",
       "   1.0394969548781712,\n",
       "   1.0421548144022623,\n",
       "   1.0147918675343195,\n",
       "   1.0338614972432454,\n",
       "   1.0523937249183655,\n",
       "   1.0539509703715642,\n",
       "   1.0358632665872574,\n",
       "   1.032536778052648,\n",
       "   1.0154057870308557,\n",
       "   1.0534862021605174,\n",
       "   1.0033953974644343,\n",
       "   1.104992275039355,\n",
       "   1.0407660112778345,\n",
       "   1.0861460946003596,\n",
       "   1.0112395534912746,\n",
       "   1.0234007529417675,\n",
       "   1.0157522306839626,\n",
       "   1.031027969121933,\n",
       "   1.0310223817825317,\n",
       "   1.0234680936733882,\n",
       "   1.015620813568433,\n",
       "   1.0512659196058909,\n",
       "   1.0324392352501552,\n",
       "   1.0282961785793305,\n",
       "   1.0500487770636877,\n",
       "   1.0060121111075084,\n",
       "   1.0200449188550313,\n",
       "   1.025437722404798,\n",
       "   1.0228043832381566,\n",
       "   1.027930878798167,\n",
       "   1.006664238770803,\n",
       "   1.0095369939009349,\n",
       "   1.019173193971316,\n",
       "   1.0214796455701192,\n",
       "   1.0150180768966675,\n",
       "   1.0089046855767567,\n",
       "   0.9941347759962081,\n",
       "   1.0192620128393173,\n",
       "   1.0024719393253327,\n",
       "   0.9914079656203588,\n",
       "   1.0050751491387686,\n",
       "   1.028025538722674,\n",
       "   1.0384909588098525,\n",
       "   1.0113098166386287,\n",
       "   1.0035858611265818,\n",
       "   0.9946130307515463,\n",
       "   0.9963187477986017,\n",
       "   1.0186052946249644],\n",
       "  'val_loss_std': [0.09581657314969845,\n",
       "   0.10203790325495003,\n",
       "   0.10741523334755972,\n",
       "   0.10994163477535494,\n",
       "   0.11266248686003626,\n",
       "   0.11923291236329818,\n",
       "   0.11678275521267276,\n",
       "   0.1195070601412562,\n",
       "   0.11016343258715433,\n",
       "   0.10912409624830652,\n",
       "   0.12133485876254979,\n",
       "   0.12589509341541824,\n",
       "   0.12561214294419012,\n",
       "   0.1317035888998971,\n",
       "   0.12659682988321402,\n",
       "   0.13112222038548976,\n",
       "   0.13303447235957136,\n",
       "   0.12318689845978333,\n",
       "   0.12240746426416713,\n",
       "   0.12234582045839966,\n",
       "   0.1273682325488972,\n",
       "   0.13145676201959847,\n",
       "   0.12765315646059527,\n",
       "   0.13277057002904755,\n",
       "   0.14041302211727125,\n",
       "   0.1352829853902586,\n",
       "   0.13330951058723037,\n",
       "   0.13799422865265723,\n",
       "   0.14084698675577226,\n",
       "   0.13699683685439548,\n",
       "   0.13712044141091664,\n",
       "   0.13656525689017227,\n",
       "   0.14571218221199553,\n",
       "   0.13006788014964496,\n",
       "   0.1390853532285255,\n",
       "   0.132595094773633,\n",
       "   0.13303933325224726,\n",
       "   0.14528082359928934,\n",
       "   0.14025688220555768,\n",
       "   0.13410655703199614,\n",
       "   0.1345983584444533,\n",
       "   0.1356120833345153,\n",
       "   0.13622388855500148,\n",
       "   0.13418376235044388,\n",
       "   0.1362710681911687,\n",
       "   0.14171401076337134,\n",
       "   0.13828504001001635,\n",
       "   0.13061555954651882,\n",
       "   0.1421869591044378,\n",
       "   0.14951777499333513,\n",
       "   0.13256231092426626,\n",
       "   0.14316005417906913,\n",
       "   0.13548853673609096,\n",
       "   0.13802546178260097,\n",
       "   0.13624233287756082,\n",
       "   0.14613632771568003,\n",
       "   0.144706152698425,\n",
       "   0.13569260867192134,\n",
       "   0.14093799280667782,\n",
       "   0.14262201452364334,\n",
       "   0.13985219614253577,\n",
       "   0.13798584627795352,\n",
       "   0.14844940116163755,\n",
       "   0.13814537922243883,\n",
       "   0.14905746437859804,\n",
       "   0.141112385152457,\n",
       "   0.14599454585944546,\n",
       "   0.13725956504380157,\n",
       "   0.13566504395181325,\n",
       "   0.14630991852722408,\n",
       "   0.1352544605872256,\n",
       "   0.1411334717250993,\n",
       "   0.14509238614678266,\n",
       "   0.14779201820703614,\n",
       "   0.1414722135021841,\n",
       "   0.14494353416785866,\n",
       "   0.14456092522297626,\n",
       "   0.14731247934731284,\n",
       "   0.1499835820916479,\n",
       "   0.14282578920431327,\n",
       "   0.1381255959182702,\n",
       "   0.1406628250543862,\n",
       "   0.146626907229354,\n",
       "   0.14769745787302474,\n",
       "   0.153437004005239,\n",
       "   0.1429353935132325,\n",
       "   0.13652320773671037,\n",
       "   0.1392889148600874,\n",
       "   0.13901333453415055,\n",
       "   0.1362624431186942,\n",
       "   0.14240417759770838,\n",
       "   0.14119649823498864,\n",
       "   0.14482723446484433,\n",
       "   0.13949298887862957,\n",
       "   0.14338460026606292,\n",
       "   0.14555761047688975,\n",
       "   0.13755545077889825,\n",
       "   0.1358203306192887,\n",
       "   0.14589173851099957],\n",
       "  'val_accuracy_mean': [0.4272000007828077,\n",
       "   0.43584444468220074,\n",
       "   0.46773333152135216,\n",
       "   0.4838444447517395,\n",
       "   0.4909111120303472,\n",
       "   0.47444444268941877,\n",
       "   0.4979333331187566,\n",
       "   0.5088222220540046,\n",
       "   0.5170000010728836,\n",
       "   0.5269555538892746,\n",
       "   0.5245555553833644,\n",
       "   0.5164888871709505,\n",
       "   0.53126666645209,\n",
       "   0.5392222220698992,\n",
       "   0.5375111115972201,\n",
       "   0.549088889459769,\n",
       "   0.538066665927569,\n",
       "   0.5578444443146388,\n",
       "   0.5605555534362793,\n",
       "   0.5723555538058281,\n",
       "   0.568355555931727,\n",
       "   0.5741555536786715,\n",
       "   0.5680666669209798,\n",
       "   0.575266665816307,\n",
       "   0.5732666671276092,\n",
       "   0.5703777768214544,\n",
       "   0.5787333328525225,\n",
       "   0.5854888888200124,\n",
       "   0.5868666664759318,\n",
       "   0.5870444432894388,\n",
       "   0.5841777770717939,\n",
       "   0.587844444712003,\n",
       "   0.5867333320776621,\n",
       "   0.5881111099322637,\n",
       "   0.5955111103256544,\n",
       "   0.5825333338975907,\n",
       "   0.5715777752796809,\n",
       "   0.5913999989628792,\n",
       "   0.5844444440801938,\n",
       "   0.5969555560747782,\n",
       "   0.5836222199598948,\n",
       "   0.5788666649659475,\n",
       "   0.578244442443053,\n",
       "   0.5950222205122312,\n",
       "   0.5931999992330869,\n",
       "   0.5850666660070419,\n",
       "   0.5916444445649783,\n",
       "   0.5838888890544573,\n",
       "   0.6006888884305954,\n",
       "   0.5972666655977567,\n",
       "   0.5788444444537163,\n",
       "   0.5902666656176249,\n",
       "   0.5904888873298962,\n",
       "   0.602133332490921,\n",
       "   0.5948444417119026,\n",
       "   0.5873333313067755,\n",
       "   0.585177776614825,\n",
       "   0.5940888872742653,\n",
       "   0.5957555522521337,\n",
       "   0.6020888868967692,\n",
       "   0.5885999983549118,\n",
       "   0.6079555534323057,\n",
       "   0.5646222225824992,\n",
       "   0.5913111091653506,\n",
       "   0.5801333316167195,\n",
       "   0.6057777767380078,\n",
       "   0.5997333326935768,\n",
       "   0.6011999995509784,\n",
       "   0.595288887321949,\n",
       "   0.5934666656454404,\n",
       "   0.5975111093123754,\n",
       "   0.6008444447318713,\n",
       "   0.5885777773459753,\n",
       "   0.5973555542031924,\n",
       "   0.5972666666905085,\n",
       "   0.5858444424470266,\n",
       "   0.6067555559674899,\n",
       "   0.5979111109177272,\n",
       "   0.5963333318630855,\n",
       "   0.597222220102946,\n",
       "   0.5971777761975924,\n",
       "   0.6045777798692386,\n",
       "   0.6040888872742652,\n",
       "   0.6067777748902639,\n",
       "   0.5989333322644234,\n",
       "   0.6000222204128901,\n",
       "   0.6037777775526046,\n",
       "   0.6101333323121071,\n",
       "   0.6009333340326944,\n",
       "   0.6044888864954313,\n",
       "   0.6101333323121071,\n",
       "   0.6062222225467364,\n",
       "   0.5983333339293798,\n",
       "   0.5897111097971598,\n",
       "   0.6033333319425583,\n",
       "   0.6085999993483225,\n",
       "   0.6083555556337039,\n",
       "   0.6124888875087102,\n",
       "   0.6012222215533256],\n",
       "  'val_accuracy_std': [0.05616388713584925,\n",
       "   0.05620654563747143,\n",
       "   0.06075368520751861,\n",
       "   0.06142895484967141,\n",
       "   0.062457566939145816,\n",
       "   0.06335048319530955,\n",
       "   0.06091423209653762,\n",
       "   0.0626969627599154,\n",
       "   0.05681712000093845,\n",
       "   0.05966627586403989,\n",
       "   0.0622693268284931,\n",
       "   0.062123874356503156,\n",
       "   0.06247745551374117,\n",
       "   0.06416981659721492,\n",
       "   0.06348891545376627,\n",
       "   0.06099592600245034,\n",
       "   0.06397426540399069,\n",
       "   0.06203420257038797,\n",
       "   0.061421332017447886,\n",
       "   0.06293741181933538,\n",
       "   0.061892076301466224,\n",
       "   0.06267663840550358,\n",
       "   0.060660280357858,\n",
       "   0.062448520726745754,\n",
       "   0.0647376141170667,\n",
       "   0.06394214503502126,\n",
       "   0.06503124198367055,\n",
       "   0.06286921905656591,\n",
       "   0.06318547684516969,\n",
       "   0.06056772997735231,\n",
       "   0.06327387349188396,\n",
       "   0.06320763681621043,\n",
       "   0.06469411748477853,\n",
       "   0.06366453406277607,\n",
       "   0.06486680011758708,\n",
       "   0.06299756796014645,\n",
       "   0.06332069540933737,\n",
       "   0.06570615742469396,\n",
       "   0.06422260513823264,\n",
       "   0.06312883637517572,\n",
       "   0.06319216596501825,\n",
       "   0.061613285112055126,\n",
       "   0.06373114259056495,\n",
       "   0.0627903590103849,\n",
       "   0.06024380209910301,\n",
       "   0.06290818777912895,\n",
       "   0.06339149017673293,\n",
       "   0.06085430930263338,\n",
       "   0.06437137178652975,\n",
       "   0.06366180440921133,\n",
       "   0.06295558329284182,\n",
       "   0.06670056919962755,\n",
       "   0.06050820230050336,\n",
       "   0.06196447997580087,\n",
       "   0.06393119687351059,\n",
       "   0.06439576780522545,\n",
       "   0.06694169558753711,\n",
       "   0.06026871479918141,\n",
       "   0.0654346119574674,\n",
       "   0.06345199072797024,\n",
       "   0.06539879900766228,\n",
       "   0.06305175691425847,\n",
       "   0.06542146674114821,\n",
       "   0.0627344742410408,\n",
       "   0.06364703644501052,\n",
       "   0.06185846079084472,\n",
       "   0.06581176331557238,\n",
       "   0.06381487048765537,\n",
       "   0.06181476193673923,\n",
       "   0.06725725072892305,\n",
       "   0.060371916813753784,\n",
       "   0.05969203416735589,\n",
       "   0.06489803529874395,\n",
       "   0.06258599506490885,\n",
       "   0.06346369071048025,\n",
       "   0.06778226192370602,\n",
       "   0.06582349892374918,\n",
       "   0.06628842461789129,\n",
       "   0.06497663320094267,\n",
       "   0.06406584059673628,\n",
       "   0.06346913748319413,\n",
       "   0.06392188140181204,\n",
       "   0.0662130618020829,\n",
       "   0.06234303827330927,\n",
       "   0.06493565954974,\n",
       "   0.06458184152384773,\n",
       "   0.06312323782023142,\n",
       "   0.06382253333143148,\n",
       "   0.06240384679577024,\n",
       "   0.062141994887294,\n",
       "   0.060690934700044746,\n",
       "   0.06358740424030891,\n",
       "   0.06195368666663298,\n",
       "   0.0647736086339633,\n",
       "   0.06225752907534439,\n",
       "   0.06351176017987327,\n",
       "   0.0625042175096394,\n",
       "   0.06219346956923147,\n",
       "   0.06517999839376576],\n",
       "  'val_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'val_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0]}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델을 구성한다\n",
    "model = MAMLFewShotClassifier(args=args, device=device,\n",
    "                              im_shape=(2, 3,\n",
    "                                        args.image_height, args.image_width))\n",
    "\n",
    "data = MetaLearningSystemDataLoader\n",
    "\n",
    "maml_system = ExperimentBuilder(model=model, data=data, args=args, device=device)\n",
    "maml_system.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3803d5ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MAMLFewShotClassifier(\n",
       "  (classifier): VGGReLUNormNetwork(\n",
       "    (layer_dict): ModuleDict(\n",
       "      (conv0): MetaConvNormLayerReLU(\n",
       "        (layer_dict): ModuleDict()\n",
       "        (conv): MetaConv2dLayer()\n",
       "        (norm_layer): MetaBatchNormLayer(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): MetaConvNormLayerReLU(\n",
       "        (layer_dict): ModuleDict()\n",
       "        (conv): MetaConv2dLayer()\n",
       "        (norm_layer): MetaBatchNormLayer(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): MetaConvNormLayerReLU(\n",
       "        (layer_dict): ModuleDict()\n",
       "        (conv): MetaConv2dLayer()\n",
       "        (norm_layer): MetaBatchNormLayer(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv3): MetaConvNormLayerReLU(\n",
       "        (layer_dict): ModuleDict()\n",
       "        (conv): MetaConv2dLayer()\n",
       "        (norm_layer): MetaBatchNormLayer(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (linear): MetaLinearLayer()\n",
       "    )\n",
       "  )\n",
       "  (inner_loop_optimizer): GradientDescentLearningRule()\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_dir = maml_system.saved_models_filepath\n",
    "model_name = \"train_model\"\n",
    "model_idx = maml_system.state['best_epoch']\n",
    "\n",
    "state = maml_system.model.load_model(model_save_dir=model_save_dir,\n",
    "                                     model_name=model_name,\n",
    "                                     model_idx=model_idx+1)\n",
    "\n",
    "state_dict_loaded = state['network']\n",
    "\n",
    "maml_system.model.load_state_dict(state_dict=state_dict_loaded)\n",
    "\n",
    "# # 잘 불러왔는지 확인하는 코드\n",
    "# print(\"state_dict_loaded == \",state_dict_loaded)\n",
    "# print(\"=\"*10)\n",
    "# for key, value in maml_system.model.named_parameters():\n",
    "#     print(key)\n",
    "#     print(value)\n",
    "maml_system.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db4855d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = maml_system.data.get_train_batches(total_batches=int(600/2), augment_images=False)\n",
    "\n",
    "figure_idx = 0\n",
    "\n",
    "for sample_idx, train_sample in enumerate(train_data):\n",
    "    \n",
    "    x_support_set, x_target_set, y_support_set, y_target_set, seed = train_sample\n",
    "    \n",
    "    x_support_set = torch.Tensor(x_support_set).float().to(device=maml_system.model.device)\n",
    "    x_target_set = torch.Tensor(x_target_set).float().to(device=maml_system.model.device)\n",
    "    y_support_set = torch.Tensor(y_support_set).long().to(device=maml_system.model.device)\n",
    "    y_target_set = torch.Tensor(y_target_set).long().to(device=maml_system.model.device)\n",
    "    \n",
    "    for task_id, (x_support_set_task, y_support_set_task, x_target_set_task, y_target_set_task) in enumerate(zip(x_support_set,\n",
    "                              y_support_set,\n",
    "                              x_target_set,\n",
    "                              y_target_set)):\n",
    "        \n",
    "        names_weights_copy = maml_system.model.get_inner_loop_parameter_dict(maml_system.model.classifier.named_parameters())\n",
    "        \n",
    "        num_devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "\n",
    "        names_weights_copy = {\n",
    "            name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
    "                [num_devices] + [1 for i in range(len(value.shape))]) for\n",
    "            name, value in names_weights_copy.items()}\n",
    "        \n",
    "        prompted_weights_copy = {}\n",
    "        if args.prompter:\n",
    "            prompted_weights_copy = {key: value for key, value in names_weights_copy.items() if 'prompt' in key}\n",
    "\n",
    "        names_weights_copy = {key: value for key, value in names_weights_copy.items() if 'layer_dict' in key}\n",
    "        \n",
    "        n, s, c, h, w = x_target_set_task.shape\n",
    "\n",
    "        x_support_set_task = x_support_set_task.view(-1, c, h, w)\n",
    "        y_support_set_task = y_support_set_task.view(-1)\n",
    "        x_target_set_task = x_target_set_task.view(-1, c, h, w)\n",
    "        y_target_set_task = y_target_set_task.view(-1)\n",
    "        \n",
    "        num_steps=5\n",
    "        \n",
    "        for num_step in range(num_steps):            \n",
    "            \n",
    "            support_loss, support_preds = maml_system.model.net_forward(\n",
    "                                                               x=x_support_set_task,\n",
    "                                                               y=y_support_set_task,\n",
    "                                                               weights=names_weights_copy,\n",
    "                                                               prompted_weights=prompted_weights_copy,\n",
    "                                                               backup_running_statistics=num_step == 0,\n",
    "                                                               prepend_prompt=args.prompter,\n",
    "                                                               training=True,\n",
    "                                                               num_step=num_step,\n",
    "                                                               training_phase=False,\n",
    "                                                               epoch=0,\n",
    "                                                               inner_loop=True)\n",
    "        \n",
    "            names_weights_copy, prompted_weights_copy = maml_system.model.apply_inner_loop_update(\n",
    "                 loss=support_loss,\n",
    "                 names_weights_copy=names_weights_copy,\n",
    "                 prompted_weights_copy=prompted_weights_copy,\n",
    "                 use_second_order=True,\n",
    "                 current_step_idx=num_step,\n",
    "                 current_iter='test',\n",
    "                 training_phase=False)\n",
    "            \n",
    "        ##########Inner level 종료 #############\n",
    "        # Query set으로 Grad-CAM을 구한다\n",
    "        individual_images = torch.split(x_target_set_task, 1, dim=0)  # (25, 3, 84, 84) -> (1, 3, 84, 84) 텐서로 분리\n",
    "        individual_labels = torch.split(y_target_set_task, 1, dim=0)\n",
    "        \n",
    "        \n",
    "        original_save_path = \"Grad_CAM/\" + datasets + \"/MAML/origin/\" + \"sample_idx_\" + str(sample_idx) + \"/\" + \"task_id_\" + str(task_id) + \"/\"\n",
    "        grad_cam_save_path = \"Grad_CAM/\" + datasets + \"/MAML/gradcam/\"+ \"sample_idx_\" + str(sample_idx) + \"/\" + \"task_id_\" + str(task_id) + \"/\"\n",
    "        \n",
    "        make_folder(original_save_path)\n",
    "        make_folder(grad_cam_save_path)\n",
    "        \n",
    "        # 각 이미지 텐서 확인\n",
    "        for i in range(y_target_set_task.shape[0]):\n",
    "\n",
    "            input_image = individual_images[i]\n",
    "            y_label = individual_labels[i]\n",
    "\n",
    "            _, target_preds, feature_map_list = maml_system.model.net_forward_feature_extractor(x=input_image,\n",
    "                                                             y=y_label, weights=names_weights_copy,\n",
    "                                                             backup_running_statistics=False, training=True,\n",
    "                                                             num_step=num_step, training_phase='test',\n",
    "                                                             epoch=0)\n",
    "            \n",
    "            feature_map = feature_map_list[3]\n",
    "            \n",
    "            # 선택한 클래스에 대한 스칼라 출력값을 기준으로 gradient 계산\n",
    "            target_class = y_label\n",
    "            output = target_preds\n",
    "\n",
    "            class_score = output[:, target_class].sum()\n",
    "            gradients = torch.autograd.grad(outputs=class_score, inputs=feature_map, grad_outputs=torch.ones_like(class_score))\n",
    "\n",
    "            # gradients를 사용하여 feature map의 가중치를 계산\n",
    "            weights = torch.mean(gradients[0], dim=(2, 3), keepdim=True)\n",
    "            grad_cam = F.relu(torch.sum(weights * feature_map, dim=1, keepdim=True))\n",
    "\n",
    "            # grad_cam을 이미지 크기로 리사이즈\n",
    "            grad_cam = F.interpolate(grad_cam, size=(input_image.shape[2], input_image.shape[3]), mode='bilinear', align_corners=False)\n",
    "            grad_cam = grad_cam.squeeze().cpu().detach().numpy()\n",
    "\n",
    "            visualize_and_save_grad_cam(\n",
    "                input_image=input_image,\n",
    "                grad_cam=grad_cam,\n",
    "                grad_cam_save_path=grad_cam_save_path + str(i) + \".png\",\n",
    "                original_save_path=original_save_path + str(i) + \".png\",\n",
    "                datasets=datasets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
