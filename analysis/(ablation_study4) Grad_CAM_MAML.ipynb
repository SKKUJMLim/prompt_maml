{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd87b33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pytorchcv.model_provider import get_model as ptcv_get_model # model\n",
    "import sys, os\n",
    "import easydict\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline \n",
    "\n",
    "# enable cuda devices\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from meta_neural_network_architectures import VGGReLUNormNetwork, ResNet12\n",
    "from utils.parser_utils import get_args\n",
    "from data import MetaLearningSystemDataLoader\n",
    "from experiment_builder import ExperimentBuilder\n",
    "from utils.basic import show_batch\n",
    "from few_shot_learning_system import MAMLFewShotClassifier\n",
    "import prompters\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from data_augmentation import mixup_data, cutmix_data, random_flip, random_flip_like_torchvision\n",
    "from utils.grad_cam import visualize_and_save_grad_cam, make_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c638561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets_name = \"mini_imagenet\"\n",
    "# datasets_name = \"tiered_imagenet\"\n",
    "datasets_name = \"CIFAR_FS\"\n",
    "# datasets_name = \"CUB\"\n",
    "\n",
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "# os.environ['TEST_DATASET'] = \"tiered_imagenet\" # https://mtl.yyliu.net/download/Lmzjm9tX.html\n",
    "os.environ['TEST_DATASET'] = \"CIFAR_FS\" # https://drive.google.com/file/d/1pTsCCMDj45kzFYgrnO67BWVbKs48Q3NI/view\n",
    "# os.environ['TEST_DATASET'] = \"CUB\" # https://data.caltech.edu/records/65de6-vp158"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8daf114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.environ['DATASET_DIR'] ===  C:/Users/JM/PycharmProjects/MAML/datasets\n"
     ]
    }
   ],
   "source": [
    "print(\"os.environ['DATASET_DIR'] === \", os.environ['DATASET_DIR'])\n",
    "args = easydict.EasyDict(\n",
    "{\n",
    "  \"batch_size\":2,\n",
    "  \"image_height\":84,\n",
    "  \"image_width\":84,\n",
    "  \"image_channels\":3,\n",
    "  \"gpu_to_use\":0,\n",
    "  \"num_dataprovider_workers\":4,\n",
    "  \"max_models_to_save\":5,\n",
    "  \"dataset_name\":\"mini_imagenet_full_size\",\n",
    "  \"dataset_path\":\"mini_imagenet_full_size\",\n",
    "  \"reset_stored_paths\":False,\n",
    "  \"experiment_name\":\"../MAML_5way_5shot_filter128_miniImagenet\",\n",
    "  \"train_seed\": 0, \"val_seed\": 0,\n",
    "  \"indexes_of_folders_indicating_class\": [-3, -2],\n",
    "  \"sets_are_pre_split\": True,\n",
    "  \"train_val_test_split\": [0.64, 0.16, 0.20],\n",
    "  \"evaluate_on_test_set_only\": False,\n",
    "\n",
    "  \"total_epochs\": 100,\n",
    "  \"total_iter_per_epoch\":500, \"continue_from_epoch\": -2,\n",
    "  \"num_evaluation_tasks\":600,\n",
    "  \"multi_step_loss_num_epochs\": 15,\n",
    "  \"minimum_per_task_contribution\": 0.01,\n",
    "  \"learnable_per_layer_per_step_inner_loop_learning_rate\": False,\n",
    "  \"enable_inner_loop_optimizable_bn_params\": False,\n",
    "  \"evalute_on_test_set_only\": False,\n",
    "\n",
    "  \"max_pooling\": True,\n",
    "  \"per_step_bn_statistics\": False,\n",
    "  \"learnable_batch_norm_momentum\": False,\n",
    "  \"load_into_memory\": False,\n",
    "  \"init_inner_loop_learning_rate\": 0.01,\n",
    "  \"init_inner_loop_weight_decay\": 0.0005,\n",
    "  \"learnable_bn_gamma\": True,\n",
    "  \"learnable_bn_beta\": True,\n",
    "\n",
    "  \"dropout_rate_value\":0.0,\n",
    "  \"min_learning_rate\":0.001,\n",
    "  \"meta_learning_rate\":0.001,   \"total_epochs_before_pause\": 100,\n",
    "  \"first_order_to_second_order_epoch\":-1,\n",
    "  \"weight_decay\": 0.0,\n",
    "\n",
    "  \"norm_layer\":\"batch_norm\",\n",
    "  \"cnn_num_filters\":128,\n",
    "  \"num_stages\":4,\n",
    "  \"conv_padding\": True,\n",
    "  \"number_of_training_steps_per_iter\":5,\n",
    "  \"number_of_evaluation_steps_per_iter\":5,\n",
    "  \"cnn_blocks_per_stage\":1,\n",
    "  \"num_classes_per_set\":5,\n",
    "  \"num_samples_per_class\":5,\n",
    "  \"num_target_samples\": 15,\n",
    "  \"samples_per_iter\" : 1,\n",
    "\n",
    "  \"second_order\": True,\n",
    "  \"use_multi_step_loss_optimization\":False,\n",
    "  \"backbone\": \"4-CONV\",\n",
    "  \"use_bias\": True,\n",
    "  \"prompter\": False,\n",
    "  \"prompt_engineering\": 'not',\n",
    "  \"prompt_size\" : 5,\n",
    "  \"image_size\" : 84,\n",
    "  \"prompt_random_init\": False,\n",
    "  \"outer_prompt_learning_rate\": 0.001,\n",
    "  \"inner_prompt_learning_rate\": 0.01,\n",
    "  \"ANIL\": False,\n",
    "  \"BOIL\": False,\n",
    "  \"data_aug\" : \"none\",\n",
    "  \"ablation_record\": False,\n",
    "  \"random_prompt_init\": False\n",
    "}\n",
    ")\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "args.im_shape = (2, 3, args.image_height, args.image_width)\n",
    "\n",
    "args.use_cuda = torch.cuda.is_available()\n",
    "args.seed = 104\n",
    "args.reverse_channels=False\n",
    "args.labels_as_int=False\n",
    "args.reset_stored_filepaths=False\n",
    "args.num_of_gpus=1\n",
    "\n",
    "args.continue_from_epoch='latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dba8492c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using max pooling\n",
      "meta network params\n",
      "Inner Loop parameters\n",
      "Outer Loop parameters\n",
      "classifier.layer_dict.conv0.conv.weight torch.Size([128, 3, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv0.conv.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv0.norm_layer.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv0.norm_layer.weight torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv1.conv.weight torch.Size([128, 128, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv1.conv.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv1.norm_layer.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv1.norm_layer.weight torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv2.conv.weight torch.Size([128, 128, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv2.conv.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv2.norm_layer.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv2.norm_layer.weight torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv3.conv.weight torch.Size([128, 128, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv3.conv.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv3.norm_layer.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv3.norm_layer.weight torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.linear.weights torch.Size([5, 3200]) cuda:0 True\n",
      "classifier.layer_dict.linear.bias torch.Size([5]) cuda:0 True\n",
      "1\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\MAML_5way_5shot_filter128_miniImagenet\n",
      "attempting to find existing checkpoint\n",
      "data {'test': 12000, 'train': 38400, 'val': 9600}\n",
      "train_seed 985773, val_seed: 985773, at start time\n",
      "50000 50000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_val_acc': 0.6138222214579582,\n",
       " 'best_val_iter': 45000,\n",
       " 'current_iter': 50000,\n",
       " 'best_epoch': 90,\n",
       " 'train_loss_mean': 0.5814862896502018,\n",
       " 'train_loss_std': 0.12721647610816997,\n",
       " 'train_accuracy_mean': 0.7875600011348725,\n",
       " 'train_accuracy_std': 0.05884519899358138,\n",
       " 'train_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_0_std': 0.0,\n",
       " 'train_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_1_std': 0.0,\n",
       " 'train_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_2_std': 0.0,\n",
       " 'train_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_3_std': 0.0,\n",
       " 'train_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'train_loss_importance_vector_4_std': 0.0,\n",
       " 'train_learning_rate_mean': 0.0010000000000000005,\n",
       " 'train_learning_rate_std': 4.336808689942018e-19,\n",
       " 'val_loss_mean': 1.0568136115868887,\n",
       " 'val_loss_std': 0.15827062479566187,\n",
       " 'val_accuracy_mean': 0.5880888885259629,\n",
       " 'val_accuracy_std': 0.06520889128387614,\n",
       " 'val_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_0_std': 0.0,\n",
       " 'val_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_1_std': 0.0,\n",
       " 'val_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_2_std': 0.0,\n",
       " 'val_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_3_std': 0.0,\n",
       " 'val_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'val_loss_importance_vector_4_std': 0.0,\n",
       " 'network': OrderedDict([('classifier.layer_dict.conv0.conv.weight',\n",
       "               tensor([[[[ 1.2752e-01, -3.8145e-01,  2.0409e-01],\n",
       "                         [-1.2580e-01, -6.3707e-02,  6.6579e-02],\n",
       "                         [-1.0911e-01,  3.0030e-01, -5.8555e-02]],\n",
       "               \n",
       "                        [[ 1.4483e-01, -3.2933e-01,  2.3947e-01],\n",
       "                         [-1.0096e-01,  1.5914e-02,  1.1105e-01],\n",
       "                         [-1.9702e-01,  1.9213e-01, -9.2583e-02]],\n",
       "               \n",
       "                        [[ 3.0199e-01, -2.2339e-01, -3.7985e-02],\n",
       "                         [ 1.3134e-01,  6.8101e-02, -2.0355e-01],\n",
       "                         [-8.1707e-02,  1.6982e-01, -2.2569e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 3.3667e-18,  1.8764e-18,  1.9798e-18],\n",
       "                         [ 6.8712e-18,  6.0583e-18,  4.7780e-18],\n",
       "                         [ 1.0034e-17,  7.3690e-18,  4.8040e-18]],\n",
       "               \n",
       "                        [[-3.1348e-17, -3.0545e-17, -3.1028e-17],\n",
       "                         [-3.1391e-17, -3.0340e-17, -3.2436e-17],\n",
       "                         [-3.2209e-17, -3.2800e-17, -3.4103e-17]],\n",
       "               \n",
       "                        [[-6.4888e-18, -1.1846e-17, -9.4621e-18],\n",
       "                         [-8.8111e-18, -1.1500e-17, -1.1398e-17],\n",
       "                         [ 1.7639e-18,  4.1225e-18,  3.0816e-18]]],\n",
       "               \n",
       "               \n",
       "                       [[[-3.8416e-12, -3.3472e-12, -3.0930e-12],\n",
       "                         [-3.3565e-12, -2.8371e-12, -2.7487e-12],\n",
       "                         [-2.6264e-12, -2.0783e-12, -1.6835e-12]],\n",
       "               \n",
       "                        [[ 7.7980e-12,  7.6627e-12,  7.9578e-12],\n",
       "                         [ 8.8633e-12,  8.1026e-12,  8.2620e-12],\n",
       "                         [ 8.6782e-12,  9.0170e-12,  9.1509e-12]],\n",
       "               \n",
       "                        [[ 1.3759e-11,  1.4221e-11,  1.3273e-11],\n",
       "                         [ 1.3668e-11,  1.3676e-11,  1.3900e-11],\n",
       "                         [ 1.4089e-11,  1.4354e-11,  1.4268e-11]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 6.6834e-09,  4.4550e-09,  4.8198e-09],\n",
       "                         [ 7.3475e-09,  5.1327e-09,  4.3283e-09],\n",
       "                         [ 5.9327e-09,  4.3756e-09,  2.9979e-09]],\n",
       "               \n",
       "                        [[-3.3088e-09, -5.9567e-09, -6.4521e-09],\n",
       "                         [-2.9824e-09, -5.1976e-09, -6.0285e-09],\n",
       "                         [-2.3799e-09, -3.4012e-09, -5.1779e-09]],\n",
       "               \n",
       "                        [[ 2.8238e-09,  8.9361e-10, -5.4545e-10],\n",
       "                         [ 1.6759e-09,  6.2652e-11, -1.5165e-09],\n",
       "                         [ 1.4345e-09,  1.2327e-09, -8.7014e-10]]],\n",
       "               \n",
       "               \n",
       "                       [[[-3.0706e-09, -1.7800e-09, -2.4970e-09],\n",
       "                         [-2.8191e-09, -1.7002e-09, -2.4115e-09],\n",
       "                         [-4.1103e-09, -3.5960e-09, -3.6590e-09]],\n",
       "               \n",
       "                        [[-5.5543e-09, -5.0188e-09, -5.4937e-09],\n",
       "                         [-4.6797e-09, -4.3100e-09, -4.4593e-09],\n",
       "                         [-4.9977e-09, -4.3704e-09, -4.0221e-09]],\n",
       "               \n",
       "                        [[-3.5142e-09,  2.1669e-09, -4.9893e-10],\n",
       "                         [-3.4857e-09, -1.0228e-09,  2.4739e-09],\n",
       "                         [-5.3795e-09, -4.1425e-09, -1.5916e-09]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 9.6481e-26,  8.7030e-26, -5.5359e-26],\n",
       "                         [ 7.8851e-26,  1.1662e-25,  2.8604e-26],\n",
       "                         [-2.3846e-26, -5.7348e-26, -1.7480e-26]],\n",
       "               \n",
       "                        [[ 6.8847e-26,  6.4630e-26,  7.4681e-26],\n",
       "                         [ 8.1530e-26,  6.6519e-26,  4.4052e-26],\n",
       "                         [ 7.6753e-26,  9.7325e-26,  7.8692e-26]],\n",
       "               \n",
       "                        [[-1.5927e-26, -2.6288e-26, -3.5685e-26],\n",
       "                         [-1.9852e-26, -3.6905e-26, -4.2108e-26],\n",
       "                         [-1.4295e-26, -3.0553e-26, -3.9856e-26]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.conv.bias',\n",
       "               tensor([-8.9601e-05,  8.4944e-12,  6.9598e-10,  2.0631e-07, -1.6735e-13,\n",
       "                       -1.5687e-04,  7.8251e-08, -2.1903e-06, -4.4300e-07, -4.3070e-15,\n",
       "                       -8.4352e-06,  2.7616e-05,  7.5222e-05,  3.9639e-11, -1.1226e-06,\n",
       "                        1.0157e-04, -3.6850e-07,  2.0959e-05,  1.3311e-06, -3.2139e-05,\n",
       "                        2.7632e-07,  1.3853e-09, -3.2478e-05,  4.4511e-07,  3.5452e-05,\n",
       "                        2.5611e-07, -4.6128e-08,  9.3184e-05,  3.9690e-06,  2.3204e-06,\n",
       "                       -5.0007e-05,  6.0176e-07,  1.1417e-10,  6.9186e-07, -1.4342e-07,\n",
       "                        4.8747e-07,  4.3894e-05,  2.4704e-05,  2.4603e-07,  1.5313e-06,\n",
       "                       -3.0938e-05,  1.4105e-04,  5.2166e-05, -5.0446e-06, -3.4049e-06,\n",
       "                        2.2875e-08,  1.8068e-05, -1.3239e-09,  1.1401e-07, -2.1413e-05,\n",
       "                        1.1551e-06,  5.5974e-08, -5.3988e-10, -1.1347e-05,  1.4207e-05,\n",
       "                       -5.2035e-06, -1.0199e-05,  8.4722e-09, -4.0619e-11, -3.0147e-04,\n",
       "                       -1.0445e-06, -2.3997e-14, -4.4412e-07, -4.5641e-09, -8.0191e-09,\n",
       "                       -4.2241e-06,  5.1331e-06,  2.1982e-05,  2.8092e-06, -1.1995e-08,\n",
       "                        2.8361e-07,  2.0555e-13,  2.7083e-05,  1.4541e-06, -2.4453e-07,\n",
       "                       -4.0038e-05,  9.6255e-06, -1.1791e-04, -1.8251e-07, -5.0839e-05,\n",
       "                       -2.9845e-07, -4.1132e-05,  5.5617e-07, -2.3814e-05, -1.0909e-04,\n",
       "                        1.2071e-04, -9.1586e-05,  1.2167e-04,  1.7152e-06,  4.2896e-12,\n",
       "                        2.1259e-09, -3.4742e-17, -5.7448e-07, -1.9413e-09,  4.1967e-08,\n",
       "                        2.1906e-08, -9.9807e-14, -8.1814e-06,  6.5157e-06,  3.7172e-05,\n",
       "                        1.6340e-07,  5.8913e-06,  3.1998e-07,  6.7131e-05, -3.4589e-05,\n",
       "                        9.4000e-10,  2.6112e-05,  1.1555e-08, -6.0242e-07, -6.6692e-14,\n",
       "                       -1.2256e-08,  4.4534e-05,  1.4942e-15,  1.0959e-04, -1.3069e-08,\n",
       "                       -4.1612e-05, -2.7530e-06, -3.7873e-05,  1.9426e-07, -4.9507e-06,\n",
       "                       -1.0908e-15,  2.8356e-08, -3.9559e-09, -1.6076e-10, -2.1947e-06,\n",
       "                       -3.4575e-06, -7.1513e-12,  3.7944e-14], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.bias',\n",
       "               tensor([-4.1286e-02, -1.3141e-10, -5.2680e-08, -6.7203e-03, -1.4447e-07,\n",
       "                       -4.2683e-02, -2.4264e-08, -3.6316e-02, -8.4859e-03, -2.7811e-07,\n",
       "                       -4.6842e-05,  1.1575e-01, -9.4470e-03, -7.1721e-08, -1.5662e-03,\n",
       "                       -1.5851e-02, -1.1766e-02, -7.5598e-03, -2.4959e-02, -6.6769e-04,\n",
       "                       -1.0381e-09, -2.6716e-09, -3.2715e-01, -5.1294e-02, -5.1332e-01,\n",
       "                       -3.8594e-02, -2.6827e-04,  1.6834e-01, -8.0787e-04, -5.7998e-02,\n",
       "                        4.8212e-03, -4.7730e-04, -7.6772e-07, -1.0964e-08, -3.5568e-02,\n",
       "                        8.9736e-01, -7.1337e-01,  1.7220e-01, -2.9232e-10, -3.5786e-02,\n",
       "                       -4.0933e-02,  2.0144e-01,  3.2934e-01,  4.5029e-02, -1.5107e-02,\n",
       "                       -4.1542e-02, -7.8220e-05, -5.0744e-02, -5.7728e-02, -2.2449e-01,\n",
       "                       -2.8602e-02, -6.6500e-02, -2.7601e-10, -2.8588e-01,  8.7816e-02,\n",
       "                       -1.2473e-02, -3.5435e-01, -1.2274e-04, -1.4672e-08,  3.2196e-02,\n",
       "                       -4.1055e-02, -1.0994e-06, -4.1026e-01, -4.7345e-04, -1.6663e-04,\n",
       "                       -6.3590e-06, -3.2442e-01, -3.2938e-01, -3.1205e-02, -2.9422e-10,\n",
       "                       -6.2974e-03, -4.9425e-09, -9.5502e-02, -2.5102e-01, -1.3837e-03,\n",
       "                       -4.2698e-01, -3.5534e-01,  2.0723e-01, -3.5357e-02, -7.2470e-01,\n",
       "                       -2.8449e-02,  2.7757e-01, -5.4068e-10, -3.1129e-01,  2.3712e-02,\n",
       "                        3.3315e-01,  8.1584e-02,  2.5959e-01, -4.3031e-04, -1.9257e-06,\n",
       "                       -4.1828e-03, -1.6191e-08, -2.5575e-02, -2.8312e-06, -1.4260e-02,\n",
       "                       -1.4799e-03, -1.3442e-09,  3.9235e-03, -2.5852e-03, -6.2148e-01,\n",
       "                       -3.8307e-02, -3.5375e-01, -6.9629e-02, -7.0138e-02, -2.5625e-03,\n",
       "                       -3.4844e-07, -3.7570e-01, -5.1766e-04, -1.2477e-05, -3.2086e-07,\n",
       "                       -7.8215e-04,  1.3921e-01, -9.4714e-08, -3.4523e-02, -2.8087e-06,\n",
       "                       -4.3342e-02, -2.7524e-02,  7.2227e-02, -2.7032e-03,  9.2418e-01,\n",
       "                       -1.4841e-10, -4.4358e-03, -3.0715e-03, -3.5976e-10, -5.0803e-05,\n",
       "                       -1.9960e-05, -8.2382e-06, -2.4032e-10], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.weight',\n",
       "               tensor([ 2.9039e-01,  9.1143e-12,  3.3542e-07, -1.6947e-03,  4.1554e-18,\n",
       "                        2.8558e-01, -4.0050e-11, -1.9006e-03,  1.7938e-03, -1.3676e-20,\n",
       "                        6.6466e-06,  1.1492e-01,  3.3404e-01,  8.4649e-09,  3.4864e-04,\n",
       "                        6.0186e-01, -8.4055e-04,  3.1657e-01,  1.6109e-03, -3.1248e-04,\n",
       "                       -9.8472e-19,  1.2852e-08,  4.9315e-01,  9.9873e-03,  3.7232e-01,\n",
       "                        1.7633e-02, -3.5383e-05,  3.3167e-01, -7.0470e-05,  1.9098e-02,\n",
       "                        4.5151e-01,  1.1140e-05, -6.6607e-18,  9.9466e-19, -3.8207e-04,\n",
       "                       -2.6506e-03,  4.7010e-01,  2.8681e-01, -1.4732e-17,  7.0808e-03,\n",
       "                       -8.6500e-04,  3.5124e-01,  4.0006e-01,  6.2601e-01,  7.7218e-02,\n",
       "                        1.5618e-02,  8.7883e-06,  8.4411e-04,  2.5089e-02,  2.7258e-01,\n",
       "                        4.9812e-03,  1.4797e-02,  7.9258e-11,  2.2090e-01,  1.2331e-01,\n",
       "                       -9.8288e-04,  4.5528e-01,  1.0461e-05, -6.9862e-13,  5.0291e-01,\n",
       "                       -5.8298e-03,  5.4402e-27,  2.2154e-01,  3.6331e-05,  4.7226e-04,\n",
       "                       -4.3186e-06,  3.6150e-01,  3.2858e-01, -7.5597e-03,  5.8155e-14,\n",
       "                        5.1748e-04,  1.1079e-07,  2.8937e-01,  1.2657e-01, -5.2062e-05,\n",
       "                        2.9987e-01,  2.0778e-01,  5.6662e-01,  1.3104e-02,  4.7661e-01,\n",
       "                        8.2509e-03,  4.6092e-01,  5.0679e-13,  3.7736e-01,  3.2760e-01,\n",
       "                        3.2247e-01,  2.3590e-01,  3.3323e-01, -3.1819e-05,  2.0988e-06,\n",
       "                       -3.4943e-04,  4.1041e-24, -2.5904e-03,  2.4152e-06, -1.5335e-03,\n",
       "                       -1.5833e-04,  8.9047e-10,  6.7163e-03,  5.1118e-04,  4.2892e-01,\n",
       "                        8.3945e-03,  3.1980e-01, -1.7806e-02,  3.4814e-01,  2.4574e-04,\n",
       "                       -9.4153e-18,  3.7741e-01, -3.7130e-05, -4.2523e-06,  7.7668e-08,\n",
       "                        2.3572e-04,  1.2481e-01,  1.6445e-13,  3.3900e-01, -4.8571e-05,\n",
       "                        6.2085e-01,  3.9526e-03,  2.4387e-01,  2.2174e-04,  4.3772e-03,\n",
       "                       -7.0157e-08,  1.0985e-03,  8.1437e-04,  2.9775e-11, -1.3619e-05,\n",
       "                       -3.3611e-06,  5.2129e-06, -1.6493e-21], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.weight',\n",
       "               tensor([[[[-1.0500e-02, -7.3015e-03, -2.6549e-03],\n",
       "                         [-1.7727e-02,  1.8065e-03,  1.0746e-02],\n",
       "                         [-1.4601e-02,  4.0654e-04,  2.0069e-02]],\n",
       "               \n",
       "                        [[ 5.3492e-10, -7.1734e-12, -3.1604e-12],\n",
       "                         [-5.9707e-12,  3.2211e-08, -9.1016e-09],\n",
       "                         [-1.9223e-12, -8.7841e-09,  5.4209e-07]],\n",
       "               \n",
       "                        [[ 2.1325e-08,  1.3273e-07, -1.4151e-08],\n",
       "                         [-4.7016e-10, -3.4226e-12, -2.1789e-12],\n",
       "                         [-5.4290e-09,  1.6070e-09, -3.4286e-11]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 9.6621e-07, -7.8711e-10,  1.2439e-08],\n",
       "                         [-1.0437e-09, -2.6048e-06,  5.3395e-09],\n",
       "                         [-1.6373e-10,  2.0804e-08,  5.7673e-09]],\n",
       "               \n",
       "                        [[-7.5726e-10, -8.3830e-10, -1.7857e-09],\n",
       "                         [-2.3028e-10,  1.4099e-10,  1.9276e-05],\n",
       "                         [-5.9577e-10,  4.4426e-07,  1.7884e-06]],\n",
       "               \n",
       "                        [[-6.5390e-14, -1.2663e-14,  2.5690e-09],\n",
       "                         [-1.4792e-13,  1.0655e-12,  6.2996e-09],\n",
       "                         [ 4.4246e-09,  1.9950e-13,  3.8183e-13]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.4567e-01, -9.7285e-02,  1.0524e-01],\n",
       "                         [-1.3138e-01, -4.0282e-02,  2.4381e-01],\n",
       "                         [-1.8034e-01, -3.2793e-02,  2.3563e-01]],\n",
       "               \n",
       "                        [[ 6.6848e-13, -4.6317e-14, -1.3100e-12],\n",
       "                         [ 2.3266e-12,  8.6673e-17, -1.3589e-12],\n",
       "                         [ 4.1866e-12,  1.4292e-12,  7.7460e-13]],\n",
       "               \n",
       "                        [[-1.2569e-09, -3.9708e-10, -5.5389e-10],\n",
       "                         [-8.7996e-10, -3.1998e-14, -1.4700e-10],\n",
       "                         [-1.5902e-09, -9.1484e-10, -1.4897e-09]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-3.1179e-07, -9.1723e-08, -7.4961e-08],\n",
       "                         [-2.5351e-07,  9.1083e-11,  2.7153e-08],\n",
       "                         [-1.4395e-07,  1.2295e-07,  1.5139e-08]],\n",
       "               \n",
       "                        [[-1.9818e-07, -5.9985e-08, -8.4212e-08],\n",
       "                         [-1.3980e-07, -1.7460e-11, -2.3888e-08],\n",
       "                         [-2.5318e-07, -1.4009e-07, -2.3478e-07]],\n",
       "               \n",
       "                        [[-1.3478e-11,  8.1172e-12,  4.9091e-12],\n",
       "                         [-1.9665e-11, -1.0019e-16,  2.8490e-12],\n",
       "                         [-2.4134e-11,  5.4254e-12,  6.8325e-13]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.0420e-01,  1.6266e-02, -8.3780e-03],\n",
       "                         [ 4.5838e-02,  4.8878e-03,  4.6072e-02],\n",
       "                         [-3.4028e-02,  7.3315e-02, -6.3013e-02]],\n",
       "               \n",
       "                        [[ 8.6111e-13, -6.6400e-13, -8.5544e-12],\n",
       "                         [ 1.2042e-12,  9.4249e-17, -7.5728e-12],\n",
       "                         [ 1.6097e-11,  9.9768e-12, -2.6593e-13]],\n",
       "               \n",
       "                        [[ 1.1439e-10, -1.6151e-10,  1.8115e-09],\n",
       "                         [ 3.3819e-10, -6.1041e-15,  1.4776e-09],\n",
       "                         [ 1.3573e-09,  1.7298e-09,  1.1450e-09]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 4.8768e-08, -5.8241e-08,  5.2482e-07],\n",
       "                         [ 1.3068e-07, -3.4032e-11,  7.1713e-07],\n",
       "                         [ 7.6628e-07,  2.2986e-07,  1.1728e-06]],\n",
       "               \n",
       "                        [[-1.5772e-08, -2.7685e-08,  2.9244e-07],\n",
       "                         [ 3.8883e-08, -1.8613e-10, -1.0301e-08],\n",
       "                         [ 1.6368e-07,  7.4155e-07,  3.5363e-07]],\n",
       "               \n",
       "                        [[ 9.1894e-12,  1.4664e-11, -3.5437e-12],\n",
       "                         [ 3.5404e-12,  2.9841e-16, -3.6474e-11],\n",
       "                         [ 1.3562e-11,  2.4151e-11, -3.4905e-11]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-5.3272e-05,  4.9134e-04,  2.1341e-04],\n",
       "                         [ 2.1924e-04,  2.9912e-04,  2.8346e-04],\n",
       "                         [ 4.8665e-04,  3.9278e-04,  1.2959e-04]],\n",
       "               \n",
       "                        [[ 9.0218e-06, -6.1648e-09,  1.0736e-08],\n",
       "                         [ 8.2190e-11,  2.1754e-10,  4.5069e-10],\n",
       "                         [ 3.2549e-11, -2.0786e-09, -1.1746e-06]],\n",
       "               \n",
       "                        [[-6.0129e-11, -3.9922e-08,  4.8692e-08],\n",
       "                         [-4.0720e-06, -3.5824e-13,  2.3870e-07],\n",
       "                         [-2.0363e-05, -7.4383e-08, -2.4452e-11]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-3.5896e-09, -3.2094e-07,  1.3890e-08],\n",
       "                         [-1.6283e-08, -5.1409e-08, -2.2713e-09],\n",
       "                         [-2.3309e-06, -2.4674e-10, -1.8454e-07]],\n",
       "               \n",
       "                        [[-3.7760e-08,  1.3200e-06,  4.7680e-07],\n",
       "                         [-8.6636e-10,  2.3013e-08, -3.1650e-09],\n",
       "                         [-1.4693e-09, -3.3290e-09,  1.2620e-06]],\n",
       "               \n",
       "                        [[-1.1949e-06, -5.5955e-08, -2.9255e-10],\n",
       "                         [-6.3060e-08,  2.6966e-11,  1.7918e-09],\n",
       "                         [-2.6150e-11, -5.1354e-10, -5.7868e-10]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 8.1469e-05,  5.3048e-05,  1.3973e-04],\n",
       "                         [ 1.5525e-04,  1.4966e-05,  1.0590e-04],\n",
       "                         [ 6.0221e-05,  1.2926e-04,  1.5608e-04]],\n",
       "               \n",
       "                        [[ 8.4357e-11,  2.1903e-13, -2.5664e-09],\n",
       "                         [-1.6493e-13, -2.5729e-14, -2.5503e-13],\n",
       "                         [-1.3834e-11, -2.8795e-11, -2.2378e-10]],\n",
       "               \n",
       "                        [[-3.1672e-12,  1.6752e-08,  3.3743e-11],\n",
       "                         [ 2.3504e-10,  1.7514e-13,  4.7543e-11],\n",
       "                         [-1.0972e-05,  7.4057e-10,  5.1833e-09]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.0758e-06, -2.1418e-07, -3.1682e-05],\n",
       "                         [-2.7151e-06,  1.4460e-08,  8.3278e-07],\n",
       "                         [ 1.8959e-07, -7.3695e-08,  4.2036e-06]],\n",
       "               \n",
       "                        [[-1.8677e-06,  5.6098e-10,  7.0209e-09],\n",
       "                         [-3.8847e-06,  6.7261e-11,  1.1648e-09],\n",
       "                         [ 1.3952e-06,  2.4745e-06,  4.8639e-06]],\n",
       "               \n",
       "                        [[ 7.3558e-10,  5.8683e-07,  9.8628e-09],\n",
       "                         [-1.4607e-07, -7.0655e-06, -2.2932e-05],\n",
       "                         [ 1.3242e-07,  1.3379e-12, -2.0557e-06]]],\n",
       "               \n",
       "               \n",
       "                       [[[-2.1013e-02, -7.8097e-02,  5.0398e-02],\n",
       "                         [-1.6189e-04, -2.9942e-02,  6.2445e-04],\n",
       "                         [-2.2455e-02, -5.0753e-02, -2.7177e-02]],\n",
       "               \n",
       "                        [[ 1.5947e-12, -6.0310e-13,  5.2973e-13],\n",
       "                         [ 1.4285e-12, -1.3597e-16,  1.2482e-12],\n",
       "                         [ 1.5850e-12,  3.1671e-12,  5.2302e-12]],\n",
       "               \n",
       "                        [[ 6.3553e-10, -4.2213e-10,  1.9750e-10],\n",
       "                         [ 1.0272e-09, -2.3058e-14,  3.9708e-10],\n",
       "                         [ 3.0873e-09,  2.0102e-09,  2.1409e-09]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-5.1480e-08,  1.6396e-08, -2.2713e-07],\n",
       "                         [ 3.8062e-08, -3.8845e-11, -2.2718e-07],\n",
       "                         [-1.1309e-07, -3.2826e-07, -8.2943e-07]],\n",
       "               \n",
       "                        [[-1.4652e-07, -4.9763e-09,  2.6520e-08],\n",
       "                         [-2.0457e-07, -2.5391e-11,  1.2746e-08],\n",
       "                         [-4.5133e-07, -1.5373e-07, -7.8115e-08]],\n",
       "               \n",
       "                        [[ 2.9520e-11, -4.3105e-12,  1.5719e-12],\n",
       "                         [ 4.8030e-13, -4.2243e-16,  1.2401e-11],\n",
       "                         [ 3.8206e-11, -5.0852e-11,  6.8554e-11]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.bias',\n",
       "               tensor([-1.4937e-08,  3.6905e-05,  7.0855e-06, -3.0730e-05,  1.5296e-05,\n",
       "                       -3.3792e-05, -1.7071e-11, -6.2560e-05, -2.2891e-05, -1.8041e-09,\n",
       "                        2.1149e-06, -4.3449e-07, -9.5993e-05, -2.5369e-07, -8.4984e-07,\n",
       "                       -1.5437e-11, -4.2545e-12,  1.6195e-07, -1.4136e-05,  2.4850e-05,\n",
       "                        1.7598e-05,  2.0220e-07, -1.0116e-05, -8.2375e-05,  3.5836e-05,\n",
       "                       -4.5432e-08, -2.1021e-05, -6.8162e-06, -3.0998e-05,  1.8948e-06,\n",
       "                       -9.5121e-06,  7.1193e-06,  1.6394e-06, -3.4553e-05,  2.3772e-05,\n",
       "                        4.0741e-05,  3.8603e-12, -1.4140e-05,  1.2740e-05, -1.7057e-06,\n",
       "                       -1.1113e-06,  2.1391e-06,  3.9387e-05, -5.3015e-05,  1.8604e-12,\n",
       "                        7.2378e-05,  1.2409e-07, -8.8560e-06, -5.1263e-05,  2.8185e-05,\n",
       "                        4.8152e-05,  1.4464e-07,  5.4036e-05,  1.5254e-07, -3.6132e-05,\n",
       "                       -1.4913e-09, -6.6568e-06,  2.1585e-05,  8.2528e-07, -1.8585e-05,\n",
       "                        1.1600e-11, -4.0479e-11,  3.0284e-13,  8.6787e-06,  2.2887e-05,\n",
       "                        2.4357e-05, -3.5417e-05, -6.8079e-05, -4.0439e-06, -5.6721e-06,\n",
       "                       -7.0761e-07,  1.6993e-05, -9.2189e-05, -5.2896e-05, -4.3999e-11,\n",
       "                        1.1455e-05,  9.4956e-06,  8.8068e-06,  1.1501e-08,  3.3969e-05,\n",
       "                        6.2716e-05, -1.2056e-08, -1.4061e-06, -9.8405e-06,  1.6129e-05,\n",
       "                        2.6670e-05,  1.5320e-05, -3.1611e-05,  3.7323e-05, -4.1606e-16,\n",
       "                       -2.4092e-05, -4.9012e-05, -6.6779e-08,  6.7076e-06, -4.4948e-05,\n",
       "                        5.6028e-07, -1.0364e-05,  1.9625e-15,  4.8977e-14,  1.0333e-07,\n",
       "                       -4.8102e-08,  1.6384e-12, -3.5463e-05, -8.3265e-10, -1.1076e-05,\n",
       "                        4.1107e-05, -3.6083e-05,  4.2263e-05,  1.7595e-06, -9.6313e-06,\n",
       "                       -3.9606e-05, -3.9761e-06, -2.1151e-14, -6.1701e-06,  4.4974e-06,\n",
       "                       -6.8773e-05,  1.4525e-07,  3.5633e-05,  2.3668e-06, -8.8391e-13,\n",
       "                        1.1460e-05,  3.1504e-05, -1.9531e-06,  3.2730e-08,  9.5344e-06,\n",
       "                       -1.7508e-08, -4.6311e-08, -2.7246e-05], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.bias',\n",
       "               tensor([-7.1050e-02, -2.8568e-01, -3.0045e-01, -2.4433e-01, -2.0564e-01,\n",
       "                       -2.9385e-01, -3.5109e-06, -2.8828e-01, -2.4123e-01, -1.0179e-08,\n",
       "                       -2.3399e-01, -5.9303e-02, -2.1817e-01, -2.2397e-06, -3.3646e-01,\n",
       "                       -2.4536e-06, -2.3926e-09, -2.9081e-02, -3.0497e-01, -2.2960e-01,\n",
       "                       -4.1852e-01, -2.5544e-02, -2.5996e-01, -2.8127e-01, -3.0474e-01,\n",
       "                       -7.0456e-03, -2.2467e-01, -2.7521e-02, -1.9459e-01, -3.0952e-01,\n",
       "                       -2.5993e-01, -4.2107e-07, -3.4347e-01, -3.5621e-01, -2.6498e-01,\n",
       "                       -2.8244e-01, -5.5044e-08, -3.9876e-01, -2.6307e-01, -7.2605e-08,\n",
       "                       -2.1502e-01, -1.4416e-06, -3.0399e-01, -1.9356e-01, -1.3294e-06,\n",
       "                       -2.0537e-02, -2.4425e-04, -1.9473e-01, -2.4553e-01, -3.1523e-01,\n",
       "                       -2.6063e-01, -2.1439e-02, -3.8598e-01, -2.1502e-02, -2.3911e-01,\n",
       "                       -1.5144e-05, -1.6368e-01, -2.5013e-02, -3.8905e-02, -7.8271e-03,\n",
       "                       -6.9043e-07, -1.0284e-09, -1.5251e-07, -3.9944e-01, -3.3973e-01,\n",
       "                       -3.0951e-01, -2.6852e-01, -1.3953e-01, -1.3830e-01, -3.3518e-02,\n",
       "                       -1.2434e-02, -2.2375e-01, -2.1066e-01, -3.7249e-01, -1.1836e-02,\n",
       "                       -2.7480e-01, -3.1704e-01, -2.3248e-01, -1.9509e-05, -1.8987e-01,\n",
       "                       -3.0153e-01, -4.6851e-04, -1.0231e-01, -3.2114e-01, -6.3072e-01,\n",
       "                       -2.0815e-01, -3.0224e-01, -4.1387e-01, -2.1373e-01, -1.5526e-06,\n",
       "                       -3.3092e-01, -3.4679e-01, -9.9268e-05, -3.8958e-01, -4.5395e-01,\n",
       "                       -9.3391e-03, -2.3692e-01, -1.1740e-08, -1.0550e-07, -1.1705e-01,\n",
       "                       -3.4503e-02, -4.4692e-09, -1.8899e-01, -5.4842e-09, -4.5216e-04,\n",
       "                       -2.9216e-01, -2.4470e-01, -1.9109e-01, -1.6255e-05, -3.5463e-01,\n",
       "                       -2.9440e-01, -3.0749e-01, -1.9956e-08, -3.6674e-02, -3.1833e-01,\n",
       "                       -2.3249e-01, -4.1730e-02, -2.5883e-01, -5.7486e-05, -2.1608e-04,\n",
       "                       -1.6771e-01, -2.3529e-01, -1.4864e-01, -1.5703e-02, -3.2254e-02,\n",
       "                       -2.6096e-02, -1.6035e-02, -4.7901e-01], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.weight',\n",
       "               tensor([-5.9789e-03,  4.7444e-01,  2.6138e-01,  2.9016e-01,  4.0032e-01,\n",
       "                        3.7839e-01,  6.3970e-05,  3.8827e-01,  4.4479e-01,  2.9253e-05,\n",
       "                        4.1520e-01, -9.4893e-03,  3.5128e-01, -6.0300e-05,  3.8119e-01,\n",
       "                        6.1730e-05,  3.5693e-06,  2.3226e-03,  2.6887e-01,  3.3415e-01,\n",
       "                        2.6425e-01,  7.8339e-03,  3.7583e-01,  5.7132e-01,  4.6166e-01,\n",
       "                       -9.4939e-04,  3.2631e-01,  8.3045e-04,  2.5066e-01,  2.1005e-01,\n",
       "                        2.6199e-01, -2.7706e-14,  2.6666e-01,  3.6367e-01,  4.4210e-01,\n",
       "                        2.6693e-01, -6.3785e-05,  4.0517e-01,  3.9314e-01, -9.3601e-06,\n",
       "                        3.0251e-01,  5.5068e-12,  3.8046e-01,  2.6482e-01, -4.8789e-12,\n",
       "                        4.0765e-01,  2.1273e-04,  2.2252e-01,  4.0039e-01,  4.4297e-01,\n",
       "                        2.9841e-01,  2.3273e-03,  5.7452e-01,  6.8827e-03,  4.9537e-01,\n",
       "                       -8.3743e-05,  3.1616e-01,  5.5658e-03,  5.4468e-03, -9.6078e-04,\n",
       "                        1.6075e-04, -3.2690e-06, -1.8535e-08,  5.1547e-01,  3.1001e-01,\n",
       "                        4.0982e-01,  5.1172e-01,  3.8695e-01,  2.4743e-01, -1.0951e-02,\n",
       "                       -1.3170e-03,  2.9696e-01,  3.0032e-01,  4.0071e-01,  3.2612e-04,\n",
       "                        2.8821e-01,  2.0725e-01,  1.6603e-01,  8.1681e-05,  3.7139e-01,\n",
       "                        2.7396e-01, -3.5161e-04,  1.8938e-01,  3.1237e-01,  7.8959e-01,\n",
       "                        3.4892e-01,  3.0580e-01,  4.2557e-01,  3.9026e-01,  1.9292e-11,\n",
       "                        3.8688e-01,  4.5357e-01,  1.4255e-04,  5.6373e-01,  4.3227e-01,\n",
       "                        1.0990e-03,  5.5933e-01,  1.0114e-09,  6.0646e-13,  2.2762e-01,\n",
       "                       -3.9441e-03,  2.6566e-06,  3.1990e-01,  2.5421e-05, -2.7106e-04,\n",
       "                        2.9547e-01,  4.4139e-01,  2.9607e-01, -7.5767e-11,  4.1539e-01,\n",
       "                        4.0492e-01,  4.1748e-01,  3.1012e-09,  4.5895e-03,  3.7506e-01,\n",
       "                        3.3594e-01,  1.1787e-02,  3.3017e-01, -1.4841e-10, -3.5406e-08,\n",
       "                        2.7691e-01,  3.3514e-01,  7.3610e-02, -2.0607e-03, -5.7059e-03,\n",
       "                       -2.4288e-03, -1.6261e-03,  4.1020e-01], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.weight',\n",
       "               tensor([[[[-7.9618e-03, -1.0307e-02, -1.0710e-02],\n",
       "                         [ 7.2538e-04, -1.2854e-02,  9.0603e-05],\n",
       "                         [-7.2834e-03, -1.2725e-02,  5.5659e-03]],\n",
       "               \n",
       "                        [[ 7.5539e-02, -3.1164e-02,  5.5003e-03],\n",
       "                         [ 4.9267e-02, -6.5519e-03, -9.2343e-02],\n",
       "                         [ 2.0336e-02,  1.3542e-02, -6.2395e-02]],\n",
       "               \n",
       "                        [[ 7.2963e-02, -4.8609e-02, -5.6259e-03],\n",
       "                         [ 3.9995e-02, -7.6415e-02, -3.0648e-02],\n",
       "                         [ 2.2311e-02, -4.3356e-02, -1.5089e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 6.6736e-04,  9.8623e-04,  9.0306e-04],\n",
       "                         [ 3.4849e-04,  4.4768e-04,  4.2009e-04],\n",
       "                         [ 2.3350e-04,  2.6537e-04,  3.3479e-04]],\n",
       "               \n",
       "                        [[ 2.5090e-04,  1.3239e-04,  1.2628e-04],\n",
       "                         [-1.7790e-05, -1.8605e-05,  6.2129e-06],\n",
       "                         [ 5.3118e-06, -7.2831e-05, -1.8537e-05]],\n",
       "               \n",
       "                        [[ 2.3296e-02, -1.8061e-02,  3.9821e-02],\n",
       "                         [ 2.2150e-03, -3.0748e-02,  8.8412e-03],\n",
       "                         [ 2.5338e-02,  2.2263e-02,  4.3199e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.1727e-03, -5.3923e-03, -1.1305e-03],\n",
       "                         [-2.4228e-04, -1.5712e-03, -7.2579e-04],\n",
       "                         [-2.1041e-04, -4.3080e-04, -1.1439e-03]],\n",
       "               \n",
       "                        [[-5.7352e-02, -3.3954e-02, -3.1146e-02],\n",
       "                         [-2.8065e-02, -4.8249e-02,  1.0143e-02],\n",
       "                         [ 3.0325e-03, -3.1767e-02,  3.3818e-02]],\n",
       "               \n",
       "                        [[ 3.8020e-03,  1.2874e-01,  4.1448e-02],\n",
       "                         [ 3.0296e-02,  1.4681e-01,  1.1165e-02],\n",
       "                         [ 3.1029e-02,  7.4158e-02, -1.5005e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.6671e-04, -8.6076e-05, -6.9636e-04],\n",
       "                         [ 1.5311e-04,  2.1452e-04, -4.2367e-04],\n",
       "                         [-1.0035e-04,  2.6002e-05, -4.9563e-04]],\n",
       "               \n",
       "                        [[-2.2296e-04, -1.6643e-04, -4.5937e-04],\n",
       "                         [ 3.8451e-05,  7.5299e-05, -3.1920e-04],\n",
       "                         [-1.1005e-04, -6.2131e-05, -3.9386e-04]],\n",
       "               \n",
       "                        [[-1.5306e-02, -4.9539e-02, -3.6840e-02],\n",
       "                         [ 6.8604e-02, -1.9385e-02, -4.5311e-02],\n",
       "                         [ 2.8976e-02, -8.5431e-03,  1.7028e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 2.4055e-02,  1.0360e-02, -1.8612e-02],\n",
       "                         [ 1.7364e-02,  2.1633e-02,  7.9329e-03],\n",
       "                         [ 6.6560e-03,  1.6941e-02,  2.6198e-02]],\n",
       "               \n",
       "                        [[-1.5228e-01, -1.1635e-01,  7.5360e-02],\n",
       "                         [-5.5590e-02, -1.3062e-01,  8.9956e-03],\n",
       "                         [ 9.9610e-02, -1.9794e-03, -1.1341e-02]],\n",
       "               \n",
       "                        [[ 9.6152e-02,  1.3521e-01, -1.3460e-01],\n",
       "                         [ 5.4868e-02,  7.7807e-02, -5.2707e-03],\n",
       "                         [ 7.6686e-02,  8.6121e-02,  3.0323e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.7737e-04,  2.5883e-05,  1.0721e-03],\n",
       "                         [ 1.4281e-04, -1.0616e-04,  8.6017e-04],\n",
       "                         [ 4.4274e-04,  1.6353e-05,  9.1530e-04]],\n",
       "               \n",
       "                        [[-5.8509e-05, -1.0410e-04,  6.4770e-04],\n",
       "                         [-3.6777e-05, -1.7024e-04,  4.8200e-04],\n",
       "                         [ 2.9372e-04, -1.1582e-04,  4.8196e-04]],\n",
       "               \n",
       "                        [[-3.0448e-02,  5.2196e-02,  2.4823e-02],\n",
       "                         [-1.0969e-01, -5.3867e-02,  4.3040e-02],\n",
       "                         [-3.8322e-03, -1.6889e-01,  7.8864e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-4.1778e-03, -7.1020e-04,  2.5066e-03],\n",
       "                         [-9.6838e-03, -2.2944e-02, -1.1217e-02],\n",
       "                         [-4.7991e-03, -3.1668e-02,  8.3459e-04]],\n",
       "               \n",
       "                        [[ 2.9581e-02, -5.8125e-02,  7.2883e-03],\n",
       "                         [ 3.0763e-03, -2.9443e-02,  1.0412e-01],\n",
       "                         [-3.1444e-02,  7.9190e-02,  1.3048e-02]],\n",
       "               \n",
       "                        [[ 1.9943e-02,  1.4246e-02, -5.5858e-02],\n",
       "                         [-3.1656e-02, -5.0902e-02, -9.4191e-02],\n",
       "                         [ 8.5282e-02,  3.9418e-02, -2.7749e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.1535e-04,  1.0935e-04,  8.5999e-05],\n",
       "                         [ 7.5970e-05,  1.4729e-04,  1.4102e-04],\n",
       "                         [-2.4319e-04,  5.3871e-05,  6.1072e-05]],\n",
       "               \n",
       "                        [[ 1.8635e-04,  1.9288e-04,  9.2689e-05],\n",
       "                         [ 1.5654e-04,  1.7202e-04,  1.0257e-04],\n",
       "                         [ 2.4399e-05,  6.0904e-05, -2.5058e-05]],\n",
       "               \n",
       "                        [[-1.6279e-01, -6.1513e-02, -6.0562e-02],\n",
       "                         [-1.6022e-01,  1.0266e-01, -4.1258e-02],\n",
       "                         [-2.1225e-02,  1.4143e-01,  7.4169e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 5.5794e-03,  8.1139e-04,  1.9290e-03],\n",
       "                         [ 1.9397e-02,  3.7653e-03,  6.4681e-04],\n",
       "                         [ 2.2436e-02,  1.9790e-03, -5.8910e-04]],\n",
       "               \n",
       "                        [[ 5.0188e-02,  3.1155e-02, -4.9794e-02],\n",
       "                         [-7.5795e-02, -5.0637e-02, -1.2471e-02],\n",
       "                         [ 3.4755e-02, -4.1632e-02, -3.5276e-02]],\n",
       "               \n",
       "                        [[-4.4845e-02,  1.6986e-02,  2.6441e-02],\n",
       "                         [-1.4164e-01, -4.5180e-02, -1.8914e-02],\n",
       "                         [-1.0707e-01, -2.2484e-02,  6.2197e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.8391e-04, -1.7204e-04, -3.1792e-04],\n",
       "                         [-2.9703e-04, -3.2909e-04, -6.1397e-04],\n",
       "                         [-6.1012e-04, -6.3779e-04, -1.0947e-03]],\n",
       "               \n",
       "                        [[-7.8783e-05, -8.1970e-05, -2.5310e-04],\n",
       "                         [-6.1571e-05, -1.5967e-04, -3.6162e-04],\n",
       "                         [-2.5682e-04, -3.6125e-04, -5.4483e-04]],\n",
       "               \n",
       "                        [[-4.4888e-02, -3.3983e-02, -5.1432e-02],\n",
       "                         [-1.4066e-02, -7.9896e-03, -5.4974e-02],\n",
       "                         [ 1.1104e-01,  1.0678e-01, -2.1346e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 2.6150e-03,  9.5080e-03,  3.8706e-03],\n",
       "                         [-5.7854e-04,  6.3734e-03,  4.9505e-03],\n",
       "                         [-3.4849e-04,  4.4767e-03,  2.7746e-03]],\n",
       "               \n",
       "                        [[-2.0307e-02, -4.8043e-02,  6.0700e-02],\n",
       "                         [-3.3075e-02, -1.4555e-02, -3.2844e-02],\n",
       "                         [ 1.3917e-02, -2.0795e-02, -7.0790e-02]],\n",
       "               \n",
       "                        [[-4.1874e-02, -6.6557e-02, -3.6402e-02],\n",
       "                         [-2.7391e-02, -7.0128e-02, -3.6931e-02],\n",
       "                         [-6.2165e-02, -5.8901e-02, -5.9478e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 3.9888e-04,  4.2869e-04,  5.2469e-04],\n",
       "                         [-5.1177e-05,  5.2602e-05,  2.4946e-04],\n",
       "                         [-1.0202e-04, -3.3488e-05,  2.4738e-04]],\n",
       "               \n",
       "                        [[ 3.7617e-05, -1.3405e-06,  1.0988e-04],\n",
       "                         [-1.3047e-04, -1.6869e-04, -7.4607e-05],\n",
       "                         [-1.5385e-05, -7.8338e-05,  1.3410e-05]],\n",
       "               \n",
       "                        [[-1.7427e-02, -9.7981e-03,  9.7489e-03],\n",
       "                         [-7.0243e-03, -3.2790e-02, -2.3833e-02],\n",
       "                         [ 3.1831e-02,  1.4522e-02, -2.7762e-02]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.bias',\n",
       "               tensor([-5.7232e-05,  4.6341e-05, -2.1574e-05,  1.7900e-05, -5.5158e-05,\n",
       "                        1.6586e-06, -3.2881e-05,  2.6940e-05,  6.4804e-05,  1.4678e-05,\n",
       "                       -3.1463e-05,  8.9905e-05, -9.0423e-06,  3.0986e-05, -1.1061e-04,\n",
       "                        1.9841e-08,  1.0399e-04, -3.4206e-05,  2.5227e-06,  9.1430e-06,\n",
       "                       -5.0984e-05,  3.6644e-05, -3.2539e-05, -2.7301e-05, -1.0996e-06,\n",
       "                        9.3714e-06,  4.6176e-06, -7.4281e-06, -2.1649e-08, -1.1856e-07,\n",
       "                       -1.0403e-06,  2.5514e-05, -2.4369e-05,  1.8870e-08, -3.1350e-05,\n",
       "                        5.4914e-05,  1.9493e-05,  8.8017e-05, -6.4239e-05, -5.2071e-05,\n",
       "                        3.0949e-07, -6.7790e-05,  7.0260e-07,  2.7873e-07, -1.5259e-05,\n",
       "                       -7.2484e-05,  8.7449e-05,  2.2523e-05, -2.5743e-05, -5.6032e-05,\n",
       "                        4.5626e-05, -3.5888e-05, -1.9130e-05,  5.1376e-05,  1.2180e-08,\n",
       "                        1.1825e-07,  1.3701e-06, -7.0230e-05, -1.1501e-05, -1.0830e-05,\n",
       "                        4.9489e-06, -4.9748e-05, -9.1965e-06,  3.5851e-06,  1.1346e-05,\n",
       "                        3.9100e-05, -9.9462e-10,  2.6427e-05, -5.5833e-05, -5.4469e-07,\n",
       "                        4.5782e-05, -8.1423e-05, -6.8505e-06,  4.0680e-05, -8.4042e-05,\n",
       "                        5.9575e-05, -1.5400e-06,  4.5432e-07, -5.5602e-06,  2.2433e-05,\n",
       "                       -1.0248e-05,  5.3307e-07,  5.1695e-06,  3.2929e-08, -1.0983e-05,\n",
       "                        6.1591e-05,  3.3802e-06,  1.7272e-07,  3.8020e-05, -3.9646e-05,\n",
       "                       -6.4780e-05, -3.5348e-05,  5.0129e-05,  1.5910e-05, -1.0823e-07,\n",
       "                       -1.4340e-05,  1.4134e-05,  6.1231e-05, -2.9073e-05, -2.6500e-05,\n",
       "                        3.8089e-05, -8.3867e-05, -3.0744e-06,  3.8994e-05, -1.4680e-12,\n",
       "                       -2.4687e-06,  4.9483e-09,  4.2275e-06,  7.1489e-05, -2.4528e-05,\n",
       "                        1.1266e-05,  3.6097e-06, -1.2965e-04, -1.0554e-04,  8.2715e-07,\n",
       "                        9.7029e-08,  7.9184e-06,  4.7391e-05, -5.0276e-07, -2.3261e-05,\n",
       "                        1.4799e-12, -2.6116e-05,  3.9046e-05, -1.2705e-07,  2.6476e-08,\n",
       "                        4.0102e-06,  9.0863e-05, -9.5779e-05], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.bias',\n",
       "               tensor([-3.6793e-01, -2.5078e-01, -6.0857e-01, -3.4833e-01, -3.0343e-01,\n",
       "                       -5.4664e-02, -2.8105e-01, -3.1427e-01, -4.5420e-01, -3.7782e-01,\n",
       "                       -3.4578e-01, -3.9379e-01, -6.7464e-01, -3.9033e-01, -4.1676e-01,\n",
       "                       -2.3141e-03, -5.6520e-01, -4.5738e-01, -1.9661e-01, -2.5970e-01,\n",
       "                       -4.5370e-01, -4.4177e-01, -5.9373e-01, -1.9711e-01, -8.4981e-05,\n",
       "                       -5.3690e-01, -3.5705e-01, -2.9346e-01, -4.0434e-02, -4.7633e-01,\n",
       "                       -4.4011e-01, -6.5173e-01, -5.3252e-01, -4.8892e-02, -9.5916e-02,\n",
       "                       -2.8359e-01, -3.2119e-01, -4.3438e-01,  1.2990e-01, -3.6271e-01,\n",
       "                       -1.9632e-04, -2.6723e-01, -8.5768e-04, -1.8851e-01, -5.7735e-01,\n",
       "                       -2.3583e-01, -5.0253e-01, -4.0657e-01, -3.2385e-01, -3.6909e-01,\n",
       "                       -4.0625e-01, -3.4221e-01, -1.8734e-01, -2.8141e-01, -1.9358e-02,\n",
       "                       -2.0658e-02, -3.2489e-03, -4.4883e-01, -5.5397e-01, -4.8946e-01,\n",
       "                       -1.3933e-02, -1.9047e-01, -3.3778e-01, -2.6645e-01, -2.4059e-01,\n",
       "                       -3.7889e-01, -9.8026e-04, -3.8562e-01, -3.1479e-01, -2.5518e-02,\n",
       "                       -2.9427e-01, -1.7783e-01, -5.1989e-01, -2.2470e-01, -3.6919e-01,\n",
       "                       -5.2954e-01, -4.2979e-01, -3.8721e-07, -2.8108e-01, -3.0248e-01,\n",
       "                       -4.1465e-01, -1.1636e-03, -3.0397e-01, -3.7652e-03, -3.5012e-01,\n",
       "                       -7.4918e-01, -5.0726e-01, -1.6459e-02, -3.0570e-01, -3.0129e-01,\n",
       "                       -3.6203e-01, -3.2020e-01, -2.3199e-01, -4.0858e-02, -1.5449e-02,\n",
       "                       -6.5301e-01, -4.9220e-01, -3.5197e-01, -3.5674e-03, -5.3459e-01,\n",
       "                       -3.4714e-03, -4.3443e-01, -2.0819e-01, -3.1391e-01,  1.2400e-03,\n",
       "                       -4.0252e-01, -5.1804e-07, -4.4944e-01, -4.0942e-01, -3.1685e-01,\n",
       "                       -3.9025e-01, -4.6038e-01, -3.8636e-01, -3.4741e-01, -1.8323e-02,\n",
       "                       -8.5692e-03, -2.1002e-03, -3.6438e-01, -3.6392e-01, -3.2574e-01,\n",
       "                       -3.5693e-08, -2.5504e-01, -4.0166e-01, -4.4182e-02, -1.1674e-05,\n",
       "                       -3.2704e-01, -3.7845e-01, -2.6078e-01], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.weight',\n",
       "               tensor([ 2.6118e-01,  2.3350e-01,  5.7988e-01,  2.7211e-01,  2.2198e-01,\n",
       "                       -1.3246e-02,  2.4754e-01,  2.1719e-01,  3.7699e-01,  2.5617e-01,\n",
       "                        2.2192e-01,  2.2337e-01,  4.2243e-01,  3.3865e-01,  3.2711e-01,\n",
       "                        8.0174e-04,  5.5772e-01,  3.5114e-01,  3.2852e-02,  1.9912e-01,\n",
       "                        3.1192e-01,  2.8079e-01,  2.3343e-01,  1.5305e-01,  3.3190e-04,\n",
       "                        3.4006e-01,  3.0347e-01,  1.2316e-01, -1.6509e-03,  1.7622e-01,\n",
       "                        1.6566e-01,  5.5668e-01,  4.1835e-01, -2.7754e-03,  1.1541e-01,\n",
       "                        2.1683e-01,  2.7720e-01,  3.0041e-01, -6.1692e-02,  4.0094e-01,\n",
       "                       -4.4600e-04,  2.7032e-01,  4.3144e-04,  1.3897e-01,  4.7947e-01,\n",
       "                        1.6679e-01,  2.8605e-01,  2.8902e-01,  3.0256e-01,  2.6215e-01,\n",
       "                        3.2991e-01,  2.4515e-01,  2.5134e-01,  2.5278e-01, -2.0708e-05,\n",
       "                       -4.2267e-03, -2.2424e-04,  3.8888e-01,  3.9720e-01,  3.8523e-01,\n",
       "                        7.8523e-04,  2.0797e-01,  2.4187e-01,  1.7604e-01,  7.8201e-02,\n",
       "                        1.7666e-01, -1.2043e-04,  2.3630e-01,  2.8266e-01, -2.8021e-03,\n",
       "                        2.7550e-01,  1.9251e-01,  2.4976e-01,  1.4059e-01,  3.5615e-01,\n",
       "                        3.4520e-01,  1.5828e-01, -8.9218e-09,  3.3791e-01,  2.0077e-01,\n",
       "                        4.1727e-01,  2.5252e-04,  2.7778e-01,  1.1654e-03,  3.2495e-01,\n",
       "                        5.5609e-01,  1.4733e-01,  1.2040e-03,  3.1067e-01,  2.1333e-01,\n",
       "                        3.6544e-01,  2.6706e-01,  3.3311e-01, -1.4290e-03, -2.2980e-03,\n",
       "                        5.7983e-01,  4.0603e-01,  2.8037e-01, -4.3457e-04,  3.6420e-01,\n",
       "                        3.8785e-05,  3.7129e-01,  2.0461e-01,  2.2028e-01, -7.4169e-08,\n",
       "                        2.7166e-01, -7.1799e-05,  1.4868e-01,  4.3667e-01,  1.8317e-01,\n",
       "                        3.8402e-01,  2.1509e-01,  3.2254e-01,  2.2693e-01,  3.5307e-03,\n",
       "                        1.1079e-03,  7.1403e-04,  2.8571e-01,  3.0204e-01,  3.1859e-01,\n",
       "                       -3.0151e-05,  3.2693e-01,  3.5666e-01, -2.5269e-03, -1.2618e-04,\n",
       "                        2.7548e-01,  2.4226e-01,  1.7353e-01], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.weight',\n",
       "               tensor([[[[ 9.2306e-02,  3.1405e-02,  2.5851e-02],\n",
       "                         [ 3.6979e-02, -2.8270e-02,  5.0255e-02],\n",
       "                         [ 1.2573e-01, -2.0005e-02,  9.9708e-02]],\n",
       "               \n",
       "                        [[-2.6987e-02,  4.6603e-02, -2.7767e-02],\n",
       "                         [-5.6577e-03, -3.5402e-02, -6.4181e-02],\n",
       "                         [-3.6616e-02, -2.3870e-02, -5.0792e-02]],\n",
       "               \n",
       "                        [[ 9.3338e-02,  1.3554e-01,  7.9953e-02],\n",
       "                         [ 7.0715e-02,  6.0832e-02,  3.8987e-02],\n",
       "                         [ 3.8024e-02,  9.2472e-02,  3.9045e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-5.3909e-02, -7.9882e-02, -7.5643e-02],\n",
       "                         [-3.5632e-02, -6.5819e-02, -6.5240e-02],\n",
       "                         [-5.6156e-02, -2.8490e-02, -6.3526e-02]],\n",
       "               \n",
       "                        [[-6.9243e-02, -8.7856e-02, -9.3693e-02],\n",
       "                         [-4.5755e-02, -4.5908e-02, -6.1379e-02],\n",
       "                         [-8.4978e-02, -4.7056e-02, -7.5766e-02]],\n",
       "               \n",
       "                        [[ 5.6270e-02,  4.7896e-02,  1.1028e-01],\n",
       "                         [ 1.5233e-02, -3.2797e-02,  3.7918e-03],\n",
       "                         [ 7.3863e-02, -2.7613e-02, -3.1035e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 5.7249e-02,  5.4601e-02, -1.4537e-02],\n",
       "                         [ 4.9983e-02,  3.1788e-02,  3.1343e-02],\n",
       "                         [ 6.0028e-02,  2.9636e-02,  5.3472e-02]],\n",
       "               \n",
       "                        [[ 2.8263e-02,  7.9787e-03, -4.6547e-02],\n",
       "                         [ 1.8441e-02,  1.9782e-02, -5.9326e-02],\n",
       "                         [ 4.4507e-02,  2.0157e-02,  2.7760e-02]],\n",
       "               \n",
       "                        [[-9.8495e-02, -6.7864e-02, -1.1443e-01],\n",
       "                         [-5.5861e-02, -5.0672e-02, -5.0754e-02],\n",
       "                         [-6.1502e-02, -4.4263e-03, -5.1490e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.0104e-01, -1.0828e-01, -7.7966e-02],\n",
       "                         [-7.0038e-02, -5.2385e-02, -2.7292e-02],\n",
       "                         [-2.8369e-02, -4.6611e-02, -9.9328e-02]],\n",
       "               \n",
       "                        [[ 1.4721e-02,  2.0310e-02,  1.2649e-02],\n",
       "                         [ 3.8817e-03, -2.4070e-02, -1.1852e-02],\n",
       "                         [-3.4515e-03,  2.2552e-02,  8.8513e-03]],\n",
       "               \n",
       "                        [[ 4.9819e-02,  1.3658e-02, -3.7502e-02],\n",
       "                         [-5.3615e-02, -1.1591e-02, -2.0984e-02],\n",
       "                         [ 8.4612e-03, -2.3559e-02, -2.7210e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-2.1653e-06, -5.3333e-06, -2.3478e-06],\n",
       "                         [-3.0629e-06, -4.2582e-06, -1.5448e-06],\n",
       "                         [-5.5187e-06, -5.1821e-06, -5.2305e-06]],\n",
       "               \n",
       "                        [[ 5.5206e-07,  2.6511e-06,  2.1551e-08],\n",
       "                         [-5.0557e-07,  4.7116e-08,  8.7620e-07],\n",
       "                         [ 1.1069e-06,  4.2049e-07,  2.1533e-06]],\n",
       "               \n",
       "                        [[ 1.6030e-07,  2.1516e-06,  3.7841e-07],\n",
       "                         [ 7.6932e-07,  1.8074e-06,  1.1762e-06],\n",
       "                         [ 2.5866e-06,  1.2673e-06,  2.1761e-06]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.5814e-06,  5.0941e-06,  2.3740e-06],\n",
       "                         [ 6.3715e-06,  5.2135e-06,  4.3584e-06],\n",
       "                         [ 3.6965e-06,  3.8096e-07,  1.4994e-06]],\n",
       "               \n",
       "                        [[ 2.2769e-06, -1.4335e-07,  6.4041e-07],\n",
       "                         [-1.3904e-06,  2.3179e-06,  1.0245e-06],\n",
       "                         [ 2.6789e-07,  1.0616e-06, -2.3248e-07]],\n",
       "               \n",
       "                        [[ 7.6043e-06,  5.5132e-06,  2.7563e-06],\n",
       "                         [ 1.7941e-06,  3.8328e-06,  4.6400e-06],\n",
       "                         [-4.1896e-06, -3.3669e-06, -2.0409e-06]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 1.1513e-09,  1.0641e-09,  9.0059e-10],\n",
       "                         [ 1.5473e-09,  1.3218e-09,  7.9299e-10],\n",
       "                         [-3.5958e-11,  6.7503e-10, -1.7043e-10]],\n",
       "               \n",
       "                        [[ 7.1283e-10,  5.1583e-10,  4.0271e-10],\n",
       "                         [ 7.2088e-10,  2.3650e-10,  3.3552e-10],\n",
       "                         [ 6.2594e-10,  3.6621e-10,  7.7113e-11]],\n",
       "               \n",
       "                        [[-7.9805e-10, -3.5025e-10, -2.3844e-10],\n",
       "                         [-7.4426e-10,  2.6173e-10, -2.1093e-10],\n",
       "                         [-2.7085e-10, -1.1699e-11, -1.5721e-10]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.8463e-09, -1.5110e-09, -3.3019e-10],\n",
       "                         [-1.0315e-09, -6.3924e-10, -1.0950e-09],\n",
       "                         [-1.8514e-09, -1.4591e-09, -2.3607e-09]],\n",
       "               \n",
       "                        [[-9.1216e-10, -1.1352e-09, -1.1099e-09],\n",
       "                         [-6.7320e-10, -1.4215e-10, -6.1027e-10],\n",
       "                         [-1.0149e-09, -7.1730e-10, -1.0377e-09]],\n",
       "               \n",
       "                        [[-7.1667e-10, -3.3896e-10, -4.7490e-10],\n",
       "                         [-1.3766e-11,  2.0905e-10,  7.3043e-10],\n",
       "                         [-5.8078e-10, -6.6410e-10, -7.7109e-12]]],\n",
       "               \n",
       "               \n",
       "                       [[[-3.4963e-03, -5.6492e-03,  4.3915e-03],\n",
       "                         [-2.0045e-03,  7.4174e-04,  5.0947e-03],\n",
       "                         [ 5.3926e-03, -8.3320e-03,  1.5928e-03]],\n",
       "               \n",
       "                        [[ 5.4547e-03, -2.9699e-03,  1.2310e-03],\n",
       "                         [ 1.6754e-02, -3.8871e-03,  6.8030e-03],\n",
       "                         [ 7.6962e-03, -2.1203e-03, -4.6658e-04]],\n",
       "               \n",
       "                        [[-5.2574e-03,  3.6917e-03, -1.1505e-02],\n",
       "                         [-7.3070e-03, -3.8786e-03,  7.5933e-03],\n",
       "                         [-8.4547e-04,  5.8873e-03,  3.6578e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.4192e-02,  1.5367e-02, -4.4071e-05],\n",
       "                         [ 8.6418e-03,  1.0731e-02,  7.7944e-03],\n",
       "                         [ 1.1998e-02,  5.7061e-03,  6.0919e-03]],\n",
       "               \n",
       "                        [[-3.6848e-02, -3.0675e-02, -2.7630e-02],\n",
       "                         [-2.4743e-02, -2.1992e-02, -2.5960e-02],\n",
       "                         [-1.5858e-02, -1.7806e-02, -2.0496e-02]],\n",
       "               \n",
       "                        [[-1.4760e-02, -8.9201e-04, -7.4672e-03],\n",
       "                         [-1.0732e-02, -1.2150e-02, -2.1480e-02],\n",
       "                         [-2.1063e-02, -2.4913e-03, -7.9468e-03]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 3.4939e-02,  1.9619e-02,  2.0320e-02],\n",
       "                         [ 6.3129e-03, -3.2009e-02,  1.8124e-02],\n",
       "                         [ 4.6580e-04, -1.6146e-03,  7.9173e-02]],\n",
       "               \n",
       "                        [[-5.3502e-02, -2.2448e-03,  2.8876e-02],\n",
       "                         [-2.1427e-02,  8.2426e-03,  1.4897e-02],\n",
       "                         [-1.4253e-02, -6.7697e-04,  9.1195e-03]],\n",
       "               \n",
       "                        [[-7.7495e-02, -6.2015e-02, -7.5262e-02],\n",
       "                         [-3.6331e-02,  7.9829e-03, -7.4145e-02],\n",
       "                         [-3.8464e-02, -5.2995e-02, -7.4912e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-5.3950e-02,  2.7541e-03,  9.8277e-03],\n",
       "                         [-2.0796e-02, -6.2982e-02,  9.5049e-03],\n",
       "                         [-2.6093e-02,  2.2936e-02,  2.4942e-02]],\n",
       "               \n",
       "                        [[-6.7340e-02, -1.5071e-02, -5.4728e-02],\n",
       "                         [-5.6909e-02, -3.4358e-02, -4.8884e-02],\n",
       "                         [-1.5749e-02, -1.4201e-02, -5.4528e-02]],\n",
       "               \n",
       "                        [[ 3.0562e-02,  1.2157e-02,  9.6003e-04],\n",
       "                         [ 1.1526e-02,  5.3747e-03, -5.1139e-02],\n",
       "                         [ 3.9405e-02,  7.5291e-02,  8.8668e-02]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.bias',\n",
       "               tensor([ 7.1086e-05,  4.1761e-05, -3.3488e-05, -1.1207e-10, -1.4413e-14,\n",
       "                        2.2265e-06, -1.9731e-16, -9.6798e-05, -3.3294e-07, -8.2874e-07,\n",
       "                        2.5932e-05,  4.7807e-09, -2.0184e-08,  4.5603e-06,  3.1556e-07,\n",
       "                        7.1379e-05,  3.5153e-06, -6.1705e-05, -3.0333e-15,  3.8083e-05,\n",
       "                       -5.4894e-05,  6.6775e-06,  2.1882e-04,  9.2213e-09, -5.0698e-09,\n",
       "                       -1.3286e-04,  9.1595e-05,  2.9078e-05,  2.2191e-08, -1.1145e-10,\n",
       "                       -3.3226e-05,  1.0101e-06, -3.1314e-06, -4.5140e-13, -7.7711e-05,\n",
       "                       -7.1287e-15, -5.4302e-07,  4.8251e-13,  1.4154e-08, -8.5817e-13,\n",
       "                       -1.7394e-09,  6.9516e-08,  4.5858e-13, -3.4311e-14,  8.5754e-07,\n",
       "                       -1.8612e-07, -4.9425e-06,  1.0168e-06, -7.0248e-05,  2.6708e-08,\n",
       "                        4.5055e-06, -1.3672e-07, -5.8786e-11, -6.4779e-14, -3.1396e-12,\n",
       "                        1.5401e-14,  1.5581e-08,  2.5758e-06, -1.3309e-16,  2.9786e-06,\n",
       "                       -9.1326e-06,  5.8251e-12,  2.5886e-06,  1.0894e-06,  1.3938e-14,\n",
       "                        2.2405e-13,  1.0222e-07,  9.0305e-15, -6.1918e-05,  1.1342e-06,\n",
       "                       -7.2660e-06,  3.5228e-16,  4.4286e-09, -2.2639e-13, -8.2292e-11,\n",
       "                       -1.0820e-07,  8.3502e-14,  7.4042e-06,  2.3801e-07, -5.3963e-15,\n",
       "                       -1.0623e-07, -5.1845e-08,  1.9537e-16, -2.4388e-06, -5.1320e-06,\n",
       "                        4.0064e-06,  2.5010e-14,  3.4628e-06, -5.3572e-05, -1.5297e-09,\n",
       "                        7.8305e-15, -3.6146e-07, -8.8475e-05,  7.0723e-08,  4.7553e-16,\n",
       "                        1.0226e-07,  2.4888e-10, -5.2379e-05,  8.5860e-08,  3.7391e-08,\n",
       "                       -9.5655e-08,  2.1949e-15, -1.4282e-10,  1.1953e-11, -3.7114e-08,\n",
       "                        9.7119e-15,  1.3559e-16, -8.6764e-09, -1.0559e-07,  9.8831e-05,\n",
       "                        5.5652e-05,  3.7492e-10,  6.5280e-05,  2.4272e-13,  1.4539e-06,\n",
       "                        1.3779e-06,  1.9746e-08,  3.9895e-05, -4.7387e-08,  3.0416e-07,\n",
       "                        1.1192e-13,  1.4086e-04, -1.3280e-12, -2.2743e-11,  2.5483e-08,\n",
       "                       -6.8651e-12,  8.0021e-07,  6.0942e-05], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.bias',\n",
       "               tensor([-4.4336e-01, -1.8426e-01, -9.4879e-03, -1.2074e-05, -1.0193e-28,\n",
       "                       -2.8042e-03, -1.8947e-10, -5.2389e-01, -1.8011e-20, -2.0039e-01,\n",
       "                       -5.9793e-15, -6.3029e-05, -1.8357e-03, -8.2478e-04, -2.2283e-02,\n",
       "                       -2.3554e-01, -1.8663e-02, -2.8657e-01, -3.5288e-11,  1.4671e-03,\n",
       "                       -1.5605e-01, -4.7839e-02,  1.2226e-01, -1.9319e-11, -1.8358e-04,\n",
       "                       -2.1880e-01, -2.7861e-01, -1.8724e-01, -2.8786e-07, -5.3529e-09,\n",
       "                       -8.5241e-02,  1.2097e-02, -2.8345e-11, -2.8498e-23, -1.7622e-01,\n",
       "                       -2.4225e-23, -6.3115e-02, -2.2055e-23, -4.6684e-03, -4.6376e-11,\n",
       "                       -1.3753e-17, -2.3315e-02, -2.3333e-13, -1.2601e-09, -3.0292e-03,\n",
       "                       -7.6780e-19, -7.1590e-03, -1.1625e-05, -2.1551e-01, -1.6392e-02,\n",
       "                       -1.6504e-01, -4.2713e-08, -4.8727e-16, -2.9348e-11, -4.9517e-21,\n",
       "                       -1.0596e-10, -1.2045e-03, -5.5859e-03, -3.1987e-09, -2.0443e-03,\n",
       "                       -1.4769e-09, -1.7610e-05, -5.9926e-03, -3.0276e-02, -2.0397e-09,\n",
       "                       -1.1094e-05, -5.7930e-07, -6.2695e-13, -2.0573e-01, -3.0874e-03,\n",
       "                       -4.6876e-05, -1.3416e-16, -1.2793e-12, -1.2430e-32, -3.0081e-07,\n",
       "                       -2.6244e-02, -7.6310e-09, -1.5659e-16, -1.4760e-03, -1.8187e-18,\n",
       "                       -2.7925e-02, -5.1396e-05, -2.6036e-18, -6.9737e-03, -8.6194e-08,\n",
       "                       -3.0572e-05, -7.2585e-04, -5.9148e-04, -3.3466e-02, -9.1938e-07,\n",
       "                       -1.4283e-23, -1.3720e-02, -2.0719e-01, -1.5796e-02, -9.2563e-21,\n",
       "                       -5.5906e-12, -1.1390e-04, -4.3634e-01, -1.9106e-03, -1.8218e-12,\n",
       "                       -1.6545e-03, -1.4817e-27, -2.8155e-11, -3.8991e-05, -2.3081e-02,\n",
       "                       -1.7477e-07, -3.5630e-12, -3.0078e-05, -1.4274e-15, -3.6920e-01,\n",
       "                       -4.3953e-01, -2.2798e-05, -7.6253e-03, -6.7545e-27, -3.9434e-09,\n",
       "                       -5.0921e-12, -6.2027e-22,  3.8567e-02, -4.1460e-03, -2.0830e-02,\n",
       "                       -3.3308e-15, -7.7934e-02, -6.9630e-07, -5.6818e-06, -3.1065e-05,\n",
       "                       -3.7242e-06, -4.9954e-02, -1.1988e-01], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.weight',\n",
       "               tensor([ 2.7865e+00,  2.3477e+00, -2.9660e-03,  4.7420e-06,  1.3315e-33,\n",
       "                       -5.2886e-04,  3.4483e-13,  3.5035e+00, -3.6828e-26,  1.3801e-01,\n",
       "                       -1.7974e-19,  1.0362e-11, -4.8167e-04, -7.3824e-04,  2.5542e-04,\n",
       "                        2.3949e+00, -8.3279e-03,  2.5299e+00,  1.3233e-08,  2.5466e+00,\n",
       "                        2.1848e+00,  2.1919e+00,  3.0513e+00,  3.1978e-08,  1.3201e-04,\n",
       "                        2.8436e+00,  2.4064e+00,  2.3961e+00,  3.7781e-05,  7.6418e-07,\n",
       "                        2.1816e+00, -3.8686e-03, -2.0418e-06,  2.5006e-29,  1.9513e+00,\n",
       "                        2.6672e-24,  1.4272e-02, -9.9188e-21, -1.1772e-06,  4.3492e-08,\n",
       "                       -3.5795e-13,  2.8694e-03,  2.0699e-10,  1.0526e-10, -1.8226e-03,\n",
       "                       -2.8756e-24,  6.1446e-04,  3.4037e-04,  2.2632e+00, -2.7331e-03,\n",
       "                        5.9289e-02,  2.7982e-06, -2.0300e-13, -2.1887e-07, -1.9669e-17,\n",
       "                        4.5760e-07,  1.8533e-07, -4.7694e-04,  2.3699e-15,  1.2038e-03,\n",
       "                        1.1642e-06,  5.2536e-05,  4.0808e-03,  1.7506e-03,  4.9522e-08,\n",
       "                        9.5731e-12,  8.0844e-08,  4.8068e-18,  2.1510e+00,  8.1761e-05,\n",
       "                        1.2020e-10, -1.9447e-13, -9.4218e-09, -2.8739e-32,  3.0206e-07,\n",
       "                       -5.0582e-03,  3.2982e-07, -1.2039e-12,  6.1100e-04, -8.4132e-22,\n",
       "                       -4.7784e-03,  3.2294e-04, -5.1082e-19, -7.2187e-04,  5.2295e-06,\n",
       "                        5.5855e-06,  5.1062e-11, -4.4068e-04,  2.5744e+00,  3.6293e-05,\n",
       "                        1.1162e-29,  2.3608e-03,  2.4040e+00,  4.8619e-03,  1.7786e-22,\n",
       "                        2.0778e-08,  4.1998e-04,  2.4967e+00,  1.3835e-03,  4.5247e-05,\n",
       "                        1.6900e-03, -9.8782e-34, -2.7468e-10,  4.0194e-05, -5.9263e-04,\n",
       "                        1.6720e-13, -4.2262e-10,  9.3691e-05,  3.2333e-10,  1.2377e+00,\n",
       "                        2.9242e+00,  4.5986e-05,  2.2229e+00,  7.7827e-32, -8.4150e-16,\n",
       "                        1.0199e-08,  3.1750e-28,  2.7055e+00, -1.7634e-03,  4.7701e-03,\n",
       "                        1.6196e-22,  2.5035e+00, -1.8688e-06, -3.1556e-05,  4.3079e-05,\n",
       "                       -9.2320e-05,  2.0012e-03,  2.0136e+00], device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.weights',\n",
       "               tensor([[-0.0288, -0.0132,  0.0064,  ..., -0.0017,  0.0005, -0.0067],\n",
       "                       [ 0.0043,  0.0113, -0.0058,  ...,  0.0026,  0.0082,  0.0185],\n",
       "                       [ 0.0073,  0.0126,  0.0116,  ...,  0.0016,  0.0081, -0.0013],\n",
       "                       [-0.0014, -0.0110, -0.0113,  ..., -0.0180, -0.0025, -0.0331],\n",
       "                       [ 0.0194, -0.0161,  0.0004,  ..., -0.0007,  0.0024, -0.0065]],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.bias',\n",
       "               tensor([-0.1974,  0.2965,  0.4488, -0.2723, -0.2486], device='cuda:0'))]),\n",
       " 'per_epoch_statistics': {'train_loss_mean': [1.3964135072231292,\n",
       "   1.2752144742012024,\n",
       "   1.2302939254045486,\n",
       "   1.1908126748800278,\n",
       "   1.148055531144142,\n",
       "   1.1200508754253387,\n",
       "   1.0946748892068863,\n",
       "   1.0646279028654098,\n",
       "   1.0476867884397507,\n",
       "   1.0374736145734786,\n",
       "   0.9953752520084381,\n",
       "   0.9825711916685105,\n",
       "   0.9757475962638855,\n",
       "   0.9312514889240265,\n",
       "   0.9124414831399917,\n",
       "   0.9021071534156799,\n",
       "   0.8772549514770508,\n",
       "   0.8682146328687668,\n",
       "   0.8702834696769715,\n",
       "   0.8515160855054855,\n",
       "   0.8389425154924393,\n",
       "   0.8066611856222152,\n",
       "   0.8049740948677063,\n",
       "   0.806400563776493,\n",
       "   0.7971608828902245,\n",
       "   0.772989372074604,\n",
       "   0.7689504561424255,\n",
       "   0.7663676652908326,\n",
       "   0.7626488138437271,\n",
       "   0.7678568828701973,\n",
       "   0.7480019012093544,\n",
       "   0.7394276539087296,\n",
       "   0.7436857013702393,\n",
       "   0.7302546924948692,\n",
       "   0.7499731214046478,\n",
       "   0.7065267487168312,\n",
       "   0.7157375577688218,\n",
       "   0.7128869076967239,\n",
       "   0.7187631876468659,\n",
       "   0.6997101403474808,\n",
       "   0.7128469305634498,\n",
       "   0.6915928853750228,\n",
       "   0.6988294006586074,\n",
       "   0.7024870350956917,\n",
       "   0.6929781484603882,\n",
       "   0.6776044164896011,\n",
       "   0.6820184682011604,\n",
       "   0.6726458131074905,\n",
       "   0.679235668182373,\n",
       "   0.6713978750705719,\n",
       "   0.6813826475143433,\n",
       "   0.6606179778575897,\n",
       "   0.6675700828433037,\n",
       "   0.65941478484869,\n",
       "   0.6741656668782234,\n",
       "   0.6630126795172692,\n",
       "   0.6566157265901565,\n",
       "   0.6482015027999878,\n",
       "   0.6564260697364808,\n",
       "   0.642287045121193,\n",
       "   0.6541453097462654,\n",
       "   0.6396252736449242,\n",
       "   0.6334865258336068,\n",
       "   0.6313861788511276,\n",
       "   0.6381710269451142,\n",
       "   0.641573390841484,\n",
       "   0.6476540412902833,\n",
       "   0.6326918061375618,\n",
       "   0.6284828606247902,\n",
       "   0.6395158768892288,\n",
       "   0.6293193819522858,\n",
       "   0.6271663135290146,\n",
       "   0.639690104007721,\n",
       "   0.6262340896129608,\n",
       "   0.6221976612210274,\n",
       "   0.619012985765934,\n",
       "   0.6285568754673004,\n",
       "   0.6210005061626435,\n",
       "   0.6118246250450611,\n",
       "   0.6274391765892505,\n",
       "   0.611426229596138,\n",
       "   0.616690392434597,\n",
       "   0.6225437664985657,\n",
       "   0.6141085449457169,\n",
       "   0.6012782045006752,\n",
       "   0.6090472363829613,\n",
       "   0.6149458696246147,\n",
       "   0.6095568494796753,\n",
       "   0.6076630163788795,\n",
       "   0.6022856568098068,\n",
       "   0.6044286361038684,\n",
       "   0.6053322129249573,\n",
       "   0.5971787528991699,\n",
       "   0.6081058636307717,\n",
       "   0.5975217327475548,\n",
       "   0.6160425456762314,\n",
       "   0.6105331205129624,\n",
       "   0.5957160738706588,\n",
       "   0.5858395889103413],\n",
       "  'train_loss_std': [0.18034262879829238,\n",
       "   0.13085841462318726,\n",
       "   0.13671169889169718,\n",
       "   0.13749199748237834,\n",
       "   0.1383487280900498,\n",
       "   0.13888812696386707,\n",
       "   0.1405895665114901,\n",
       "   0.14168200170506084,\n",
       "   0.14271376891016163,\n",
       "   0.1395350461521767,\n",
       "   0.14092269196794174,\n",
       "   0.1473270843874367,\n",
       "   0.13766863450293532,\n",
       "   0.14623473479503182,\n",
       "   0.139881839027217,\n",
       "   0.14427419582848017,\n",
       "   0.13899572573253233,\n",
       "   0.14389969001745198,\n",
       "   0.1476639803246798,\n",
       "   0.14290961968885105,\n",
       "   0.14749169632899833,\n",
       "   0.1356692068974213,\n",
       "   0.14575795679118683,\n",
       "   0.15477704656986327,\n",
       "   0.1428941119426693,\n",
       "   0.1412216353386054,\n",
       "   0.14813135414991208,\n",
       "   0.14884168262400338,\n",
       "   0.14549696402412407,\n",
       "   0.15027502522276887,\n",
       "   0.14681948594128547,\n",
       "   0.14048119871442416,\n",
       "   0.14397061818949364,\n",
       "   0.15034243380575987,\n",
       "   0.14100472736727454,\n",
       "   0.1421494440562852,\n",
       "   0.1347376981783929,\n",
       "   0.14544151399678112,\n",
       "   0.15118340917462422,\n",
       "   0.15325231147122387,\n",
       "   0.13857153427209543,\n",
       "   0.1365381862408919,\n",
       "   0.1473061630451568,\n",
       "   0.1424103430658685,\n",
       "   0.142155866314491,\n",
       "   0.13668289610685774,\n",
       "   0.14852465853151756,\n",
       "   0.1424556989671052,\n",
       "   0.14293652149921016,\n",
       "   0.1441498263650176,\n",
       "   0.1374526803318021,\n",
       "   0.13907153742697262,\n",
       "   0.13855879428084655,\n",
       "   0.14350404604842493,\n",
       "   0.14665772439753447,\n",
       "   0.14742085922159376,\n",
       "   0.13696016936077962,\n",
       "   0.14042545162901318,\n",
       "   0.1350976296557966,\n",
       "   0.13392141004413086,\n",
       "   0.14335212050548876,\n",
       "   0.14351735495139542,\n",
       "   0.13964731951603318,\n",
       "   0.14274111770016706,\n",
       "   0.14186492093064346,\n",
       "   0.13485535483477587,\n",
       "   0.1496225473664019,\n",
       "   0.1337542725677398,\n",
       "   0.14187874524432748,\n",
       "   0.14852862821645793,\n",
       "   0.13828002378415555,\n",
       "   0.1405839987682479,\n",
       "   0.14154859282103457,\n",
       "   0.13664076935729041,\n",
       "   0.14120246134271114,\n",
       "   0.13669215441183252,\n",
       "   0.14397666963166209,\n",
       "   0.1387662110938472,\n",
       "   0.13567112049758565,\n",
       "   0.14939664868651478,\n",
       "   0.14268159583191006,\n",
       "   0.13776530456410668,\n",
       "   0.14684631567477086,\n",
       "   0.13725201549745347,\n",
       "   0.13870710280749501,\n",
       "   0.1330645074038315,\n",
       "   0.14161221737328378,\n",
       "   0.14067829265788093,\n",
       "   0.13998369897861898,\n",
       "   0.13480723207631004,\n",
       "   0.13396932271735387,\n",
       "   0.13710519959936593,\n",
       "   0.13038069851855505,\n",
       "   0.14408017058487593,\n",
       "   0.13342055483554927,\n",
       "   0.1440402915977341,\n",
       "   0.13305246472261162,\n",
       "   0.13562821619435722,\n",
       "   0.13205924670849029],\n",
       "  'train_accuracy_mean': [0.43690666678547857,\n",
       "   0.4812533329129219,\n",
       "   0.5039999997615814,\n",
       "   0.5242266666293144,\n",
       "   0.5432933316230774,\n",
       "   0.5558266670107842,\n",
       "   0.5710933329463005,\n",
       "   0.5816666659116745,\n",
       "   0.5910133324861526,\n",
       "   0.5945599983930587,\n",
       "   0.6137866659760475,\n",
       "   0.6212799989581108,\n",
       "   0.6205733323097229,\n",
       "   0.6407066655755043,\n",
       "   0.6521733323335648,\n",
       "   0.6545199990868569,\n",
       "   0.6636799993515015,\n",
       "   0.6695466663837433,\n",
       "   0.669053334236145,\n",
       "   0.6766266667246819,\n",
       "   0.6799999992847443,\n",
       "   0.6947466654777527,\n",
       "   0.6952266674041748,\n",
       "   0.6940266669392586,\n",
       "   0.699506666302681,\n",
       "   0.707240001142025,\n",
       "   0.7125866671800614,\n",
       "   0.713519998908043,\n",
       "   0.713773333966732,\n",
       "   0.7099466660022735,\n",
       "   0.7185333332419396,\n",
       "   0.721853334069252,\n",
       "   0.7215999999642372,\n",
       "   0.725426666855812,\n",
       "   0.7166933326721191,\n",
       "   0.7347599999904633,\n",
       "   0.7315066664218902,\n",
       "   0.7342400006055831,\n",
       "   0.732306667983532,\n",
       "   0.7394266663789749,\n",
       "   0.7317066655158997,\n",
       "   0.7401866676807404,\n",
       "   0.7398000003099442,\n",
       "   0.73667999958992,\n",
       "   0.7395199983716011,\n",
       "   0.746799998998642,\n",
       "   0.7468799999952316,\n",
       "   0.7510533341169358,\n",
       "   0.7483733344078064,\n",
       "   0.7486400021314621,\n",
       "   0.7471599997282028,\n",
       "   0.756280000448227,\n",
       "   0.7518266675472259,\n",
       "   0.7548133343458175,\n",
       "   0.7466133320331574,\n",
       "   0.7533466668128967,\n",
       "   0.7583200006484986,\n",
       "   0.7597733340263366,\n",
       "   0.755226667046547,\n",
       "   0.7621066663265228,\n",
       "   0.7575733321905136,\n",
       "   0.7631999987363816,\n",
       "   0.7654799990653992,\n",
       "   0.765613332748413,\n",
       "   0.7627466659545898,\n",
       "   0.7619733327627182,\n",
       "   0.7584799988269806,\n",
       "   0.7663999998569488,\n",
       "   0.7680266671180725,\n",
       "   0.7625066655874252,\n",
       "   0.7658666670322418,\n",
       "   0.7674133323431015,\n",
       "   0.7640266679525375,\n",
       "   0.7669466658830643,\n",
       "   0.7696666657924652,\n",
       "   0.7713733330965042,\n",
       "   0.7659599999189377,\n",
       "   0.7703599990606308,\n",
       "   0.7747600004673004,\n",
       "   0.7665866668224335,\n",
       "   0.7748399993181229,\n",
       "   0.770479999780655,\n",
       "   0.7712933328151703,\n",
       "   0.7715466655492783,\n",
       "   0.7774399998188019,\n",
       "   0.7755466669797897,\n",
       "   0.7730933351516723,\n",
       "   0.7746533331871033,\n",
       "   0.7760533343553543,\n",
       "   0.7774933340549469,\n",
       "   0.7773733327388763,\n",
       "   0.7750800000429153,\n",
       "   0.7791466665267944,\n",
       "   0.7743200005292893,\n",
       "   0.780013332605362,\n",
       "   0.7719733321666717,\n",
       "   0.7745866675376892,\n",
       "   0.7790933343172073,\n",
       "   0.7843733330965043],\n",
       "  'train_accuracy_std': [0.07098566501571596,\n",
       "   0.07195327261265541,\n",
       "   0.07111430463232131,\n",
       "   0.07130280989462792,\n",
       "   0.06966139114976855,\n",
       "   0.06880249287915091,\n",
       "   0.07113402173578617,\n",
       "   0.07090760814269802,\n",
       "   0.0731582440462926,\n",
       "   0.06981822197835608,\n",
       "   0.07042202085890227,\n",
       "   0.07207007098085995,\n",
       "   0.06923538160568078,\n",
       "   0.07106765801772949,\n",
       "   0.06793582730646155,\n",
       "   0.07131224785906914,\n",
       "   0.0688674403622176,\n",
       "   0.06860300457810307,\n",
       "   0.07047705415056688,\n",
       "   0.06890443085160579,\n",
       "   0.0726538229681589,\n",
       "   0.06602459367128517,\n",
       "   0.06845707233240783,\n",
       "   0.07324030110803474,\n",
       "   0.06735528517502865,\n",
       "   0.06789357905275095,\n",
       "   0.06824643531552617,\n",
       "   0.06753936008875214,\n",
       "   0.06859142928006903,\n",
       "   0.07029902364538214,\n",
       "   0.06791746789831195,\n",
       "   0.06566876919411412,\n",
       "   0.06806349893328108,\n",
       "   0.06979060846474362,\n",
       "   0.06640280654771515,\n",
       "   0.06682255138878061,\n",
       "   0.06575203375324362,\n",
       "   0.06767028003838664,\n",
       "   0.0701801762433526,\n",
       "   0.06990870279571633,\n",
       "   0.0634393710979739,\n",
       "   0.06487979130725575,\n",
       "   0.06767146096417186,\n",
       "   0.06609017352234921,\n",
       "   0.06520082228732696,\n",
       "   0.06154008600710984,\n",
       "   0.06776133889745166,\n",
       "   0.06398369090843131,\n",
       "   0.06440374832053412,\n",
       "   0.06681596213896443,\n",
       "   0.06269805014029385,\n",
       "   0.062428852753394964,\n",
       "   0.06406209605626313,\n",
       "   0.06707101190930842,\n",
       "   0.06855847068231928,\n",
       "   0.06702901313758022,\n",
       "   0.06183580202591928,\n",
       "   0.06643721999227731,\n",
       "   0.06165255483294641,\n",
       "   0.06170760543097286,\n",
       "   0.0644496353364846,\n",
       "   0.06640134015192903,\n",
       "   0.06338535322858624,\n",
       "   0.06515248183229859,\n",
       "   0.06427519087162845,\n",
       "   0.06364218768455544,\n",
       "   0.06829348916632402,\n",
       "   0.06275114833465181,\n",
       "   0.06494112937076231,\n",
       "   0.06830084535193921,\n",
       "   0.0639060429975997,\n",
       "   0.06610276694754622,\n",
       "   0.06279532200845471,\n",
       "   0.06402229959821563,\n",
       "   0.06241919171013953,\n",
       "   0.06382878554820122,\n",
       "   0.06630661516853786,\n",
       "   0.06368780857738861,\n",
       "   0.06145357830935029,\n",
       "   0.06728244118993987,\n",
       "   0.06445081560934204,\n",
       "   0.06327956079492389,\n",
       "   0.06656270758855035,\n",
       "   0.061986441873139106,\n",
       "   0.06389872032286512,\n",
       "   0.059231289313961336,\n",
       "   0.06453757014681656,\n",
       "   0.0644631848141265,\n",
       "   0.06398664305455096,\n",
       "   0.06278663088589885,\n",
       "   0.06289983686126902,\n",
       "   0.0631391249723629,\n",
       "   0.05915238954395095,\n",
       "   0.06450154138826508,\n",
       "   0.061662160502801044,\n",
       "   0.06544696888356885,\n",
       "   0.06119283059464528,\n",
       "   0.06509258555519666,\n",
       "   0.06138608756114903],\n",
       "  'train_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'train_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_learning_rate_mean': [0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005],\n",
       "  'train_learning_rate_std': [4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19],\n",
       "  'val_loss_mean': [1.3876997216542561,\n",
       "   1.368763699134191,\n",
       "   1.3808363791306812,\n",
       "   1.2845906293392182,\n",
       "   1.2618953680992127,\n",
       "   1.2686110935608546,\n",
       "   1.2399143473307292,\n",
       "   1.2155419466892878,\n",
       "   1.1807628170649211,\n",
       "   1.2036402567227682,\n",
       "   1.1642734531561534,\n",
       "   1.1692318640152612,\n",
       "   1.1546032398939132,\n",
       "   1.161598687171936,\n",
       "   1.1231279146671296,\n",
       "   1.1198896263043085,\n",
       "   1.1369570509592692,\n",
       "   1.135751462181409,\n",
       "   1.0883020122845968,\n",
       "   1.0772391813993454,\n",
       "   1.0953187255064647,\n",
       "   1.0632869817813237,\n",
       "   1.0761917799711227,\n",
       "   1.0637538192669551,\n",
       "   1.072016444603602,\n",
       "   1.0486490108569464,\n",
       "   1.0578456509113312,\n",
       "   1.0359915516773859,\n",
       "   1.0426755746205647,\n",
       "   1.0396343972285589,\n",
       "   1.0350255767504375,\n",
       "   1.0186537969112397,\n",
       "   1.013898446559906,\n",
       "   1.0426814792553585,\n",
       "   1.0386661007006963,\n",
       "   1.0576876105864843,\n",
       "   1.030489247441292,\n",
       "   1.0360035345951717,\n",
       "   1.0505849691232045,\n",
       "   1.0375201280911763,\n",
       "   1.0422140975793204,\n",
       "   1.0204767906665801,\n",
       "   1.0498089104890824,\n",
       "   1.0311589127779006,\n",
       "   1.0321407520771027,\n",
       "   1.068314519325892,\n",
       "   1.021491900285085,\n",
       "   1.0309439663092295,\n",
       "   1.0341881595055262,\n",
       "   1.0403256899118423,\n",
       "   1.0641760820150374,\n",
       "   1.0387593589226405,\n",
       "   1.0413962390025457,\n",
       "   1.012323366800944,\n",
       "   1.0308739721775055,\n",
       "   1.0147114497423173,\n",
       "   1.0355764901638032,\n",
       "   1.0003155837456386,\n",
       "   1.0069367770353954,\n",
       "   1.0097508090734482,\n",
       "   1.014523623784383,\n",
       "   1.009491335550944,\n",
       "   1.0345306746164957,\n",
       "   1.0242119212945302,\n",
       "   1.0339610034227371,\n",
       "   1.0181408685445785,\n",
       "   1.0399553368488947,\n",
       "   1.028639412522316,\n",
       "   1.0154422614971796,\n",
       "   1.0116477698087691,\n",
       "   1.0174491931994756,\n",
       "   1.0163130035003027,\n",
       "   1.077779041926066,\n",
       "   1.0076473673184714,\n",
       "   1.043330605427424,\n",
       "   1.0140309131145477,\n",
       "   0.9984333568811417,\n",
       "   1.0275578918059667,\n",
       "   1.0389728629589081,\n",
       "   1.0117483856280645,\n",
       "   1.03337417781353,\n",
       "   1.0121075532833734,\n",
       "   1.0436381280422211,\n",
       "   1.0318673596779506,\n",
       "   1.0436430295308432,\n",
       "   1.022965386112531,\n",
       "   1.0088144705692927,\n",
       "   1.0199989287058513,\n",
       "   1.043161410888036,\n",
       "   0.9861289292573929,\n",
       "   1.0127662913004558,\n",
       "   1.0286547509829203,\n",
       "   1.009249255855878,\n",
       "   1.020052375793457,\n",
       "   1.0037086113293965,\n",
       "   1.0448955434560776,\n",
       "   1.0128965761264166,\n",
       "   1.0078910001118977,\n",
       "   1.031059092283249],\n",
       "  'val_loss_std': [0.10038837730388611,\n",
       "   0.10260757100750612,\n",
       "   0.11016344391336806,\n",
       "   0.10917997491203958,\n",
       "   0.11383214816827199,\n",
       "   0.12643059763851255,\n",
       "   0.1230684879799547,\n",
       "   0.13022644538227945,\n",
       "   0.12413330480956565,\n",
       "   0.1260019638957191,\n",
       "   0.13311803681526987,\n",
       "   0.13299392676159444,\n",
       "   0.13537558189086124,\n",
       "   0.13701860377595745,\n",
       "   0.13348787793267047,\n",
       "   0.1385359178024229,\n",
       "   0.14079482066214025,\n",
       "   0.14450659952056916,\n",
       "   0.1341612838072778,\n",
       "   0.14082483681593078,\n",
       "   0.13299705327946149,\n",
       "   0.1474914475827995,\n",
       "   0.14580302192698735,\n",
       "   0.14718594794882714,\n",
       "   0.1526568806655996,\n",
       "   0.15393917511846936,\n",
       "   0.14330825951850235,\n",
       "   0.14781172903615858,\n",
       "   0.15393299974117128,\n",
       "   0.15449136598116509,\n",
       "   0.159497699564022,\n",
       "   0.14967999202186963,\n",
       "   0.15010565930369107,\n",
       "   0.1547232981792278,\n",
       "   0.14893217109715073,\n",
       "   0.15128406927545193,\n",
       "   0.15337501486762395,\n",
       "   0.15883681425660615,\n",
       "   0.15027642015941217,\n",
       "   0.14839065290428205,\n",
       "   0.14992616418638594,\n",
       "   0.14931057135023626,\n",
       "   0.14707329733495672,\n",
       "   0.15371664204272717,\n",
       "   0.14690686919710463,\n",
       "   0.15537291436498837,\n",
       "   0.15130927384062667,\n",
       "   0.14273822441713283,\n",
       "   0.15203850003381067,\n",
       "   0.15351266970800903,\n",
       "   0.1505878598239639,\n",
       "   0.1587829481758882,\n",
       "   0.1469928732747852,\n",
       "   0.15072830930139333,\n",
       "   0.15109367298883408,\n",
       "   0.15461977577409838,\n",
       "   0.1545644245995111,\n",
       "   0.15325083587887228,\n",
       "   0.15602419017795202,\n",
       "   0.14694406094238271,\n",
       "   0.1587062118121638,\n",
       "   0.15529394185380682,\n",
       "   0.15335337018297754,\n",
       "   0.15387797502722114,\n",
       "   0.15639685926292185,\n",
       "   0.15350899428422599,\n",
       "   0.1682949855367356,\n",
       "   0.14944545084577768,\n",
       "   0.1572139618012217,\n",
       "   0.1490699543983805,\n",
       "   0.14826212383428794,\n",
       "   0.1503295871021462,\n",
       "   0.15934553693204923,\n",
       "   0.16295833681593896,\n",
       "   0.14808321793299276,\n",
       "   0.15867351625566847,\n",
       "   0.1456926641176557,\n",
       "   0.1504922768459961,\n",
       "   0.15771553996839294,\n",
       "   0.15879798285091087,\n",
       "   0.1546040875700343,\n",
       "   0.1578620584128105,\n",
       "   0.15984334963290514,\n",
       "   0.15921643728500282,\n",
       "   0.1538902272213496,\n",
       "   0.15487058685631422,\n",
       "   0.15570225448500677,\n",
       "   0.1601040337005361,\n",
       "   0.15292351089561979,\n",
       "   0.14512075683913242,\n",
       "   0.15906433726200095,\n",
       "   0.16092158596365425,\n",
       "   0.15228263802085576,\n",
       "   0.155369151799667,\n",
       "   0.1542712137560882,\n",
       "   0.16152212458780682,\n",
       "   0.15371628183257313,\n",
       "   0.15497951432578816,\n",
       "   0.1650584429642703],\n",
       "  'val_accuracy_mean': [0.4209777781367302,\n",
       "   0.4343111127614975,\n",
       "   0.4347111121813456,\n",
       "   0.4785111098488172,\n",
       "   0.4870666672786077,\n",
       "   0.4866444437702497,\n",
       "   0.4960444446404775,\n",
       "   0.5065999982754389,\n",
       "   0.5257777775327365,\n",
       "   0.5134222212433815,\n",
       "   0.531822222272555,\n",
       "   0.5287111103534698,\n",
       "   0.5343333319822947,\n",
       "   0.5355777783195178,\n",
       "   0.5489777780572573,\n",
       "   0.5512444415688514,\n",
       "   0.5483777770400047,\n",
       "   0.5486000000437101,\n",
       "   0.5678888887166977,\n",
       "   0.5693777769804,\n",
       "   0.5649555540084839,\n",
       "   0.5786888892451922,\n",
       "   0.579066665271918,\n",
       "   0.5851333327094714,\n",
       "   0.5759999990463257,\n",
       "   0.5870222199956576,\n",
       "   0.584133331477642,\n",
       "   0.5958222238222758,\n",
       "   0.5908888877431552,\n",
       "   0.5951999990145366,\n",
       "   0.5970666643977165,\n",
       "   0.6019999996821086,\n",
       "   0.6018444432814916,\n",
       "   0.5895777786771457,\n",
       "   0.5988666658600171,\n",
       "   0.5859333316485087,\n",
       "   0.5967333328723907,\n",
       "   0.5935111114382744,\n",
       "   0.5908000002304713,\n",
       "   0.5963777760664623,\n",
       "   0.5952888881166776,\n",
       "   0.5989111088713011,\n",
       "   0.5886666661500931,\n",
       "   0.6007555549343427,\n",
       "   0.5961777751644453,\n",
       "   0.5809333318471909,\n",
       "   0.6020444432894388,\n",
       "   0.5972444450855255,\n",
       "   0.5974444457888604,\n",
       "   0.5947555555899938,\n",
       "   0.5814222213625908,\n",
       "   0.5972222206989924,\n",
       "   0.5935555553436279,\n",
       "   0.6070222215851148,\n",
       "   0.599933331211408,\n",
       "   0.6039555548628172,\n",
       "   0.5981777788201967,\n",
       "   0.6118444447716077,\n",
       "   0.608377777338028,\n",
       "   0.6045111116766929,\n",
       "   0.6043333327770233,\n",
       "   0.6063999979694684,\n",
       "   0.5947333331902822,\n",
       "   0.6028888873259226,\n",
       "   0.601622222562631,\n",
       "   0.604288886586825,\n",
       "   0.5983555551369985,\n",
       "   0.6009555553396543,\n",
       "   0.6044222222765286,\n",
       "   0.608688890337944,\n",
       "   0.605,\n",
       "   0.6031555562218031,\n",
       "   0.579244444767634,\n",
       "   0.6092444427808126,\n",
       "   0.597000000278155,\n",
       "   0.6024222214023273,\n",
       "   0.611266666551431,\n",
       "   0.6008444426457087,\n",
       "   0.5950222210089365,\n",
       "   0.6029999994238218,\n",
       "   0.5974888884027799,\n",
       "   0.6049333344896635,\n",
       "   0.5942000000675519,\n",
       "   0.6023333320021629,\n",
       "   0.5943333317836126,\n",
       "   0.6011777751644453,\n",
       "   0.6110888882478078,\n",
       "   0.6038222207625707,\n",
       "   0.5959333339333535,\n",
       "   0.6138222214579582,\n",
       "   0.6080888886253039,\n",
       "   0.600422222216924,\n",
       "   0.6094444435834885,\n",
       "   0.6004666659235954,\n",
       "   0.6077555541197459,\n",
       "   0.5980888878305753,\n",
       "   0.6080666663249333,\n",
       "   0.607533333003521,\n",
       "   0.6004222196340561],\n",
       "  'val_accuracy_std': [0.05731982646163474,\n",
       "   0.056060215574100694,\n",
       "   0.05593009898956827,\n",
       "   0.059489228266920595,\n",
       "   0.06183480868298129,\n",
       "   0.06103186259346482,\n",
       "   0.06412994487305844,\n",
       "   0.06366226703094416,\n",
       "   0.06059050840264287,\n",
       "   0.06536727694765715,\n",
       "   0.06621914939676836,\n",
       "   0.06458274667572127,\n",
       "   0.06665138742142886,\n",
       "   0.06458872376500156,\n",
       "   0.061081544671151206,\n",
       "   0.06583542467316995,\n",
       "   0.06358107917057848,\n",
       "   0.06706742787158831,\n",
       "   0.061254680984808116,\n",
       "   0.06534737657826563,\n",
       "   0.06363930409905129,\n",
       "   0.06360922055770254,\n",
       "   0.061713940685058054,\n",
       "   0.06363918033265495,\n",
       "   0.06924353039814744,\n",
       "   0.06500501092262412,\n",
       "   0.06265091969688039,\n",
       "   0.06323405892510478,\n",
       "   0.06489611726894791,\n",
       "   0.0658713440755826,\n",
       "   0.06722424565760443,\n",
       "   0.06580892622490842,\n",
       "   0.06846251892267753,\n",
       "   0.06724834877502954,\n",
       "   0.062209121766690176,\n",
       "   0.06651690901234407,\n",
       "   0.06507086654147543,\n",
       "   0.06827806573017503,\n",
       "   0.06827686511362269,\n",
       "   0.06372444912215516,\n",
       "   0.06190337424435444,\n",
       "   0.06459789689357126,\n",
       "   0.06451987822367933,\n",
       "   0.0619296467745699,\n",
       "   0.060299728627113296,\n",
       "   0.06571106105820741,\n",
       "   0.06697903199706837,\n",
       "   0.0629058638361555,\n",
       "   0.0663290533957194,\n",
       "   0.06418070261929668,\n",
       "   0.06551035817127845,\n",
       "   0.06674763508169733,\n",
       "   0.06475900306059212,\n",
       "   0.0669716869266545,\n",
       "   0.06452214259123398,\n",
       "   0.06721153829140661,\n",
       "   0.06640569585416162,\n",
       "   0.06547381253889652,\n",
       "   0.06731158599043063,\n",
       "   0.06154027627375303,\n",
       "   0.0682438446292102,\n",
       "   0.06714662816609696,\n",
       "   0.06317262680380543,\n",
       "   0.06690365361014634,\n",
       "   0.06575514392790016,\n",
       "   0.06515489890212123,\n",
       "   0.06721964892234462,\n",
       "   0.06656084605885819,\n",
       "   0.06607627055620902,\n",
       "   0.06325890117110364,\n",
       "   0.06402574820134997,\n",
       "   0.06319023077514789,\n",
       "   0.07067730174205762,\n",
       "   0.06555647509860908,\n",
       "   0.06293588873544816,\n",
       "   0.06630278138191277,\n",
       "   0.06346088856554957,\n",
       "   0.06584289595563594,\n",
       "   0.06806178692811649,\n",
       "   0.06364513693545124,\n",
       "   0.06676263821152242,\n",
       "   0.06562262383396095,\n",
       "   0.0656679985640724,\n",
       "   0.06242298929200402,\n",
       "   0.06546500480114233,\n",
       "   0.06377929080179423,\n",
       "   0.06530270939260356,\n",
       "   0.06900068318459839,\n",
       "   0.06490262858150647,\n",
       "   0.06400044601964736,\n",
       "   0.06397261728850093,\n",
       "   0.06758895033069882,\n",
       "   0.06666990666518961,\n",
       "   0.06630516432710662,\n",
       "   0.06631127212751237,\n",
       "   0.0683989897576501,\n",
       "   0.06496468575182837,\n",
       "   0.06584823284829877,\n",
       "   0.06805639907986383],\n",
       "  'val_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'val_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0]}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델을 구성한다\n",
    "model = MAMLFewShotClassifier(args=args, device=device,\n",
    "                              im_shape=(2, 3,\n",
    "                                        args.image_height, args.image_width))\n",
    "\n",
    "data = MetaLearningSystemDataLoader\n",
    "\n",
    "maml_system = ExperimentBuilder(model=model, data=data, args=args, device=device)\n",
    "maml_system.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3803d5ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MAMLFewShotClassifier(\n",
       "  (classifier): VGGReLUNormNetwork(\n",
       "    (layer_dict): ModuleDict(\n",
       "      (conv0): MetaConvNormLayerReLU(\n",
       "        (layer_dict): ModuleDict()\n",
       "        (conv): MetaConv2dLayer()\n",
       "        (norm_layer): MetaBatchNormLayer(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): MetaConvNormLayerReLU(\n",
       "        (layer_dict): ModuleDict()\n",
       "        (conv): MetaConv2dLayer()\n",
       "        (norm_layer): MetaBatchNormLayer(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): MetaConvNormLayerReLU(\n",
       "        (layer_dict): ModuleDict()\n",
       "        (conv): MetaConv2dLayer()\n",
       "        (norm_layer): MetaBatchNormLayer(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv3): MetaConvNormLayerReLU(\n",
       "        (layer_dict): ModuleDict()\n",
       "        (conv): MetaConv2dLayer()\n",
       "        (norm_layer): MetaBatchNormLayer(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (linear): MetaLinearLayer()\n",
       "    )\n",
       "  )\n",
       "  (inner_loop_optimizer): GradientDescentLearningRule()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_dir = maml_system.saved_models_filepath\n",
    "model_name = \"train_model\"\n",
    "model_idx = maml_system.state['best_epoch']\n",
    "\n",
    "state = maml_system.model.load_model(model_save_dir=model_save_dir,\n",
    "                                     model_name=model_name,\n",
    "                                     model_idx=model_idx+1)\n",
    "\n",
    "state_dict_loaded = state['network']\n",
    "\n",
    "maml_system.model.load_state_dict(state_dict=state_dict_loaded)\n",
    "\n",
    "# # 잘 불러왔는지 확인하는 코드\n",
    "# print(\"state_dict_loaded == \",state_dict_loaded)\n",
    "# print(\"=\"*10)\n",
    "# for key, value in maml_system.model.named_parameters():\n",
    "#     print(key)\n",
    "#     print(value)\n",
    "maml_system.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db4855d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\utils\\grad_cam.py:24: RuntimeWarning: invalid value encountered in true_divide\n",
      "  grad_cam = (grad_cam - np.min(grad_cam)) / (np.max(grad_cam) - np.min(grad_cam))\n"
     ]
    }
   ],
   "source": [
    "# train_data = maml_system.data.get_train_batches(total_batches=int(600/2), augment_images=False)\n",
    "train_data = maml_system.data.get_test_batches(total_batches=int(600/2), augment_images=False)\n",
    "\n",
    "figure_idx = 0\n",
    "\n",
    "for sample_idx, train_sample in enumerate(train_data):\n",
    "    \n",
    "    x_support_set, x_target_set, y_support_set, y_target_set, seed = train_sample\n",
    "    \n",
    "    x_support_set = torch.Tensor(x_support_set).float().to(device=maml_system.model.device)\n",
    "    x_target_set = torch.Tensor(x_target_set).float().to(device=maml_system.model.device)\n",
    "    y_support_set = torch.Tensor(y_support_set).long().to(device=maml_system.model.device)\n",
    "    y_target_set = torch.Tensor(y_target_set).long().to(device=maml_system.model.device)\n",
    "    \n",
    "    for task_id, (x_support_set_task, y_support_set_task, x_target_set_task, y_target_set_task) in enumerate(zip(x_support_set,\n",
    "                              y_support_set,\n",
    "                              x_target_set,\n",
    "                              y_target_set)):\n",
    "        \n",
    "        names_weights_copy = maml_system.model.get_inner_loop_parameter_dict(maml_system.model.classifier.named_parameters())\n",
    "        \n",
    "        num_devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "\n",
    "        names_weights_copy = {\n",
    "            name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
    "                [num_devices] + [1 for i in range(len(value.shape))]) for\n",
    "            name, value in names_weights_copy.items()}\n",
    "        \n",
    "        prompted_weights_copy = {}\n",
    "        if args.prompter:\n",
    "            prompted_weights_copy = {key: value for key, value in names_weights_copy.items() if 'prompt' in key}\n",
    "\n",
    "        names_weights_copy = {key: value for key, value in names_weights_copy.items() if 'layer_dict' in key}\n",
    "        \n",
    "        n, s, c, h, w = x_target_set_task.shape\n",
    "\n",
    "        x_support_set_task = x_support_set_task.view(-1, c, h, w)\n",
    "        y_support_set_task = y_support_set_task.view(-1)\n",
    "        x_target_set_task = x_target_set_task.view(-1, c, h, w)\n",
    "        y_target_set_task = y_target_set_task.view(-1)\n",
    "        \n",
    "        num_steps=5\n",
    "        \n",
    "        for num_step in range(num_steps):\n",
    "            \n",
    "            # show_batch(images=x_support_set_task, labels=y_support_set_task, datasets=datasets_name)\n",
    "            \n",
    "            support_loss, support_preds = maml_system.model.net_forward(\n",
    "                                                               x=x_support_set_task,\n",
    "                                                               y=y_support_set_task,\n",
    "                                                               weights=names_weights_copy,\n",
    "                                                               prompted_weights=prompted_weights_copy,\n",
    "                                                               backup_running_statistics=num_step == 0,\n",
    "                                                               prepend_prompt=args.prompter,\n",
    "                                                               training=True,\n",
    "                                                               num_step=num_step,\n",
    "                                                               training_phase=False,\n",
    "                                                               epoch=0,\n",
    "                                                               inner_loop=True)\n",
    "        \n",
    "            names_weights_copy, prompted_weights_copy = maml_system.model.apply_inner_loop_update(\n",
    "                 loss=support_loss,\n",
    "                 names_weights_copy=names_weights_copy,\n",
    "                 prompted_weights_copy=prompted_weights_copy,\n",
    "                 use_second_order=True,\n",
    "                 current_step_idx=num_step,\n",
    "                 current_iter='test',\n",
    "                 training_phase=False)\n",
    "            \n",
    "        ##########Inner level 종료 #############\n",
    "        # Query set으로 Grad-CAM을 구한다\n",
    "        individual_images = torch.split(x_target_set_task, 1, dim=0)  # (25, 3, 84, 84) -> (1, 3, 84, 84) 텐서로 분리\n",
    "        individual_labels = torch.split(y_target_set_task, 1, dim=0)\n",
    "        \n",
    "        \n",
    "        original_save_path = \"Grad_CAM/\" + datasets_name + \"/MAML/origin/\" + \"sample_idx_\" + str(sample_idx) + \"/\" + \"task_id_\" + str(task_id) + \"/\"\n",
    "        grad_cam_save_path = \"Grad_CAM/\" + datasets_name + \"/MAML/gradcam/\"+ \"sample_idx_\" + str(sample_idx) + \"/\" + \"task_id_\" + str(task_id) + \"/\"\n",
    "        \n",
    "        make_folder(original_save_path)\n",
    "        make_folder(grad_cam_save_path)\n",
    "        \n",
    "        # 각 이미지 텐서 확인\n",
    "        for i in range(y_target_set_task.shape[0]):\n",
    "\n",
    "            input_image = individual_images[i]\n",
    "            y_label = individual_labels[i]\n",
    "\n",
    "            _, target_preds, feature_map_list = maml_system.model.net_forward_feature_extractor(x=input_image,\n",
    "                                                             y=y_label, weights=names_weights_copy,\n",
    "                                                             backup_running_statistics=False, training=True,\n",
    "                                                             num_step=num_step, training_phase='test',\n",
    "                                                             epoch=0)\n",
    "            \n",
    "            feature_map = feature_map_list[3]\n",
    "            \n",
    "            # 선택한 클래스에 대한 스칼라 출력값을 기준으로 gradient 계산\n",
    "            target_class = y_label\n",
    "            output = target_preds\n",
    "\n",
    "            class_score = output[:, target_class].sum()\n",
    "            gradients = torch.autograd.grad(outputs=class_score, inputs=feature_map, grad_outputs=torch.ones_like(class_score))\n",
    "\n",
    "            # gradients를 사용하여 feature map의 가중치를 계산\n",
    "            weights = torch.mean(gradients[0], dim=(2, 3), keepdim=True)\n",
    "            grad_cam = F.relu(torch.sum(weights * feature_map, dim=1, keepdim=True))\n",
    "\n",
    "            # grad_cam을 이미지 크기로 리사이즈\n",
    "            grad_cam = F.interpolate(grad_cam, size=(input_image.shape[2], input_image.shape[3]), mode='bilinear', align_corners=False)\n",
    "            grad_cam = grad_cam.squeeze().cpu().detach().numpy()\n",
    "\n",
    "            visualize_and_save_grad_cam(\n",
    "                input_image=input_image,\n",
    "                grad_cam=grad_cam,\n",
    "                grad_cam_save_path=grad_cam_save_path + str(i) + \".png\",\n",
    "                original_save_path=original_save_path + str(i) + \".png\",\n",
    "                datasets=datasets_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
