{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd87b33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pytorchcv.model_provider import get_model as ptcv_get_model # model\n",
    "import sys, os\n",
    "import easydict\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline \n",
    "\n",
    "# enable cuda devices\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from meta_neural_network_architectures import VGGReLUNormNetwork, ResNet12\n",
    "from utils.parser_utils import get_args\n",
    "from data import MetaLearningSystemDataLoader\n",
    "from experiment_builder import ExperimentBuilder\n",
    "from utils.basic import show_batch\n",
    "from few_shot_learning_system import MAMLFewShotClassifier\n",
    "import prompters\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from data_augmentation import mixup_data, cutmix_data, random_flip, random_flip_like_torchvision\n",
    "from utils.grad_cam import visualize_and_save_grad_cam, make_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c638561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets_name = \"mini_imagenet\"\n",
    "# datasets_name = \"tiered_imagenet\"\n",
    "# datasets_name = \"CIFAR_FS\"\n",
    "datasets_name = \"CUB\"\n",
    "\n",
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "# os.environ['TEST_DATASET'] = \"tiered_imagenet\" # https://mtl.yyliu.net/download/Lmzjm9tX.html\n",
    "# os.environ['TEST_DATASET'] = \"CIFAR_FS\" # https://drive.google.com/file/d/1pTsCCMDj45kzFYgrnO67BWVbKs48Q3NI/view\n",
    "os.environ['TEST_DATASET'] = \"CUB\" # https://data.caltech.edu/records/65de6-vp158"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8daf114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.environ['DATASET_DIR'] ===  C:/Users/JM/PycharmProjects/MAML/datasets\n"
     ]
    }
   ],
   "source": [
    "# os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "print(\"os.environ['DATASET_DIR'] === \", os.environ['DATASET_DIR'])\n",
    "\n",
    "args = easydict.EasyDict(\n",
    "{\n",
    "  \"batch_size\":2,\n",
    "  \"image_height\":84,\n",
    "  \"image_width\":84,\n",
    "  \"image_channels\":3,\n",
    "  \"gpu_to_use\":0,\n",
    "  \"num_dataprovider_workers\":4,\n",
    "  \"max_models_to_save\":5,\n",
    "  \"dataset_name\":\"mini_imagenet_full_size\",\n",
    "  \"dataset_path\":\"mini_imagenet_full_size\",\n",
    "  \"reset_stored_paths\":False,\n",
    "  \"experiment_name\":\"../MAML_Prompt_padding_5way_5shot_filter128_miniImagenet\",\n",
    "  \"train_seed\": 0, \"val_seed\": 0,\n",
    "  \"indexes_of_folders_indicating_class\": [-3, -2],\n",
    "  \"sets_are_pre_split\": True,\n",
    "  \"train_val_test_split\": [0.64, 0.16, 0.20],\n",
    "  \"evaluate_on_test_set_only\": False,\n",
    "\n",
    "  \"total_epochs\": 100,\n",
    "  \"total_iter_per_epoch\":500, \"continue_from_epoch\": -2,\n",
    "  \"num_evaluation_tasks\":600,\n",
    "  \"multi_step_loss_num_epochs\": 15,\n",
    "  \"minimum_per_task_contribution\": 0.01,\n",
    "  \"learnable_per_layer_per_step_inner_loop_learning_rate\": True,\n",
    "  \"enable_inner_loop_optimizable_bn_params\": False,\n",
    "  \"evalute_on_test_set_only\": False,\n",
    "\n",
    "  \"max_pooling\": True,\n",
    "  \"per_step_bn_statistics\": False,\n",
    "  \"learnable_batch_norm_momentum\": False,\n",
    "  \"load_into_memory\": False,\n",
    "  \"init_inner_loop_learning_rate\": 0.01,\n",
    "  \"init_inner_loop_weight_decay\": 0.0005,\n",
    "  \"learnable_bn_gamma\": True,\n",
    "  \"learnable_bn_beta\": True,\n",
    "\n",
    "  \"dropout_rate_value\":0.0,\n",
    "  \"min_learning_rate\":0.001,\n",
    "  \"meta_learning_rate\":0.001,   \"total_epochs_before_pause\": 101,\n",
    "  \"first_order_to_second_order_epoch\":-1,\n",
    "  \"weight_decay\": 0.0,\n",
    "\n",
    "  \"norm_layer\":\"batch_norm\",\n",
    "  \"cnn_num_filters\":128,\n",
    "  \"num_stages\":4,\n",
    "  \"conv_padding\": True,\n",
    "  \"number_of_training_steps_per_iter\":5,\n",
    "  \"number_of_evaluation_steps_per_iter\":5,\n",
    "  \"cnn_blocks_per_stage\":1,\n",
    "  \"num_classes_per_set\":5,\n",
    "  \"num_samples_per_class\":5,\n",
    "  \"num_target_samples\": 15,\n",
    "  \"samples_per_iter\" : 1,\n",
    "\n",
    "  \"second_order\": True,\n",
    "  \"use_multi_step_loss_optimization\":False,\n",
    "  \"backbone\": \"4-CONV\",\n",
    "  \"arbiter\": False,\n",
    "  \"use_bias\": True,\n",
    "  \"prompter\": True,\n",
    "  \"prompt_engineering\": 'padding',\n",
    "  \"prompt_size\" : 5,\n",
    "  \"image_size\" : 84,\n",
    "  \"prompt_random_init\": False,\n",
    "  \"outer_prompt_learning_rate\": 0.001,\n",
    "  \"inner_prompt_learning_rate\": 0.01,\n",
    "  \"data_aug\" : \"not_mixup\",\n",
    "  \"ablation_record\": False,\n",
    "  \"random_prompt_init\": False\n",
    "}\n",
    ")\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "args.im_shape = (2, 3, args.image_height, args.image_width)\n",
    "\n",
    "args.use_cuda = torch.cuda.is_available()\n",
    "args.seed = 104\n",
    "args.reverse_channels=False\n",
    "args.labels_as_int=False\n",
    "args.reset_stored_filepaths=False\n",
    "args.num_of_gpus=1\n",
    "\n",
    "args.continue_from_epoch='latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dba8492c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using max pooling\n",
      "meta network params\n",
      "0.01\n",
      "Inner Loop parameters\n",
      "prompt_learning_rates_dict.prompt_weight_learning_rate torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv0-conv-weight torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv0-conv-bias torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv1-conv-weight torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv1-conv-bias torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv2-conv-weight torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv2-conv-bias torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv3-conv-weight torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv3-conv-bias torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-linear-weights torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-linear-bias torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv0-conv-weight torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv0-conv-bias torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv1-conv-weight torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv1-conv-bias torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv2-conv-weight torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv2-conv-bias torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv3-conv-weight torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv3-conv-bias torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-linear-weights torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-linear-bias torch.Size([6])\n",
      "Outer Loop parameters\n",
      "classifier.layer_dict.conv0.conv.weight torch.Size([128, 3, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv0.conv.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv0.norm_layer.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv0.norm_layer.weight torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv1.conv.weight torch.Size([128, 128, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv1.conv.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv1.norm_layer.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv1.norm_layer.weight torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv2.conv.weight torch.Size([128, 128, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv2.conv.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv2.norm_layer.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv2.norm_layer.weight torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv3.conv.weight torch.Size([128, 128, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv3.conv.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv3.norm_layer.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv3.norm_layer.weight torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.linear.weights torch.Size([5, 3200]) cuda:0 True\n",
      "classifier.layer_dict.linear.bias torch.Size([5]) cuda:0 True\n",
      "classifier.prompt.prompt_dict.pad_up torch.Size([3, 5, 84]) cuda:0 True\n",
      "classifier.prompt.prompt_dict.pad_down torch.Size([3, 5, 84]) cuda:0 True\n",
      "classifier.prompt.prompt_dict.pad_left torch.Size([3, 74, 5]) cuda:0 True\n",
      "classifier.prompt.prompt_dict.pad_right torch.Size([3, 74, 5]) cuda:0 True\n",
      "inner_loop_optimizer.prompt_learning_rates_dict.prompt_weight_learning_rate torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv0-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv0-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv1-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv1-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv2-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv2-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv3-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv3-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-weights torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv0-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv0-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv1-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv1-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv2-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv2-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv3-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv3-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-linear-weights torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-linear-bias torch.Size([6]) cuda:0 True\n",
      "1\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\MAML_Prompt_padding_5way_5shot_filter128_miniImagenet\n",
      "attempting to find existing checkpoint\n",
      "data {'test': 2953, 'train': 5885, 'val': 2950}\n",
      "train_seed 985773, val_seed: 985773, at start time\n",
      "50000 50000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_val_acc': 0.6893555573622385,\n",
       " 'best_val_iter': 46000,\n",
       " 'current_iter': 50000,\n",
       " 'best_epoch': 92,\n",
       " 'train_loss_mean': 0.4218806900084019,\n",
       " 'train_loss_std': 0.11633159977043864,\n",
       " 'train_accuracy_mean': 0.8458666672706604,\n",
       " 'train_accuracy_std': 0.04957311298936435,\n",
       " 'train_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_0_std': 0.0,\n",
       " 'train_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_1_std': 0.0,\n",
       " 'train_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_2_std': 0.0,\n",
       " 'train_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_3_std': 0.0,\n",
       " 'train_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'train_loss_importance_vector_4_std': 0.0,\n",
       " 'train_learning_rate_mean': 0.0010000000000000005,\n",
       " 'train_learning_rate_std': 4.336808689942018e-19,\n",
       " 'val_loss_mean': 0.8366678491234779,\n",
       " 'val_loss_std': 0.14215259579417638,\n",
       " 'val_accuracy_mean': 0.6766888898611069,\n",
       " 'val_accuracy_std': 0.06025746955351326,\n",
       " 'val_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_0_std': 0.0,\n",
       " 'val_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_1_std': 0.0,\n",
       " 'val_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_2_std': 0.0,\n",
       " 'val_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_3_std': 0.0,\n",
       " 'val_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'val_loss_importance_vector_4_std': 0.0,\n",
       " 'network': OrderedDict([('classifier.layer_dict.conv0.conv.weight',\n",
       "               tensor([[[[ 1.1943e-18,  3.0424e-18,  3.4479e-18],\n",
       "                         [ 2.7335e-18,  4.0985e-18,  4.8780e-18],\n",
       "                         [ 2.7021e-18,  4.8803e-18,  5.7132e-18]],\n",
       "               \n",
       "                        [[ 2.5279e-18,  4.1613e-18,  4.5616e-18],\n",
       "                         [ 3.4050e-18,  4.3568e-18,  4.9179e-18],\n",
       "                         [ 3.1353e-18,  5.1975e-18,  5.5489e-18]],\n",
       "               \n",
       "                        [[-4.7770e-19,  1.5046e-19,  5.3308e-19],\n",
       "                         [-4.8802e-20,  4.2384e-19,  5.7209e-19],\n",
       "                         [-8.1170e-20,  7.4235e-19,  9.0603e-19]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 3.2778e-09,  3.1811e-09,  3.2582e-09],\n",
       "                         [ 3.0362e-09,  3.0276e-09,  3.0222e-09],\n",
       "                         [ 2.5388e-09,  2.7771e-09,  2.7383e-09]],\n",
       "               \n",
       "                        [[ 1.5563e-09,  1.8604e-09,  1.5755e-09],\n",
       "                         [ 1.1331e-09,  1.5384e-09,  1.2569e-09],\n",
       "                         [ 6.4964e-10,  1.1986e-09,  8.8503e-10]],\n",
       "               \n",
       "                        [[ 5.6613e-11,  3.6395e-11, -1.7946e-10],\n",
       "                         [-3.1339e-10, -1.9562e-10, -4.1506e-10],\n",
       "                         [-7.5679e-10, -5.0691e-10, -6.9447e-10]]],\n",
       "               \n",
       "               \n",
       "                       [[[-4.1192e-02, -2.8390e-02, -2.3778e-02],\n",
       "                         [-3.9143e-02, -2.7630e-02, -2.3272e-02],\n",
       "                         [-4.3812e-02, -3.0420e-02, -2.6554e-02]],\n",
       "               \n",
       "                        [[-2.0370e-02, -9.3513e-03, -7.6436e-03],\n",
       "                         [-1.7322e-02, -8.5155e-03, -7.4824e-03],\n",
       "                         [-1.8963e-02, -9.9365e-03, -9.8090e-03]],\n",
       "               \n",
       "                        [[ 7.0550e-03,  1.5571e-02,  1.7465e-02],\n",
       "                         [ 1.0838e-02,  1.6318e-02,  1.8075e-02],\n",
       "                         [ 1.2896e-02,  1.7256e-02,  1.7917e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 1.1258e-01, -2.6157e-01,  1.0076e-01],\n",
       "                         [ 6.6907e-02, -1.4049e-01,  9.6846e-02],\n",
       "                         [-1.5860e-01,  4.0214e-01, -2.2144e-01]],\n",
       "               \n",
       "                        [[ 1.5212e-01, -2.5632e-01,  1.8537e-01],\n",
       "                         [ 6.5207e-02, -1.2565e-01,  4.0391e-02],\n",
       "                         [-2.1094e-01,  3.5588e-01, -1.9827e-01]],\n",
       "               \n",
       "                        [[ 1.2651e-01, -2.2765e-01,  6.4708e-02],\n",
       "                         [ 1.9519e-02, -5.5145e-02,  7.2135e-02],\n",
       "                         [-1.6862e-01,  2.9446e-01, -1.4037e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 8.9614e-02,  2.6964e-01,  7.4524e-02],\n",
       "                         [-2.6143e-02, -2.0005e-02, -6.3546e-03],\n",
       "                         [-9.3772e-02, -2.4325e-01, -8.7058e-02]],\n",
       "               \n",
       "                        [[ 4.1229e-02,  2.1870e-01,  3.8055e-02],\n",
       "                         [ 9.9392e-03,  1.4414e-02,  3.2787e-02],\n",
       "                         [-3.2369e-02, -1.6998e-01, -2.1208e-02]],\n",
       "               \n",
       "                        [[-2.5068e-02,  1.2958e-01,  1.7445e-02],\n",
       "                         [-1.2693e-02,  1.6019e-02,  3.6611e-02],\n",
       "                         [-2.6890e-02, -1.4297e-01, -1.0114e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-3.6279e-24, -1.4622e-24, -3.0472e-26],\n",
       "                         [-2.5765e-24, -4.5967e-25,  5.6020e-24],\n",
       "                         [-4.0944e-24, -1.7575e-25,  5.7036e-24]],\n",
       "               \n",
       "                        [[-4.4470e-24, -4.5579e-24, -3.6726e-24],\n",
       "                         [-2.3385e-24, -5.3788e-24, -2.5801e-24],\n",
       "                         [-1.3413e-24, -5.7814e-24, -4.3411e-24]],\n",
       "               \n",
       "                        [[-4.1279e-25, -2.2953e-25,  8.2167e-25],\n",
       "                         [-5.5822e-25, -1.0694e-24, -6.2801e-25],\n",
       "                         [-1.0883e-24, -1.7799e-24, -1.6728e-24]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.conv.bias',\n",
       "               tensor([ 1.0524e-06,  1.6541e-09, -1.9102e-08, -2.6465e-06, -6.7701e-06,\n",
       "                        6.4297e-11,  1.4988e-11, -4.5770e-06, -4.2623e-08, -1.4698e-06,\n",
       "                       -1.1463e-11,  2.5219e-06, -3.6892e-08, -1.7170e-07,  3.1355e-11,\n",
       "                        9.3389e-05,  3.5274e-09, -5.5816e-12, -1.7676e-09,  1.5461e-06,\n",
       "                        4.6881e-13,  7.8481e-08, -1.7575e-08, -8.9531e-05,  4.0527e-05,\n",
       "                       -3.0930e-09,  2.6233e-15,  4.2624e-11, -1.8141e-07,  1.1101e-04,\n",
       "                       -1.0304e-07,  6.9069e-06, -6.9338e-06,  6.0615e-08, -1.5969e-05,\n",
       "                       -1.3752e-05,  3.3475e-05, -5.5421e-05,  3.5265e-16,  1.8670e-06,\n",
       "                        2.3537e-07, -2.8127e-07, -1.5133e-04, -4.2607e-12,  2.5957e-05,\n",
       "                       -8.6109e-09,  3.0591e-08,  6.6969e-05,  6.7065e-08,  3.2946e-10,\n",
       "                        7.1491e-05,  1.3182e-11,  7.5910e-09,  2.1896e-05,  1.0240e-07,\n",
       "                        1.2161e-08,  2.5051e-05,  3.4993e-16,  3.3137e-13, -4.7086e-05,\n",
       "                       -2.6095e-05, -2.9702e-10, -9.6883e-13,  5.3326e-07,  1.5609e-07,\n",
       "                        1.4771e-07,  8.3883e-14, -2.1798e-15,  2.5047e-06, -6.2930e-05,\n",
       "                       -6.6387e-08,  1.1175e-07,  6.5500e-10,  2.6351e-08,  9.7207e-17,\n",
       "                        1.3163e-05, -2.1916e-12, -4.2216e-06,  7.7358e-05, -4.3889e-05,\n",
       "                       -1.6732e-05,  4.7594e-06,  1.8315e-05,  9.6631e-12, -6.6386e-07,\n",
       "                        4.5200e-05,  2.4961e-06, -1.8810e-08,  2.4084e-13,  5.6978e-11,\n",
       "                       -2.9544e-10,  9.2345e-11,  1.2572e-04, -4.7430e-07,  3.2911e-06,\n",
       "                        4.7756e-09, -1.0578e-06, -7.0224e-05,  3.4127e-05,  1.5996e-06,\n",
       "                        4.1024e-12,  2.3691e-08, -1.0529e-04, -2.7037e-08,  5.2684e-05,\n",
       "                       -1.9711e-06, -9.8735e-11,  5.0731e-09, -4.3694e-06,  2.6797e-07,\n",
       "                        1.3043e-07,  1.5711e-07,  7.9768e-12, -5.0554e-12, -6.2005e-09,\n",
       "                        9.6566e-05,  9.6014e-05,  4.5012e-10, -4.0852e-09, -2.8504e-05,\n",
       "                       -1.1248e-08, -2.4769e-07, -1.6400e-04,  1.4262e-04,  3.0202e-13,\n",
       "                       -1.4934e-04, -2.3117e-05, -5.9797e-10], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.bias',\n",
       "               tensor([-1.6492e-08, -4.5241e-07, -4.4118e-02, -4.4546e-02,  6.2903e-02,\n",
       "                       -3.5892e-07, -4.0695e-05, -1.0882e-04, -4.9154e-02, -9.1948e-02,\n",
       "                       -1.0625e-08, -2.1015e-04, -1.8222e-03, -5.7343e-02, -1.1519e-05,\n",
       "                       -3.5309e-02, -1.6239e-03, -1.0606e-07,  1.7807e-02, -6.8064e-02,\n",
       "                       -5.0391e-09, -1.6847e-03, -1.9076e-10, -1.9125e-02, -2.0972e-02,\n",
       "                       -9.3744e-03, -1.7557e-08, -7.5977e-07, -2.7847e-02,  2.0685e-01,\n",
       "                       -2.1872e-02, -5.0239e-02, -1.7742e-02, -6.8264e-03, -3.6126e-02,\n",
       "                       -2.9984e-02, -2.5546e-02, -1.8032e-01, -1.1711e-07, -3.3711e-01,\n",
       "                       -1.0100e-02,  1.9228e-03,  1.8426e-01, -1.9861e-07, -5.8361e-02,\n",
       "                       -2.0360e-08, -2.9197e-02,  1.7876e-01, -1.1714e-02, -1.4166e-06,\n",
       "                       -4.1320e-02, -2.3553e-07, -4.1509e-04, -1.1620e-05, -6.1055e-03,\n",
       "                       -4.5461e-04, -1.0581e-01, -1.4787e-10, -1.6861e-07, -3.0643e-02,\n",
       "                       -2.7748e-01, -9.9302e-10, -4.3542e-11, -8.5134e-04, -1.6779e-04,\n",
       "                       -6.7215e-03, -8.8319e-07, -1.7342e-07, -8.1052e-03,  4.2243e-01,\n",
       "                       -9.6234e-04, -4.7858e-02, -7.5041e-08, -8.1302e-03, -1.1961e-07,\n",
       "                       -2.6615e-01, -1.6426e-07,  1.9465e-02,  9.6692e-02,  4.5558e-01,\n",
       "                       -1.0278e-05, -6.0816e-01, -2.8875e-02, -2.7313e-06, -4.2625e-02,\n",
       "                       -1.8725e-02, -2.0067e-02, -2.3645e-11, -1.3560e-08, -1.2448e-06,\n",
       "                       -5.8555e-08, -4.3977e-09, -1.7268e-02, -2.3565e-02, -3.2383e-03,\n",
       "                       -1.0971e-05, -5.5447e-03, -2.3239e-01, -1.8410e-01, -2.2666e-02,\n",
       "                       -3.3598e-07, -8.4370e-05,  5.7293e-01, -1.3998e-07, -3.4352e-02,\n",
       "                       -3.1219e-02, -9.0036e-07, -7.2049e-09, -4.7093e-02, -4.3036e-02,\n",
       "                       -3.2777e-06, -7.2565e-06, -2.2784e-05, -2.5831e-06, -1.3423e-03,\n",
       "                       -3.4418e-02,  4.8266e-02,  1.9350e-09, -2.8314e-05,  3.0774e-02,\n",
       "                       -1.1339e-02, -2.4296e-02, -3.6537e-02, -1.3802e-02, -6.5758e-10,\n",
       "                       -1.8796e-02, -6.3146e-01, -6.8342e-09], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.weight',\n",
       "               tensor([ 2.7367e-15, -3.0779e-05, -1.6701e-04,  8.2459e-03,  1.9188e-01,\n",
       "                        5.8912e-08,  7.4359e-05,  1.9610e-05, -4.6592e-03,  4.8134e-02,\n",
       "                        4.4677e-07,  2.6885e-04,  6.7176e-04,  1.8329e-02,  2.2293e-04,\n",
       "                        6.1774e-01,  6.3645e-04,  2.7761e-07,  2.8934e-05,  1.7785e-01,\n",
       "                       -6.0013e-07, -2.2998e-04,  1.3065e-13,  7.5127e-01, -2.8732e-03,\n",
       "                       -7.6978e-04, -6.6277e-11, -6.0765e-06, -7.5175e-03,  2.6424e-01,\n",
       "                       -4.3484e-05,  2.5101e-01,  7.1467e-03,  2.5195e-03, -5.1647e-03,\n",
       "                        3.3129e-01,  8.1563e-03,  2.5122e-01,  2.5060e-28,  3.3521e-01,\n",
       "                        4.4079e-03,  1.7546e-03,  3.8876e-01, -1.9165e-08,  4.6298e-01,\n",
       "                        1.1973e-10,  6.5934e-03,  3.6285e-01, -3.7652e-03, -2.0102e-06,\n",
       "                        2.7235e-01,  5.7872e-07, -4.4342e-05, -9.7958e-06, -2.1633e-03,\n",
       "                       -3.9307e-05,  3.6865e-01,  4.0561e-20,  1.9206e-12,  5.8715e-01,\n",
       "                        1.9247e-01,  2.6162e-19, -1.3851e-07,  8.1998e-05,  1.3155e-04,\n",
       "                       -3.3098e-04, -2.2367e-11,  1.2320e-16,  7.2642e-04,  1.8690e-01,\n",
       "                        3.4007e-04, -2.5301e-02,  9.3213e-12, -1.1202e-03, -2.3330e-26,\n",
       "                        1.5002e-01, -4.9453e-07,  6.9024e-03,  5.2749e-01,  4.0836e-01,\n",
       "                       -1.2582e-15,  3.8281e-01, -7.4731e-03,  3.0277e-22,  1.8065e-02,\n",
       "                        4.9102e-01,  7.7089e-03, -7.9006e-06, -6.1253e-10,  5.3237e-07,\n",
       "                        8.8925e-05, -2.9303e-08,  4.2450e-01, -8.0343e-03,  4.2536e-04,\n",
       "                       -3.7552e-06,  9.1380e-04,  4.0719e-01,  3.0915e-01, -3.1789e-03,\n",
       "                        5.7767e-06, -6.3009e-12,  4.4379e-01, -1.2662e-19,  3.5823e-01,\n",
       "                        7.6183e-03,  1.7214e-05, -4.8602e-09, -3.1484e-03,  8.6783e-03,\n",
       "                        1.3171e-17, -2.3575e-15,  3.6312e-05, -5.6745e-07,  1.5871e-04,\n",
       "                        6.2112e-01,  3.4509e-01,  6.9948e-23,  9.3391e-18,  5.3377e-01,\n",
       "                       -5.0115e-04, -7.1237e-03,  4.6017e-01,  3.9404e-01,  5.8744e-16,\n",
       "                        4.3954e-01,  3.8971e-01,  8.2749e-21], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.weight',\n",
       "               tensor([[[[ 6.6278e-09,  7.0833e-09,  6.7503e-09],\n",
       "                         [-3.5831e-10, -2.3446e-14, -3.0449e-10],\n",
       "                         [ 2.0159e-09,  2.2331e-09,  2.0453e-09]],\n",
       "               \n",
       "                        [[-1.7288e-09, -1.8795e-09, -1.2361e-09],\n",
       "                         [-3.1704e-11,  9.8948e-11, -3.1824e-10],\n",
       "                         [-3.1753e-09, -2.6806e-09, -3.3519e-09]],\n",
       "               \n",
       "                        [[-4.1868e-05, -1.1279e-04, -2.1701e-04],\n",
       "                         [-2.0974e-05, -1.1195e-04, -2.4247e-04],\n",
       "                         [-4.5701e-05, -1.4551e-04, -2.3877e-04]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 9.4221e-02,  5.4859e-03,  9.8189e-02],\n",
       "                         [-4.0616e-02, -1.0975e-01, -2.3560e-03],\n",
       "                         [ 9.0836e-02,  1.6620e-03, -1.6809e-02]],\n",
       "               \n",
       "                        [[-5.9540e-02, -4.1217e-02, -5.8852e-02],\n",
       "                         [-9.8070e-03, -3.1699e-02, -9.2495e-03],\n",
       "                         [-1.9617e-02, -3.3666e-02, -1.3720e-02]],\n",
       "               \n",
       "                        [[-2.6684e-10, -3.0160e-10, -3.6857e-10],\n",
       "                         [ 2.3273e-11,  1.5202e-14, -1.3265e-11],\n",
       "                         [-2.9654e-11, -5.8316e-12, -7.8959e-12]]],\n",
       "               \n",
       "               \n",
       "                       [[[-5.5981e-09, -2.2351e-09, -4.0166e-09],\n",
       "                         [-4.9434e-09,  5.0699e-14,  7.5350e-10],\n",
       "                         [-8.8780e-09, -6.1946e-09, -8.1922e-09]],\n",
       "               \n",
       "                        [[ 4.6999e-09, -7.3313e-09,  7.9593e-09],\n",
       "                         [ 6.1325e-09, -7.2503e-11,  6.0658e-09],\n",
       "                         [ 2.2682e-08,  1.7976e-08,  2.2995e-08]],\n",
       "               \n",
       "                        [[ 9.4384e-06, -6.9208e-05, -5.4734e-05],\n",
       "                         [ 1.0900e-04, -1.2384e-06,  9.8615e-06],\n",
       "                         [ 1.2048e-04, -4.5437e-05, -5.4930e-05]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.4932e-02, -5.8412e-02,  1.0370e-01],\n",
       "                         [-2.5604e-02, -2.6183e-03,  8.3763e-02],\n",
       "                         [ 5.1413e-03,  1.1169e-02, -3.2663e-02]],\n",
       "               \n",
       "                        [[-5.7412e-02, -1.9582e-02, -2.2906e-03],\n",
       "                         [-9.2099e-02, -1.2173e-01, -8.3524e-02],\n",
       "                         [-6.5382e-02, -7.3276e-02, -7.1680e-02]],\n",
       "               \n",
       "                        [[ 3.1661e-10,  2.2291e-10,  3.2832e-10],\n",
       "                         [ 3.9602e-11, -1.6383e-15,  1.0288e-10],\n",
       "                         [ 1.6816e-11, -1.1171e-10, -5.0309e-11]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.4621e-09, -3.1849e-09, -3.5086e-09],\n",
       "                         [ 4.9513e-09, -6.1502e-14, -1.0864e-09],\n",
       "                         [ 6.7791e-09,  1.0191e-09,  3.7969e-11]],\n",
       "               \n",
       "                        [[ 2.9178e-09,  1.5126e-09,  6.7321e-09],\n",
       "                         [ 1.5266e-09,  1.2450e-10,  5.6602e-09],\n",
       "                         [ 2.8594e-09,  1.7877e-09,  7.3055e-09]],\n",
       "               \n",
       "                        [[-9.5561e-04,  2.1545e-04, -7.9914e-04],\n",
       "                         [-1.0077e-03,  1.0481e-04, -5.4583e-04],\n",
       "                         [-2.8431e-04,  5.6609e-04, -4.5099e-04]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.1257e-02, -5.2822e-03,  4.1019e-02],\n",
       "                         [ 4.2501e-02, -5.5792e-02,  1.7058e-02],\n",
       "                         [-5.3511e-02, -5.7037e-02, -3.5724e-02]],\n",
       "               \n",
       "                        [[-7.2770e-02, -3.5676e-02,  4.0076e-02],\n",
       "                         [-1.2862e-01, -9.4058e-02,  4.0301e-03],\n",
       "                         [-3.9669e-02, -1.3539e-01, -8.0446e-02]],\n",
       "               \n",
       "                        [[-2.8030e-10,  4.9703e-11, -1.4815e-11],\n",
       "                         [-3.1979e-10, -7.3634e-15, -7.1997e-11],\n",
       "                         [-4.0096e-10, -9.7011e-11, -1.4980e-10]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 2.2880e-11, -6.8604e-09, -1.0350e-10],\n",
       "                         [-1.0027e-09, -3.0671e-11, -6.0668e-12],\n",
       "                         [ 7.3188e-07, -2.8836e-11, -3.6492e-12]],\n",
       "               \n",
       "                        [[ 3.5373e-13, -4.0400e-12, -1.4284e-12],\n",
       "                         [-2.4369e-12, -1.0146e-13, -3.4939e-12],\n",
       "                         [ 9.5291e-12,  7.3065e-12,  9.1562e-12]],\n",
       "               \n",
       "                        [[-2.2517e-07, -8.6856e-08,  1.5494e-08],\n",
       "                         [-1.5625e-07, -1.5722e-08, -1.2224e-07],\n",
       "                         [ 5.2741e-09,  1.2460e-07, -1.5822e-07]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.3873e-05,  4.3708e-05,  2.1692e-05],\n",
       "                         [ 4.0341e-05, -7.0156e-06, -2.1583e-06],\n",
       "                         [ 2.6772e-05,  6.1512e-06, -2.5975e-06]],\n",
       "               \n",
       "                        [[ 6.2590e-05,  5.8450e-05,  5.5750e-05],\n",
       "                         [ 6.5459e-05,  6.1291e-05,  7.6915e-05],\n",
       "                         [ 6.0467e-05,  3.1937e-05,  4.0671e-05]],\n",
       "               \n",
       "                        [[ 2.4777e-12, -9.9457e-12, -6.0266e-13],\n",
       "                         [-1.7419e-07, -3.0055e-05, -8.1210e-12],\n",
       "                         [-2.9430e-12,  3.6882e-11, -2.5580e-10]]],\n",
       "               \n",
       "               \n",
       "                       [[[-9.2894e-06, -5.3182e-12, -1.3408e-05],\n",
       "                         [-4.1456e-06,  1.0104e-05, -3.4569e-11],\n",
       "                         [ 1.5860e-08, -3.8778e-11,  4.5460e-09]],\n",
       "               \n",
       "                        [[ 1.8446e-11,  3.8953e-05, -7.1026e-10],\n",
       "                         [-1.3736e-11, -1.2280e-06,  1.6687e-10],\n",
       "                         [-1.7391e-11,  2.3083e-12,  7.8132e-12]],\n",
       "               \n",
       "                        [[ 1.1763e-07, -5.8738e-08,  2.7405e-07],\n",
       "                         [ 1.1521e-07, -1.9392e-08,  2.3753e-07],\n",
       "                         [-2.7445e-07, -3.1383e-07,  6.7299e-08]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.4438e-06, -4.6207e-05, -6.6110e-05],\n",
       "                         [ 1.0338e-05, -1.1991e-04,  2.5502e-05],\n",
       "                         [-2.6466e-05, -1.0866e-04,  2.4234e-05]],\n",
       "               \n",
       "                        [[-4.9199e-05, -6.7689e-05, -2.7360e-05],\n",
       "                         [-1.2512e-05, -2.1336e-05, -1.2906e-05],\n",
       "                         [-1.7121e-05, -9.8807e-06, -1.8550e-05]],\n",
       "               \n",
       "                        [[-1.5282e-09,  6.8045e-13,  3.6644e-13],\n",
       "                         [-6.9498e-12,  1.4798e-13, -1.1098e-12],\n",
       "                         [-3.5303e-11,  9.0883e-06,  1.6583e-09]]],\n",
       "               \n",
       "               \n",
       "                       [[[-4.2470e-17,  9.0869e-18,  3.8624e-17],\n",
       "                         [-3.3798e-17, -9.6813e-21,  1.5410e-16],\n",
       "                         [-1.1759e-16, -9.8882e-17, -3.4711e-17]],\n",
       "               \n",
       "                        [[-1.7217e-13, -1.8038e-05, -3.5763e-10],\n",
       "                         [-2.3519e-08,  2.4552e-10,  1.0851e-10],\n",
       "                         [-1.1301e-07, -4.3517e-13,  8.8006e-12]],\n",
       "               \n",
       "                        [[-6.3730e-06, -2.3685e-10,  2.6822e-08],\n",
       "                         [-1.8144e-06, -1.1842e-07, -7.2378e-06],\n",
       "                         [ 4.1883e-07,  2.4364e-11, -1.8780e-09]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-4.6021e-09, -9.0652e-09, -6.2093e-09],\n",
       "                         [-5.6509e-09, -6.7279e-09, -4.7417e-09],\n",
       "                         [-1.0260e-08, -1.2658e-08, -6.4496e-09]],\n",
       "               \n",
       "                        [[-2.1360e-09, -6.1340e-10, -3.8333e-10],\n",
       "                         [ 5.3931e-09,  2.1428e-09,  3.5012e-09],\n",
       "                         [ 3.3609e-09,  2.4704e-09,  4.2788e-09]],\n",
       "               \n",
       "                        [[-6.3984e-12, -3.0834e-10, -4.0177e-12],\n",
       "                         [-1.4922e-06, -4.8941e-18,  3.5910e-07],\n",
       "                         [ 1.4504e-06,  2.3292e-07, -1.0962e-07]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.bias',\n",
       "               tensor([-2.0406e-05,  3.5173e-05, -1.5419e-05,  2.6365e-15, -3.6628e-05,\n",
       "                       -1.8834e-05,  1.6278e-05, -6.5249e-07, -6.8522e-06,  3.2679e-05,\n",
       "                       -1.9763e-08, -2.3694e-08,  4.0541e-10, -2.6957e-05,  2.6397e-09,\n",
       "                       -6.0672e-06,  2.2586e-15, -5.4183e-06,  2.0764e-05, -9.8987e-10,\n",
       "                        3.2684e-05,  1.4946e-10, -4.1078e-06,  1.5639e-05, -3.4745e-06,\n",
       "                        1.6995e-10,  1.0755e-12,  7.6707e-06,  2.1176e-07, -1.0038e-08,\n",
       "                       -7.2126e-09,  1.3586e-06, -3.1899e-08, -6.3760e-05,  6.9999e-07,\n",
       "                       -2.6418e-05, -1.7852e-06, -4.3299e-09,  5.8841e-05,  2.8174e-11,\n",
       "                        1.8878e-05,  1.4557e-05, -1.1693e-05,  9.7738e-06, -8.2288e-12,\n",
       "                       -1.5900e-05, -3.7226e-05,  1.8814e-05,  2.2460e-07, -1.6291e-05,\n",
       "                       -9.1343e-05, -1.2932e-05,  1.2253e-13, -1.1363e-15,  1.1431e-12,\n",
       "                        2.9468e-07, -9.4830e-11,  4.2850e-05,  5.7869e-05,  8.4614e-05,\n",
       "                        3.6495e-06,  3.0059e-08, -2.4405e-06,  7.9308e-06,  1.2737e-05,\n",
       "                        9.9165e-15,  6.0042e-05,  4.5768e-05, -4.2438e-07, -4.3027e-05,\n",
       "                        4.3703e-05,  4.2728e-08, -3.4229e-05, -1.2527e-06,  1.1962e-05,\n",
       "                       -1.8988e-13,  5.8409e-05,  3.8328e-05,  1.1938e-09,  1.8600e-07,\n",
       "                        6.4447e-06, -1.1560e-07,  1.2321e-05,  3.2855e-05,  1.2316e-05,\n",
       "                        4.9405e-05,  5.2613e-05, -1.2604e-08, -1.0587e-05, -1.4876e-05,\n",
       "                       -1.4652e-05, -3.9212e-06, -2.7241e-06,  1.0594e-05, -1.0739e-07,\n",
       "                        3.5286e-06,  4.4818e-05,  6.3394e-07,  1.6551e-05, -2.3077e-08,\n",
       "                        1.0002e-05, -4.5723e-06,  1.5702e-05, -3.7171e-07,  3.6112e-05,\n",
       "                        2.5335e-05,  1.2401e-11,  3.0312e-05, -1.7065e-05, -1.7602e-05,\n",
       "                       -1.5203e-05,  7.5026e-05,  1.4802e-07,  4.9596e-05, -1.4031e-06,\n",
       "                        1.7149e-07,  1.2262e-05,  4.5327e-05,  5.7158e-06, -2.9078e-05,\n",
       "                       -2.4831e-05, -2.8516e-05,  1.9137e-05, -3.1485e-05,  2.8334e-05,\n",
       "                        1.5225e-06,  1.6736e-05,  1.8386e-06], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.bias',\n",
       "               tensor([-2.4314e-01, -2.9312e-01, -2.2859e-01, -2.9805e-08, -2.6118e-01,\n",
       "                       -2.4971e-01, -2.4589e-01, -3.1100e-04, -2.7819e-08, -2.1591e-01,\n",
       "                       -1.7417e-02, -3.2340e-07, -6.2893e-04, -1.0585e-01, -4.4664e-03,\n",
       "                       -2.5977e-01, -3.3347e-09, -3.2564e-06, -1.4709e-01, -6.2392e-06,\n",
       "                       -2.5352e-01, -2.4089e-06, -1.6861e-01, -4.1476e-03, -2.4386e-01,\n",
       "                       -5.1360e-10, -4.3788e-10, -8.0612e-05, -4.7549e-02, -2.3328e-03,\n",
       "                       -2.3646e-03, -1.6846e-01, -1.2571e-02, -1.8101e-01, -4.7438e-02,\n",
       "                       -1.4776e-01, -3.0470e-01, -5.8783e-03, -2.0750e-01, -1.4154e-09,\n",
       "                       -2.4875e-01, -1.3429e-01, -2.6566e-02, -2.9824e-01, -2.3981e-07,\n",
       "                       -2.0607e-01, -1.4942e-01, -1.6507e-01, -2.6775e-03, -1.5419e-01,\n",
       "                       -1.5356e-01, -2.6554e-06, -3.7578e-11, -3.5204e-11, -2.0860e-11,\n",
       "                       -5.9436e-02, -1.6219e-11, -1.7647e-01, -3.4265e-01, -2.3697e-01,\n",
       "                       -3.2613e-01, -1.8613e-08, -1.9743e-01, -2.0404e-01, -5.0049e-02,\n",
       "                       -1.7521e-10, -5.2685e-02, -2.7390e-01, -1.8548e-02, -1.2294e-01,\n",
       "                       -1.7823e-01, -1.0978e-02, -2.0746e-01, -4.6118e-02, -2.6810e-01,\n",
       "                       -1.3843e-11, -1.6714e-01, -1.2479e-01, -3.4795e-03, -1.6691e-01,\n",
       "                       -1.8222e-01, -1.0354e-02, -2.2556e-01, -1.9901e-01, -2.1406e-01,\n",
       "                       -1.8517e-01, -2.5913e-01, -3.8549e-05, -1.6392e-01, -2.1399e-01,\n",
       "                       -4.5790e-02, -4.2655e-03, -2.1778e-01, -1.0557e-01, -1.6053e-03,\n",
       "                       -5.3120e-01, -2.0070e-01, -5.1839e-02, -2.1774e-01, -3.3985e-03,\n",
       "                       -2.5171e-01, -1.5980e-01, -1.5808e-01, -2.9667e-02, -1.2905e-01,\n",
       "                       -2.8953e-01, -9.4949e-09, -1.7311e-01,  4.2157e-04, -1.7877e-01,\n",
       "                       -1.8861e-02, -1.6365e-01, -2.5693e-05, -2.7420e-01, -2.1900e-01,\n",
       "                       -2.5426e-02, -1.4915e-01, -2.8173e-01, -1.5140e-01, -2.3000e-01,\n",
       "                       -8.2773e-02,  2.8032e-02, -1.4648e-01, -2.9048e-01, -1.8561e-01,\n",
       "                       -8.4501e-03, -2.7620e-02, -2.2373e-06], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.weight',\n",
       "               tensor([ 3.2580e-01,  3.0331e-01,  4.0500e-01,  3.6898e-11,  3.8390e-01,\n",
       "                        4.4220e-01,  3.1219e-01, -2.7435e-04, -4.5110e-06,  3.4924e-01,\n",
       "                       -2.3434e-03,  1.0425e-06, -1.7786e-04,  2.1262e-01, -9.4724e-04,\n",
       "                        4.1983e-01,  1.4971e-13,  3.2078e-05,  2.5351e-01, -1.2322e-04,\n",
       "                        4.1877e-01, -4.5642e-05,  1.9690e-01,  7.9828e-04,  3.3277e-01,\n",
       "                       -3.5722e-07, -2.6916e-11, -1.0832e-04, -6.1943e-03,  5.3533e-04,\n",
       "                        5.5096e-04,  3.2269e-02, -1.0511e-03,  4.0669e-01,  1.9807e-03,\n",
       "                        3.1191e-01,  3.5965e-01, -8.9268e-04,  4.2313e-01, -2.7873e-06,\n",
       "                        3.2429e-01,  2.8309e-01, -4.0523e-03,  2.1717e-01,  7.9368e-06,\n",
       "                        4.6340e-01,  4.1104e-01,  3.1907e-01, -6.9437e-04,  2.5711e-01,\n",
       "                        4.4330e-01, -5.7296e-05, -2.3955e-11,  6.4831e-15, -3.5885e-14,\n",
       "                        7.2721e-03, -2.9823e-10,  3.4877e-01,  3.9328e-01,  4.1184e-01,\n",
       "                        3.4909e-01,  1.6782e-05,  3.5294e-01,  1.5899e-01,  2.8694e-01,\n",
       "                       -1.3511e-07,  2.8676e-01,  2.2307e-01,  2.5296e-03,  3.1783e-01,\n",
       "                        3.3190e-01, -1.4214e-03,  3.4460e-01,  5.5165e-03,  1.5487e-01,\n",
       "                        1.4479e-09,  6.3576e-01,  2.5802e-01, -8.7167e-04,  3.8136e-01,\n",
       "                        3.1344e-01,  1.5164e-03,  2.2566e-01,  2.6587e-01,  3.2659e-01,\n",
       "                        5.2100e-01,  3.3760e-01, -6.1800e-05,  3.3968e-01,  2.9186e-01,\n",
       "                        9.6890e-03,  6.5048e-04,  3.8361e-01,  2.9948e-01,  3.6021e-04,\n",
       "                        3.7547e-01,  4.2933e-01, -5.7489e-03,  3.8708e-01,  6.2165e-04,\n",
       "                        3.6411e-01,  4.6567e-01,  2.3415e-01,  3.8900e-03,  2.7701e-01,\n",
       "                        3.6609e-01,  3.7925e-05,  3.1057e-01,  1.7314e-02,  2.5449e-01,\n",
       "                       -2.7270e-03,  6.4993e-01,  9.5306e-05,  3.8480e-01,  2.3695e-01,\n",
       "                       -3.0346e-03,  2.6352e-01,  3.9519e-01,  2.8656e-01,  3.0420e-01,\n",
       "                        2.8708e-01,  2.6791e-01,  2.2514e-01,  4.9756e-01,  2.6420e-01,\n",
       "                        1.2177e-03, -3.7303e-03, -2.5307e-05], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.weight',\n",
       "               tensor([[[[-6.1960e-02, -1.6569e-02, -6.6203e-02],\n",
       "                         [-3.6323e-02, -5.9095e-02, -2.6631e-03],\n",
       "                         [-6.9517e-02, -2.1625e-02,  2.8976e-02]],\n",
       "               \n",
       "                        [[ 2.6452e-02,  1.8811e-02,  1.6651e-02],\n",
       "                         [-1.5386e-02, -1.3704e-03, -2.5053e-02],\n",
       "                         [ 6.7156e-02, -8.2382e-03, -1.5394e-02]],\n",
       "               \n",
       "                        [[ 8.9769e-02, -3.9372e-02,  3.8401e-02],\n",
       "                         [-2.5864e-02,  2.9448e-02, -3.1073e-02],\n",
       "                         [ 3.4205e-02,  3.3725e-02,  1.4222e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.3395e-04,  9.3650e-05,  2.9191e-04],\n",
       "                         [ 9.4381e-05, -2.9532e-06,  1.8337e-04],\n",
       "                         [ 2.7281e-04,  1.9220e-04,  3.7205e-04]],\n",
       "               \n",
       "                        [[ 3.5244e-04,  1.4927e-04,  1.3675e-04],\n",
       "                         [ 2.5572e-04,  1.4768e-06,  8.7981e-05],\n",
       "                         [ 3.1859e-04,  1.4703e-04,  2.1130e-04]],\n",
       "               \n",
       "                        [[ 1.4427e-07,  7.4549e-08,  1.5940e-07],\n",
       "                         [ 8.1899e-08, -5.1598e-10,  9.8489e-08],\n",
       "                         [ 1.8100e-07,  1.0639e-07,  1.9193e-07]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.6325e-02,  1.8671e-02, -9.5657e-03],\n",
       "                         [-2.0848e-02, -1.1758e-02, -7.7211e-03],\n",
       "                         [-8.2603e-03, -1.0107e-01, -3.1424e-02]],\n",
       "               \n",
       "                        [[ 4.3208e-02, -2.6571e-02, -7.0447e-02],\n",
       "                         [ 5.4213e-02, -1.1906e-01, -1.0027e-01],\n",
       "                         [ 6.5878e-02,  2.5202e-02, -5.1793e-02]],\n",
       "               \n",
       "                        [[ 3.6720e-02,  4.6455e-04,  3.4982e-02],\n",
       "                         [ 3.7334e-02, -1.4459e-02, -3.3287e-02],\n",
       "                         [-4.0532e-04,  1.2515e-01,  3.7805e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 7.6166e-05,  5.9613e-06, -2.0802e-06],\n",
       "                         [ 1.5584e-04,  6.9365e-05,  9.9578e-05],\n",
       "                         [ 1.5798e-04,  7.4947e-05,  6.0668e-05]],\n",
       "               \n",
       "                        [[ 1.0155e-04, -1.1584e-05,  3.3116e-05],\n",
       "                         [ 1.4812e-04, -1.4802e-05,  6.9185e-05],\n",
       "                         [ 1.3885e-04,  1.8587e-05,  4.6411e-05]],\n",
       "               \n",
       "                        [[-2.7862e-08,  1.6845e-08,  3.3894e-08],\n",
       "                         [-3.4410e-08, -2.5474e-10,  1.1687e-08],\n",
       "                         [-3.1877e-08, -7.6659e-10,  1.7430e-08]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 3.2263e-03,  2.0115e-02,  3.4748e-02],\n",
       "                         [-3.6406e-02, -7.1295e-02, -1.7224e-02],\n",
       "                         [-3.6617e-02, -3.7218e-02, -1.4759e-02]],\n",
       "               \n",
       "                        [[-9.9831e-03, -2.2717e-02,  2.3014e-02],\n",
       "                         [-2.3883e-02, -5.5210e-02,  5.0784e-02],\n",
       "                         [ 8.4228e-03, -6.7079e-02,  6.2211e-02]],\n",
       "               \n",
       "                        [[-3.3314e-02,  7.7782e-02,  2.3944e-02],\n",
       "                         [-2.0357e-03,  5.7686e-02, -1.9311e-02],\n",
       "                         [ 6.6255e-02, -8.1824e-02, -6.5641e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-5.5092e-05, -2.0661e-05, -8.4872e-05],\n",
       "                         [-1.3893e-04, -8.5195e-05, -1.3507e-04],\n",
       "                         [-1.9281e-04, -1.3179e-04, -1.4386e-04]],\n",
       "               \n",
       "                        [[ 4.0300e-05,  1.5035e-05,  1.2742e-04],\n",
       "                         [-3.1271e-05, -3.7972e-05,  8.8305e-05],\n",
       "                         [ 1.0692e-06,  2.2948e-05,  1.7588e-04]],\n",
       "               \n",
       "                        [[-1.7012e-08, -2.7865e-08, -4.0207e-09],\n",
       "                         [ 8.8916e-09, -1.0357e-09,  2.5528e-08],\n",
       "                         [ 3.9924e-08,  2.8614e-08,  5.1537e-08]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-5.9649e-02,  1.1379e-03, -6.6844e-03],\n",
       "                         [-5.4955e-02,  8.4823e-03,  3.3630e-02],\n",
       "                         [-4.1656e-02,  6.7145e-03,  6.2612e-02]],\n",
       "               \n",
       "                        [[ 3.4316e-02,  4.8737e-02,  2.3129e-02],\n",
       "                         [ 2.7036e-02, -4.7869e-04,  5.3420e-02],\n",
       "                         [ 4.6899e-02,  3.7891e-02,  3.4531e-02]],\n",
       "               \n",
       "                        [[ 1.0286e-02,  1.2970e-02,  2.2359e-02],\n",
       "                         [-3.0882e-02, -7.8964e-02, -1.5085e-01],\n",
       "                         [-8.1531e-02, -1.2103e-01, -7.2910e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.0593e-04,  6.3892e-05,  3.5797e-05],\n",
       "                         [ 7.2101e-05,  2.8521e-05,  8.5600e-06],\n",
       "                         [ 7.6535e-05,  7.5224e-05,  7.7826e-05]],\n",
       "               \n",
       "                        [[ 5.5127e-05,  8.8739e-05,  1.2570e-04],\n",
       "                         [ 2.6534e-05,  9.0444e-05,  1.3672e-04],\n",
       "                         [ 6.0486e-05,  1.8172e-04,  2.0080e-04]],\n",
       "               \n",
       "                        [[-6.1314e-08, -1.0170e-07, -1.5296e-07],\n",
       "                         [ 2.2078e-08,  5.8465e-10, -6.5384e-08],\n",
       "                         [-5.9621e-08, -7.9732e-08, -1.4225e-07]]],\n",
       "               \n",
       "               \n",
       "                       [[[-6.9475e-02, -2.5261e-02, -1.6468e-02],\n",
       "                         [-5.6557e-02, -7.4668e-02, -3.3056e-02],\n",
       "                         [-7.6941e-02, -5.7562e-02,  1.9859e-02]],\n",
       "               \n",
       "                        [[-4.1933e-02,  1.4343e-02,  1.8715e-02],\n",
       "                         [ 1.4841e-02,  5.6727e-02,  9.0832e-02],\n",
       "                         [-2.6755e-02,  9.9046e-03,  2.4459e-02]],\n",
       "               \n",
       "                        [[ 1.0409e-02,  1.0096e-01,  3.8744e-02],\n",
       "                         [ 8.9367e-02,  2.9718e-02,  3.4387e-02],\n",
       "                         [ 3.7007e-04,  3.4291e-02,  8.3277e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 3.7865e-04,  3.1881e-04,  4.2882e-04],\n",
       "                         [-1.1599e-05, -4.8525e-05,  8.0282e-05],\n",
       "                         [ 4.7951e-05,  8.1192e-05,  2.0544e-04]],\n",
       "               \n",
       "                        [[ 6.1804e-04,  4.5126e-04,  3.8024e-04],\n",
       "                         [ 1.9698e-04, -8.0195e-05, -7.1391e-05],\n",
       "                         [ 1.2887e-04, -1.4206e-04, -7.9313e-05]],\n",
       "               \n",
       "                        [[-2.1397e-07, -1.6462e-07, -1.8448e-07],\n",
       "                         [-9.6336e-08,  1.6786e-10, -4.5572e-08],\n",
       "                         [-1.5287e-07, -5.7114e-08, -9.5188e-08]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 3.3883e-02,  2.6999e-02,  3.2858e-02],\n",
       "                         [ 1.7226e-03, -3.5770e-02, -8.6720e-03],\n",
       "                         [ 5.2711e-02,  4.6235e-02,  4.1992e-02]],\n",
       "               \n",
       "                        [[-1.5386e-02,  5.8942e-03, -1.9233e-02],\n",
       "                         [-1.8292e-02, -1.2981e-02, -1.9507e-02],\n",
       "                         [-1.7768e-02,  1.4432e-02,  2.4108e-02]],\n",
       "               \n",
       "                        [[ 5.8530e-02,  2.5678e-02,  7.3654e-02],\n",
       "                         [ 1.9554e-02,  6.8520e-02,  4.9164e-02],\n",
       "                         [-1.6953e-02,  3.9406e-02,  8.6250e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.2195e-05, -8.6619e-05, -7.8640e-05],\n",
       "                         [ 3.4793e-05, -4.5191e-05, -1.4722e-05],\n",
       "                         [ 5.8857e-05, -2.2794e-05, -1.1195e-05]],\n",
       "               \n",
       "                        [[ 2.5800e-05, -8.2159e-05, -2.7798e-05],\n",
       "                         [ 1.0496e-04,  1.0162e-05,  5.0213e-05],\n",
       "                         [ 5.4140e-05, -1.8655e-05,  1.8996e-05]],\n",
       "               \n",
       "                        [[ 3.8606e-08, -7.4479e-09, -1.2587e-11],\n",
       "                         [ 3.0645e-08,  9.3067e-11, -1.5663e-10],\n",
       "                         [ 2.4819e-08, -4.5697e-09, -4.4357e-09]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.bias',\n",
       "               tensor([-4.0175e-06, -3.6263e-06,  5.4038e-05, -2.2176e-05, -4.2153e-06,\n",
       "                       -7.2098e-05, -6.6451e-05, -3.4996e-05,  6.1417e-05,  4.3286e-05,\n",
       "                       -2.9088e-05,  1.9201e-05,  3.8114e-05, -5.4571e-05, -2.6501e-05,\n",
       "                       -2.1652e-05, -3.1895e-05,  7.6149e-06, -6.3673e-05, -1.0885e-05,\n",
       "                        2.5168e-05, -3.1701e-05, -2.4665e-05, -3.1779e-05, -1.9439e-05,\n",
       "                        1.7696e-05, -1.2069e-05,  1.1697e-05, -3.8357e-05, -4.8268e-05,\n",
       "                       -4.5662e-05, -1.7509e-05,  1.3484e-05, -8.8117e-06, -6.8841e-06,\n",
       "                       -1.6619e-05,  3.0056e-05,  3.6852e-05,  1.6994e-05,  1.1243e-05,\n",
       "                       -3.8680e-05, -5.5314e-05, -1.9058e-05, -2.2871e-05,  6.6182e-05,\n",
       "                       -7.2356e-05, -4.5958e-05, -9.5446e-05, -5.9705e-05,  3.4374e-06,\n",
       "                       -1.4409e-05,  1.3720e-05, -1.1359e-05, -5.3315e-06,  3.1879e-05,\n",
       "                       -4.4204e-05, -7.9723e-06, -9.8079e-06, -1.0962e-05,  8.8072e-06,\n",
       "                        9.9203e-14,  4.0236e-05,  7.9325e-05,  2.6618e-05,  1.9072e-05,\n",
       "                        8.8354e-06,  6.5234e-06, -4.8920e-06, -8.0500e-05, -4.1660e-08,\n",
       "                       -2.9376e-05, -1.9406e-05,  9.4025e-06,  2.0915e-06, -1.5076e-05,\n",
       "                       -5.2262e-06,  1.5099e-05,  8.9577e-05, -3.2747e-06,  6.1327e-05,\n",
       "                        4.6521e-06, -4.8170e-05, -7.4651e-07, -3.1341e-05, -4.1830e-05,\n",
       "                        6.3689e-05,  9.2538e-07, -4.1745e-05, -2.8395e-05, -1.8551e-05,\n",
       "                       -3.7951e-05,  3.4444e-05, -2.3774e-05, -2.2038e-05, -3.2681e-05,\n",
       "                        7.1134e-05, -2.0752e-05, -1.1304e-05,  2.5880e-05, -5.0285e-05,\n",
       "                       -8.0923e-06,  7.9369e-06,  8.4295e-06, -4.2367e-05,  1.3188e-05,\n",
       "                        2.3921e-05, -1.2868e-05, -5.4303e-05,  5.0225e-05, -4.2147e-05,\n",
       "                        6.3390e-06,  4.8375e-05, -4.6489e-05, -6.1534e-07,  2.1714e-05,\n",
       "                       -1.6768e-05,  5.5734e-06, -3.6248e-05, -4.6292e-05,  2.2542e-05,\n",
       "                        4.7836e-05, -1.2732e-05,  2.2801e-05,  6.4414e-05,  2.1190e-05,\n",
       "                        3.7467e-05,  3.6082e-05,  5.5052e-07], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.bias',\n",
       "               tensor([-3.1589e-01, -4.1927e-01, -3.5825e-01, -4.7573e-01, -2.7399e-01,\n",
       "                       -3.3854e-01, -4.3674e-01, -3.4337e-01, -1.9997e-01, -2.9109e-01,\n",
       "                       -3.6079e-01, -2.6478e-01, -3.4245e-01, -3.5442e-01, -4.9047e-01,\n",
       "                       -3.4828e-01, -3.9765e-01, -2.7572e-01, -3.7176e-01, -3.7581e-01,\n",
       "                       -2.3270e-01, -2.6813e-01, -3.7735e-01, -3.3378e-01, -5.1989e-01,\n",
       "                       -4.0050e-01, -3.2999e-01, -2.2467e-01, -3.7878e-01, -2.6067e-01,\n",
       "                       -3.2735e-01, -2.9100e-01, -2.4813e-01, -1.0912e-03, -2.7846e-01,\n",
       "                       -2.5409e-01, -2.6175e-01, -2.8854e-01, -3.4902e-01, -3.7556e-01,\n",
       "                       -3.5267e-01, -4.3581e-01, -2.6827e-01, -2.4023e-01, -4.1707e-01,\n",
       "                       -4.1558e-01, -2.8909e-01, -3.7008e-01, -1.8639e-01, -3.5482e-01,\n",
       "                       -4.2350e-01, -4.4763e-01, -2.9387e-01, -3.1081e-01, -4.4574e-01,\n",
       "                       -3.6355e-01, -4.2133e-01, -3.3121e-01, -3.2590e-01, -3.1683e-01,\n",
       "                       -4.2918e-09, -3.2574e-01, -2.8021e-01, -3.6148e-01, -3.1358e-01,\n",
       "                       -3.7669e-01, -2.9969e-01, -5.0242e-01, -3.6992e-01, -1.5394e-02,\n",
       "                       -3.3490e-01, -3.4427e-01, -3.8804e-01, -2.6965e-01, -3.4636e-01,\n",
       "                       -3.9857e-01, -3.1257e-01, -3.3254e-01, -2.9349e-01, -4.0332e-01,\n",
       "                       -2.6554e-01, -2.8679e-01, -3.3347e-01, -3.9555e-01, -3.6703e-01,\n",
       "                       -4.8744e-01, -2.7543e-01, -3.4304e-01, -3.0239e-01, -3.5533e-01,\n",
       "                       -2.1562e-01, -3.8308e-01, -3.1366e-01, -4.6224e-01, -2.9973e-01,\n",
       "                       -3.2180e-01, -2.6166e-01, -4.9887e-01, -3.6083e-01, -2.8288e-01,\n",
       "                       -2.3219e-01, -2.4220e-01, -4.3521e-01, -3.0871e-01, -3.1682e-01,\n",
       "                       -3.0380e-01, -2.9187e-01, -6.1552e-01, -3.5794e-01, -4.1306e-01,\n",
       "                       -3.7652e-01, -4.4189e-01, -3.4541e-01, -2.3493e-02, -3.3259e-01,\n",
       "                       -2.9053e-01, -3.4541e-01, -2.9701e-01, -2.5153e-01, -3.2045e-01,\n",
       "                       -3.2744e-01, -1.9501e-01, -3.2432e-01, -4.1838e-01, -3.4242e-01,\n",
       "                       -3.5950e-01, -4.1823e-01, -3.5952e-01], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.weight',\n",
       "               tensor([ 2.8773e-01,  3.2645e-01,  3.1784e-01,  3.5053e-01,  2.7200e-01,\n",
       "                        3.5852e-01,  3.5756e-01,  2.7948e-01,  3.1942e-01,  3.0325e-01,\n",
       "                        1.9271e-01,  1.6913e-01,  2.8690e-01,  2.8496e-01,  3.6482e-01,\n",
       "                        3.4104e-01,  3.6028e-01,  3.6793e-01,  3.0561e-01,  3.9667e-01,\n",
       "                        2.8285e-01,  2.3345e-01,  2.7313e-01,  2.8076e-01,  3.1961e-01,\n",
       "                        3.5237e-01,  2.2746e-01,  1.5749e-01,  3.4746e-01,  2.7880e-01,\n",
       "                        2.4333e-01,  2.2918e-01,  2.2394e-01, -6.4871e-04,  3.3011e-01,\n",
       "                        2.7247e-01,  2.8749e-01,  2.7702e-01,  2.4496e-01,  3.7956e-01,\n",
       "                        3.2581e-01,  3.7993e-01,  2.7388e-01,  2.8098e-01,  3.2295e-01,\n",
       "                        3.0931e-01,  1.9935e-01,  3.3328e-01,  1.7780e-01,  2.3090e-01,\n",
       "                        4.0989e-01,  2.4771e-01,  2.4737e-01,  3.5122e-01,  4.3881e-01,\n",
       "                        2.7300e-01,  3.4199e-01,  2.7164e-01,  2.7939e-01,  2.6560e-01,\n",
       "                        4.6979e-08,  2.9568e-01,  2.6773e-01,  3.6727e-01,  2.8461e-01,\n",
       "                        3.3054e-01,  2.7620e-01,  3.7905e-01,  2.8925e-01,  1.6852e-03,\n",
       "                        3.2017e-01,  3.2003e-01,  3.1019e-01,  2.0393e-01,  2.5361e-01,\n",
       "                        2.4771e-01,  2.5711e-01,  3.1160e-01,  2.1538e-01,  3.4892e-01,\n",
       "                        2.3879e-01,  3.4815e-01,  2.8578e-01,  3.8354e-01,  3.3464e-01,\n",
       "                        3.9922e-01,  1.2935e-01,  3.1378e-01,  1.3791e-01,  2.8273e-01,\n",
       "                        3.1537e-01,  2.7034e-01,  2.7625e-01,  4.0920e-01,  2.7444e-01,\n",
       "                        2.7936e-01,  2.7500e-01,  3.2000e-01,  3.1437e-01,  2.9187e-01,\n",
       "                        2.2841e-01,  2.5427e-01,  2.9564e-01,  2.2779e-01,  3.3538e-01,\n",
       "                        2.2070e-01,  2.7579e-01,  3.1461e-01,  3.6574e-01,  2.5385e-01,\n",
       "                        1.9968e-01,  3.7138e-01,  2.7304e-01,  4.4665e-03,  2.8051e-01,\n",
       "                        1.6528e-01,  3.2688e-01,  2.8579e-01,  2.7798e-01,  2.4691e-01,\n",
       "                        2.9926e-01,  1.5304e-01,  2.4167e-01,  3.5986e-01,  2.5297e-01,\n",
       "                        2.6766e-01,  4.0788e-01,  1.5750e-01], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.weight',\n",
       "               tensor([[[[ 9.9778e-04,  3.3163e-04, -9.2129e-03],\n",
       "                         [ 7.0417e-03,  1.2431e-02, -9.9109e-03],\n",
       "                         [ 2.5050e-02, -2.4722e-02,  4.3863e-02]],\n",
       "               \n",
       "                        [[ 5.3958e-02,  2.1432e-02, -3.5378e-03],\n",
       "                         [-2.3956e-02,  2.5122e-02,  2.0594e-02],\n",
       "                         [ 9.3363e-03,  2.1138e-02,  2.3883e-02]],\n",
       "               \n",
       "                        [[-4.9568e-02,  4.1430e-02,  1.2536e-02],\n",
       "                         [-5.5346e-02, -5.0161e-02, -7.9876e-02],\n",
       "                         [-6.9574e-02, -5.7016e-02, -4.6269e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.6818e-02, -1.2710e-02,  2.9447e-02],\n",
       "                         [ 3.9709e-03, -1.6218e-02,  1.3024e-02],\n",
       "                         [ 3.5152e-02,  3.8046e-02,  7.1858e-02]],\n",
       "               \n",
       "                        [[-1.2795e-02, -5.2389e-02, -9.3185e-03],\n",
       "                         [-1.6508e-02, -2.6275e-02,  9.5672e-03],\n",
       "                         [-2.0318e-02, -2.5747e-02, -3.5922e-02]],\n",
       "               \n",
       "                        [[-3.7105e-02, -2.1829e-02, -2.5974e-02],\n",
       "                         [-3.5122e-02, -1.4930e-02, -4.2481e-02],\n",
       "                         [-2.0184e-02, -3.7544e-03, -6.0110e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.6584e-02, -6.0337e-03,  2.5484e-02],\n",
       "                         [-9.5829e-03,  6.0405e-04, -9.1913e-03],\n",
       "                         [-1.3705e-02, -2.1452e-02,  7.7514e-03]],\n",
       "               \n",
       "                        [[-1.1449e-02,  4.7827e-02,  3.8491e-02],\n",
       "                         [-9.7258e-03, -6.0006e-03, -2.8851e-02],\n",
       "                         [ 4.6326e-02,  2.8360e-02, -3.8896e-03]],\n",
       "               \n",
       "                        [[-1.5261e-02,  1.2969e-02, -4.2735e-03],\n",
       "                         [-4.5670e-04, -8.2751e-03,  2.3258e-02],\n",
       "                         [ 2.0378e-02,  2.9149e-02,  2.0408e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.2469e-03, -3.8900e-02, -4.6715e-02],\n",
       "                         [-1.8973e-02,  5.2199e-03,  2.7200e-02],\n",
       "                         [-1.2225e-02, -3.2126e-02, -3.9801e-02]],\n",
       "               \n",
       "                        [[-1.4183e-02, -4.8064e-02, -5.7053e-02],\n",
       "                         [-8.8992e-03, -2.1227e-02, -1.2265e-02],\n",
       "                         [-2.9779e-03,  4.0814e-02, -4.8016e-02]],\n",
       "               \n",
       "                        [[ 1.7102e-02, -3.5757e-03,  1.8066e-02],\n",
       "                         [ 1.5270e-02,  2.6521e-02,  1.6161e-02],\n",
       "                         [-2.6738e-03,  9.7962e-03,  2.4171e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-7.3522e-32,  1.2039e-31, -8.3366e-32],\n",
       "                         [-1.7927e-31,  1.3583e-31,  2.8559e-32],\n",
       "                         [ 8.1357e-32,  1.9568e-31, -2.5618e-31]],\n",
       "               \n",
       "                        [[ 6.9684e-33, -1.8830e-32, -7.6069e-32],\n",
       "                         [-1.0805e-31, -1.5628e-31,  4.0574e-31],\n",
       "                         [-2.2430e-32,  2.1856e-31, -9.3721e-32]],\n",
       "               \n",
       "                        [[-7.0594e-32, -7.7102e-32,  5.1208e-32],\n",
       "                         [-7.0389e-32,  7.2547e-32, -8.0496e-32],\n",
       "                         [-1.1347e-31, -2.5095e-31, -3.1023e-31]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-3.0213e-33,  6.9534e-32, -4.1005e-32],\n",
       "                         [ 2.1617e-31,  2.3366e-31,  1.4105e-32],\n",
       "                         [-2.6127e-31, -1.8371e-31,  9.6908e-32]],\n",
       "               \n",
       "                        [[ 4.7716e-32,  7.1799e-32,  5.3337e-32],\n",
       "                         [ 9.1727e-31, -4.6724e-31,  7.9919e-31],\n",
       "                         [-1.9667e-31, -4.9191e-31, -5.4787e-32]],\n",
       "               \n",
       "                        [[ 5.2456e-34,  1.8097e-32,  3.0099e-32],\n",
       "                         [-8.2418e-32,  2.3112e-31, -1.0252e-31],\n",
       "                         [-9.6066e-33,  1.2679e-31, -1.4505e-33]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 5.1845e-02, -1.8135e-03,  1.5210e-02],\n",
       "                         [ 4.2588e-02, -1.4843e-02, -2.4964e-02],\n",
       "                         [-3.9550e-02, -3.7463e-02, -4.7718e-02]],\n",
       "               \n",
       "                        [[ 6.5846e-02, -5.1748e-02, -4.6844e-02],\n",
       "                         [ 7.3461e-02, -3.8932e-02, -7.1846e-02],\n",
       "                         [ 5.1154e-02,  2.0614e-02, -3.6266e-02]],\n",
       "               \n",
       "                        [[-8.5820e-03,  5.1156e-02,  2.2966e-02],\n",
       "                         [ 4.3015e-02,  4.6919e-02, -4.0666e-04],\n",
       "                         [ 5.0392e-02,  5.2287e-03,  2.9757e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.7710e-02, -2.3285e-02, -3.6574e-02],\n",
       "                         [ 5.7922e-02, -1.8546e-02, -2.0589e-02],\n",
       "                         [ 3.0850e-02,  4.1965e-04, -3.5892e-02]],\n",
       "               \n",
       "                        [[ 1.4062e-02, -1.0276e-02,  3.9592e-02],\n",
       "                         [-2.9619e-02, -5.4621e-02, -1.8322e-02],\n",
       "                         [ 3.8410e-03, -4.9907e-02,  3.4613e-03]],\n",
       "               \n",
       "                        [[-1.3507e-02, -3.1780e-02, -4.9562e-02],\n",
       "                         [-3.1390e-02,  1.7744e-02,  3.6704e-03],\n",
       "                         [ 2.7440e-02,  1.2006e-02, -3.1826e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 4.1635e-20,  6.9628e-20, -6.5431e-20],\n",
       "                         [-4.3868e-20,  2.0405e-20,  2.3490e-20],\n",
       "                         [-5.4386e-20, -3.7526e-20,  4.6789e-20]],\n",
       "               \n",
       "                        [[-1.4150e-19, -5.6220e-20, -2.1022e-20],\n",
       "                         [-7.2307e-20, -4.0779e-20, -1.0818e-19],\n",
       "                         [-6.4599e-20, -1.5369e-20, -1.2913e-19]],\n",
       "               \n",
       "                        [[-2.5254e-20, -8.0391e-20,  1.8564e-19],\n",
       "                         [-6.8719e-20, -1.1733e-19,  1.2199e-19],\n",
       "                         [-2.1766e-20, -1.1647e-19,  8.3617e-20]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.3893e-19,  1.2382e-19,  6.3498e-20],\n",
       "                         [ 1.2753e-19,  4.6538e-20,  1.2453e-19],\n",
       "                         [ 1.2412e-20,  6.6510e-20,  1.9580e-20]],\n",
       "               \n",
       "                        [[-1.5391e-20, -4.5116e-21,  4.8791e-20],\n",
       "                         [-1.3868e-19, -8.9483e-20,  1.8641e-20],\n",
       "                         [ 4.8393e-21,  5.9754e-20,  3.5231e-20]],\n",
       "               \n",
       "                        [[ 1.2656e-20,  1.4579e-20, -4.8593e-21],\n",
       "                         [ 2.2461e-21,  3.9061e-20,  8.9407e-20],\n",
       "                         [ 1.3760e-20,  3.3376e-20,  2.7835e-20]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 4.0213e-02,  1.5852e-02, -1.8317e-03],\n",
       "                         [ 3.0827e-02, -1.9880e-03, -1.9123e-03],\n",
       "                         [ 1.6210e-02, -6.9317e-03, -2.3761e-02]],\n",
       "               \n",
       "                        [[-7.3398e-03, -4.4194e-02, -1.4897e-03],\n",
       "                         [ 5.6764e-03,  1.6559e-02, -2.3187e-02],\n",
       "                         [-2.4680e-02, -6.6219e-02, -1.8164e-02]],\n",
       "               \n",
       "                        [[-1.1870e-02, -7.6274e-02, -4.5903e-02],\n",
       "                         [-3.0319e-02, -5.8130e-02, -3.2318e-02],\n",
       "                         [-1.0270e-02, -7.3530e-02, -9.6724e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 4.6543e-02, -5.4769e-03,  6.2513e-02],\n",
       "                         [ 2.4757e-02, -1.5262e-03,  1.8143e-02],\n",
       "                         [-5.0629e-03, -2.0088e-02,  7.7641e-03]],\n",
       "               \n",
       "                        [[-1.3439e-02, -9.5536e-03, -4.9379e-02],\n",
       "                         [-9.2892e-03, -1.1310e-02, -5.5121e-02],\n",
       "                         [-4.6662e-02, -6.9417e-02, -5.6757e-02]],\n",
       "               \n",
       "                        [[ 1.0956e-02,  4.2728e-04,  2.2605e-02],\n",
       "                         [-5.7650e-02, -5.7817e-02, -4.7846e-02],\n",
       "                         [-4.5206e-03,  2.9690e-02,  1.5006e-02]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.bias',\n",
       "               tensor([ 9.6703e-06, -3.5970e-05,  3.1211e-12,  6.1388e-05,  1.7874e-05,\n",
       "                        5.8685e-05,  9.1803e-06, -2.1708e-05, -2.0324e-05,  7.6525e-06,\n",
       "                       -1.3525e-05, -1.0637e-05,  4.9558e-05,  7.3112e-05,  4.0728e-05,\n",
       "                       -2.0209e-06, -4.9782e-05,  1.5406e-04, -4.8330e-05,  7.4068e-06,\n",
       "                        2.0808e-15,  1.5229e-05, -1.6200e-05,  5.4444e-06, -2.8764e-08,\n",
       "                        1.8828e-05, -3.2556e-05,  5.6171e-05, -1.0620e-05,  4.1994e-09,\n",
       "                       -3.8437e-06, -4.8063e-05, -6.8620e-06, -2.0273e-09,  9.6956e-05,\n",
       "                       -8.1326e-05, -5.3347e-05, -2.3228e-05,  1.7747e-11,  4.2928e-08,\n",
       "                       -3.3818e-05,  3.7678e-10,  2.1473e-07,  3.9316e-05, -3.0768e-06,\n",
       "                       -2.3144e-05,  5.6010e-05, -3.1779e-10,  1.1325e-11, -1.5985e-05,\n",
       "                       -1.6338e-05,  1.3645e-08,  9.9768e-06,  9.2179e-05,  5.9342e-05,\n",
       "                       -3.9798e-05, -4.5867e-05,  5.9324e-05,  1.8932e-05, -1.4394e-10,\n",
       "                        6.7355e-06, -3.4306e-05, -5.3630e-06, -1.7245e-08, -2.3361e-05,\n",
       "                       -8.9321e-06,  5.6263e-09, -2.7547e-05,  1.5326e-05,  4.0490e-06,\n",
       "                       -6.1372e-10,  6.8497e-05,  2.3184e-05, -4.0864e-05, -8.6491e-14,\n",
       "                        3.6110e-05,  8.3932e-14, -1.5955e-06, -3.7353e-05,  1.2028e-05,\n",
       "                       -1.5293e-12,  2.5528e-05,  4.1647e-05, -4.3002e-05,  3.6074e-05,\n",
       "                       -4.3564e-05, -8.7098e-06,  3.8460e-05,  1.9869e-09, -4.5810e-05,\n",
       "                       -1.8050e-06,  2.4061e-05, -3.8786e-06, -8.4766e-05, -9.2282e-08,\n",
       "                       -1.6641e-05, -1.4381e-05,  2.2035e-05,  8.2427e-05,  1.4642e-11,\n",
       "                       -1.0297e-05,  1.7583e-05,  7.1910e-16,  4.4886e-06, -3.2320e-05,\n",
       "                        3.7186e-05, -2.7574e-05,  1.8435e-05,  3.8400e-05,  2.8993e-05,\n",
       "                       -8.6971e-13, -3.3808e-05, -7.8410e-06, -3.3024e-06,  3.3083e-09,\n",
       "                       -1.5789e-05,  4.6268e-05,  8.6598e-05, -6.9326e-05,  9.3323e-05,\n",
       "                        2.6182e-05, -2.4035e-05,  4.2989e-05,  3.4343e-05,  1.6517e-05,\n",
       "                        5.0137e-05,  5.4730e-06, -2.6197e-05], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.bias',\n",
       "               tensor([-2.2587e-01, -3.9856e-01, -1.6369e-19, -5.3218e-01, -2.8019e-01,\n",
       "                       -3.4946e-01, -3.8546e-01, -2.2735e-01, -2.1765e-01, -3.2111e-01,\n",
       "                       -2.0272e-01, -2.8099e-01, -2.8501e-01, -5.9971e-01, -3.2713e-01,\n",
       "                       -3.5474e-01, -2.7770e-01, -4.1304e-01, -1.9632e-01, -3.3584e-01,\n",
       "                       -3.7688e-12, -2.8482e-01, -2.8249e-01, -1.8560e-01, -1.1939e-02,\n",
       "                       -2.4347e-01, -4.7516e-01, -3.5184e-01, -3.2741e-01, -9.9806e-04,\n",
       "                       -2.5298e-01, -4.2949e-01, -2.0589e-01, -2.4780e-06, -4.7399e-01,\n",
       "                       -3.9725e-01, -3.3330e-01, -3.7234e-01, -1.4204e-05, -5.8807e-03,\n",
       "                       -4.1337e-01, -5.6727e-03, -3.1176e-03, -2.1542e-01, -9.9206e-13,\n",
       "                       -3.2327e-01, -6.3490e-01, -1.6125e-05, -4.6570e-05, -3.4208e-01,\n",
       "                       -2.7091e-01, -5.8304e-02, -4.7288e-01, -2.6030e-01, -1.5500e-01,\n",
       "                       -7.1799e-01, -4.3764e-01, -5.1276e-01, -4.9490e-01, -3.0752e-16,\n",
       "                       -2.7945e-01, -3.2335e-01, -4.4902e-01, -9.1313e-04, -4.2615e-01,\n",
       "                       -3.1529e-01, -8.3074e-15, -6.1216e-01, -3.8553e-01, -7.4686e-08,\n",
       "                       -9.6777e-15, -2.5207e-01, -5.2309e-01, -1.6470e-01, -9.7801e-19,\n",
       "                       -2.3598e-01, -1.1695e-13, -2.0835e-06, -6.7930e-01, -4.8144e-01,\n",
       "                       -9.8088e-16, -3.9415e-01, -2.7965e-01, -1.8783e-01,  1.7655e-01,\n",
       "                       -3.3147e-01, -4.2328e-01, -3.2910e-01, -1.9216e-05, -3.5716e-01,\n",
       "                       -3.3422e-01, -4.7756e-01, -1.5743e-01, -5.4120e-01,  1.2027e-03,\n",
       "                       -2.2688e-01, -4.9420e-01, -5.2267e-01, -3.4492e-01, -8.9248e-23,\n",
       "                       -3.0083e-01, -3.6628e-01, -5.3977e-10, -3.0607e-01, -3.5501e-01,\n",
       "                       -4.4738e-01, -9.3769e-02, -5.5229e-01, -2.8492e-01, -2.1327e-01,\n",
       "                       -2.6829e-06, -3.8990e-01, -3.4688e-01, -1.8370e-01, -8.5014e-03,\n",
       "                       -3.3563e-01, -4.0044e-01, -3.5783e-01, -5.2294e-01, -1.9877e-01,\n",
       "                       -1.3308e-01, -2.3851e-01, -5.8316e-01, -2.1292e-01, -2.6719e-01,\n",
       "                       -4.2370e-01, -2.0554e-04, -1.7815e-01], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.weight',\n",
       "               tensor([ 2.1364e-01,  2.9156e-01,  3.2651e-14,  3.4456e-01,  2.7674e-01,\n",
       "                        3.4306e-01,  3.1873e-01,  2.6258e-01,  2.3512e-01,  3.0270e-01,\n",
       "                        2.2958e-01,  2.5511e-01,  2.7800e-01,  4.8839e-01,  2.7574e-01,\n",
       "                        2.5351e-01,  2.3172e-01,  3.7012e-01,  2.1877e-01,  3.2003e-01,\n",
       "                       -3.4490e-07,  1.9324e-01,  2.4054e-01,  2.4472e-01,  4.1734e-03,\n",
       "                        2.8514e-01,  3.7322e-01,  3.0824e-01,  2.4770e-01,  3.4835e-03,\n",
       "                        3.0780e-01,  3.2564e-01,  2.7500e-01, -7.9920e-04,  4.0601e-01,\n",
       "                        3.4834e-01,  3.3616e-01,  3.6558e-01, -4.6862e-04,  1.0733e-02,\n",
       "                        3.3850e-01,  3.0315e-03,  1.6212e-02,  2.4417e-01, -3.5564e-08,\n",
       "                        2.0876e-01,  4.4620e-01,  5.8623e-05, -2.7202e-05,  2.9748e-01,\n",
       "                        2.4362e-01,  2.3539e-02,  4.0232e-01,  2.5480e-01,  2.8222e-01,\n",
       "                        5.0413e-01,  4.1570e-01,  3.9614e-01,  4.1481e-01,  2.0357e-07,\n",
       "                        3.0994e-01,  2.6586e-01,  3.5626e-01, -6.0979e-05,  3.5780e-01,\n",
       "                        3.0880e-01,  1.4661e-08,  5.1687e-01,  3.7694e-01, -3.8902e-04,\n",
       "                        7.0708e-29,  2.7012e-01,  4.2860e-01,  3.1944e-01,  7.7059e-23,\n",
       "                        2.6223e-01,  1.9096e-11,  4.3193e-04,  4.4775e-01,  4.1863e-01,\n",
       "                        1.0657e-09,  3.1072e-01,  2.8106e-01,  2.2644e-01,  1.2612e-01,\n",
       "                        2.6758e-01,  4.0652e-01,  2.2166e-01, -1.6405e-04,  2.8076e-01,\n",
       "                        2.4515e-01,  3.5698e-01,  1.7924e-01,  4.5139e-01,  7.2523e-04,\n",
       "                        2.2266e-01,  4.0222e-01,  4.5546e-01,  3.2512e-01,  4.1698e-17,\n",
       "                        2.8894e-01,  3.3250e-01, -2.4816e-16,  2.8250e-01,  3.2923e-01,\n",
       "                        4.3549e-01,  2.3146e-01,  4.6419e-01,  2.6198e-01,  2.5980e-01,\n",
       "                        3.0101e-04,  3.1886e-01,  2.8884e-01,  2.7570e-01, -3.5193e-03,\n",
       "                        2.9061e-01,  3.3436e-01,  3.0178e-01,  3.9992e-01,  2.3395e-01,\n",
       "                        1.7482e-01,  2.8176e-01,  4.9741e-01,  2.5656e-01,  2.4575e-01,\n",
       "                        3.4270e-01, -1.5723e-16,  1.9770e-01], device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.weights',\n",
       "               tensor([[-0.0064, -0.0039, -0.0010,  ...,  0.0071,  0.0083,  0.0093],\n",
       "                       [-0.0043,  0.0096, -0.0154,  ..., -0.0046,  0.0007,  0.0072],\n",
       "                       [ 0.0011, -0.0066, -0.0046,  ...,  0.0148,  0.0051,  0.0049],\n",
       "                       [ 0.0208,  0.0179,  0.0320,  ..., -0.0132, -0.0152, -0.0225],\n",
       "                       [-0.0106, -0.0131, -0.0090,  ..., -0.0023,  0.0028,  0.0029]],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.bias',\n",
       "               tensor([-0.0214,  0.0204, -0.0107,  0.0036,  0.0089], device='cuda:0')),\n",
       "              ('classifier.prompt.prompt_dict.pad_up',\n",
       "               tensor([[[ 0.0077,  0.0097,  0.0043,  ..., -0.0100, -0.0119, -0.0250],\n",
       "                        [ 0.0212,  0.0009,  0.0003,  ...,  0.0224, -0.0011, -0.0389],\n",
       "                        [ 0.0474,  0.0126,  0.0478,  ...,  0.0008,  0.0210, -0.0144],\n",
       "                        [ 0.0036,  0.0115, -0.0086,  ..., -0.0588,  0.0053,  0.0188],\n",
       "                        [-0.0219, -0.0484, -0.0176,  ...,  0.0413,  0.0139, -0.0342]],\n",
       "               \n",
       "                       [[-0.0261, -0.0067, -0.0160,  ..., -0.0328,  0.0025, -0.0250],\n",
       "                        [-0.0103, -0.0181, -0.0055,  ...,  0.0255,  0.0380, -0.0055],\n",
       "                        [-0.0309, -0.0558, -0.0043,  ...,  0.0047,  0.0105, -0.0238],\n",
       "                        [ 0.0060,  0.0181,  0.0003,  ..., -0.0111,  0.0041,  0.0230],\n",
       "                        [ 0.0422,  0.0200,  0.0160,  ...,  0.0258,  0.0140, -0.0220]],\n",
       "               \n",
       "                       [[-0.0061,  0.0278,  0.0168,  ..., -0.0097,  0.0110, -0.0176],\n",
       "                        [ 0.0089, -0.0005, -0.0022,  ...,  0.0080,  0.0235, -0.0015],\n",
       "                        [ 0.0317,  0.0133,  0.0597,  ...,  0.0016,  0.0173, -0.0109],\n",
       "                        [-0.0044, -0.0243, -0.0371,  ..., -0.0448, -0.0180,  0.0046],\n",
       "                        [-0.0242, -0.0803, -0.0429,  ...,  0.0241, -0.0175, -0.0304]]],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.prompt.prompt_dict.pad_down',\n",
       "               tensor([[[ 0.0348,  0.0151, -0.0201,  ..., -0.0145, -0.0025, -0.0459],\n",
       "                        [-0.0122, -0.0080,  0.0021,  ...,  0.0426, -0.0079, -0.0722],\n",
       "                        [-0.0153, -0.0369, -0.0453,  ...,  0.0512,  0.0016, -0.0161],\n",
       "                        [ 0.0156, -0.0040, -0.0251,  ..., -0.0037,  0.0074,  0.0009],\n",
       "                        [-0.0088, -0.0014,  0.0060,  ..., -0.0296, -0.0142, -0.0344]],\n",
       "               \n",
       "                       [[-0.0144, -0.0327, -0.0356,  ..., -0.0106,  0.0365, -0.0252],\n",
       "                        [-0.0127,  0.0022,  0.0045,  ..., -0.0305,  0.0281,  0.0042],\n",
       "                        [ 0.0313,  0.0573,  0.0520,  ..., -0.0043, -0.0122, -0.0352],\n",
       "                        [ 0.0248,  0.0131,  0.0012,  ..., -0.0128, -0.0061, -0.0299],\n",
       "                        [-0.0303, -0.0051, -0.0161,  ..., -0.0221, -0.0013, -0.0278]],\n",
       "               \n",
       "                       [[ 0.0392,  0.0225,  0.0124,  ..., -0.0374,  0.0148, -0.0157],\n",
       "                        [-0.0045,  0.0049,  0.0070,  ..., -0.0129,  0.0251,  0.0271],\n",
       "                        [-0.0290, -0.0506, -0.0311,  ..., -0.0117, -0.0043,  0.0153],\n",
       "                        [ 0.0214, -0.0123, -0.0170,  ..., -0.0071,  0.0240,  0.0129],\n",
       "                        [-0.0057, -0.0105, -0.0020,  ..., -0.0135,  0.0107, -0.0107]]],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.prompt.prompt_dict.pad_left',\n",
       "               tensor([[[-0.0100,  0.0315, -0.0067, -0.0123,  0.0133],\n",
       "                        [-0.0041, -0.0295,  0.0147, -0.0135, -0.0118],\n",
       "                        [ 0.0165, -0.0166,  0.0111,  0.0317,  0.0376],\n",
       "                        ...,\n",
       "                        [-0.0006,  0.0093, -0.0101, -0.0121, -0.0133],\n",
       "                        [-0.0148, -0.0059, -0.0255,  0.0315, -0.0150],\n",
       "                        [ 0.0004, -0.0223, -0.0027,  0.0064, -0.0120]],\n",
       "               \n",
       "                       [[-0.0077,  0.0145, -0.0053, -0.0177,  0.0037],\n",
       "                        [ 0.0052,  0.0040,  0.0009,  0.0027, -0.0225],\n",
       "                        [-0.0251,  0.0045, -0.0493,  0.0077, -0.0008],\n",
       "                        ...,\n",
       "                        [ 0.0081, -0.0130, -0.0165,  0.0090,  0.0076],\n",
       "                        [ 0.0152,  0.0223, -0.0176,  0.0603,  0.0102],\n",
       "                        [-0.0476, -0.0237,  0.0371,  0.0377, -0.0112]],\n",
       "               \n",
       "                       [[ 0.0040, -0.0150, -0.0155, -0.0087,  0.0116],\n",
       "                        [ 0.0111,  0.0038,  0.0115,  0.0012, -0.0304],\n",
       "                        [ 0.0087,  0.0318, -0.0119,  0.0340,  0.0267],\n",
       "                        ...,\n",
       "                        [-0.0033, -0.0096, -0.0308, -0.0128,  0.0062],\n",
       "                        [-0.0126, -0.0025, -0.0290,  0.0142, -0.0045],\n",
       "                        [-0.0088, -0.0387, -0.0038, -0.0094, -0.0173]]], device='cuda:0')),\n",
       "              ('classifier.prompt.prompt_dict.pad_right',\n",
       "               tensor([[[-0.0558, -0.0206,  0.0194,  0.0265,  0.0145],\n",
       "                        [-0.0030, -0.0767,  0.0513,  0.0118,  0.0138],\n",
       "                        [-0.0119, -0.0419,  0.0530,  0.0170, -0.0452],\n",
       "                        ...,\n",
       "                        [-0.0312,  0.0029, -0.0167,  0.0182, -0.0240],\n",
       "                        [-0.0265,  0.0303,  0.0124, -0.0119, -0.0538],\n",
       "                        [-0.0454,  0.0057,  0.0380, -0.0485,  0.0604]],\n",
       "               \n",
       "                       [[-0.0213,  0.0076, -0.0521, -0.0039,  0.0051],\n",
       "                        [ 0.0101, -0.0095, -0.0416, -0.0713,  0.0030],\n",
       "                        [ 0.0068,  0.0006,  0.0199, -0.0045, -0.0360],\n",
       "                        ...,\n",
       "                        [ 0.0082,  0.0398,  0.0039,  0.0139, -0.0031],\n",
       "                        [-0.0196, -0.0033, -0.0004,  0.0415, -0.0080],\n",
       "                        [-0.0039, -0.0227,  0.0202, -0.0537,  0.0310]],\n",
       "               \n",
       "                       [[-0.0401,  0.0064, -0.0116, -0.0403, -0.0113],\n",
       "                        [ 0.0179, -0.0184,  0.0216, -0.0125,  0.0504],\n",
       "                        [-0.0008,  0.0060,  0.0377,  0.0524,  0.0204],\n",
       "                        ...,\n",
       "                        [ 0.0179,  0.0267,  0.0077,  0.0051, -0.0166],\n",
       "                        [ 0.0202, -0.0204,  0.0165, -0.0043,  0.0052],\n",
       "                        [ 0.0296, -0.0364,  0.0081, -0.0382,  0.0676]]], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.prompt_learning_rates_dict.prompt_weight_learning_rate',\n",
       "               tensor([-2.3035e-01, -2.7197e-01, -2.8120e-01,  2.0952e-03,  6.1659e-01,\n",
       "                       -1.1517e-14], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv0-conv-weight',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv0-conv-bias',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv1-conv-weight',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv1-conv-bias',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv2-conv-weight',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv2-conv-bias',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv3-conv-weight',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv3-conv-bias',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-weights',\n",
       "               tensor([ 2.7539e-01,  3.6843e-01,  5.3869e-01,  9.3650e-01,  1.4978e+00,\n",
       "                       -1.1517e-14], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-bias',\n",
       "               tensor([ 1.2263e-01,  1.0044e-02, -1.1430e-01, -1.7502e-01,  6.7042e-02,\n",
       "                       -1.1517e-14], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv0-conv-weight',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv0-conv-bias',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv1-conv-weight',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv1-conv-bias',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv2-conv-weight',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv2-conv-bias',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv3-conv-weight',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv3-conv-bias',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-linear-weights',\n",
       "               tensor([ 1.0058e+00, -9.4281e-02, -4.5651e-01, -9.7358e-01, -1.8349e+00,\n",
       "                       -1.7893e-16], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-linear-bias',\n",
       "               tensor([ 1.7287e-01,  1.4833e-01,  1.1883e-01, -6.0842e-03, -3.0088e-02,\n",
       "                       -1.7893e-16], device='cuda:0'))]),\n",
       " 'per_epoch_statistics': {'train_loss_mean': [1.5287191603183747,\n",
       "   1.3087079682350158,\n",
       "   1.1822218327522278,\n",
       "   1.1003865857124329,\n",
       "   1.0334410020112992,\n",
       "   0.9851122982501984,\n",
       "   0.9623334238529205,\n",
       "   0.9205508681535721,\n",
       "   0.8959451113939285,\n",
       "   0.8831795489788056,\n",
       "   0.8322586427330971,\n",
       "   0.8166595655083656,\n",
       "   0.7976653907299042,\n",
       "   0.7602433665394783,\n",
       "   0.7354601004123688,\n",
       "   0.7234197436571121,\n",
       "   0.7017989028096199,\n",
       "   0.6896196339130402,\n",
       "   0.6873844412565231,\n",
       "   0.6684585362076759,\n",
       "   0.6399076988697052,\n",
       "   0.6238135381937027,\n",
       "   0.6169031699299812,\n",
       "   0.6117314789295196,\n",
       "   0.59836458081007,\n",
       "   0.5819817011654377,\n",
       "   0.5754283549189567,\n",
       "   0.5698440008461475,\n",
       "   0.5688133637309074,\n",
       "   0.5705624460875988,\n",
       "   0.5475086936056613,\n",
       "   0.5464293903708458,\n",
       "   0.5456357058882714,\n",
       "   0.5364898134469986,\n",
       "   0.542416247099638,\n",
       "   0.5150938726365566,\n",
       "   0.5178878527283669,\n",
       "   0.515435187369585,\n",
       "   0.5203195397555828,\n",
       "   0.5040173936784268,\n",
       "   0.5196221300959587,\n",
       "   0.49897146281599997,\n",
       "   0.4999581035375595,\n",
       "   0.5058206704258918,\n",
       "   0.49845578768849375,\n",
       "   0.49108028116822244,\n",
       "   0.4932515753507614,\n",
       "   0.4891705230772495,\n",
       "   0.4843356150984764,\n",
       "   0.4847707284092903,\n",
       "   0.49144859132170676,\n",
       "   0.47075734156370164,\n",
       "   0.490650477796793,\n",
       "   0.47474593853950503,\n",
       "   0.4865857430994511,\n",
       "   0.48378262984752657,\n",
       "   0.47965867495536807,\n",
       "   0.47056997668743134,\n",
       "   0.47400851199030875,\n",
       "   0.47548300966620444,\n",
       "   0.47774769315123555,\n",
       "   0.46909257826209066,\n",
       "   0.46378291383385656,\n",
       "   0.45593588668107987,\n",
       "   0.46167937058210373,\n",
       "   0.47067978835105895,\n",
       "   0.47563692194223406,\n",
       "   0.45847623893618583,\n",
       "   0.4605610853433609,\n",
       "   0.470991806358099,\n",
       "   0.46062133413553236,\n",
       "   0.459903327435255,\n",
       "   0.46107315018773076,\n",
       "   0.4566495282948017,\n",
       "   0.4522366107404232,\n",
       "   0.4493947006762028,\n",
       "   0.45476002484560013,\n",
       "   0.45706255131959916,\n",
       "   0.44901121297478674,\n",
       "   0.45477797678112986,\n",
       "   0.44523624670505524,\n",
       "   0.4516067501604557,\n",
       "   0.4493849837779999,\n",
       "   0.4543998059630394,\n",
       "   0.4428413875699043,\n",
       "   0.4514333799481392,\n",
       "   0.4483924883902073,\n",
       "   0.4426089172363281,\n",
       "   0.44204348304867747,\n",
       "   0.4383564358651638,\n",
       "   0.4477545205056667,\n",
       "   0.4460466632246971,\n",
       "   0.4324578695595264,\n",
       "   0.44037217074632645,\n",
       "   0.43973952212929723,\n",
       "   0.4484758940935135,\n",
       "   0.4462255824804306,\n",
       "   0.44238554120063783,\n",
       "   0.42900713527202605],\n",
       "  'train_loss_std': [0.4105616120362475,\n",
       "   0.12276517599950341,\n",
       "   0.1386081086794683,\n",
       "   0.14111836845542164,\n",
       "   0.14366292788409682,\n",
       "   0.14167205123396887,\n",
       "   0.14400990938185138,\n",
       "   0.14331276296167644,\n",
       "   0.14865812408310664,\n",
       "   0.14176701888502563,\n",
       "   0.14860905420434684,\n",
       "   0.1485243399342706,\n",
       "   0.1447375906437896,\n",
       "   0.14537454201312186,\n",
       "   0.13747706320964148,\n",
       "   0.1476423826980195,\n",
       "   0.1381223132974934,\n",
       "   0.14494073843513358,\n",
       "   0.14441247455165462,\n",
       "   0.1412759978484202,\n",
       "   0.14199263260947997,\n",
       "   0.1344662908317849,\n",
       "   0.14463304942718316,\n",
       "   0.14681709405361337,\n",
       "   0.13555741904020183,\n",
       "   0.1343447492959571,\n",
       "   0.13707114601760162,\n",
       "   0.13910407917000023,\n",
       "   0.1391971293620295,\n",
       "   0.1441045100466731,\n",
       "   0.1397909070250828,\n",
       "   0.132258911751491,\n",
       "   0.1326962219659648,\n",
       "   0.1462306997082187,\n",
       "   0.12968541303928738,\n",
       "   0.12871809586059232,\n",
       "   0.1287209196195633,\n",
       "   0.13232758447832813,\n",
       "   0.13857414752921784,\n",
       "   0.13661852747053407,\n",
       "   0.13584345330111797,\n",
       "   0.13044629325428064,\n",
       "   0.13058856205031552,\n",
       "   0.13264800652559464,\n",
       "   0.1322078211015645,\n",
       "   0.13148518216884267,\n",
       "   0.13396402309540134,\n",
       "   0.13036136597122436,\n",
       "   0.13350345135213632,\n",
       "   0.12767903788227633,\n",
       "   0.12646029253248608,\n",
       "   0.12194822445642585,\n",
       "   0.1319238864577992,\n",
       "   0.1334418663512463,\n",
       "   0.13646298907378107,\n",
       "   0.13157205965642738,\n",
       "   0.12699780317032605,\n",
       "   0.13478901980263694,\n",
       "   0.1299599289377846,\n",
       "   0.12248533118597406,\n",
       "   0.12676940913837012,\n",
       "   0.13287682290895172,\n",
       "   0.12990956744154747,\n",
       "   0.1272348634523428,\n",
       "   0.1276241741917382,\n",
       "   0.12412663358904452,\n",
       "   0.13024612908669736,\n",
       "   0.12799192769994738,\n",
       "   0.13029136469640593,\n",
       "   0.13560010723785618,\n",
       "   0.1313639138761694,\n",
       "   0.13015542686861614,\n",
       "   0.13156428837270653,\n",
       "   0.1269416901902629,\n",
       "   0.12940871702415813,\n",
       "   0.1223419933988531,\n",
       "   0.12938541079533952,\n",
       "   0.12829341033573258,\n",
       "   0.12790120189963947,\n",
       "   0.13263397043297662,\n",
       "   0.12564384969603415,\n",
       "   0.12640739486888614,\n",
       "   0.12785987194410478,\n",
       "   0.13473992784626654,\n",
       "   0.12733567250743885,\n",
       "   0.12337427476071171,\n",
       "   0.13262881187377232,\n",
       "   0.13370309779100564,\n",
       "   0.12566843659601332,\n",
       "   0.12056490598826683,\n",
       "   0.12401533590991608,\n",
       "   0.1307537214618921,\n",
       "   0.12156998867494692,\n",
       "   0.12767036608391433,\n",
       "   0.12136888971545996,\n",
       "   0.1300115951771381,\n",
       "   0.12216699492261734,\n",
       "   0.12645168936600426,\n",
       "   0.12477755686603871],\n",
       "  'train_accuracy_mean': [0.3802133333086967,\n",
       "   0.4579200002551079,\n",
       "   0.5235733328461647,\n",
       "   0.5631466654539108,\n",
       "   0.5960400002002716,\n",
       "   0.6177599985599518,\n",
       "   0.6273999982476235,\n",
       "   0.6432266663908959,\n",
       "   0.6557333332300186,\n",
       "   0.6593466665744782,\n",
       "   0.6817333336472511,\n",
       "   0.6890133336782456,\n",
       "   0.6947866672277451,\n",
       "   0.7112133345603943,\n",
       "   0.7226266665458679,\n",
       "   0.7265999998450279,\n",
       "   0.7341866670846939,\n",
       "   0.7400133318305016,\n",
       "   0.7420266664028168,\n",
       "   0.7499466673135757,\n",
       "   0.7589999997615814,\n",
       "   0.765893334031105,\n",
       "   0.7709466682672501,\n",
       "   0.7715466665029526,\n",
       "   0.77732000041008,\n",
       "   0.783466667175293,\n",
       "   0.786693333029747,\n",
       "   0.7871866661310196,\n",
       "   0.788906665444374,\n",
       "   0.786639997959137,\n",
       "   0.7977066661119461,\n",
       "   0.7986266670227051,\n",
       "   0.798053334236145,\n",
       "   0.8008666663169861,\n",
       "   0.7987866673469544,\n",
       "   0.8095199998617172,\n",
       "   0.8062666665315628,\n",
       "   0.8096666671037674,\n",
       "   0.8074800001382828,\n",
       "   0.8131466677188873,\n",
       "   0.80664000082016,\n",
       "   0.8170000002384186,\n",
       "   0.8166266676187516,\n",
       "   0.813399999499321,\n",
       "   0.8142933323383331,\n",
       "   0.8193733336925506,\n",
       "   0.8187733343839645,\n",
       "   0.820226665854454,\n",
       "   0.8210800020694733,\n",
       "   0.8212399985790253,\n",
       "   0.8194000010490418,\n",
       "   0.825240000128746,\n",
       "   0.8178666664361953,\n",
       "   0.8243866677284241,\n",
       "   0.8205600007772446,\n",
       "   0.82221333360672,\n",
       "   0.8228800004720688,\n",
       "   0.8246266669034958,\n",
       "   0.8245199992656708,\n",
       "   0.8249599999189376,\n",
       "   0.8234666671752929,\n",
       "   0.8275866668224334,\n",
       "   0.8286133320331573,\n",
       "   0.8332266681194306,\n",
       "   0.829253332734108,\n",
       "   0.8259066661596298,\n",
       "   0.8234133330583573,\n",
       "   0.8303866683244705,\n",
       "   0.8308533327579498,\n",
       "   0.827880000114441,\n",
       "   0.8302400002479553,\n",
       "   0.8311066659688949,\n",
       "   0.831266665816307,\n",
       "   0.83216000020504,\n",
       "   0.8330933343172073,\n",
       "   0.8347599997520446,\n",
       "   0.8329466655254364,\n",
       "   0.831306666970253,\n",
       "   0.8365066666603088,\n",
       "   0.831666667342186,\n",
       "   0.8358533339500427,\n",
       "   0.8340133343935012,\n",
       "   0.8348400011062622,\n",
       "   0.8326400002241134,\n",
       "   0.8380666663646698,\n",
       "   0.8317066659927368,\n",
       "   0.8333200010061264,\n",
       "   0.8380933343172073,\n",
       "   0.8383199999332428,\n",
       "   0.8392000004053116,\n",
       "   0.8357066669464112,\n",
       "   0.8352533342838288,\n",
       "   0.8402800005674362,\n",
       "   0.837946667432785,\n",
       "   0.838360001206398,\n",
       "   0.8335466669797897,\n",
       "   0.8369333337545395,\n",
       "   0.8387066668272019,\n",
       "   0.8435600010156632],\n",
       "  'train_accuracy_std': [0.07594705040518926,\n",
       "   0.06995066658820595,\n",
       "   0.07235244149417475,\n",
       "   0.0735503490555953,\n",
       "   0.07296914560509415,\n",
       "   0.06874465059255365,\n",
       "   0.0694441111372226,\n",
       "   0.06761253935266848,\n",
       "   0.07035731069704462,\n",
       "   0.06795959645689581,\n",
       "   0.0699105451116678,\n",
       "   0.07075594894584616,\n",
       "   0.0665527278743608,\n",
       "   0.06630330264642781,\n",
       "   0.06371281519283503,\n",
       "   0.06710982773719928,\n",
       "   0.06127374525676644,\n",
       "   0.06510230882990428,\n",
       "   0.06378421341348044,\n",
       "   0.06442840483568806,\n",
       "   0.06302574399394649,\n",
       "   0.05914597531597589,\n",
       "   0.060685650711630894,\n",
       "   0.06364351355773587,\n",
       "   0.057625764539132714,\n",
       "   0.05841674088158454,\n",
       "   0.05876960073178867,\n",
       "   0.060841475621097874,\n",
       "   0.06046012736316961,\n",
       "   0.06186867228911505,\n",
       "   0.058912238562062715,\n",
       "   0.05709079193020603,\n",
       "   0.05813040339754879,\n",
       "   0.06099949079203981,\n",
       "   0.055745803145135955,\n",
       "   0.0554163298170291,\n",
       "   0.0571315830721022,\n",
       "   0.05639680838975872,\n",
       "   0.057774703055680485,\n",
       "   0.05688320174504852,\n",
       "   0.05753761635472281,\n",
       "   0.05526642563949351,\n",
       "   0.05611653201155049,\n",
       "   0.05522072853487052,\n",
       "   0.057650003758100817,\n",
       "   0.055275137242087116,\n",
       "   0.05625898227804738,\n",
       "   0.055552516021784154,\n",
       "   0.05648982558843622,\n",
       "   0.05393139988638592,\n",
       "   0.05410520796279241,\n",
       "   0.05091527067443555,\n",
       "   0.056649252200227734,\n",
       "   0.05561815771492859,\n",
       "   0.05819238080155073,\n",
       "   0.055801943200840726,\n",
       "   0.052644458937455334,\n",
       "   0.05820895806415095,\n",
       "   0.05514518901193114,\n",
       "   0.05170620785355149,\n",
       "   0.05307001638184046,\n",
       "   0.055932879958553,\n",
       "   0.05532559849339139,\n",
       "   0.0527668871608742,\n",
       "   0.05339057791651572,\n",
       "   0.05278931290528658,\n",
       "   0.05509662484601043,\n",
       "   0.053087199106927226,\n",
       "   0.05475830572426518,\n",
       "   0.05781075520529324,\n",
       "   0.054796880555373795,\n",
       "   0.0568015826141126,\n",
       "   0.054975510892539485,\n",
       "   0.054047314746084575,\n",
       "   0.05393935125899646,\n",
       "   0.05153303527811758,\n",
       "   0.05279673322970694,\n",
       "   0.054036854717195215,\n",
       "   0.05220533171037162,\n",
       "   0.05703157226179082,\n",
       "   0.05180953142828685,\n",
       "   0.053975961898701116,\n",
       "   0.05465809843560185,\n",
       "   0.0567102528568179,\n",
       "   0.052994716649600385,\n",
       "   0.05204120879776605,\n",
       "   0.05537990132093405,\n",
       "   0.05698818849821525,\n",
       "   0.05385432970561127,\n",
       "   0.0496179202124232,\n",
       "   0.05118387947376754,\n",
       "   0.05708358778092637,\n",
       "   0.050501591695935814,\n",
       "   0.05270342098441955,\n",
       "   0.050782974849950015,\n",
       "   0.05278508993289983,\n",
       "   0.04977478353201444,\n",
       "   0.052883253253310704,\n",
       "   0.05099753619832773],\n",
       "  'train_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'train_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_learning_rate_mean': [0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005],\n",
       "  'train_learning_rate_std': [4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19],\n",
       "  'val_loss_mean': [1.427607449690501,\n",
       "   1.355423356294632,\n",
       "   1.238869721889496,\n",
       "   1.2361089517672856,\n",
       "   1.1779483050107955,\n",
       "   1.1548319971561432,\n",
       "   1.1179563317696253,\n",
       "   1.0949846142530442,\n",
       "   1.0793093107144038,\n",
       "   1.090482276280721,\n",
       "   1.0072305901845295,\n",
       "   1.025243609547615,\n",
       "   0.9994777470827103,\n",
       "   0.9901943699518839,\n",
       "   0.9881963040431341,\n",
       "   0.9569184712568919,\n",
       "   1.015507728854815,\n",
       "   0.9486051599184672,\n",
       "   0.9589750822385152,\n",
       "   0.918096215526263,\n",
       "   0.9085845875740052,\n",
       "   0.9177135090033214,\n",
       "   0.88259836435318,\n",
       "   0.9363117738564809,\n",
       "   0.9007854537169139,\n",
       "   0.909209347764651,\n",
       "   0.8878708622852961,\n",
       "   0.870238119562467,\n",
       "   0.8484021153052648,\n",
       "   0.879757069349289,\n",
       "   0.8667336882154146,\n",
       "   0.8737553125619888,\n",
       "   0.8892110047737757,\n",
       "   0.8610018362601598,\n",
       "   0.8720020510752996,\n",
       "   0.8874425222476323,\n",
       "   0.873099956313769,\n",
       "   0.8950113979975383,\n",
       "   0.845685358941555,\n",
       "   0.856176683207353,\n",
       "   0.8758414610226949,\n",
       "   0.8576216434439023,\n",
       "   0.8495531592766444,\n",
       "   0.8555127996206283,\n",
       "   0.8752733165025711,\n",
       "   0.8580940951903661,\n",
       "   0.8242248575886091,\n",
       "   0.8688240025440852,\n",
       "   0.8225788829723993,\n",
       "   0.8427860170602799,\n",
       "   0.86344957113266,\n",
       "   0.8517489070693652,\n",
       "   0.876144364575545,\n",
       "   0.8457188413540522,\n",
       "   0.8871164228518804,\n",
       "   0.853955558637778,\n",
       "   0.8450690978765487,\n",
       "   0.8534416818618774,\n",
       "   0.8704815830787023,\n",
       "   0.8682989494005839,\n",
       "   0.8661549690365792,\n",
       "   0.8445505809783935,\n",
       "   0.8486118237177531,\n",
       "   0.8360474710663159,\n",
       "   0.8423686969280243,\n",
       "   0.8549561901887258,\n",
       "   0.8772177807490031,\n",
       "   0.8556726477543513,\n",
       "   0.8522529445091883,\n",
       "   0.854788020948569,\n",
       "   0.8815171444416046,\n",
       "   0.8432030828793844,\n",
       "   0.8756036379933357,\n",
       "   0.8628294625878334,\n",
       "   0.8590245674053828,\n",
       "   0.8447099632024765,\n",
       "   0.8674948708216349,\n",
       "   0.8485527056455612,\n",
       "   0.8700715384880702,\n",
       "   0.837767469783624,\n",
       "   0.8324728884299596,\n",
       "   0.8393935668468475,\n",
       "   0.842447656194369,\n",
       "   0.8481998974084854,\n",
       "   0.8440300254027049,\n",
       "   0.8598569051424663,\n",
       "   0.833506552974383,\n",
       "   0.834223389128844,\n",
       "   0.8536451830466588,\n",
       "   0.8526864764094353,\n",
       "   0.8599296865860621,\n",
       "   0.8123516945044199,\n",
       "   0.8309151195486386,\n",
       "   0.8447122397025426,\n",
       "   0.8502085846662522,\n",
       "   0.8423720703522364,\n",
       "   0.8272320338090261,\n",
       "   0.846259113252163,\n",
       "   0.8617218359311422],\n",
       "  'val_loss_std': [0.0913674028012535,\n",
       "   0.09478691572325407,\n",
       "   0.10914183593497477,\n",
       "   0.11400710791684968,\n",
       "   0.12099520675300733,\n",
       "   0.12636277847362107,\n",
       "   0.12925698951143794,\n",
       "   0.12451984539427141,\n",
       "   0.11925905850965532,\n",
       "   0.12551081484337384,\n",
       "   0.1354267605840889,\n",
       "   0.12711775425893637,\n",
       "   0.12293110011160764,\n",
       "   0.12266260174238168,\n",
       "   0.13506477864538952,\n",
       "   0.13815858345997548,\n",
       "   0.14179228891317727,\n",
       "   0.1296147081026128,\n",
       "   0.14067093100640315,\n",
       "   0.14145187194083858,\n",
       "   0.13145672270605233,\n",
       "   0.13795511085997475,\n",
       "   0.13708955542147427,\n",
       "   0.14508450122246613,\n",
       "   0.13796067047686733,\n",
       "   0.14307131966930955,\n",
       "   0.1458231735445474,\n",
       "   0.1439510848848066,\n",
       "   0.14080470668891104,\n",
       "   0.13055008526536702,\n",
       "   0.14808384513855039,\n",
       "   0.15598427240005822,\n",
       "   0.1450476690299777,\n",
       "   0.14030328106298134,\n",
       "   0.14347306136273993,\n",
       "   0.14331766873129267,\n",
       "   0.14768140928063914,\n",
       "   0.15595204840313784,\n",
       "   0.14180698980475173,\n",
       "   0.1485694462172881,\n",
       "   0.14451629401276722,\n",
       "   0.1508344402046029,\n",
       "   0.14095067556048765,\n",
       "   0.15227775225045534,\n",
       "   0.13852328188963536,\n",
       "   0.1435430138363371,\n",
       "   0.14281342992340482,\n",
       "   0.14260216757182678,\n",
       "   0.14934902645938683,\n",
       "   0.14430367516005513,\n",
       "   0.14575289039418907,\n",
       "   0.15169148721374254,\n",
       "   0.1457984971181133,\n",
       "   0.14173961660668644,\n",
       "   0.15276352334460866,\n",
       "   0.14856718221422482,\n",
       "   0.13689177718384257,\n",
       "   0.141456573264913,\n",
       "   0.15198501006274004,\n",
       "   0.15286076995346756,\n",
       "   0.14521911047579364,\n",
       "   0.144403699586049,\n",
       "   0.14970520869314446,\n",
       "   0.1480352734205415,\n",
       "   0.14485509119025064,\n",
       "   0.13581519388353022,\n",
       "   0.1590783347899854,\n",
       "   0.1451482429905536,\n",
       "   0.14832827748835126,\n",
       "   0.14639606446093703,\n",
       "   0.15233963740028075,\n",
       "   0.1447526181155258,\n",
       "   0.1446043640418105,\n",
       "   0.14689790679020306,\n",
       "   0.14678953320248705,\n",
       "   0.14783244938247722,\n",
       "   0.157530832909095,\n",
       "   0.14237406937435673,\n",
       "   0.1449425160579328,\n",
       "   0.14516617500567347,\n",
       "   0.14995666330394503,\n",
       "   0.15125108258621006,\n",
       "   0.1433910862809281,\n",
       "   0.14832578035371208,\n",
       "   0.14401752202220913,\n",
       "   0.15124030059065507,\n",
       "   0.1488431100357577,\n",
       "   0.14972363063971691,\n",
       "   0.14394966271045675,\n",
       "   0.14126630595538991,\n",
       "   0.15083760799867735,\n",
       "   0.14248239054487155,\n",
       "   0.15513362889222254,\n",
       "   0.14351534563982357,\n",
       "   0.1431143384447884,\n",
       "   0.1480498681988259,\n",
       "   0.14488765505519305,\n",
       "   0.14315181213806027,\n",
       "   0.13337858401148045],\n",
       "  'val_accuracy_mean': [0.3945333327849706,\n",
       "   0.4338000007470449,\n",
       "   0.49471111049254735,\n",
       "   0.5007111103336016,\n",
       "   0.5287555538614591,\n",
       "   0.5324000000953675,\n",
       "   0.549911109606425,\n",
       "   0.5621555550893148,\n",
       "   0.5702444449067116,\n",
       "   0.5677555543184281,\n",
       "   0.5998888874053955,\n",
       "   0.596844442387422,\n",
       "   0.6071555541952451,\n",
       "   0.6120888897776604,\n",
       "   0.6146444422006607,\n",
       "   0.6268666662772496,\n",
       "   0.6076222205162048,\n",
       "   0.6302222213149071,\n",
       "   0.6300222200155258,\n",
       "   0.6426888889074326,\n",
       "   0.6460222200552622,\n",
       "   0.6436666672428449,\n",
       "   0.6612888874610265,\n",
       "   0.64064444343249,\n",
       "   0.6526444457968076,\n",
       "   0.647622220714887,\n",
       "   0.6586888899405797,\n",
       "   0.6656888882319133,\n",
       "   0.6753555551171303,\n",
       "   0.6603777786095937,\n",
       "   0.6698666667938232,\n",
       "   0.668822221060594,\n",
       "   0.6617111101746559,\n",
       "   0.668622221549352,\n",
       "   0.6630222219228744,\n",
       "   0.6607777790228526,\n",
       "   0.6661555566390356,\n",
       "   0.6614222213625908,\n",
       "   0.6787555565436681,\n",
       "   0.6707999995350837,\n",
       "   0.6647111119826635,\n",
       "   0.6736666662494342,\n",
       "   0.674399998386701,\n",
       "   0.6783777768413226,\n",
       "   0.6641333315769832,\n",
       "   0.6726666645208994,\n",
       "   0.6847777791817983,\n",
       "   0.6669111092885335,\n",
       "   0.6870222210884094,\n",
       "   0.680199999610583,\n",
       "   0.6674000006914139,\n",
       "   0.6758444438378016,\n",
       "   0.6678222223122915,\n",
       "   0.6766222220659256,\n",
       "   0.6650888892014821,\n",
       "   0.6755777770280837,\n",
       "   0.6722444440921148,\n",
       "   0.6730666650334994,\n",
       "   0.668466666340828,\n",
       "   0.6669111107786496,\n",
       "   0.6704888886213303,\n",
       "   0.6788666653633117,\n",
       "   0.6761555547515551,\n",
       "   0.6819555563728015,\n",
       "   0.6794666655858358,\n",
       "   0.6729555569092432,\n",
       "   0.6666000004609426,\n",
       "   0.6724222232898076,\n",
       "   0.6765111100673675,\n",
       "   0.6746666683753332,\n",
       "   0.6650888900955518,\n",
       "   0.6811777778466542,\n",
       "   0.6635999993483226,\n",
       "   0.6754666656255722,\n",
       "   0.676511110663414,\n",
       "   0.6799111118912697,\n",
       "   0.6776444449027379,\n",
       "   0.6745111101865768,\n",
       "   0.6693111092845598,\n",
       "   0.6803333321213723,\n",
       "   0.6828444455067316,\n",
       "   0.6809555542469025,\n",
       "   0.6805777771274248,\n",
       "   0.6788222227493922,\n",
       "   0.6779777759313583,\n",
       "   0.6751777760187785,\n",
       "   0.6846222229798635,\n",
       "   0.6856444442272186,\n",
       "   0.6746666661898295,\n",
       "   0.6735999980568885,\n",
       "   0.6771333331863085,\n",
       "   0.6893555573622385,\n",
       "   0.6836666656533877,\n",
       "   0.6782888903220494,\n",
       "   0.6724888891975085,\n",
       "   0.6816444445649783,\n",
       "   0.6863333338499069,\n",
       "   0.677755557000637,\n",
       "   0.6733111109336217],\n",
       "  'val_accuracy_std': [0.056322195402102246,\n",
       "   0.052925565371835376,\n",
       "   0.05983828343441192,\n",
       "   0.05796625613900188,\n",
       "   0.05885896009508777,\n",
       "   0.06261477732857804,\n",
       "   0.06162124021692486,\n",
       "   0.06104235055949339,\n",
       "   0.059733477228064116,\n",
       "   0.060844667572801985,\n",
       "   0.06372733123485687,\n",
       "   0.06045847815746269,\n",
       "   0.05901460658968797,\n",
       "   0.05591732889040717,\n",
       "   0.058668431028584836,\n",
       "   0.060802937307830034,\n",
       "   0.06179636335301938,\n",
       "   0.0611817776912406,\n",
       "   0.061880405503239096,\n",
       "   0.06238019533377505,\n",
       "   0.06102603846573174,\n",
       "   0.06462455714937333,\n",
       "   0.06086145423355294,\n",
       "   0.06185310623618493,\n",
       "   0.062284646624795846,\n",
       "   0.061630724082146,\n",
       "   0.06262102128329956,\n",
       "   0.06155461036308616,\n",
       "   0.05998937771064819,\n",
       "   0.05790418306568148,\n",
       "   0.06062743585578667,\n",
       "   0.0652058138208835,\n",
       "   0.0597516305652165,\n",
       "   0.059362273790392904,\n",
       "   0.062487152246936545,\n",
       "   0.062634646941601,\n",
       "   0.06105642463360739,\n",
       "   0.06278752858902903,\n",
       "   0.06206991007718483,\n",
       "   0.06230234574688589,\n",
       "   0.06119814477042362,\n",
       "   0.0625841682889957,\n",
       "   0.06327403133842406,\n",
       "   0.0628852440701946,\n",
       "   0.05816283699011324,\n",
       "   0.05992217250829404,\n",
       "   0.06073734317648342,\n",
       "   0.059817745618005326,\n",
       "   0.061172672675419766,\n",
       "   0.06159933857962307,\n",
       "   0.06124797419004979,\n",
       "   0.061800680865338214,\n",
       "   0.06203581799212128,\n",
       "   0.059594914747609196,\n",
       "   0.06384262802107754,\n",
       "   0.06257794860746445,\n",
       "   0.05875624094860218,\n",
       "   0.06268028353275913,\n",
       "   0.06200015748303053,\n",
       "   0.06372696055389995,\n",
       "   0.06133017790305159,\n",
       "   0.06037454819689401,\n",
       "   0.0627295622480581,\n",
       "   0.060039725299622586,\n",
       "   0.061824220600123415,\n",
       "   0.056274507715483445,\n",
       "   0.06394322817230465,\n",
       "   0.061986135797470816,\n",
       "   0.062297775586263614,\n",
       "   0.06243455945581022,\n",
       "   0.06343990765052372,\n",
       "   0.059922960359636526,\n",
       "   0.06238532455177055,\n",
       "   0.060454182523848835,\n",
       "   0.0609247727257439,\n",
       "   0.06001968449650819,\n",
       "   0.06024953998612299,\n",
       "   0.0631044246265479,\n",
       "   0.06178552661413131,\n",
       "   0.061728498250291954,\n",
       "   0.0641426499545077,\n",
       "   0.06210246947078856,\n",
       "   0.060046583857102584,\n",
       "   0.060255797265610596,\n",
       "   0.061650624190688834,\n",
       "   0.060989418806289546,\n",
       "   0.061732743202974005,\n",
       "   0.06247363659381636,\n",
       "   0.06047711481502197,\n",
       "   0.061376216650320586,\n",
       "   0.05867869934896099,\n",
       "   0.05980486405259861,\n",
       "   0.06103581380487323,\n",
       "   0.05984824781826279,\n",
       "   0.058874867140730036,\n",
       "   0.05853994842525849,\n",
       "   0.059861879024932355,\n",
       "   0.06109736836895111,\n",
       "   0.05944309112193898],\n",
       "  'val_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'val_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0]}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델을 구성한다\n",
    "model = MAMLFewShotClassifier(args=args, device=device,\n",
    "                              im_shape=(2, 3,\n",
    "                                        args.image_height, args.image_width))\n",
    "\n",
    "data = MetaLearningSystemDataLoader\n",
    "\n",
    "maml_system = ExperimentBuilder(model=model, data=data, args=args, device=device)\n",
    "maml_system.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3803d5ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MAMLFewShotClassifier(\n",
       "  (classifier): VGGReLUNormNetwork(\n",
       "    (layer_dict): ModuleDict(\n",
       "      (conv0): MetaConvNormLayerReLU(\n",
       "        (layer_dict): ModuleDict()\n",
       "        (conv): MetaConv2dLayer()\n",
       "        (norm_layer): MetaBatchNormLayer(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): MetaConvNormLayerReLU(\n",
       "        (layer_dict): ModuleDict()\n",
       "        (conv): MetaConv2dLayer()\n",
       "        (norm_layer): MetaBatchNormLayer(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): MetaConvNormLayerReLU(\n",
       "        (layer_dict): ModuleDict()\n",
       "        (conv): MetaConv2dLayer()\n",
       "        (norm_layer): MetaBatchNormLayer(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv3): MetaConvNormLayerReLU(\n",
       "        (layer_dict): ModuleDict()\n",
       "        (conv): MetaConv2dLayer()\n",
       "        (norm_layer): MetaBatchNormLayer(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (linear): MetaLinearLayer()\n",
       "    )\n",
       "    (prompt): PadPrompter(\n",
       "      (prompt_dict): ParameterDict(\n",
       "          (pad_up): Parameter containing: [torch.cuda.FloatTensor of size 3x5x84 (GPU 0)]\n",
       "          (pad_down): Parameter containing: [torch.cuda.FloatTensor of size 3x5x84 (GPU 0)]\n",
       "          (pad_left): Parameter containing: [torch.cuda.FloatTensor of size 3x74x5 (GPU 0)]\n",
       "          (pad_right): Parameter containing: [torch.cuda.FloatTensor of size 3x74x5 (GPU 0)]\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (inner_loop_optimizer): LSLRGradientDescentLearningRule(\n",
       "    (prompt_learning_rates_dict): ParameterDict(  (prompt_weight_learning_rate): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)])\n",
       "    (names_learning_rates_dict): ParameterDict(\n",
       "        (layer_dict-conv0-conv-weight): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv0-conv-bias): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv1-conv-weight): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv1-conv-bias): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv2-conv-weight): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv2-conv-bias): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv3-conv-weight): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv3-conv-bias): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-linear-weights): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-linear-bias): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "    )\n",
       "    (names_weight_decay_dict): ParameterDict(\n",
       "        (layer_dict-conv0-conv-weight): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv0-conv-bias): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv1-conv-weight): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv1-conv-bias): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv2-conv-weight): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv2-conv-bias): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv3-conv-weight): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv3-conv-bias): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-linear-weights): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-linear-bias): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_dir = maml_system.saved_models_filepath\n",
    "model_name = \"train_model\"\n",
    "model_idx = maml_system.state['best_epoch']\n",
    "\n",
    "state = maml_system.model.load_model(model_save_dir=model_save_dir,\n",
    "                                     model_name=model_name,\n",
    "                                     model_idx=model_idx+1)\n",
    "\n",
    "state_dict_loaded = state['network']\n",
    "\n",
    "maml_system.model.load_state_dict(state_dict=state_dict_loaded)\n",
    "\n",
    "# # 잘 불러왔는지 확인하는 코드\n",
    "# print(\"state_dict_loaded == \",state_dict_loaded)\n",
    "# print(\"=\"*10)\n",
    "# for key, value in maml_system.model.named_parameters():\n",
    "#     print(key)\n",
    "#     print(value)\n",
    "maml_system.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db4855d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\utils\\grad_cam.py:24: RuntimeWarning: invalid value encountered in true_divide\n",
      "  grad_cam = (grad_cam - np.min(grad_cam)) / (np.max(grad_cam) - np.min(grad_cam))\n"
     ]
    }
   ],
   "source": [
    "# train_data = maml_system.data.get_train_batches(total_batches=int(600/2), augment_images=False)\n",
    "train_data = maml_system.data.get_test_batches(total_batches=int(600/2), augment_images=False)\n",
    "\n",
    "figure_idx = 0\n",
    "\n",
    "for sample_idx, train_sample in enumerate(train_data):\n",
    "    \n",
    "    x_support_set, x_target_set, y_support_set, y_target_set, seed = train_sample\n",
    "    \n",
    "    x_support_set = torch.Tensor(x_support_set).float().to(device=maml_system.model.device)\n",
    "    x_target_set = torch.Tensor(x_target_set).float().to(device=maml_system.model.device)\n",
    "    y_support_set = torch.Tensor(y_support_set).long().to(device=maml_system.model.device)\n",
    "    y_target_set = torch.Tensor(y_target_set).long().to(device=maml_system.model.device)\n",
    "    \n",
    "    for task_id, (x_support_set_task, y_support_set_task, x_target_set_task, y_target_set_task) in enumerate(zip(x_support_set,\n",
    "                              y_support_set,\n",
    "                              x_target_set,\n",
    "                              y_target_set)):\n",
    "        \n",
    "        names_weights_copy = maml_system.model.get_inner_loop_parameter_dict(maml_system.model.classifier.named_parameters())\n",
    "        \n",
    "        num_devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "\n",
    "        names_weights_copy = {\n",
    "            name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
    "                [num_devices] + [1 for i in range(len(value.shape))]) for\n",
    "            name, value in names_weights_copy.items()}\n",
    "        \n",
    "        prompted_weights_copy = {}\n",
    "        if args.prompter:\n",
    "            prompted_weights_copy = {key: value for key, value in names_weights_copy.items() if 'prompt' in key}\n",
    "\n",
    "        names_weights_copy = {key: value for key, value in names_weights_copy.items() if 'layer_dict' in key}\n",
    "        \n",
    "        n, s, c, h, w = x_target_set_task.shape\n",
    "\n",
    "        x_support_set_task = x_support_set_task.view(-1, c, h, w)\n",
    "        y_support_set_task = y_support_set_task.view(-1)\n",
    "        x_target_set_task = x_target_set_task.view(-1, c, h, w)\n",
    "        y_target_set_task = y_target_set_task.view(-1)\n",
    "        \n",
    "        num_steps=5\n",
    "        \n",
    "        for num_step in range(num_steps):\n",
    "                        \n",
    "            support_loss, support_preds = maml_system.model.net_forward(\n",
    "                                                               x=x_support_set_task,\n",
    "                                                               y=y_support_set_task,\n",
    "                                                               weights=names_weights_copy,\n",
    "                                                               prompted_weights=prompted_weights_copy,\n",
    "                                                               backup_running_statistics=num_step == 0,\n",
    "                                                               prepend_prompt=args.prompter,\n",
    "                                                               training=True,\n",
    "                                                               num_step=num_step,\n",
    "                                                               training_phase=False,\n",
    "                                                               epoch=0,\n",
    "                                                               inner_loop=True)\n",
    "        \n",
    "            names_weights_copy, prompted_weights_copy = maml_system.model.apply_inner_loop_update(\n",
    "                 loss=support_loss,\n",
    "                 names_weights_copy=names_weights_copy,\n",
    "                 prompted_weights_copy=prompted_weights_copy,\n",
    "                 use_second_order=True,\n",
    "                 current_step_idx=num_step,\n",
    "                 current_iter='test',\n",
    "                 training_phase=False)\n",
    "            \n",
    "        ##########Inner level 종료 #############\n",
    "        # Query set으로 Grad-CAM을 구한다\n",
    "        individual_images = torch.split(x_target_set_task, 1, dim=0)  # (25, 3, 84, 84) -> (1, 3, 84, 84) 텐서로 분리\n",
    "        individual_labels = torch.split(y_target_set_task, 1, dim=0)\n",
    "        \n",
    "        \n",
    "        original_save_path = \"Grad_CAM/\" + datasets_name + \"/DCML/origin/\" + \"sample_idx_\" + str(sample_idx) + \"/\" + \"task_id_\" + str(task_id) + \"/\"\n",
    "        grad_cam_save_path = \"Grad_CAM/\" + datasets_name + \"/DCML/gradcam/\"+ \"sample_idx_\" + str(sample_idx) + \"/\" + \"task_id_\" + str(task_id) + \"/\"\n",
    "        \n",
    "        make_folder(original_save_path)\n",
    "        make_folder(grad_cam_save_path)\n",
    "        \n",
    "        # 각 이미지 텐서 확인\n",
    "        for i in range(y_target_set_task.shape[0]):\n",
    "\n",
    "            input_image = individual_images[i]\n",
    "            y_label = individual_labels[i]\n",
    "\n",
    "            _, target_preds, feature_map_list = maml_system.model.net_forward_feature_extractor(x=input_image,\n",
    "                                                             y=y_label, weights=names_weights_copy,\n",
    "                                                             backup_running_statistics=False, training=True,\n",
    "                                                             num_step=num_step, training_phase='test',\n",
    "                                                             epoch=0)\n",
    "            \n",
    "            feature_map = feature_map_list[3]\n",
    "            \n",
    "            # 선택한 클래스에 대한 스칼라 출력값을 기준으로 gradient 계산\n",
    "            target_class = y_label\n",
    "            output = target_preds\n",
    "\n",
    "            class_score = output[:, target_class].sum()\n",
    "            gradients = torch.autograd.grad(outputs=class_score, inputs=feature_map, grad_outputs=torch.ones_like(class_score))\n",
    "\n",
    "            # gradients를 사용하여 feature map의 가중치를 계산\n",
    "            weights = torch.mean(gradients[0], dim=(2, 3), keepdim=True)\n",
    "            grad_cam = F.relu(torch.sum(weights * feature_map, dim=1, keepdim=True))\n",
    "\n",
    "            # grad_cam을 이미지 크기로 리사이즈\n",
    "            grad_cam = F.interpolate(grad_cam, size=(input_image.shape[2], input_image.shape[3]), mode='bilinear', align_corners=False)\n",
    "            grad_cam = grad_cam.squeeze().cpu().detach().numpy()\n",
    "\n",
    "            visualize_and_save_grad_cam(\n",
    "                input_image=input_image,\n",
    "                grad_cam=grad_cam,\n",
    "                grad_cam_save_path=grad_cam_save_path + str(i) + \".png\",\n",
    "                original_save_path=original_save_path + str(i) + \".png\",\n",
    "                datasets=datasets_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
