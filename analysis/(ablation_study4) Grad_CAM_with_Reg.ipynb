{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bd87b33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pytorchcv.model_provider import get_model as ptcv_get_model # model\n",
    "import sys, os\n",
    "import easydict\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline \n",
    "\n",
    "# enable cuda devices\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from meta_neural_network_architectures import VGGReLUNormNetwork, ResNet12\n",
    "from utils.parser_utils import get_args\n",
    "from data import MetaLearningSystemDataLoader\n",
    "from experiment_builder import ExperimentBuilder\n",
    "from utils.basic import show_batch\n",
    "from few_shot_learning_system import MAMLFewShotClassifier\n",
    "import prompters\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from data_augmentation import mixup_data, cutmix_data, random_flip, random_flip_like_torchvision\n",
    "from utils.grad_cam import visualize_and_save_grad_cam, make_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6c638561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets_name = \"mini_imagenet\"\n",
    "# datasets_name = \"tiered_imagenet\"\n",
    "# datasets_name = \"CIFAR_FS\"\n",
    "datasets_name = \"CUB\"\n",
    "\n",
    "# os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/prompt_maml/datasets'\n",
    "# os.environ['TEST_DATASET'] = \"tiered_imagenet\" # https://mtl.yyliu.net/download/Lmzjm9tX.html\n",
    "# os.environ['TEST_DATASET'] = \"CIFAR_FS\" # https://drive.google.com/file/d/1pTsCCMDj45kzFYgrnO67BWVbKs48Q3NI/view\n",
    "os.environ['TEST_DATASET'] = \"CUB\" # https://data.caltech.edu/records/65de6-vp158"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f8daf114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.environ['DATASET_DIR'] ===  C:/Users/JM/PycharmProjects/prompt_maml/datasets\n"
     ]
    }
   ],
   "source": [
    "# os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "print(\"os.environ['DATASET_DIR'] === \", os.environ['DATASET_DIR'])\n",
    "\n",
    "args = easydict.EasyDict(\n",
    "{\n",
    "  \"batch_size\":2,\n",
    "  \"image_height\":84,\n",
    "  \"image_width\":84,\n",
    "  \"image_channels\":3,\n",
    "  \"gpu_to_use\":0,\n",
    "  \"num_dataprovider_workers\":4,\n",
    "  \"max_models_to_save\":5,\n",
    "  \"dataset_name\":\"mini_imagenet_full_size\",\n",
    "  \"dataset_path\":\"mini_imagenet_full_size\",\n",
    "  \"reset_stored_paths\":False,\n",
    "  \"experiment_name\":\"../MAML_Prompt_padding_5way_5shot_filter128_miniImagenet\",\n",
    "  \"train_seed\": 0, \"val_seed\": 0,\n",
    "  \"indexes_of_folders_indicating_class\": [-3, -2],\n",
    "  \"sets_are_pre_split\": True,\n",
    "  \"train_val_test_split\": [0.64, 0.16, 0.20],\n",
    "  \"evaluate_on_test_set_only\": False,\n",
    "\n",
    "  \"total_epochs\": 100,\n",
    "  \"total_iter_per_epoch\":500, \"continue_from_epoch\": -2,\n",
    "  \"num_evaluation_tasks\":600,\n",
    "  \"multi_step_loss_num_epochs\": 15,\n",
    "  \"minimum_per_task_contribution\": 0.01,\n",
    "  \"learnable_per_layer_per_step_inner_loop_learning_rate\": True,\n",
    "  \"enable_inner_loop_optimizable_bn_params\": False,\n",
    "  \"evalute_on_test_set_only\": False,\n",
    "\n",
    "  \"max_pooling\": True,\n",
    "  \"per_step_bn_statistics\": False,\n",
    "  \"learnable_batch_norm_momentum\": False,\n",
    "  \"load_into_memory\": False,\n",
    "  \"init_inner_loop_learning_rate\": 0.01,\n",
    "  \"init_inner_loop_weight_decay\": 0.0005,\n",
    "  \"learnable_bn_gamma\": True,\n",
    "  \"learnable_bn_beta\": True,\n",
    "\n",
    "  \"dropout_rate_value\":0.0,\n",
    "  \"min_learning_rate\":0.001,\n",
    "  \"meta_learning_rate\":0.001,   \"total_epochs_before_pause\": 101,\n",
    "  \"first_order_to_second_order_epoch\":-1,\n",
    "  \"weight_decay\": 0.0,\n",
    "\n",
    "  \"norm_layer\":\"batch_norm\",\n",
    "  \"cnn_num_filters\":128,\n",
    "  \"num_stages\":4,\n",
    "  \"conv_padding\": True,\n",
    "  \"number_of_training_steps_per_iter\":5,\n",
    "  \"number_of_evaluation_steps_per_iter\":5,\n",
    "  \"cnn_blocks_per_stage\":1,\n",
    "  \"num_classes_per_set\":5,\n",
    "  \"num_samples_per_class\":5,\n",
    "  \"num_target_samples\": 15,\n",
    "  \"samples_per_iter\" : 1,\n",
    "\n",
    "  \"second_order\": True,\n",
    "  \"use_multi_step_loss_optimization\":False,\n",
    "  \"backbone\": \"4-CONV\",\n",
    "  \"arbiter\": False,\n",
    "  \"use_bias\": True,\n",
    "  \"prompter\": True,\n",
    "  \"prompt_engineering\": 'padding',\n",
    "  \"prompt_size\" : 5,\n",
    "  \"image_size\" : 84,\n",
    "  \"prompt_random_init\": False,\n",
    "  \"outer_prompt_learning_rate\": 0.001,\n",
    "  \"inner_prompt_learning_rate\": 0.01,\n",
    "  \"data_aug\" : \"not_mixup\",\n",
    "  \"ablation_record\": False,\n",
    "  \"random_prompt_init\": False\n",
    "}\n",
    ")\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "args.im_shape = (2, 3, args.image_height, args.image_width)\n",
    "\n",
    "args.use_cuda = torch.cuda.is_available()\n",
    "args.seed = 104\n",
    "args.reverse_channels=False\n",
    "args.labels_as_int=False\n",
    "args.reset_stored_filepaths=False\n",
    "args.num_of_gpus=1\n",
    "\n",
    "args.continue_from_epoch='latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "dba8492c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using max pooling\n",
      "meta network params\n",
      "0.01\n",
      "Inner Loop parameters\n",
      "prompt_learning_rates_dict.prompt_weight_learning_rate torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv0-conv-weight torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv0-conv-bias torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv1-conv-weight torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv1-conv-bias torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv2-conv-weight torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv2-conv-bias torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv3-conv-weight torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv3-conv-bias torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-linear-weights torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-linear-bias torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv0-conv-weight torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv0-conv-bias torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv1-conv-weight torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv1-conv-bias torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv2-conv-weight torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv2-conv-bias torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv3-conv-weight torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv3-conv-bias torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-linear-weights torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-linear-bias torch.Size([6])\n",
      "Outer Loop parameters\n",
      "classifier.layer_dict.conv0.conv.weight torch.Size([128, 3, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv0.conv.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv0.norm_layer.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv0.norm_layer.weight torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv1.conv.weight torch.Size([128, 128, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv1.conv.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv1.norm_layer.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv1.norm_layer.weight torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv2.conv.weight torch.Size([128, 128, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv2.conv.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv2.norm_layer.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv2.norm_layer.weight torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv3.conv.weight torch.Size([128, 128, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv3.conv.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv3.norm_layer.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv3.norm_layer.weight torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.linear.weights torch.Size([5, 3200]) cuda:0 True\n",
      "classifier.layer_dict.linear.bias torch.Size([5]) cuda:0 True\n",
      "classifier.prompt.prompt_dict.pad_up torch.Size([3, 5, 84]) cuda:0 True\n",
      "classifier.prompt.prompt_dict.pad_down torch.Size([3, 5, 84]) cuda:0 True\n",
      "classifier.prompt.prompt_dict.pad_left torch.Size([3, 74, 5]) cuda:0 True\n",
      "classifier.prompt.prompt_dict.pad_right torch.Size([3, 74, 5]) cuda:0 True\n",
      "inner_loop_optimizer.prompt_learning_rates_dict.prompt_weight_learning_rate torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv0-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv0-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv1-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv1-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv2-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv2-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv3-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv3-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-weights torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv0-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv0-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv1-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv1-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv2-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv2-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv3-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv3-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-linear-weights torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-linear-bias torch.Size([6]) cuda:0 True\n",
      "1\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\MAML_Prompt_padding_5way_5shot_filter128_miniImagenet\n",
      "attempting to find existing checkpoint\n",
      "data {'test': 2953, 'train': 5885, 'val': 2950}\n",
      "train_seed 985773, val_seed: 985773, at start time\n",
      "50000 50000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_val_acc': 0.6918444450696309,\n",
       " 'best_val_iter': 48500,\n",
       " 'current_iter': 50000,\n",
       " 'best_epoch': 97,\n",
       " 'train_loss_mean': 0.4655487828552723,\n",
       " 'train_loss_std': 0.1196020963444165,\n",
       " 'train_accuracy_mean': 0.8247199999094009,\n",
       " 'train_accuracy_std': 0.05141151567399771,\n",
       " 'train_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_0_std': 0.0,\n",
       " 'train_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_1_std': 0.0,\n",
       " 'train_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_2_std': 0.0,\n",
       " 'train_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_3_std': 0.0,\n",
       " 'train_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'train_loss_importance_vector_4_std': 0.0,\n",
       " 'train_learning_rate_mean': 0.00010000000000000003,\n",
       " 'train_learning_rate_std': 2.710505431213761e-20,\n",
       " 'val_loss_mean': 0.823131273984909,\n",
       " 'val_loss_std': 0.13771559480258458,\n",
       " 'val_accuracy_mean': 0.6838222232460975,\n",
       " 'val_accuracy_std': 0.057533741266991185,\n",
       " 'val_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_0_std': 0.0,\n",
       " 'val_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_1_std': 0.0,\n",
       " 'val_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_2_std': 0.0,\n",
       " 'val_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_3_std': 0.0,\n",
       " 'val_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'val_loss_importance_vector_4_std': 0.0,\n",
       " 'network': OrderedDict([('classifier.layer_dict.conv0.conv.weight',\n",
       "               tensor([[[[ 0.0091, -0.0804,  0.0548],\n",
       "                         [-0.0369, -0.0424,  0.0003],\n",
       "                         [-0.0162,  0.1013, -0.0280]],\n",
       "               \n",
       "                        [[ 0.0475, -0.0735,  0.0702],\n",
       "                         [-0.0257,  0.0113,  0.0555],\n",
       "                         [-0.0420,  0.0359, -0.0284]],\n",
       "               \n",
       "                        [[ 0.0496, -0.0216, -0.0295],\n",
       "                         [ 0.0292,  0.0547, -0.0566],\n",
       "                         [-0.0424,  0.0280, -0.0445]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0668,  0.0851,  0.0619],\n",
       "                         [ 0.0610, -0.0515,  0.0079],\n",
       "                         [-0.0796, -0.0788, -0.0609]],\n",
       "               \n",
       "                        [[-0.0116, -0.0077,  0.0065],\n",
       "                         [ 0.0459, -0.0638, -0.0601],\n",
       "                         [ 0.0604, -0.0416,  0.0641]],\n",
       "               \n",
       "                        [[-0.0506,  0.0334, -0.0634],\n",
       "                         [-0.0256,  0.0573,  0.0648],\n",
       "                         [-0.0133,  0.0073, -0.0106]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0017, -0.0286,  0.0533],\n",
       "                         [-0.0191, -0.0457, -0.0185],\n",
       "                         [ 0.0758,  0.0424, -0.0521]],\n",
       "               \n",
       "                        [[-0.0564, -0.0301,  0.0449],\n",
       "                         [ 0.0774, -0.0126,  0.0271],\n",
       "                         [-0.0553, -0.0483,  0.0457]],\n",
       "               \n",
       "                        [[ 0.0431, -0.0735,  0.0350],\n",
       "                         [ 0.0648,  0.0021, -0.0566],\n",
       "                         [-0.0536,  0.0222,  0.0196]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-0.0366, -0.0755, -0.0592],\n",
       "                         [-0.0027, -0.0669,  0.0699],\n",
       "                         [-0.0121,  0.0716, -0.0328]],\n",
       "               \n",
       "                        [[ 0.0448,  0.0195,  0.0640],\n",
       "                         [ 0.0330, -0.0246, -0.0319],\n",
       "                         [ 0.0005,  0.0449, -0.0136]],\n",
       "               \n",
       "                        [[ 0.0144, -0.0732, -0.0344],\n",
       "                         [-0.0393,  0.0180,  0.0356],\n",
       "                         [-0.0361,  0.0299,  0.0418]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0168, -0.0017, -0.0125],\n",
       "                         [ 0.0009,  0.0578, -0.0346],\n",
       "                         [ 0.0346,  0.0651, -0.0434]],\n",
       "               \n",
       "                        [[ 0.0670,  0.0335,  0.0826],\n",
       "                         [ 0.0522, -0.0128, -0.0196],\n",
       "                         [ 0.0153, -0.0456,  0.0488]],\n",
       "               \n",
       "                        [[-0.0304, -0.0199, -0.0301],\n",
       "                         [-0.0031, -0.0301,  0.0656],\n",
       "                         [-0.0723, -0.0354,  0.0024]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0701,  0.0238, -0.0107],\n",
       "                         [-0.0501,  0.0069,  0.0216],\n",
       "                         [ 0.0463,  0.0430,  0.0635]],\n",
       "               \n",
       "                        [[-0.0541, -0.0581,  0.0356],\n",
       "                         [ 0.0437, -0.0644, -0.0398],\n",
       "                         [ 0.0080,  0.0330,  0.0219]],\n",
       "               \n",
       "                        [[-0.0025, -0.0364,  0.0268],\n",
       "                         [-0.0038, -0.0497, -0.0463],\n",
       "                         [-0.0181, -0.0233, -0.0085]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.conv.bias',\n",
       "               tensor([-1.9563e-05, -5.0184e-06, -3.2435e-05,  7.4637e-06,  2.2972e-05,\n",
       "                        2.8345e-05,  3.5577e-06, -2.2295e-05,  3.9704e-05, -1.4407e-05,\n",
       "                       -5.8196e-06, -2.9924e-05,  3.0405e-05,  8.1745e-06, -1.3714e-05,\n",
       "                        4.0079e-05,  3.0452e-05, -1.9941e-05, -8.8358e-07, -1.6501e-05,\n",
       "                       -6.4813e-05, -1.7237e-05, -6.8861e-06, -9.5309e-05,  5.3681e-05,\n",
       "                       -6.5402e-07, -1.3461e-05, -2.1457e-05, -4.0637e-05,  4.9657e-05,\n",
       "                       -2.0976e-05, -1.0297e-06,  2.0711e-05,  2.8200e-06,  5.4094e-06,\n",
       "                       -6.2818e-06,  3.7736e-07, -3.7162e-06, -2.4984e-06,  2.9105e-05,\n",
       "                        3.5542e-06,  2.4315e-05,  4.9623e-06,  2.0363e-05,  4.5748e-05,\n",
       "                       -3.1802e-05,  4.1747e-05, -2.0841e-05, -9.1288e-05, -3.0898e-05,\n",
       "                       -2.6104e-05, -1.0237e-05,  4.1973e-05, -5.2607e-05, -1.5181e-05,\n",
       "                        7.8043e-06,  1.9955e-05,  4.2865e-05, -4.1748e-06, -2.7541e-06,\n",
       "                       -1.2792e-05,  4.4561e-06, -1.5149e-05,  9.7682e-06,  1.8856e-05,\n",
       "                       -2.2423e-05,  5.9911e-06, -1.1529e-06,  1.5536e-05, -2.3470e-05,\n",
       "                       -1.5711e-05, -3.2106e-05, -9.3480e-06,  4.1520e-06,  3.2695e-05,\n",
       "                       -2.4681e-05, -1.2092e-05, -1.3332e-05,  7.6868e-06,  2.5753e-05,\n",
       "                        3.4480e-05, -2.5429e-06, -1.7358e-05, -1.1087e-05, -3.0308e-05,\n",
       "                        5.9904e-05, -2.4076e-05, -4.6866e-06,  5.9139e-06, -5.2338e-05,\n",
       "                        1.5913e-04, -3.3516e-05, -2.9693e-05,  3.8201e-05,  9.5198e-06,\n",
       "                       -2.7171e-05, -1.7170e-05, -3.7336e-05, -2.0201e-05, -1.6430e-06,\n",
       "                        3.7468e-05,  4.4325e-05,  1.5812e-05, -2.1988e-05, -1.1614e-05,\n",
       "                       -3.1324e-05, -1.9418e-05,  2.6368e-05, -1.5446e-04, -2.2241e-05,\n",
       "                        9.4951e-06, -2.3255e-05, -2.9636e-06,  3.7763e-05, -8.1398e-05,\n",
       "                        7.5119e-06,  7.7591e-06,  4.6357e-06,  4.6393e-06,  5.0373e-05,\n",
       "                       -2.0042e-05, -1.4034e-05,  5.3311e-05,  1.1709e-04, -2.5843e-05,\n",
       "                        4.6862e-05, -3.5328e-05,  1.2797e-05], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.bias',\n",
       "               tensor([ 0.1106, -0.0675,  0.0785, -0.0405, -0.0201,  0.0379, -0.0653, -0.0930,\n",
       "                       -0.0676, -0.0188, -0.2011, -0.1096,  0.0313, -0.0815, -0.0593, -0.0593,\n",
       "                        0.1174, -0.1473,  0.1191, -0.0452,  0.0251, -0.0687, -0.1412,  0.0120,\n",
       "                       -0.0846, -0.1073, -0.0860, -0.1061,  0.1393,  0.0787, -0.0069, -0.1152,\n",
       "                       -0.0578, -0.1719, -0.0721, -0.0938, -0.0634, -0.0317, -0.0229, -0.0556,\n",
       "                       -0.1369, -0.0735, -0.1051, -0.0183,  0.0629, -0.0909, -0.0347, -0.1314,\n",
       "                       -0.0029, -0.0469, -0.0684, -0.0416,  0.1535, -0.0761, -0.0385, -0.1378,\n",
       "                       -0.0334, -0.0225,  0.0164,  0.0360, -0.1212,  0.0882, -0.0290, -0.1290,\n",
       "                        0.0364, -0.1449, -0.1736, -0.1824, -0.0699, -0.0498, -0.0308,  0.0069,\n",
       "                       -0.1009, -0.1099, -0.0054, -0.0693, -0.0339, -0.0010,  0.0020, -0.0751,\n",
       "                        0.0497, -0.0249, -0.1471, -0.0224, -0.0260,  0.0680,  0.0283, -0.0801,\n",
       "                       -0.1384,  0.0563,  0.0371, -0.1234, -0.0483, -0.0040,  0.1541, -0.0030,\n",
       "                        0.0239,  0.0463, -0.0033, -0.1255, -0.0796, -0.0169, -0.1236, -0.0275,\n",
       "                       -0.0612, -0.1026,  0.0084, -0.1448,  0.0131,  0.0025,  0.0929,  0.0111,\n",
       "                       -0.0580, -0.0068, -0.0671, -0.0632, -0.0696, -0.0160, -0.0851, -0.0250,\n",
       "                       -0.0441, -0.0781, -0.1050, -0.0605,  0.1127, -0.0862, -0.0338, -0.1964],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.weight',\n",
       "               tensor([0.8121, 0.6895, 0.7727, 0.6495, 0.7340, 0.8405, 0.7446, 0.6167, 0.6505,\n",
       "                       0.5981, 0.5478, 0.6384, 0.7785, 0.6421, 0.8204, 0.9261, 0.8577, 0.6182,\n",
       "                       0.7021, 0.7958, 0.7544, 0.6854, 0.6248, 0.8284, 0.7430, 0.6943, 0.5707,\n",
       "                       0.6536, 0.5858, 0.7972, 0.7612, 0.6404, 0.8468, 0.9585, 0.7197, 0.6748,\n",
       "                       0.8086, 0.7358, 0.7190, 0.8039, 0.5925, 0.7637, 0.6081, 0.7014, 1.0320,\n",
       "                       0.6472, 0.7122, 0.5809, 0.7017, 0.6803, 0.6667, 0.6765, 0.8845, 0.8539,\n",
       "                       0.7841, 0.6305, 0.9296, 0.7747, 0.6880, 0.8885, 0.6758, 0.8714, 0.7174,\n",
       "                       0.6054, 0.6275, 0.6364, 0.5782, 0.5699, 0.6713, 0.8072, 0.6946, 0.7586,\n",
       "                       0.6389, 0.6020, 0.8913, 0.6650, 0.7736, 0.7866, 0.7251, 0.7084, 0.7882,\n",
       "                       0.8164, 0.6309, 0.6542, 0.9936, 0.7765, 0.8399, 0.7434, 0.5499, 0.7710,\n",
       "                       0.9180, 0.6813, 0.7067, 0.9218, 0.8384, 0.6479, 0.8303, 0.8398, 0.6341,\n",
       "                       0.6296, 0.6519, 0.7416, 0.6175, 0.6635, 0.5637, 0.6131, 0.7441, 0.6451,\n",
       "                       0.7705, 0.6845, 0.9198, 0.7735, 0.6352, 0.7752, 0.6589, 0.9260, 0.7460,\n",
       "                       0.7167, 0.7023, 0.8000, 0.6702, 0.7761, 0.9548, 0.8965, 0.6644, 0.7560,\n",
       "                       0.7129, 0.5304], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.weight',\n",
       "               tensor([[[[-0.0194,  0.0159, -0.0132],\n",
       "                         [-0.0262,  0.0192, -0.0230],\n",
       "                         [ 0.0017,  0.0192, -0.0075]],\n",
       "               \n",
       "                        [[-0.0014,  0.0007, -0.0373],\n",
       "                         [-0.0380, -0.0386, -0.0364],\n",
       "                         [-0.0420, -0.0059, -0.0365]],\n",
       "               \n",
       "                        [[ 0.0330,  0.0154, -0.0083],\n",
       "                         [-0.0317,  0.0099,  0.0202],\n",
       "                         [ 0.0190,  0.0340, -0.0084]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0118, -0.0184, -0.0150],\n",
       "                         [-0.0304, -0.0230, -0.0092],\n",
       "                         [ 0.0027, -0.0122, -0.0114]],\n",
       "               \n",
       "                        [[-0.0180,  0.0031, -0.0205],\n",
       "                         [-0.0011,  0.0263,  0.0055],\n",
       "                         [ 0.0195, -0.0189,  0.0078]],\n",
       "               \n",
       "                        [[-0.0049,  0.0250, -0.0059],\n",
       "                         [ 0.0109,  0.0190, -0.0001],\n",
       "                         [ 0.0039,  0.0042,  0.0216]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0091, -0.0106,  0.0420],\n",
       "                         [ 0.0328,  0.0129, -0.0085],\n",
       "                         [ 0.0146, -0.0088,  0.0192]],\n",
       "               \n",
       "                        [[ 0.0221,  0.0006, -0.0089],\n",
       "                         [-0.0222,  0.0074,  0.0010],\n",
       "                         [-0.0097,  0.0088,  0.0102]],\n",
       "               \n",
       "                        [[ 0.0341,  0.0004, -0.0048],\n",
       "                         [ 0.0077,  0.0058, -0.0028],\n",
       "                         [-0.0560,  0.0048, -0.0452]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0108, -0.0088,  0.0171],\n",
       "                         [-0.0293, -0.0430, -0.0289],\n",
       "                         [ 0.0010,  0.0033, -0.0299]],\n",
       "               \n",
       "                        [[ 0.0084, -0.0292, -0.0474],\n",
       "                         [ 0.0171,  0.0179, -0.0037],\n",
       "                         [ 0.0216, -0.0034,  0.0247]],\n",
       "               \n",
       "                        [[-0.0051, -0.0449, -0.0187],\n",
       "                         [ 0.0134, -0.0427, -0.0284],\n",
       "                         [-0.0163, -0.0129,  0.0123]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0529, -0.0443,  0.0326],\n",
       "                         [ 0.0045,  0.0043, -0.0294],\n",
       "                         [ 0.0206,  0.0217, -0.0031]],\n",
       "               \n",
       "                        [[ 0.0499,  0.0389, -0.0308],\n",
       "                         [ 0.0450,  0.0122, -0.0233],\n",
       "                         [ 0.0243,  0.0234, -0.0267]],\n",
       "               \n",
       "                        [[-0.0532, -0.0281, -0.0393],\n",
       "                         [-0.0023, -0.0370, -0.0076],\n",
       "                         [ 0.0292,  0.0063,  0.0242]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0023,  0.0278,  0.0373],\n",
       "                         [-0.0205, -0.0123, -0.0065],\n",
       "                         [-0.0062, -0.0166, -0.0318]],\n",
       "               \n",
       "                        [[ 0.0373,  0.0226,  0.0263],\n",
       "                         [-0.0413, -0.0194, -0.0310],\n",
       "                         [-0.0252,  0.0137, -0.0147]],\n",
       "               \n",
       "                        [[-0.0258, -0.0382,  0.0173],\n",
       "                         [-0.0184,  0.0261,  0.0243],\n",
       "                         [ 0.0213,  0.0092, -0.0193]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0442, -0.0018,  0.0243],\n",
       "                         [ 0.0285,  0.0099,  0.0194],\n",
       "                         [ 0.0307, -0.0198, -0.0126]],\n",
       "               \n",
       "                        [[-0.0255, -0.0539, -0.0121],\n",
       "                         [-0.0249, -0.0494, -0.0131],\n",
       "                         [-0.0363, -0.0375, -0.0362]],\n",
       "               \n",
       "                        [[ 0.0540,  0.0304,  0.0067],\n",
       "                         [ 0.0363,  0.0082,  0.0074],\n",
       "                         [ 0.0002, -0.0090,  0.0071]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0098, -0.0241, -0.0058],\n",
       "                         [ 0.0167, -0.0327,  0.0154],\n",
       "                         [ 0.0107,  0.0138, -0.0051]],\n",
       "               \n",
       "                        [[ 0.0203,  0.0211,  0.0349],\n",
       "                         [ 0.0319,  0.0032, -0.0071],\n",
       "                         [-0.0339,  0.0097,  0.0173]],\n",
       "               \n",
       "                        [[ 0.0104, -0.0016,  0.0033],\n",
       "                         [-0.0034,  0.0116,  0.0149],\n",
       "                         [ 0.0004,  0.0127, -0.0168]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0152, -0.0347,  0.0006],\n",
       "                         [ 0.0123, -0.0215,  0.0129],\n",
       "                         [ 0.0041,  0.0449,  0.0029]],\n",
       "               \n",
       "                        [[-0.0063,  0.0098,  0.0050],\n",
       "                         [ 0.0364,  0.0148, -0.0309],\n",
       "                         [ 0.0356,  0.0062, -0.0318]],\n",
       "               \n",
       "                        [[-0.0186,  0.0331,  0.0303],\n",
       "                         [ 0.0318,  0.0373, -0.0028],\n",
       "                         [-0.0153,  0.0497,  0.0309]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0099, -0.0107,  0.0335],\n",
       "                         [ 0.0039,  0.0088, -0.0313],\n",
       "                         [ 0.0036, -0.0042, -0.0340]],\n",
       "               \n",
       "                        [[ 0.0152,  0.0345,  0.0383],\n",
       "                         [ 0.0058, -0.0163, -0.0067],\n",
       "                         [ 0.0004, -0.0138,  0.0057]],\n",
       "               \n",
       "                        [[-0.0210, -0.0191, -0.0148],\n",
       "                         [-0.0081,  0.0122, -0.0087],\n",
       "                         [ 0.0122, -0.0085, -0.0024]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0366, -0.0138, -0.0068],\n",
       "                         [ 0.0649,  0.0194, -0.0350],\n",
       "                         [ 0.0046, -0.0035,  0.0049]],\n",
       "               \n",
       "                        [[-0.0101,  0.0138,  0.0119],\n",
       "                         [ 0.0372, -0.0259, -0.0105],\n",
       "                         [-0.0180, -0.0019,  0.0297]],\n",
       "               \n",
       "                        [[-0.0179, -0.0116,  0.0294],\n",
       "                         [ 0.0029, -0.0019, -0.0142],\n",
       "                         [-0.0249, -0.0611, -0.0095]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0424,  0.0369,  0.0010],\n",
       "                         [ 0.0546,  0.0565, -0.0085],\n",
       "                         [ 0.0170,  0.0159,  0.0048]],\n",
       "               \n",
       "                        [[-0.0299, -0.0172, -0.0260],\n",
       "                         [ 0.0178,  0.0459, -0.0047],\n",
       "                         [-0.0242,  0.0070, -0.0455]],\n",
       "               \n",
       "                        [[-0.0120,  0.0274,  0.0066],\n",
       "                         [-0.0057,  0.0124,  0.0161],\n",
       "                         [-0.0023, -0.0234, -0.0383]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.bias',\n",
       "               tensor([ 3.2157e-06, -5.9423e-06,  6.0723e-06, -8.2983e-06,  3.4502e-07,\n",
       "                        7.2476e-07,  1.8209e-05, -4.4528e-06,  3.1301e-06, -1.0476e-06,\n",
       "                       -2.5376e-06,  5.2537e-06, -3.2374e-06,  1.7080e-06, -2.7915e-06,\n",
       "                        3.9430e-06,  3.3594e-06,  5.7654e-06, -1.5129e-06, -5.5823e-06,\n",
       "                       -2.2806e-06,  6.9773e-06, -1.7492e-05,  3.8963e-06, -9.0273e-06,\n",
       "                        2.6937e-07, -2.4403e-06,  9.9327e-06, -4.1453e-06, -6.2131e-06,\n",
       "                        3.6498e-06, -1.5165e-06, -1.1479e-06,  7.4229e-06, -2.3044e-06,\n",
       "                       -3.6872e-06, -7.2448e-06, -4.2430e-06, -1.7623e-07,  1.7555e-06,\n",
       "                        2.2183e-06,  1.3382e-06,  6.8475e-06, -3.1947e-06, -2.6217e-06,\n",
       "                        2.6841e-07, -4.5009e-07,  6.9074e-07, -4.8114e-06, -3.6826e-07,\n",
       "                        8.4307e-06,  6.7235e-06,  6.6037e-06,  1.1950e-06,  5.1914e-06,\n",
       "                        2.6676e-06,  4.8450e-07, -5.3449e-06, -8.3830e-06, -2.7632e-07,\n",
       "                        3.1150e-06,  4.2261e-06,  1.2749e-06, -4.5822e-07, -4.2891e-06,\n",
       "                        8.5480e-06, -1.0062e-05,  1.9029e-06, -2.6616e-06, -1.5231e-05,\n",
       "                       -4.5979e-06, -8.1499e-06,  7.2877e-06,  4.5539e-06,  6.4418e-07,\n",
       "                       -1.0675e-06,  3.0118e-07, -1.8530e-06, -5.0286e-06, -3.8818e-06,\n",
       "                        2.3580e-06,  3.1504e-07,  9.8953e-07,  1.0474e-06,  1.4661e-06,\n",
       "                       -7.6606e-06,  5.9939e-06,  9.8803e-06,  9.3190e-06,  1.9933e-07,\n",
       "                       -1.0100e-07,  9.9007e-06, -3.4936e-06, -6.2591e-06,  7.3517e-06,\n",
       "                       -5.5298e-06,  7.3219e-06, -8.2829e-07, -4.3913e-07, -2.9940e-07,\n",
       "                        4.2548e-07, -5.4492e-06,  3.9186e-06, -3.2913e-06, -5.2988e-06,\n",
       "                       -1.6471e-06,  1.0477e-06, -2.1614e-06,  2.3186e-06,  1.9325e-06,\n",
       "                        4.6627e-06, -2.4952e-06,  4.4872e-06,  3.9601e-06,  3.9309e-06,\n",
       "                       -2.5744e-06, -1.7200e-05,  3.8924e-07, -8.1703e-06,  6.4104e-06,\n",
       "                        7.1412e-06,  1.0436e-06, -5.7208e-06, -6.2194e-06,  2.6778e-07,\n",
       "                        3.3295e-06,  7.7684e-06,  2.6151e-06], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.bias',\n",
       "               tensor([-0.1849, -0.1672, -0.0959, -0.1050, -0.1642, -0.1643, -0.1683, -0.2194,\n",
       "                       -0.1340, -0.1234, -0.0961, -0.1411, -0.1908, -0.1158, -0.1665, -0.0748,\n",
       "                       -0.1108, -0.1151, -0.1072, -0.1105, -0.1745, -0.0997, -0.1471, -0.1376,\n",
       "                       -0.0811, -0.1411, -0.1323, -0.1019, -0.1508, -0.1026, -0.1016, -0.1760,\n",
       "                       -0.0130, -0.0641, -0.1111, -0.1186, -0.0944, -0.1379, -0.1383, -0.1411,\n",
       "                       -0.1488, -0.1175, -0.0917, -0.1358, -0.0948, -0.1387, -0.1130, -0.1289,\n",
       "                       -0.1409, -0.1247, -0.0836, -0.0978, -0.0872, -0.1857, -0.1391, -0.0953,\n",
       "                       -0.1317, -0.1319, -0.1506, -0.0614, -0.1358, -0.0990, -0.1533, -0.0943,\n",
       "                       -0.1125, -0.0667, -0.1082, -0.1479, -0.1237, -0.1393, -0.1136, -0.1431,\n",
       "                       -0.0791, -0.0769, -0.1916, -0.1401, -0.1170, -0.0971, -0.0910, -0.1643,\n",
       "                       -0.1641, -0.1270, -0.0572, -0.0676, -0.1483, -0.1367, -0.1558, -0.1467,\n",
       "                       -0.0423, -0.1151, -0.1721, -0.1255,  0.0087, -0.1280, -0.0971, -0.0736,\n",
       "                       -0.1486, -0.0913, -0.0223, -0.1313, -0.1322, -0.1145, -0.1278, -0.1504,\n",
       "                       -0.1387, -0.0724, -0.0777, -0.1168, -0.0685, -0.1160, -0.1128, -0.1088,\n",
       "                       -0.1364, -0.1006, -0.1227, -0.0467, -0.1249, -0.1514, -0.1995, -0.1096,\n",
       "                       -0.1170, -0.0692, -0.1134, -0.1275, -0.0929, -0.1374, -0.1529, -0.1411],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.weight',\n",
       "               tensor([0.6983, 0.8097, 0.8797, 0.7652, 0.8145, 0.8768, 0.8226, 0.8112, 0.8863,\n",
       "                       0.7424, 0.8119, 0.7633, 0.7106, 0.8267, 0.8510, 0.7082, 0.7491, 0.7004,\n",
       "                       0.7601, 0.8011, 0.6148, 0.8231, 0.7775, 0.6640, 0.7941, 0.7409, 0.7782,\n",
       "                       0.8264, 0.7421, 0.6992, 0.7886, 0.7963, 0.6663, 0.7663, 0.8209, 0.6478,\n",
       "                       0.7084, 0.7994, 0.7196, 0.7038, 0.7013, 0.7877, 0.7006, 0.7104, 0.6946,\n",
       "                       0.7087, 0.6690, 0.7140, 0.8398, 0.7513, 0.7557, 0.7510, 0.7338, 0.7418,\n",
       "                       0.7998, 0.7730, 0.7657, 0.7060, 0.7498, 0.7916, 0.6947, 0.7508, 0.8104,\n",
       "                       0.8084, 0.7670, 0.7242, 0.8255, 0.7544, 0.6815, 0.6861, 0.7193, 0.7727,\n",
       "                       0.7815, 0.8094, 0.8456, 0.8125, 0.8201, 0.7645, 0.8177, 0.6495, 0.8855,\n",
       "                       0.7339, 0.8860, 0.7959, 0.6931, 0.7321, 0.8062, 0.6591, 0.8781, 0.7546,\n",
       "                       0.8195, 0.8243, 0.8791, 0.7843, 0.7970, 0.7522, 0.6354, 0.6758, 0.7746,\n",
       "                       0.6940, 0.6706, 0.6764, 0.7594, 0.8386, 0.7077, 0.7397, 0.7591, 0.8049,\n",
       "                       0.7934, 0.8122, 0.7173, 0.7873, 0.8236, 0.8593, 0.7841, 0.7493, 0.8295,\n",
       "                       0.8109, 0.7906, 0.7450, 0.7405, 0.8524, 0.7959, 0.7740, 0.7907, 0.7914,\n",
       "                       0.8420, 0.7871], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.weight',\n",
       "               tensor([[[[-2.4595e-02,  1.3520e-02, -1.4394e-02],\n",
       "                         [-2.6756e-02, -5.4421e-03, -4.7521e-03],\n",
       "                         [ 7.0669e-03, -2.4649e-02,  1.1383e-02]],\n",
       "               \n",
       "                        [[-2.4045e-02, -1.1315e-02,  4.4035e-03],\n",
       "                         [-2.8769e-02, -1.7155e-03, -1.8909e-02],\n",
       "                         [-2.3179e-02, -1.0133e-02, -3.7467e-02]],\n",
       "               \n",
       "                        [[-2.6506e-02, -6.9878e-02, -3.8785e-02],\n",
       "                         [ 3.6083e-02, -1.1028e-02,  2.0983e-02],\n",
       "                         [ 2.9276e-02, -1.8010e-02,  3.0001e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.7810e-02, -1.5608e-03, -1.8127e-02],\n",
       "                         [ 9.0712e-03,  3.2943e-02,  3.8520e-02],\n",
       "                         [-1.7064e-02, -2.2701e-03, -8.0288e-03]],\n",
       "               \n",
       "                        [[-2.2680e-02, -2.4039e-02, -1.8487e-02],\n",
       "                         [-1.2055e-02,  7.5735e-03, -2.2904e-02],\n",
       "                         [-5.0327e-03,  1.2732e-02,  9.3925e-03]],\n",
       "               \n",
       "                        [[-2.5769e-02, -4.8661e-03,  9.9620e-03],\n",
       "                         [ 1.1643e-02, -1.2880e-03, -4.5422e-03],\n",
       "                         [ 4.6523e-03,  1.7176e-03,  2.8302e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 8.0301e-03,  1.1123e-02,  7.6663e-03],\n",
       "                         [-8.7752e-03, -9.8824e-03, -3.8426e-02],\n",
       "                         [-2.6984e-02, -4.5162e-02, -1.3446e-02]],\n",
       "               \n",
       "                        [[ 1.4055e-02,  5.7532e-02,  2.6562e-02],\n",
       "                         [ 4.3132e-02,  4.5465e-02,  3.7499e-02],\n",
       "                         [ 1.0515e-02,  2.1287e-02,  1.5902e-02]],\n",
       "               \n",
       "                        [[ 7.0105e-02,  7.3814e-02,  3.8453e-02],\n",
       "                         [ 7.5416e-02,  3.9198e-02,  2.1785e-02],\n",
       "                         [ 8.3222e-02,  5.0688e-02,  5.2066e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.5674e-02, -2.2823e-02, -4.5389e-02],\n",
       "                         [-2.1212e-02, -3.9447e-02, -4.3465e-02],\n",
       "                         [-1.6874e-02, -2.6703e-02, -6.8629e-02]],\n",
       "               \n",
       "                        [[-1.8656e-03, -6.0597e-03,  1.1775e-02],\n",
       "                         [-1.0746e-02, -1.9051e-02, -2.2117e-02],\n",
       "                         [-2.4089e-02, -2.9471e-02, -3.8672e-02]],\n",
       "               \n",
       "                        [[ 4.3212e-02, -2.2902e-02, -3.3805e-02],\n",
       "                         [-1.3911e-02,  1.1296e-02,  2.5199e-04],\n",
       "                         [ 4.1731e-03, -1.2086e-04, -4.3176e-03]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 2.1735e-02, -2.3923e-02, -5.3538e-02],\n",
       "                         [-3.9810e-02, -2.9345e-02,  7.1225e-03],\n",
       "                         [-4.8878e-03, -2.5426e-02, -2.9625e-02]],\n",
       "               \n",
       "                        [[ 2.3761e-02,  3.5070e-02,  3.4651e-02],\n",
       "                         [ 1.0382e-02,  2.5385e-02,  5.5720e-02],\n",
       "                         [-7.8279e-03,  1.1049e-02,  2.8224e-02]],\n",
       "               \n",
       "                        [[ 2.8651e-02, -3.5989e-02,  2.5763e-02],\n",
       "                         [-7.8715e-03, -1.6909e-02, -2.3109e-02],\n",
       "                         [ 1.4195e-02, -1.2813e-02,  1.3847e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.4224e-02,  2.2047e-02, -2.6659e-02],\n",
       "                         [-1.7270e-04, -1.8452e-03, -1.9419e-02],\n",
       "                         [ 7.9080e-03, -6.3372e-03,  1.1534e-03]],\n",
       "               \n",
       "                        [[ 6.4972e-05, -1.8033e-02,  3.2539e-02],\n",
       "                         [ 3.1905e-02, -1.9296e-02, -1.1079e-02],\n",
       "                         [ 1.0608e-02, -3.4119e-02,  1.6116e-02]],\n",
       "               \n",
       "                        [[ 6.1240e-02,  5.6568e-02,  2.8224e-02],\n",
       "                         [ 6.5044e-02,  5.4118e-02,  4.6619e-02],\n",
       "                         [-1.9289e-03,  2.6252e-02,  3.4936e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-3.0169e-02, -2.3002e-02, -4.8751e-02],\n",
       "                         [ 4.2249e-03, -2.5053e-02, -1.7789e-02],\n",
       "                         [-1.1195e-02, -8.3992e-04,  3.1268e-04]],\n",
       "               \n",
       "                        [[-2.8366e-02, -1.4075e-02, -3.7531e-03],\n",
       "                         [-3.9336e-02, -1.2325e-02, -4.1214e-02],\n",
       "                         [-5.0429e-03,  1.1604e-02, -9.9770e-03]],\n",
       "               \n",
       "                        [[-6.0997e-03,  1.4726e-03, -3.3068e-02],\n",
       "                         [-3.5237e-02, -8.4084e-03,  5.6489e-03],\n",
       "                         [-1.3855e-02, -7.5097e-03, -1.4205e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 3.6170e-03, -9.4879e-03,  1.2436e-02],\n",
       "                         [ 3.8337e-02,  1.4326e-02,  2.2588e-02],\n",
       "                         [ 7.2899e-03, -5.7904e-03,  7.6957e-03]],\n",
       "               \n",
       "                        [[ 6.7952e-04, -3.4157e-02,  1.2025e-02],\n",
       "                         [-1.7039e-02, -1.5919e-02,  1.6544e-03],\n",
       "                         [ 4.1878e-03,  2.3924e-02, -3.0344e-02]],\n",
       "               \n",
       "                        [[ 1.3475e-02,  1.8070e-02,  1.6734e-02],\n",
       "                         [-2.9038e-02, -3.2510e-02, -3.5415e-02],\n",
       "                         [-3.9083e-02, -5.1857e-03, -3.3490e-03]]],\n",
       "               \n",
       "               \n",
       "                       [[[-4.4501e-03,  3.2289e-02, -2.2089e-02],\n",
       "                         [ 3.1885e-03, -1.8405e-02, -2.9090e-02],\n",
       "                         [ 3.2522e-02,  4.6185e-02, -2.4054e-02]],\n",
       "               \n",
       "                        [[ 1.7875e-03, -8.6619e-03,  9.8217e-03],\n",
       "                         [-3.6811e-02, -1.9771e-02,  1.7350e-02],\n",
       "                         [-2.4807e-02,  2.2758e-02,  3.7127e-02]],\n",
       "               \n",
       "                        [[-3.3118e-02, -2.5215e-02,  8.5379e-03],\n",
       "                         [-2.4028e-02, -1.9994e-02, -5.9731e-03],\n",
       "                         [-6.8162e-02, -5.8698e-02, -1.0691e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-6.4427e-02, -6.2956e-02, -1.2373e-02],\n",
       "                         [-4.3735e-02,  1.7544e-03, -3.7168e-02],\n",
       "                         [-3.9030e-02, -2.0223e-02, -2.7085e-02]],\n",
       "               \n",
       "                        [[-4.7523e-02, -2.6137e-02,  1.3201e-02],\n",
       "                         [-1.6237e-02, -2.1466e-02,  5.1712e-02],\n",
       "                         [-5.0106e-04, -2.2235e-02, -1.0544e-02]],\n",
       "               \n",
       "                        [[ 3.6459e-02,  2.0671e-02, -3.2124e-02],\n",
       "                         [ 1.2045e-02,  2.8766e-02, -2.1748e-02],\n",
       "                         [ 4.0257e-03,  1.8461e-02,  3.7930e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.1199e-02,  1.3468e-02,  2.5019e-02],\n",
       "                         [-1.2732e-02,  1.9989e-03,  6.2451e-02],\n",
       "                         [-3.6267e-02,  2.1968e-04,  5.5146e-02]],\n",
       "               \n",
       "                        [[ 4.6763e-02,  2.0073e-02, -1.7179e-02],\n",
       "                         [ 2.1057e-03,  6.6864e-03, -2.7772e-03],\n",
       "                         [-7.3693e-03,  1.9613e-02,  1.8478e-02]],\n",
       "               \n",
       "                        [[-3.0667e-02, -2.5382e-02, -9.0416e-03],\n",
       "                         [-1.9101e-02, -2.3464e-03, -4.2787e-03],\n",
       "                         [-2.7058e-02, -2.7003e-02, -3.9423e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-4.3421e-03,  6.2343e-03, -2.0070e-02],\n",
       "                         [ 2.5632e-02,  2.5631e-02,  3.1719e-03],\n",
       "                         [-4.2931e-04,  6.6523e-04,  7.8694e-03]],\n",
       "               \n",
       "                        [[ 1.3522e-02,  4.2252e-02, -7.1688e-03],\n",
       "                         [ 2.3680e-03,  4.3667e-03, -3.9501e-02],\n",
       "                         [ 2.8635e-03, -2.2989e-02, -4.9335e-02]],\n",
       "               \n",
       "                        [[ 2.2411e-02,  3.4319e-02,  2.3777e-02],\n",
       "                         [ 4.1555e-03,  2.0434e-02,  4.5098e-03],\n",
       "                         [ 5.8572e-02,  2.0755e-02,  3.9904e-03]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.bias',\n",
       "               tensor([-1.4932e-06,  5.2203e-06,  3.6981e-06,  6.0755e-06, -4.8515e-06,\n",
       "                        1.0894e-05, -5.5616e-07,  2.6314e-06, -1.3622e-06, -8.4031e-06,\n",
       "                        2.2638e-06,  2.7163e-06, -1.6064e-06, -8.0784e-06, -7.2054e-06,\n",
       "                       -4.4478e-06, -3.4633e-06, -4.6097e-06, -4.3078e-07,  2.1807e-06,\n",
       "                        5.0689e-06,  2.7022e-06,  2.6542e-06,  5.4860e-06,  4.6816e-06,\n",
       "                       -4.9744e-06,  7.3237e-07, -3.6978e-06,  2.9881e-06, -5.5264e-06,\n",
       "                        3.7471e-06,  5.1306e-06, -3.6835e-06,  4.7674e-06, -5.0012e-06,\n",
       "                        1.0519e-05,  6.1161e-07,  1.9679e-06, -3.5850e-06,  1.9692e-06,\n",
       "                        2.0287e-06,  3.7626e-06, -5.3245e-07, -8.3392e-07, -5.0159e-06,\n",
       "                       -1.1017e-06,  3.3202e-06, -5.7549e-06,  2.5323e-06,  7.6212e-06,\n",
       "                       -2.5713e-06, -1.3356e-05,  7.7826e-06, -1.7654e-06,  4.5423e-06,\n",
       "                        2.5975e-06,  2.2684e-06,  2.3423e-06,  3.5957e-06,  9.8492e-06,\n",
       "                        7.0469e-06,  2.7036e-06, -3.0388e-07,  4.0082e-08,  9.3796e-06,\n",
       "                       -2.2522e-06, -3.2065e-06, -7.2391e-06,  1.0113e-05,  5.7536e-06,\n",
       "                       -5.7366e-06, -6.4917e-06,  8.1829e-06, -3.7521e-06,  1.0029e-07,\n",
       "                       -2.3428e-06, -6.1396e-06,  1.3198e-05, -4.9618e-06, -1.7033e-05,\n",
       "                       -1.8552e-05,  9.3020e-07, -2.4104e-06, -3.1999e-06,  1.0860e-06,\n",
       "                       -2.3025e-06, -1.1772e-05, -3.9252e-06,  5.8429e-07, -1.3140e-06,\n",
       "                       -5.7573e-07,  9.2351e-06,  1.8236e-06,  1.6380e-05,  7.7140e-06,\n",
       "                        4.4863e-06, -1.3731e-06, -3.3836e-07,  6.1150e-06, -1.5502e-06,\n",
       "                        9.4840e-06,  1.0925e-06,  3.3608e-06,  4.4844e-06,  1.0718e-05,\n",
       "                        3.8783e-06,  2.9913e-06, -1.0708e-06,  1.1193e-07,  6.1856e-06,\n",
       "                       -2.9090e-06, -1.5943e-07,  2.7433e-07,  1.9144e-06,  5.9656e-07,\n",
       "                        2.5150e-06, -6.2108e-06,  6.7184e-06, -1.4370e-05,  6.7250e-07,\n",
       "                        3.3884e-07,  4.2502e-06,  1.3969e-05,  5.7274e-06, -3.0153e-06,\n",
       "                       -3.7147e-06,  3.2665e-06, -2.5375e-06], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.bias',\n",
       "               tensor([-0.1163, -0.1167, -0.1098, -0.1320, -0.1521, -0.1055, -0.1973, -0.1324,\n",
       "                       -0.1387, -0.1347, -0.1273, -0.1309, -0.1226, -0.1428, -0.0975, -0.1773,\n",
       "                       -0.1718, -0.0796, -0.1515, -0.1778, -0.1193, -0.1084, -0.1510, -0.1690,\n",
       "                       -0.1269, -0.1809, -0.2080, -0.1840, -0.1042, -0.0842, -0.1508, -0.1420,\n",
       "                       -0.1602, -0.1679, -0.1781, -0.1556, -0.1813, -0.1763, -0.1446, -0.1317,\n",
       "                       -0.0993, -0.1831, -0.1515, -0.1742, -0.1168, -0.1932, -0.0362, -0.1154,\n",
       "                       -0.1554, -0.0674, -0.1129, -0.1217, -0.1422, -0.1191, -0.1920, -0.1411,\n",
       "                       -0.1544, -0.0910, -0.1503, -0.1697, -0.0421, -0.2029, -0.1588, -0.1641,\n",
       "                       -0.1132, -0.0912, -0.1545, -0.1509, -0.1146, -0.1041, -0.1125, -0.1367,\n",
       "                       -0.1769, -0.1342, -0.1229, -0.1378, -0.1451, -0.1267, -0.1605, -0.1418,\n",
       "                       -0.1144, -0.2131, -0.1861, -0.1222, -0.1427, -0.1828, -0.1796, -0.1236,\n",
       "                       -0.1030, -0.1038, -0.1912, -0.1963, -0.1097, -0.1218, -0.0884, -0.0993,\n",
       "                       -0.1649, -0.1788, -0.1212, -0.1405, -0.1225, -0.0956, -0.1087, -0.1642,\n",
       "                       -0.0981, -0.1796, -0.1439, -0.1580, -0.1364, -0.1433, -0.1080, -0.1770,\n",
       "                       -0.1530, -0.1148, -0.1861, -0.1440, -0.1328, -0.1636, -0.1201, -0.1147,\n",
       "                       -0.1401, -0.1475, -0.0625, -0.1397, -0.1419, -0.1365, -0.1792, -0.1414],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.weight',\n",
       "               tensor([0.7121, 0.8079, 0.7920, 0.6400, 0.8080, 0.6579, 0.7937, 0.8449, 0.7861,\n",
       "                       0.7507, 0.6603, 0.8777, 0.7883, 0.7482, 0.7667, 0.8102, 0.7709, 0.7588,\n",
       "                       0.8434, 0.7312, 0.7201, 0.6183, 0.7437, 0.7899, 0.8243, 0.8632, 0.7777,\n",
       "                       0.7992, 0.7884, 0.7941, 0.7799, 0.7758, 0.8851, 0.7596, 0.8349, 0.8424,\n",
       "                       0.7024, 0.7400, 0.6829, 0.7313, 0.7788, 0.7684, 0.7263, 0.7418, 0.8034,\n",
       "                       0.8246, 0.8520, 0.6916, 0.6928, 0.6547, 0.7617, 0.7378, 0.8628, 0.6709,\n",
       "                       0.7993, 0.7855, 0.8039, 0.6873, 0.7806, 0.8326, 0.8551, 0.7734, 0.7860,\n",
       "                       0.7809, 0.7192, 0.6989, 0.6794, 0.8148, 0.6870, 0.6794, 0.6756, 0.7543,\n",
       "                       0.7884, 0.7953, 0.7566, 0.7093, 0.7385, 0.7903, 0.6927, 0.8251, 0.7264,\n",
       "                       0.7548, 0.7967, 0.6935, 0.7106, 0.7736, 0.8232, 0.6743, 0.6858, 0.7374,\n",
       "                       0.6875, 0.6937, 0.6230, 0.8056, 0.7798, 0.8122, 0.7292, 0.7340, 0.7500,\n",
       "                       0.7456, 0.7904, 0.7969, 0.8198, 0.7970, 0.7799, 0.8030, 0.7518, 0.7995,\n",
       "                       0.7477, 0.7808, 0.6901, 0.7471, 0.7045, 0.7189, 0.8569, 0.7925, 0.6930,\n",
       "                       0.6881, 0.7757, 0.7486, 0.7919, 0.7896, 0.9126, 0.8433, 0.8016, 0.7462,\n",
       "                       0.7661, 0.6958], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.weight',\n",
       "               tensor([[[[-0.0187, -0.0032, -0.0057],\n",
       "                         [ 0.0087,  0.0130,  0.0094],\n",
       "                         [-0.0025, -0.0047,  0.0236]],\n",
       "               \n",
       "                        [[ 0.0456, -0.0064,  0.0155],\n",
       "                         [ 0.0118, -0.0228, -0.0367],\n",
       "                         [ 0.0242,  0.0177, -0.0312]],\n",
       "               \n",
       "                        [[ 0.0193, -0.0334, -0.0007],\n",
       "                         [-0.0223, -0.0268,  0.0101],\n",
       "                         [-0.0182, -0.0095, -0.0152]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0311, -0.0167, -0.0286],\n",
       "                         [-0.0259, -0.0524, -0.0271],\n",
       "                         [-0.0177, -0.0039,  0.0011]],\n",
       "               \n",
       "                        [[ 0.0049, -0.0211,  0.0065],\n",
       "                         [ 0.0151, -0.0160,  0.0245],\n",
       "                         [ 0.0290,  0.0070,  0.0084]],\n",
       "               \n",
       "                        [[-0.0372, -0.0272, -0.0419],\n",
       "                         [-0.0410, -0.0279, -0.0468],\n",
       "                         [-0.0298, -0.0153, -0.0282]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0107, -0.0194, -0.0368],\n",
       "                         [ 0.0052, -0.0097, -0.0263],\n",
       "                         [-0.0145, -0.0237, -0.0382]],\n",
       "               \n",
       "                        [[-0.0016, -0.0187, -0.0143],\n",
       "                         [-0.0111, -0.0214, -0.0103],\n",
       "                         [ 0.0023,  0.0050,  0.0083]],\n",
       "               \n",
       "                        [[-0.0162, -0.0184,  0.0179],\n",
       "                         [ 0.0143,  0.0047,  0.0134],\n",
       "                         [-0.0069,  0.0051,  0.0139]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0089, -0.0167,  0.0165],\n",
       "                         [ 0.0016, -0.0035,  0.0383],\n",
       "                         [ 0.0008, -0.0065, -0.0141]],\n",
       "               \n",
       "                        [[ 0.0157, -0.0009, -0.0105],\n",
       "                         [ 0.0123,  0.0047,  0.0039],\n",
       "                         [ 0.0025, -0.0060, -0.0063]],\n",
       "               \n",
       "                        [[ 0.0148,  0.0423,  0.0158],\n",
       "                         [-0.0065,  0.0090,  0.0001],\n",
       "                         [ 0.0039,  0.0117,  0.0075]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0181,  0.0042,  0.0418],\n",
       "                         [-0.0336, -0.0139,  0.0076],\n",
       "                         [ 0.0018, -0.0306, -0.0038]],\n",
       "               \n",
       "                        [[-0.0059, -0.0023,  0.0143],\n",
       "                         [-0.0205,  0.0115, -0.0130],\n",
       "                         [ 0.0060,  0.0040, -0.0223]],\n",
       "               \n",
       "                        [[ 0.0214,  0.0031,  0.0133],\n",
       "                         [ 0.0115,  0.0116,  0.0152],\n",
       "                         [ 0.0204,  0.0215,  0.0022]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0028, -0.0081, -0.0163],\n",
       "                         [ 0.0238, -0.0367, -0.0086],\n",
       "                         [-0.0037,  0.0072,  0.0311]],\n",
       "               \n",
       "                        [[ 0.0543,  0.0489,  0.0468],\n",
       "                         [ 0.0238,  0.0552,  0.0291],\n",
       "                         [ 0.0228,  0.0282,  0.0232]],\n",
       "               \n",
       "                        [[-0.0162,  0.0134,  0.0138],\n",
       "                         [ 0.0120,  0.0129, -0.0012],\n",
       "                         [ 0.0325,  0.0207,  0.0236]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0049,  0.0026,  0.0019],\n",
       "                         [ 0.0226,  0.0026, -0.0116],\n",
       "                         [-0.0173, -0.0227, -0.0268]],\n",
       "               \n",
       "                        [[-0.0044,  0.0219, -0.0088],\n",
       "                         [ 0.0144, -0.0214, -0.0389],\n",
       "                         [ 0.0020, -0.0217, -0.0158]],\n",
       "               \n",
       "                        [[ 0.0233,  0.0245,  0.0227],\n",
       "                         [ 0.0152,  0.0103,  0.0050],\n",
       "                         [-0.0023,  0.0223,  0.0087]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0163,  0.0025, -0.0232],\n",
       "                         [ 0.0455, -0.0113, -0.0125],\n",
       "                         [ 0.0343, -0.0170, -0.0065]],\n",
       "               \n",
       "                        [[-0.0042,  0.0609,  0.0591],\n",
       "                         [-0.0021,  0.0374,  0.0335],\n",
       "                         [ 0.0136,  0.0214,  0.0145]],\n",
       "               \n",
       "                        [[ 0.0242,  0.0274,  0.0111],\n",
       "                         [-0.0062,  0.0099,  0.0094],\n",
       "                         [-0.0258, -0.0116, -0.0135]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0204,  0.0100,  0.0017],\n",
       "                         [ 0.0033, -0.0074,  0.0045],\n",
       "                         [ 0.0097,  0.0017, -0.0039]],\n",
       "               \n",
       "                        [[ 0.0320, -0.0028,  0.0020],\n",
       "                         [ 0.0302,  0.0205,  0.0255],\n",
       "                         [ 0.0153,  0.0020,  0.0069]],\n",
       "               \n",
       "                        [[ 0.0240,  0.0083, -0.0143],\n",
       "                         [ 0.0117, -0.0018, -0.0164],\n",
       "                         [-0.0117,  0.0156, -0.0090]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0307, -0.0227,  0.0090],\n",
       "                         [-0.0083, -0.0232,  0.0049],\n",
       "                         [-0.0106,  0.0127,  0.0064]],\n",
       "               \n",
       "                        [[ 0.0047, -0.0026,  0.0412],\n",
       "                         [-0.0193,  0.0108, -0.0131],\n",
       "                         [-0.0127, -0.0333,  0.0017]],\n",
       "               \n",
       "                        [[ 0.0343,  0.0392,  0.0261],\n",
       "                         [ 0.0166,  0.0363,  0.0149],\n",
       "                         [ 0.0286,  0.0129,  0.0409]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0423, -0.0104,  0.0024],\n",
       "                         [ 0.0228,  0.0297, -0.0023],\n",
       "                         [-0.0147,  0.0024,  0.0105]],\n",
       "               \n",
       "                        [[ 0.0161,  0.0297,  0.0352],\n",
       "                         [-0.0082, -0.0172,  0.0088],\n",
       "                         [ 0.0219,  0.0344,  0.0281]],\n",
       "               \n",
       "                        [[-0.0114, -0.0280, -0.0331],\n",
       "                         [-0.0412, -0.0266, -0.0328],\n",
       "                         [-0.0255, -0.0511, -0.0391]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0273, -0.0096,  0.0310],\n",
       "                         [ 0.0261, -0.0104,  0.0251],\n",
       "                         [-0.0004,  0.0156,  0.0197]],\n",
       "               \n",
       "                        [[-0.0281,  0.0068, -0.0022],\n",
       "                         [ 0.0044, -0.0087, -0.0283],\n",
       "                         [-0.0138,  0.0313,  0.0064]],\n",
       "               \n",
       "                        [[ 0.0049,  0.0061,  0.0202],\n",
       "                         [ 0.0106,  0.0034, -0.0249],\n",
       "                         [ 0.0005, -0.0184,  0.0005]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.bias',\n",
       "               tensor([ 3.6020e-06, -1.0839e-05,  8.1594e-06,  8.8432e-06,  8.1802e-06,\n",
       "                        1.1870e-05,  4.7126e-06, -1.9961e-05, -1.2284e-05,  7.0988e-07,\n",
       "                       -1.0704e-05,  4.5792e-06, -6.3601e-06,  1.7966e-05, -2.2908e-06,\n",
       "                        7.4589e-06,  1.3657e-05,  1.4940e-05,  3.0645e-06,  3.9511e-06,\n",
       "                       -1.0334e-05,  8.8767e-06, -2.3153e-06,  4.2872e-06,  1.0666e-06,\n",
       "                       -1.1513e-05, -2.4488e-09,  4.1431e-06,  1.7809e-06,  8.4192e-06,\n",
       "                       -5.6746e-07, -7.3131e-06,  6.4832e-06,  8.5536e-06, -1.2620e-06,\n",
       "                        4.7327e-07, -5.3469e-06, -5.4729e-07, -2.0215e-06, -2.7825e-06,\n",
       "                        5.3269e-06, -3.0843e-06,  9.7990e-06,  2.0677e-06, -6.7390e-09,\n",
       "                       -2.4957e-06,  9.8800e-06,  3.0653e-09, -2.8046e-06,  1.0779e-07,\n",
       "                        3.5689e-06,  2.5313e-06,  4.3596e-06, -1.0157e-05, -5.2389e-06,\n",
       "                       -1.5843e-05, -4.4361e-06,  5.4432e-06,  1.1257e-06, -4.2305e-06,\n",
       "                       -1.2695e-06, -3.5248e-06, -4.3894e-06,  1.2067e-05, -1.9005e-07,\n",
       "                        9.1324e-06,  3.1527e-06, -1.7335e-06,  7.6713e-07,  2.1473e-06,\n",
       "                        3.5830e-06, -2.9195e-07, -8.4695e-06, -2.8429e-06, -6.5446e-06,\n",
       "                       -6.0747e-06,  2.9967e-06, -9.6217e-06,  2.4432e-06, -4.3852e-06,\n",
       "                       -2.1559e-06, -9.5365e-06, -7.1436e-06,  8.3117e-07,  2.5003e-06,\n",
       "                        1.1857e-05,  3.5900e-07, -9.5380e-08, -4.2073e-06,  7.7975e-06,\n",
       "                        8.9336e-07, -4.9201e-06, -2.0943e-07, -4.4902e-06, -3.7962e-06,\n",
       "                       -3.7161e-06, -6.3088e-07, -5.6749e-06, -5.2483e-06,  1.1295e-06,\n",
       "                        1.0017e-05,  4.6501e-06,  5.4090e-06, -1.3633e-06,  8.1737e-06,\n",
       "                       -2.7111e-06,  7.5517e-06,  2.9017e-07, -3.8406e-07, -1.2372e-05,\n",
       "                       -2.3897e-06, -1.8759e-06, -1.1069e-06, -4.3113e-08,  7.2064e-06,\n",
       "                        7.3855e-06,  2.5532e-06,  4.6009e-06,  7.7682e-06, -1.1034e-06,\n",
       "                       -8.0015e-06,  7.7900e-07,  2.1979e-06,  2.1024e-06, -1.3450e-05,\n",
       "                       -1.2812e-05,  5.7737e-07,  4.9129e-06], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.bias',\n",
       "               tensor([-0.1014, -0.2420, -0.1734, -0.2016, -0.1935, -0.1774, -0.1980, -0.1396,\n",
       "                       -0.1218, -0.2467, -0.2210, -0.2131, -0.0962, -0.2353, -0.1246, -0.1796,\n",
       "                       -0.2797, -0.2490, -0.1821, -0.1280, -0.1493, -0.1849, -0.1454, -0.1910,\n",
       "                       -0.1359, -0.1870, -0.0714, -0.1429, -0.1088, -0.1724, -0.1868, -0.1649,\n",
       "                       -0.1686, -0.1376, -0.1700, -0.1666, -0.1479, -0.2092, -0.1773, -0.1282,\n",
       "                       -0.1664, -0.0954, -0.2506, -0.2344, -0.1578, -0.1723, -0.1498, -0.1118,\n",
       "                       -0.1497, -0.1079, -0.1325, -0.1652, -0.2090, -0.0496, -0.1792, -0.2472,\n",
       "                       -0.1818, -0.1440, -0.1691, -0.1334, -0.1424, -0.1749, -0.2160, -0.1576,\n",
       "                       -0.1568, -0.2470, -0.1030, -0.1312, -0.1735, -0.1202, -0.1834, -0.2547,\n",
       "                       -0.2383, -0.1326, -0.1380, -0.1452, -0.1628, -0.1304, -0.1214, -0.1823,\n",
       "                       -0.1924, -0.1316, -0.1054, -0.1482, -0.1714, -0.2112, -0.1466, -0.2166,\n",
       "                       -0.1146, -0.0937, -0.1077, -0.1833, -0.2280, -0.1416, -0.1273, -0.1090,\n",
       "                       -0.1428, -0.1688, -0.2149, -0.0910, -0.1107, -0.1330, -0.0883, -0.2036,\n",
       "                       -0.1927, -0.1222, -0.1791, -0.2118, -0.1382, -0.1977, -0.1220, -0.1421,\n",
       "                       -0.1747, -0.1670, -0.1869, -0.1382, -0.1878, -0.1494, -0.1317, -0.2097,\n",
       "                       -0.0986, -0.2598, -0.2169, -0.1997, -0.1244, -0.2064, -0.1891, -0.1036],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.weight',\n",
       "               tensor([0.7937, 0.8006, 0.7749, 0.7925, 0.8336, 0.7060, 0.7808, 0.7622, 0.7966,\n",
       "                       0.8239, 0.7395, 0.7548, 0.7034, 0.8845, 0.6191, 0.7574, 0.8244, 0.8355,\n",
       "                       0.7290, 0.7016, 0.7409, 0.8282, 0.7998, 0.7103, 0.6035, 0.7966, 0.7827,\n",
       "                       0.7188, 0.7071, 0.6258, 0.7673, 0.7926, 0.7392, 0.8439, 0.7514, 0.7551,\n",
       "                       0.7770, 0.7648, 0.8150, 0.7554, 0.8171, 0.6816, 0.8525, 0.7677, 0.7135,\n",
       "                       0.7977, 0.8216, 0.6836, 0.7854, 0.7240, 0.7443, 0.7492, 0.6549, 0.8852,\n",
       "                       0.7738, 0.7909, 0.6862, 0.7729, 0.8139, 0.7960, 0.6889, 0.7846, 0.7534,\n",
       "                       0.8089, 0.8229, 0.7854, 0.6996, 0.7902, 0.6515, 0.6874, 0.7382, 0.7521,\n",
       "                       0.7118, 0.7808, 0.8472, 0.7912, 0.7305, 0.7066, 0.7311, 0.6696, 0.8175,\n",
       "                       0.7708, 0.7332, 0.5396, 0.7814, 0.7271, 0.7715, 0.7587, 0.7947, 0.7190,\n",
       "                       0.7387, 0.7632, 0.7603, 0.7056, 0.7451, 0.6357, 0.7446, 0.7849, 0.8120,\n",
       "                       0.8181, 0.7597, 0.7575, 0.7158, 0.7826, 0.8146, 0.7219, 0.7432, 0.7300,\n",
       "                       0.8113, 0.7846, 0.7037, 0.7870, 0.8162, 0.7433, 0.7711, 0.7438, 0.7429,\n",
       "                       0.7062, 0.8033, 0.7784, 0.7162, 0.8370, 0.7099, 0.7675, 0.8176, 0.7861,\n",
       "                       0.7553, 0.7642], device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.weights',\n",
       "               tensor([[-0.0116, -0.0197, -0.0119,  ..., -0.0258, -0.0201, -0.0127],\n",
       "                       [-0.0086, -0.0049,  0.0110,  ..., -0.0176, -0.0017, -0.0245],\n",
       "                       [ 0.0167,  0.0067,  0.0037,  ...,  0.0050,  0.0140, -0.0161],\n",
       "                       [-0.0112,  0.0002,  0.0128,  ...,  0.0076,  0.0105,  0.0058],\n",
       "                       [-0.0045, -0.0011,  0.0040,  ..., -0.0018,  0.0191,  0.0114]],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.bias',\n",
       "               tensor([-0.1864, -0.1844,  0.0457,  0.1220,  0.1320], device='cuda:0')),\n",
       "              ('classifier.prompt.prompt_dict.pad_up',\n",
       "               tensor([[[-0.0124, -0.0035, -0.0061,  ...,  0.0116, -0.0138, -0.0021],\n",
       "                        [ 0.0001, -0.0072,  0.0028,  ...,  0.0060,  0.0126,  0.0029],\n",
       "                        [-0.0100, -0.0076, -0.0087,  ...,  0.0260,  0.0026,  0.0078],\n",
       "                        [-0.0195,  0.0179, -0.0061,  ...,  0.0467, -0.0548, -0.0033],\n",
       "                        [-0.0082,  0.0159,  0.0183,  ...,  0.0217,  0.0205, -0.0104]],\n",
       "               \n",
       "                       [[-0.0018,  0.0218,  0.0007,  ..., -0.0125, -0.0073,  0.0052],\n",
       "                        [ 0.0045,  0.0046, -0.0046,  ..., -0.0303,  0.0003, -0.0008],\n",
       "                        [ 0.0046,  0.0020, -0.0155,  ..., -0.0053, -0.0150,  0.0004],\n",
       "                        [ 0.0220, -0.0168,  0.0167,  ..., -0.0286,  0.0237,  0.0018],\n",
       "                        [-0.0073,  0.0077,  0.0051,  ..., -0.0557, -0.0030, -0.0006]],\n",
       "               \n",
       "                       [[-0.0013, -0.0062,  0.0017,  ..., -0.0045,  0.0130, -0.0012],\n",
       "                        [ 0.0192, -0.0002, -0.0163,  ..., -0.0086,  0.0151,  0.0079],\n",
       "                        [-0.0047,  0.0095, -0.0131,  ...,  0.0013, -0.0098, -0.0054],\n",
       "                        [-0.0106, -0.0070,  0.0131,  ...,  0.0032,  0.0226, -0.0039],\n",
       "                        [ 0.0090,  0.0115, -0.0069,  ...,  0.0207,  0.0005, -0.0025]]],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.prompt.prompt_dict.pad_down',\n",
       "               tensor([[[-0.0258,  0.0405, -0.0357,  ...,  0.0148, -0.0421, -0.0168],\n",
       "                        [-0.0215, -0.0060,  0.0127,  ...,  0.0163,  0.0169,  0.0101],\n",
       "                        [-0.0203,  0.0038, -0.0396,  ..., -0.0188,  0.0279, -0.0054],\n",
       "                        [-0.0083,  0.0043,  0.0311,  ..., -0.0179,  0.0239,  0.0011],\n",
       "                        [-0.0079, -0.0099, -0.0165,  ..., -0.0115,  0.0003, -0.0071]],\n",
       "               \n",
       "                       [[ 0.0402, -0.0294, -0.0232,  ..., -0.0584,  0.0119,  0.0112],\n",
       "                        [ 0.0075,  0.0002,  0.0102,  ...,  0.0303, -0.0109, -0.0076],\n",
       "                        [-0.0089,  0.0020, -0.0002,  ...,  0.0778, -0.0207, -0.0090],\n",
       "                        [ 0.0142,  0.0030, -0.0295,  ..., -0.0047, -0.0243, -0.0024],\n",
       "                        [-0.0048,  0.0052,  0.0070,  ..., -0.0072, -0.0177,  0.0010]],\n",
       "               \n",
       "                       [[-0.0165,  0.0030,  0.0312,  ...,  0.0017,  0.0813, -0.0105],\n",
       "                        [ 0.0042,  0.0089,  0.0112,  ..., -0.0180,  0.0005, -0.0066],\n",
       "                        [ 0.0131,  0.0097,  0.0213,  ..., -0.0249, -0.0208, -0.0090],\n",
       "                        [ 0.0010,  0.0125, -0.0293,  ..., -0.0061,  0.0165, -0.0026],\n",
       "                        [ 0.0096,  0.0066, -0.0170,  ...,  0.0058, -0.0032, -0.0029]]],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.prompt.prompt_dict.pad_left',\n",
       "               tensor([[[-3.8301e-03,  5.1876e-03, -5.1320e-02,  2.4447e-03,  3.7406e-02],\n",
       "                        [-1.0370e-03,  1.3651e-02,  1.0983e-02, -9.3205e-03,  2.1927e-02],\n",
       "                        [-1.2825e-02, -1.8202e-02,  2.4075e-02,  6.0413e-03, -6.6537e-01],\n",
       "                        ...,\n",
       "                        [-2.7424e-02,  2.6733e-02, -2.0445e-03,  1.1738e-02, -1.7631e-02],\n",
       "                        [-1.1466e-02, -2.6167e-02, -4.0172e-02,  3.6581e-02, -6.7355e-04],\n",
       "                        [-1.8826e-02,  5.5944e-03, -1.1848e-02,  2.6436e-02,  8.4733e-03]],\n",
       "               \n",
       "                       [[ 4.0372e-04, -8.5038e-03,  1.9080e-02,  5.0730e-02, -4.2103e-01],\n",
       "                        [ 2.3110e-03,  1.0393e-02, -2.5480e-02,  2.6531e-02, -2.2277e-02],\n",
       "                        [-1.2162e-02,  1.5283e-02, -2.7623e-02, -1.9938e-02,  4.5544e-02],\n",
       "                        ...,\n",
       "                        [ 6.2277e-03, -9.6932e-03,  1.7107e-03,  1.2806e-02,  2.0676e-02],\n",
       "                        [-1.0947e-02,  5.8048e-03,  2.4420e-01, -1.3820e-01, -1.1802e-02],\n",
       "                        [ 1.3254e-02, -1.5536e-02,  1.7769e-02, -7.9185e-03,  2.8614e-02]],\n",
       "               \n",
       "                       [[ 5.1977e-04, -4.3590e-04,  1.9631e-02, -3.7621e-02, -3.2431e-02],\n",
       "                        [ 1.9699e-02, -2.5223e-03, -5.0889e-02,  3.1878e-04, -1.0127e-02],\n",
       "                        [-5.3559e-03, -9.3048e-03, -8.7229e-03,  1.7657e-02,  6.4036e-02],\n",
       "                        ...,\n",
       "                        [ 1.2891e-03, -9.4529e-03, -2.9003e-02,  2.9313e-02, -3.1649e-02],\n",
       "                        [ 1.8305e-02,  1.1444e-02, -2.8054e-02, -1.8297e-02,  6.4996e-03],\n",
       "                        [-6.3092e-04,  2.2137e-02,  9.3046e-03,  5.5098e-03, -2.0806e-02]]],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.prompt.prompt_dict.pad_right',\n",
       "               tensor([[[-0.0102, -0.0033,  0.0242,  0.0012,  0.0137],\n",
       "                        [ 0.0375, -0.0189, -0.0152,  0.0138,  0.0028],\n",
       "                        [ 0.0094,  0.0266,  0.0209, -0.0431,  0.0086],\n",
       "                        ...,\n",
       "                        [-0.0286,  0.0057,  0.0453, -0.0366,  0.0110],\n",
       "                        [ 0.0187,  0.0340, -0.0518,  0.0126,  0.0097],\n",
       "                        [-0.0131,  0.0272,  0.0271, -0.0147, -0.0003]],\n",
       "               \n",
       "                       [[-0.0030,  0.0268,  0.0125, -0.0176, -0.0027],\n",
       "                        [-0.0429,  0.0055,  0.0081, -0.0117,  0.0039],\n",
       "                        [-0.0005,  0.0036, -0.0565, -0.0033, -0.0143],\n",
       "                        ...,\n",
       "                        [ 0.1554,  0.0031, -0.0326,  0.0036, -0.0284],\n",
       "                        [-0.0429, -0.0213, -0.0139,  0.0097, -0.0202],\n",
       "                        [ 0.0281, -0.0287,  0.0181,  0.0012,  0.0014]],\n",
       "               \n",
       "                       [[ 0.0137, -0.0288, -0.0432, -0.0168,  0.0098],\n",
       "                        [-0.0108,  0.0298,  0.0174, -0.0139, -0.0048],\n",
       "                        [-0.0176, -0.0504,  0.0275,  0.1139, -0.0030],\n",
       "                        ...,\n",
       "                        [ 0.0140, -0.0286, -0.0294,  0.0144,  0.0218],\n",
       "                        [ 0.0082,  0.0462,  0.0206, -0.0112, -0.0067],\n",
       "                        [-0.0033, -0.0132, -0.0338,  0.0133,  0.0113]]], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.prompt_learning_rates_dict.prompt_weight_learning_rate',\n",
       "               tensor([-2.1618e-01, -2.2555e-01, -3.0681e-01, -1.9582e-01,  4.1277e-01,\n",
       "                       -4.0645e-41], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv0-conv-weight',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv0-conv-bias',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv1-conv-weight',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv1-conv-bias',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv2-conv-weight',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv2-conv-bias',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv3-conv-weight',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv3-conv-bias',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-weights',\n",
       "               tensor([ 6.1048e-02,  4.1442e-02,  2.3240e-02,  2.1922e-02,  3.5952e-02,\n",
       "                       -4.0645e-41], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-bias',\n",
       "               tensor([-1.3559e-01, -1.3342e-01, -1.2906e-01, -1.1178e-01, -3.8437e-02,\n",
       "                       -4.0645e-41], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv0-conv-weight',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv0-conv-bias',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv1-conv-weight',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv1-conv-bias',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv2-conv-weight',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv2-conv-bias',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv3-conv-weight',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv3-conv-bias',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-linear-weights',\n",
       "               tensor([ 2.3996e-01,  2.3322e-01,  2.6336e-01, -9.5786e-02, -1.2460e+00,\n",
       "                        1.5689e-41], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-linear-bias',\n",
       "               tensor([-2.4832e-01, -2.4546e-01, -2.4298e-01,  6.4554e-02,  2.5877e-01,\n",
       "                        1.5689e-41], device='cuda:0'))]),\n",
       " 'per_epoch_statistics': {'train_loss_mean': [1.9370149495601654,\n",
       "   1.4562808067798614,\n",
       "   1.410864313840866,\n",
       "   1.3743059430122375,\n",
       "   1.344195942401886,\n",
       "   1.3052989413738252,\n",
       "   1.2845991516113282,\n",
       "   1.237692002773285,\n",
       "   1.1877229887247085,\n",
       "   1.1561659895181655,\n",
       "   1.1012975356578827,\n",
       "   1.0837789936065674,\n",
       "   1.0691990346908569,\n",
       "   1.0247279789447785,\n",
       "   1.014633845925331,\n",
       "   1.0040559077262878,\n",
       "   0.9770179619789123,\n",
       "   0.9696701219081879,\n",
       "   0.9542256994247437,\n",
       "   0.9257538548707962,\n",
       "   0.9146626864671707,\n",
       "   0.8866176278591156,\n",
       "   0.8797420855164528,\n",
       "   0.8763417447805405,\n",
       "   0.859325116634369,\n",
       "   0.8471560004353523,\n",
       "   0.8337682507038117,\n",
       "   0.8180502401590347,\n",
       "   0.8146660035848617,\n",
       "   0.8119724651575089,\n",
       "   0.7933934043049813,\n",
       "   0.7890844205617905,\n",
       "   0.7798279433250427,\n",
       "   0.7641525635123253,\n",
       "   0.7729286996722221,\n",
       "   0.7385253469944,\n",
       "   0.7378166870474815,\n",
       "   0.7278152161240578,\n",
       "   0.7314715032577515,\n",
       "   0.7108614538311958,\n",
       "   0.719451138317585,\n",
       "   0.6979149134755135,\n",
       "   0.6861349119544029,\n",
       "   0.6908896798491478,\n",
       "   0.6827074735164642,\n",
       "   0.6728926202058793,\n",
       "   0.6688370190262795,\n",
       "   0.6590634137392044,\n",
       "   0.6533143911361694,\n",
       "   0.6513242200016975,\n",
       "   0.6511516085863114,\n",
       "   0.630743555366993,\n",
       "   0.6441450489759445,\n",
       "   0.62203568649292,\n",
       "   0.6374970595240593,\n",
       "   0.6286672307848931,\n",
       "   0.6175676800608635,\n",
       "   0.6097597596049309,\n",
       "   0.6148039421439171,\n",
       "   0.6090317421257496,\n",
       "   0.6117341914772987,\n",
       "   0.5983809799551963,\n",
       "   0.5875830021500588,\n",
       "   0.5805650525093079,\n",
       "   0.5841294635534287,\n",
       "   0.5847079308629036,\n",
       "   0.5851712099909783,\n",
       "   0.5692470143437386,\n",
       "   0.5666860084831714,\n",
       "   0.5719792779684066,\n",
       "   0.5631913161873817,\n",
       "   0.5581584931612015,\n",
       "   0.5607255641222,\n",
       "   0.5495392882823944,\n",
       "   0.5538352510333061,\n",
       "   0.543788764834404,\n",
       "   0.5488181646764279,\n",
       "   0.5477107594013214,\n",
       "   0.5332815824747086,\n",
       "   0.540161500275135,\n",
       "   0.5245549229979515,\n",
       "   0.5347560205161571,\n",
       "   0.5317814947068691,\n",
       "   0.532112007856369,\n",
       "   0.5135249897241593,\n",
       "   0.5212136028707027,\n",
       "   0.5148063887655735,\n",
       "   0.5132603907287121,\n",
       "   0.5054496144652366,\n",
       "   0.49985805797576904,\n",
       "   0.5062470922470093,\n",
       "   0.5018487135469913,\n",
       "   0.4931863552927971,\n",
       "   0.49650685134530065,\n",
       "   0.4983615283071995,\n",
       "   0.5038585858643055,\n",
       "   0.4990498250722885,\n",
       "   0.48917882338166235,\n",
       "   0.4715465940833092],\n",
       "  'train_loss_std': [0.8780060620304255,\n",
       "   0.12178356899624002,\n",
       "   0.11333077396076678,\n",
       "   0.11250911279158553,\n",
       "   0.11641935587385736,\n",
       "   0.1165878065223723,\n",
       "   0.12441642347550723,\n",
       "   0.12540856872097084,\n",
       "   0.1355092878954731,\n",
       "   0.1329207545701686,\n",
       "   0.13679485569823846,\n",
       "   0.1450813079747299,\n",
       "   0.13459937090003746,\n",
       "   0.13983787660987212,\n",
       "   0.1361935786884873,\n",
       "   0.13872038078474996,\n",
       "   0.1344777543731905,\n",
       "   0.14592659220100113,\n",
       "   0.14137712228073462,\n",
       "   0.14274168218803568,\n",
       "   0.14651787847614822,\n",
       "   0.13582481486207115,\n",
       "   0.1503385331011679,\n",
       "   0.1523096753610447,\n",
       "   0.14276897590535986,\n",
       "   0.14448572984178126,\n",
       "   0.14605469009264196,\n",
       "   0.15028648325323954,\n",
       "   0.1426656413562224,\n",
       "   0.1504212197828827,\n",
       "   0.14171090147633086,\n",
       "   0.14460184261425818,\n",
       "   0.14669364726220502,\n",
       "   0.15571747961554996,\n",
       "   0.14286355565626221,\n",
       "   0.14514211067452865,\n",
       "   0.13933756421205934,\n",
       "   0.14833430450137872,\n",
       "   0.14840972139718014,\n",
       "   0.14752841238733255,\n",
       "   0.1392355763329009,\n",
       "   0.1368073282452741,\n",
       "   0.14356106667957005,\n",
       "   0.14424352875083368,\n",
       "   0.1401137724505023,\n",
       "   0.1349189564491912,\n",
       "   0.14034771073239571,\n",
       "   0.13704812710589542,\n",
       "   0.13931282781947094,\n",
       "   0.13892720212728177,\n",
       "   0.1360392388737123,\n",
       "   0.14061890799089227,\n",
       "   0.14078977910027876,\n",
       "   0.14406308755416936,\n",
       "   0.14764550670852708,\n",
       "   0.1430599648124606,\n",
       "   0.13860850878761716,\n",
       "   0.1361939247930713,\n",
       "   0.13950557321769674,\n",
       "   0.13384769177968942,\n",
       "   0.14195936297207098,\n",
       "   0.14633253104339106,\n",
       "   0.13968199876217396,\n",
       "   0.13781394401239672,\n",
       "   0.13270150258360836,\n",
       "   0.13383110697378425,\n",
       "   0.1387971015560401,\n",
       "   0.13312007021941208,\n",
       "   0.13757955650195908,\n",
       "   0.1446196442114704,\n",
       "   0.12877018963389172,\n",
       "   0.1379027957014948,\n",
       "   0.14037567915129043,\n",
       "   0.12981150676111056,\n",
       "   0.14066629974975414,\n",
       "   0.1266976993947375,\n",
       "   0.1391543037222707,\n",
       "   0.13168999816627955,\n",
       "   0.13196119220838395,\n",
       "   0.14069595163973123,\n",
       "   0.13321630704928528,\n",
       "   0.1311955608931853,\n",
       "   0.139160460260813,\n",
       "   0.13792265630612688,\n",
       "   0.13124604959018726,\n",
       "   0.12620626649428557,\n",
       "   0.13350290966963582,\n",
       "   0.13963766964436286,\n",
       "   0.13162172147785609,\n",
       "   0.12765874993700146,\n",
       "   0.12990592824639133,\n",
       "   0.13257935557763317,\n",
       "   0.12729143588901343,\n",
       "   0.1288351323264692,\n",
       "   0.1321005638871507,\n",
       "   0.13844268760875172,\n",
       "   0.12232923749830915,\n",
       "   0.12938322464992955,\n",
       "   0.12568361722899224],\n",
       "  'train_accuracy_mean': [0.33394666683673857,\n",
       "   0.3845600002408028,\n",
       "   0.4045866668522358,\n",
       "   0.4217466669380665,\n",
       "   0.4381200007200241,\n",
       "   0.46022666746377944,\n",
       "   0.4728799996972084,\n",
       "   0.49351999938488006,\n",
       "   0.5206933333873749,\n",
       "   0.5356799986958504,\n",
       "   0.5609866656661033,\n",
       "   0.569626665353775,\n",
       "   0.575173332810402,\n",
       "   0.5992133331298828,\n",
       "   0.6008133325576782,\n",
       "   0.605173333466053,\n",
       "   0.6159733330607414,\n",
       "   0.6242533326745033,\n",
       "   0.6281066660284996,\n",
       "   0.6389333326220512,\n",
       "   0.643039999961853,\n",
       "   0.6552533310055733,\n",
       "   0.6583199988007545,\n",
       "   0.6585866673588753,\n",
       "   0.6665999991893768,\n",
       "   0.6714666666388511,\n",
       "   0.6783066657185555,\n",
       "   0.683386667072773,\n",
       "   0.6855466656684875,\n",
       "   0.68465333288908,\n",
       "   0.6932133331894874,\n",
       "   0.6965999989509583,\n",
       "   0.7012266672849655,\n",
       "   0.7052533329725266,\n",
       "   0.7031999984383583,\n",
       "   0.7177066661715508,\n",
       "   0.7156933317780495,\n",
       "   0.7213866657614708,\n",
       "   0.7205199998021126,\n",
       "   0.7273333333134652,\n",
       "   0.7260133337974548,\n",
       "   0.7331333328485489,\n",
       "   0.7394533323049546,\n",
       "   0.7344400000572204,\n",
       "   0.7391066673994064,\n",
       "   0.7447333340644836,\n",
       "   0.7450533343553543,\n",
       "   0.7503466671109199,\n",
       "   0.7520800005197525,\n",
       "   0.7528399994373322,\n",
       "   0.7523599982261657,\n",
       "   0.7612000002861022,\n",
       "   0.7547199990749359,\n",
       "   0.7636000007390976,\n",
       "   0.75589333319664,\n",
       "   0.7612933323383332,\n",
       "   0.766773332118988,\n",
       "   0.7682533334493638,\n",
       "   0.7656266666650772,\n",
       "   0.7689733332395554,\n",
       "   0.7687466663122177,\n",
       "   0.7736933335065842,\n",
       "   0.7774666668176651,\n",
       "   0.7809200004339218,\n",
       "   0.7799733324050904,\n",
       "   0.7777066677808762,\n",
       "   0.7771333338022232,\n",
       "   0.7848800010681153,\n",
       "   0.7851599998474121,\n",
       "   0.784133332490921,\n",
       "   0.7873066650629044,\n",
       "   0.7865999999046326,\n",
       "   0.7891999995708465,\n",
       "   0.7906533324718475,\n",
       "   0.7920666669607163,\n",
       "   0.7948133336305618,\n",
       "   0.7934933327436448,\n",
       "   0.7936266661882401,\n",
       "   0.7980933319330216,\n",
       "   0.7971066660881042,\n",
       "   0.8031466666460038,\n",
       "   0.7983599997758866,\n",
       "   0.7982399997711181,\n",
       "   0.7994666666984558,\n",
       "   0.8073199987411499,\n",
       "   0.8048799993991852,\n",
       "   0.8063999997377396,\n",
       "   0.8062799994945526,\n",
       "   0.8104133316278458,\n",
       "   0.8118799993991852,\n",
       "   0.8089333339929581,\n",
       "   0.8113333334922791,\n",
       "   0.8157733323574066,\n",
       "   0.8123200013637543,\n",
       "   0.8110000001192093,\n",
       "   0.8088933334350586,\n",
       "   0.8118800015449524,\n",
       "   0.8166133327484131,\n",
       "   0.824146666765213],\n",
       "  'train_accuracy_std': [0.06483878665328059,\n",
       "   0.06257160970031163,\n",
       "   0.06648998460192156,\n",
       "   0.0660226752584905,\n",
       "   0.06915955052312707,\n",
       "   0.06635823407289175,\n",
       "   0.07234834686588516,\n",
       "   0.0683855778863214,\n",
       "   0.0734136478079714,\n",
       "   0.07213447264308334,\n",
       "   0.07121862887731974,\n",
       "   0.0736896996523401,\n",
       "   0.0691072679294036,\n",
       "   0.07223174868034463,\n",
       "   0.06900792733826559,\n",
       "   0.06874035639047522,\n",
       "   0.06653559871332118,\n",
       "   0.0707844794871344,\n",
       "   0.06872468384994626,\n",
       "   0.06909523243986078,\n",
       "   0.06904300362425907,\n",
       "   0.06399064538944457,\n",
       "   0.07248984581359773,\n",
       "   0.0733778840784167,\n",
       "   0.06871580771707533,\n",
       "   0.06938254805456767,\n",
       "   0.06858230632989058,\n",
       "   0.0712909349334478,\n",
       "   0.06584688603245163,\n",
       "   0.07025629262334736,\n",
       "   0.06741898040536222,\n",
       "   0.06831362421530052,\n",
       "   0.06807125364246877,\n",
       "   0.07266087509888447,\n",
       "   0.06516904562273997,\n",
       "   0.06830443715476293,\n",
       "   0.06549934068480172,\n",
       "   0.0696035551747974,\n",
       "   0.06683708622336783,\n",
       "   0.0692403212704513,\n",
       "   0.0634043088059534,\n",
       "   0.06309537681501684,\n",
       "   0.06737994169717902,\n",
       "   0.0645884216935051,\n",
       "   0.06457520793334985,\n",
       "   0.061844931742764384,\n",
       "   0.0645171584346676,\n",
       "   0.06284347339979582,\n",
       "   0.06250783130769827,\n",
       "   0.06158806556350368,\n",
       "   0.0631177682829716,\n",
       "   0.06275440612677265,\n",
       "   0.0637762528803471,\n",
       "   0.06486786415869349,\n",
       "   0.06641035536653862,\n",
       "   0.06626021475599567,\n",
       "   0.06193320657444158,\n",
       "   0.06273377967219496,\n",
       "   0.06383996187074861,\n",
       "   0.05874475269549742,\n",
       "   0.0609694481441821,\n",
       "   0.06420767699227727,\n",
       "   0.06138389306458649,\n",
       "   0.0621535578892584,\n",
       "   0.05842829641963356,\n",
       "   0.05983149223648936,\n",
       "   0.06268974728202582,\n",
       "   0.060648413008333686,\n",
       "   0.06284051312622121,\n",
       "   0.06230805214789808,\n",
       "   0.058625283041909365,\n",
       "   0.06375575075631035,\n",
       "   0.06119099374728899,\n",
       "   0.058694083175211284,\n",
       "   0.062482672638784216,\n",
       "   0.05653896059272798,\n",
       "   0.06112261792180441,\n",
       "   0.05823708787953822,\n",
       "   0.058129435223150186,\n",
       "   0.06326580395278254,\n",
       "   0.05782933313018517,\n",
       "   0.05800746487118594,\n",
       "   0.06351283519536154,\n",
       "   0.06088590477571845,\n",
       "   0.057307125312169895,\n",
       "   0.0551362666400024,\n",
       "   0.05985404557333469,\n",
       "   0.06155273753126278,\n",
       "   0.05858409267252704,\n",
       "   0.057465922960210636,\n",
       "   0.057366038178719,\n",
       "   0.057343411501131224,\n",
       "   0.05514427463117606,\n",
       "   0.05766450789194745,\n",
       "   0.05702221874658377,\n",
       "   0.06009268612318059,\n",
       "   0.053306024151922954,\n",
       "   0.05745179143235778,\n",
       "   0.0534129067409578],\n",
       "  'train_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'train_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_learning_rate_mean': [0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003,\n",
       "   0.00010000000000000003],\n",
       "  'train_learning_rate_std': [2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20,\n",
       "   2.710505431213761e-20],\n",
       "  'val_loss_mean': [1.5368456550439198,\n",
       "   1.4702723077932993,\n",
       "   1.4415518633524578,\n",
       "   1.4204287079970042,\n",
       "   1.4002978070576986,\n",
       "   1.4316806451479593,\n",
       "   1.3534727235635122,\n",
       "   1.3294741018613179,\n",
       "   1.2917014237244924,\n",
       "   1.2493535659710566,\n",
       "   1.2091143866380056,\n",
       "   1.1926137262582779,\n",
       "   1.1829772444566091,\n",
       "   1.1677393954992294,\n",
       "   1.1555629557371139,\n",
       "   1.1446360735098522,\n",
       "   1.1304019169012705,\n",
       "   1.1954529307285944,\n",
       "   1.127083267569542,\n",
       "   1.0879301341374714,\n",
       "   1.080846542318662,\n",
       "   1.047259936730067,\n",
       "   1.0422012796004614,\n",
       "   1.033605043888092,\n",
       "   1.0226188166936239,\n",
       "   1.0301212831338247,\n",
       "   1.0158571477731069,\n",
       "   1.0136775342623392,\n",
       "   0.9985103652874628,\n",
       "   0.9794186850388845,\n",
       "   0.9758490840593974,\n",
       "   0.9789720714092255,\n",
       "   0.9819259488582611,\n",
       "   0.9676692742109299,\n",
       "   0.9626403464873632,\n",
       "   0.9586723842223486,\n",
       "   0.9552449438969294,\n",
       "   0.9716779534022013,\n",
       "   0.9343377159039179,\n",
       "   0.9269266698757808,\n",
       "   0.9363478877147039,\n",
       "   0.9332611954212189,\n",
       "   0.9199896889925003,\n",
       "   0.9153678804636002,\n",
       "   0.9087653231620788,\n",
       "   0.9281620595852534,\n",
       "   0.88619093755881,\n",
       "   0.903720966776212,\n",
       "   0.8937573234240214,\n",
       "   0.8950553548336029,\n",
       "   0.9076319781939188,\n",
       "   0.893612075249354,\n",
       "   0.8802302159865697,\n",
       "   0.8736940219004949,\n",
       "   0.9193269463380178,\n",
       "   0.9031357606252034,\n",
       "   0.8638760042190552,\n",
       "   0.8825533151626587,\n",
       "   0.8880187849203746,\n",
       "   0.8820393345753352,\n",
       "   0.880986710190773,\n",
       "   0.8755776258309682,\n",
       "   0.8789567307631174,\n",
       "   0.8721649748086929,\n",
       "   0.8684706356128057,\n",
       "   0.864197650551796,\n",
       "   0.8583401042222977,\n",
       "   0.8646856673558553,\n",
       "   0.8557937149206797,\n",
       "   0.8557512787977855,\n",
       "   0.8589330397049586,\n",
       "   0.8450110048055649,\n",
       "   0.8572435222069422,\n",
       "   0.852886148293813,\n",
       "   0.8405706825852394,\n",
       "   0.8502134752273559,\n",
       "   0.8546396162112554,\n",
       "   0.8433248579502106,\n",
       "   0.8551207512617112,\n",
       "   0.8343261323372523,\n",
       "   0.8264496464530627,\n",
       "   0.8396005328496298,\n",
       "   0.8409292451540629,\n",
       "   0.8248848433295886,\n",
       "   0.8252476033568382,\n",
       "   0.8495834125081698,\n",
       "   0.8164827669660251,\n",
       "   0.8304408568143845,\n",
       "   0.8277426213026047,\n",
       "   0.8342598272363345,\n",
       "   0.8509853300452233,\n",
       "   0.8393105252583821,\n",
       "   0.8149571157495181,\n",
       "   0.8490180789430937,\n",
       "   0.8236231603225073,\n",
       "   0.8126738064487775,\n",
       "   0.8054629999399185,\n",
       "   0.8194465995828311,\n",
       "   0.8387737858295441],\n",
       "  'val_loss_std': [0.09542095157957689,\n",
       "   0.08982845240045265,\n",
       "   0.08913036977822204,\n",
       "   0.08837968101217157,\n",
       "   0.08991913916358256,\n",
       "   0.115297636404548,\n",
       "   0.09285499293771322,\n",
       "   0.10384810346658743,\n",
       "   0.10947522398986642,\n",
       "   0.11207680009611916,\n",
       "   0.11725260731697802,\n",
       "   0.11711106805337906,\n",
       "   0.11990076789661981,\n",
       "   0.12237772642977392,\n",
       "   0.12263044069218382,\n",
       "   0.12048078435177051,\n",
       "   0.1228404736713987,\n",
       "   0.13109297846387682,\n",
       "   0.12527358279233908,\n",
       "   0.1244592909305544,\n",
       "   0.12131092440169408,\n",
       "   0.12445997496753668,\n",
       "   0.12479057510139505,\n",
       "   0.12640380463993306,\n",
       "   0.1274012798153527,\n",
       "   0.12948633008643437,\n",
       "   0.12831599816143865,\n",
       "   0.12583010290664046,\n",
       "   0.12672687869578678,\n",
       "   0.12702949358638693,\n",
       "   0.12736889754641756,\n",
       "   0.12860353241156725,\n",
       "   0.13627145237992902,\n",
       "   0.12907638413367392,\n",
       "   0.13156355924485955,\n",
       "   0.13156835445043147,\n",
       "   0.13123289191627285,\n",
       "   0.1346376743273137,\n",
       "   0.13089473419219153,\n",
       "   0.13033172572037607,\n",
       "   0.13285216303599356,\n",
       "   0.1290184572248652,\n",
       "   0.13611974562570578,\n",
       "   0.13340503584078994,\n",
       "   0.1259930457958077,\n",
       "   0.12889846635673285,\n",
       "   0.12860527304034083,\n",
       "   0.13065704848080573,\n",
       "   0.13645332572839688,\n",
       "   0.13586139123915167,\n",
       "   0.1351612048600966,\n",
       "   0.13907798956056808,\n",
       "   0.13357246874708611,\n",
       "   0.1408860138089634,\n",
       "   0.1374330193137035,\n",
       "   0.13939288682679427,\n",
       "   0.13373354670654522,\n",
       "   0.13868330538125423,\n",
       "   0.13768479913454307,\n",
       "   0.14061070892455868,\n",
       "   0.14064343508897284,\n",
       "   0.1361045387373358,\n",
       "   0.13517256336146646,\n",
       "   0.14461808206662147,\n",
       "   0.13495102495961356,\n",
       "   0.13664053446290977,\n",
       "   0.14214355484555105,\n",
       "   0.1389562354520368,\n",
       "   0.13228705093022747,\n",
       "   0.13980917504259263,\n",
       "   0.1442008559422596,\n",
       "   0.13411566210072073,\n",
       "   0.1405421213798975,\n",
       "   0.14284621082447138,\n",
       "   0.13964016001199803,\n",
       "   0.13290183003068054,\n",
       "   0.1378537383774227,\n",
       "   0.13588655884209164,\n",
       "   0.14026224729018502,\n",
       "   0.13667804618436968,\n",
       "   0.13665689044475066,\n",
       "   0.1386133380141849,\n",
       "   0.13290325609647524,\n",
       "   0.13766434311159095,\n",
       "   0.13678114064900893,\n",
       "   0.1367525766630329,\n",
       "   0.13529707009979833,\n",
       "   0.14089022270414778,\n",
       "   0.13331665426568606,\n",
       "   0.13782134412465302,\n",
       "   0.14142829599652615,\n",
       "   0.1369624165050036,\n",
       "   0.13980257543862656,\n",
       "   0.14676952841846033,\n",
       "   0.13746536395906506,\n",
       "   0.1432386555390256,\n",
       "   0.13774686381883494,\n",
       "   0.13445815789112156,\n",
       "   0.1355474697865724],\n",
       "  'val_accuracy_mean': [0.35768888955314954,\n",
       "   0.37802222232023874,\n",
       "   0.3885777784883976,\n",
       "   0.40064444502194724,\n",
       "   0.41117777725060783,\n",
       "   0.4079333347082138,\n",
       "   0.4380666675170263,\n",
       "   0.45228888938824335,\n",
       "   0.4704000003139178,\n",
       "   0.49120000034570693,\n",
       "   0.5106666661302248,\n",
       "   0.5181333343187968,\n",
       "   0.5234888877471288,\n",
       "   0.528888888657093,\n",
       "   0.5396444436907768,\n",
       "   0.5411333334445954,\n",
       "   0.5471777777870496,\n",
       "   0.5271111104885737,\n",
       "   0.5511333326498667,\n",
       "   0.5628888869285583,\n",
       "   0.5679555547237396,\n",
       "   0.584111111064752,\n",
       "   0.5847111102938652,\n",
       "   0.5904888874292373,\n",
       "   0.5925111132860184,\n",
       "   0.5873777774969736,\n",
       "   0.59642222036918,\n",
       "   0.597222221593062,\n",
       "   0.6042666666706403,\n",
       "   0.6115777762730916,\n",
       "   0.6114888880650202,\n",
       "   0.6111555554469427,\n",
       "   0.6137111100554467,\n",
       "   0.6163555547595024,\n",
       "   0.6209555557370185,\n",
       "   0.6213333324591319,\n",
       "   0.6224222213029862,\n",
       "   0.6169999996821086,\n",
       "   0.6337777774532636,\n",
       "   0.6347777752081553,\n",
       "   0.6319999994834264,\n",
       "   0.6344666659832001,\n",
       "   0.6389333328604698,\n",
       "   0.639866665105025,\n",
       "   0.6450444423158963,\n",
       "   0.6369111097852389,\n",
       "   0.6527333332101504,\n",
       "   0.6472222224871318,\n",
       "   0.6521555563807487,\n",
       "   0.6523333323995272,\n",
       "   0.6459111102422078,\n",
       "   0.653466666440169,\n",
       "   0.6580666668216387,\n",
       "   0.660600000123183,\n",
       "   0.6429555547237397,\n",
       "   0.6499999997019767,\n",
       "   0.6651999998092651,\n",
       "   0.6567999982833862,\n",
       "   0.6569555559754372,\n",
       "   0.6585555550456047,\n",
       "   0.6589777780572573,\n",
       "   0.6594666674733162,\n",
       "   0.6585555547475814,\n",
       "   0.6615333320697149,\n",
       "   0.6625999993085862,\n",
       "   0.6670444428920745,\n",
       "   0.6665333343545595,\n",
       "   0.6656666638453802,\n",
       "   0.6669111132621766,\n",
       "   0.6699999994039536,\n",
       "   0.665755555331707,\n",
       "   0.6721555543939273,\n",
       "   0.6673111112912496,\n",
       "   0.6718888882795969,\n",
       "   0.6757111127177874,\n",
       "   0.6699555536111196,\n",
       "   0.6706000011165937,\n",
       "   0.674977777103583,\n",
       "   0.670111110607783,\n",
       "   0.6772444438934326,\n",
       "   0.6793999985853831,\n",
       "   0.6763333321611087,\n",
       "   0.6776666676004728,\n",
       "   0.6825999998052915,\n",
       "   0.6807555537422498,\n",
       "   0.6737777772545814,\n",
       "   0.6839777787526449,\n",
       "   0.6817111108700434,\n",
       "   0.678355555832386,\n",
       "   0.6777333329121272,\n",
       "   0.6737777771552403,\n",
       "   0.6757777787248294,\n",
       "   0.6875111112991968,\n",
       "   0.6722222216924032,\n",
       "   0.682311110496521,\n",
       "   0.6867555565635364,\n",
       "   0.6918444450696309,\n",
       "   0.6844222223758698,\n",
       "   0.680000000099341],\n",
       "  'val_accuracy_std': [0.04920395457315319,\n",
       "   0.050509399981918855,\n",
       "   0.05097181052538848,\n",
       "   0.05231556258803785,\n",
       "   0.05317982952370486,\n",
       "   0.05741618156199636,\n",
       "   0.053943412967661114,\n",
       "   0.05556395900968195,\n",
       "   0.059031592758959306,\n",
       "   0.05930621090964441,\n",
       "   0.061480498323577604,\n",
       "   0.059039824143799435,\n",
       "   0.060502630023903284,\n",
       "   0.059874765826104785,\n",
       "   0.05922105080859362,\n",
       "   0.06108928461985683,\n",
       "   0.06136863177843445,\n",
       "   0.06130997929118081,\n",
       "   0.061198317657581575,\n",
       "   0.05913408026286276,\n",
       "   0.06035056333688942,\n",
       "   0.059537621276805286,\n",
       "   0.06125137911626128,\n",
       "   0.059771648626560654,\n",
       "   0.060692835457043244,\n",
       "   0.058700226565443096,\n",
       "   0.06258931082818854,\n",
       "   0.059457942431681156,\n",
       "   0.058074180154107605,\n",
       "   0.059970606677168053,\n",
       "   0.06057506566202673,\n",
       "   0.06039866039209543,\n",
       "   0.06198928955621504,\n",
       "   0.05955681686953355,\n",
       "   0.059822995555364855,\n",
       "   0.06179056845995668,\n",
       "   0.058731527526247,\n",
       "   0.0607755451370106,\n",
       "   0.059262963385626914,\n",
       "   0.0632021072001425,\n",
       "   0.057284219324059665,\n",
       "   0.06007936780017379,\n",
       "   0.060071958463554696,\n",
       "   0.06270670125793772,\n",
       "   0.0586429720850279,\n",
       "   0.059225367199205264,\n",
       "   0.061043481433844485,\n",
       "   0.05860971679224481,\n",
       "   0.061146620230372345,\n",
       "   0.06169489123811299,\n",
       "   0.061120581651300285,\n",
       "   0.06285065222063713,\n",
       "   0.060674933528642006,\n",
       "   0.060900127124393716,\n",
       "   0.061509456076077045,\n",
       "   0.061058564003532975,\n",
       "   0.06115127798201812,\n",
       "   0.060642096682565275,\n",
       "   0.062237819070575955,\n",
       "   0.06222648970306112,\n",
       "   0.06129823488239942,\n",
       "   0.06041103556386658,\n",
       "   0.06097591173466968,\n",
       "   0.06339895034985851,\n",
       "   0.05908608998839336,\n",
       "   0.05919464094996541,\n",
       "   0.06159591276591764,\n",
       "   0.06181483789471598,\n",
       "   0.05919784337080023,\n",
       "   0.06316703314061421,\n",
       "   0.061962229952148846,\n",
       "   0.060292664017824196,\n",
       "   0.06301816189577296,\n",
       "   0.0597685861676905,\n",
       "   0.06313399961252601,\n",
       "   0.05982565582186264,\n",
       "   0.06095604875825964,\n",
       "   0.05832951981500274,\n",
       "   0.0610390494813996,\n",
       "   0.06025962048438169,\n",
       "   0.061099274906457064,\n",
       "   0.0627826949718575,\n",
       "   0.060563097867520184,\n",
       "   0.05921882799293945,\n",
       "   0.05970067002181543,\n",
       "   0.060121689802725246,\n",
       "   0.06162034190498603,\n",
       "   0.061518123646211655,\n",
       "   0.05791404601899366,\n",
       "   0.061238235805224424,\n",
       "   0.061138176915379344,\n",
       "   0.06264499338616594,\n",
       "   0.06040748882425267,\n",
       "   0.061859658678483434,\n",
       "   0.059299550291131396,\n",
       "   0.059568756137207066,\n",
       "   0.05942195324695245,\n",
       "   0.05873606550165882,\n",
       "   0.06051874663811496],\n",
       "  'val_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'val_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0]}}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델을 구성한다\n",
    "model = MAMLFewShotClassifier(args=args, device=device,\n",
    "                              im_shape=(2, 3,\n",
    "                                        args.image_height, args.image_width))\n",
    "\n",
    "data = MetaLearningSystemDataLoader\n",
    "\n",
    "maml_system = ExperimentBuilder(model=model, data=data, args=args, device=device)\n",
    "maml_system.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3803d5ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MAMLFewShotClassifier(\n",
       "  (classifier): VGGReLUNormNetwork(\n",
       "    (layer_dict): ModuleDict(\n",
       "      (conv0): MetaConvNormLayerReLU(\n",
       "        (layer_dict): ModuleDict()\n",
       "        (conv): MetaConv2dLayer()\n",
       "        (norm_layer): MetaBatchNormLayer(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): MetaConvNormLayerReLU(\n",
       "        (layer_dict): ModuleDict()\n",
       "        (conv): MetaConv2dLayer()\n",
       "        (norm_layer): MetaBatchNormLayer(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): MetaConvNormLayerReLU(\n",
       "        (layer_dict): ModuleDict()\n",
       "        (conv): MetaConv2dLayer()\n",
       "        (norm_layer): MetaBatchNormLayer(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv3): MetaConvNormLayerReLU(\n",
       "        (layer_dict): ModuleDict()\n",
       "        (conv): MetaConv2dLayer()\n",
       "        (norm_layer): MetaBatchNormLayer(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (linear): MetaLinearLayer()\n",
       "    )\n",
       "    (prompt): PadPrompter(\n",
       "      (prompt_dict): ParameterDict(\n",
       "          (pad_up): Parameter containing: [torch.cuda.FloatTensor of size 3x5x84 (GPU 0)]\n",
       "          (pad_down): Parameter containing: [torch.cuda.FloatTensor of size 3x5x84 (GPU 0)]\n",
       "          (pad_left): Parameter containing: [torch.cuda.FloatTensor of size 3x74x5 (GPU 0)]\n",
       "          (pad_right): Parameter containing: [torch.cuda.FloatTensor of size 3x74x5 (GPU 0)]\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (inner_loop_optimizer): LSLRGradientDescentLearningRule(\n",
       "    (prompt_learning_rates_dict): ParameterDict(  (prompt_weight_learning_rate): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)])\n",
       "    (names_learning_rates_dict): ParameterDict(\n",
       "        (layer_dict-conv0-conv-weight): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv0-conv-bias): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv1-conv-weight): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv1-conv-bias): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv2-conv-weight): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv2-conv-bias): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv3-conv-weight): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv3-conv-bias): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-linear-weights): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-linear-bias): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "    )\n",
       "    (names_weight_decay_dict): ParameterDict(\n",
       "        (layer_dict-conv0-conv-weight): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv0-conv-bias): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv1-conv-weight): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv1-conv-bias): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv2-conv-weight): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv2-conv-bias): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv3-conv-weight): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv3-conv-bias): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-linear-weights): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-linear-bias): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_dir = maml_system.saved_models_filepath\n",
    "model_name = \"train_model\"\n",
    "model_idx = maml_system.state['best_epoch']\n",
    "\n",
    "state = maml_system.model.load_model(model_save_dir=model_save_dir,\n",
    "                                     model_name=model_name,\n",
    "                                     model_idx=model_idx+1)\n",
    "\n",
    "state_dict_loaded = state['network']\n",
    "\n",
    "maml_system.model.load_state_dict(state_dict=state_dict_loaded)\n",
    "\n",
    "# # 잘 불러왔는지 확인하는 코드\n",
    "# print(\"state_dict_loaded == \",state_dict_loaded)\n",
    "# print(\"=\"*10)\n",
    "# for key, value in maml_system.model.named_parameters():\n",
    "#     print(key)\n",
    "#     print(value)\n",
    "maml_system.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "db4855d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = maml_system.data.get_train_batches(total_batches=int(600/2), augment_images=False)\n",
    "\n",
    "figure_idx = 0\n",
    "\n",
    "for sample_idx, train_sample in enumerate(train_data):\n",
    "    \n",
    "    x_support_set, x_target_set, y_support_set, y_target_set, seed = train_sample\n",
    "    \n",
    "    x_support_set = torch.Tensor(x_support_set).float().to(device=maml_system.model.device)\n",
    "    x_target_set = torch.Tensor(x_target_set).float().to(device=maml_system.model.device)\n",
    "    y_support_set = torch.Tensor(y_support_set).long().to(device=maml_system.model.device)\n",
    "    y_target_set = torch.Tensor(y_target_set).long().to(device=maml_system.model.device)\n",
    "    \n",
    "    for task_id, (x_support_set_task, y_support_set_task, x_target_set_task, y_target_set_task) in enumerate(zip(x_support_set,\n",
    "                              y_support_set,\n",
    "                              x_target_set,\n",
    "                              y_target_set)):\n",
    "        \n",
    "        names_weights_copy = maml_system.model.get_inner_loop_parameter_dict(maml_system.model.classifier.named_parameters())\n",
    "        \n",
    "        num_devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "\n",
    "        names_weights_copy = {\n",
    "            name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
    "                [num_devices] + [1 for i in range(len(value.shape))]) for\n",
    "            name, value in names_weights_copy.items()}\n",
    "        \n",
    "        prompted_weights_copy = {}\n",
    "        if args.prompter:\n",
    "            prompted_weights_copy = {key: value for key, value in names_weights_copy.items() if 'prompt' in key}\n",
    "\n",
    "        names_weights_copy = {key: value for key, value in names_weights_copy.items() if 'layer_dict' in key}\n",
    "        \n",
    "        n, s, c, h, w = x_target_set_task.shape\n",
    "\n",
    "        x_support_set_task = x_support_set_task.view(-1, c, h, w)\n",
    "        y_support_set_task = y_support_set_task.view(-1)\n",
    "        x_target_set_task = x_target_set_task.view(-1, c, h, w)\n",
    "        y_target_set_task = y_target_set_task.view(-1)\n",
    "        \n",
    "        num_steps=5\n",
    "        \n",
    "        for num_step in range(num_steps):\n",
    "            \n",
    "            # show_batch(images=x_support_set_task, labels=y_support_set_task, datasets=datasets_name)\n",
    "            \n",
    "            support_loss, support_preds = maml_system.model.net_forward(\n",
    "                                                               x=x_support_set_task,\n",
    "                                                               y=y_support_set_task,\n",
    "                                                               weights=names_weights_copy,\n",
    "                                                               prompted_weights=prompted_weights_copy,\n",
    "                                                               backup_running_statistics=num_step == 0,\n",
    "                                                               prepend_prompt=args.prompter,\n",
    "                                                               training=True,\n",
    "                                                               num_step=num_step,\n",
    "                                                               training_phase=False,\n",
    "                                                               epoch=0,\n",
    "                                                               inner_loop=True)\n",
    "        \n",
    "            names_weights_copy, prompted_weights_copy = maml_system.model.apply_inner_loop_update(\n",
    "                 loss=support_loss,\n",
    "                 names_weights_copy=names_weights_copy,\n",
    "                 prompted_weights_copy=prompted_weights_copy,\n",
    "                 use_second_order=True,\n",
    "                 current_step_idx=num_step,\n",
    "                 current_iter='test',\n",
    "                 training_phase=False)\n",
    "            \n",
    "        ##########Inner level 종료 #############\n",
    "        # Query set으로 Grad-CAM을 구한다\n",
    "        individual_images = torch.split(x_target_set_task, 1, dim=0)  # (25, 3, 84, 84) -> (1, 3, 84, 84) 텐서로 분리\n",
    "        individual_labels = torch.split(y_target_set_task, 1, dim=0)\n",
    "        \n",
    "        \n",
    "        original_save_path = \"Grad_CAM/\" + datasets_name + \"/Ours_with_reg/origin/\" + \"sample_idx_\" + str(sample_idx) + \"/\" + \"task_id_\" + str(task_id) + \"/\"\n",
    "        grad_cam_save_path = \"Grad_CAM/\" + datasets_name + \"/Ours_with_reg/gradcam/\"+ \"sample_idx_\" + str(sample_idx) + \"/\" + \"task_id_\" + str(task_id) + \"/\"\n",
    "        \n",
    "        make_folder(original_save_path)\n",
    "        make_folder(grad_cam_save_path)\n",
    "        \n",
    "        # 각 이미지 텐서 확인\n",
    "        for i in range(y_target_set_task.shape[0]):\n",
    "\n",
    "            input_image = individual_images[i]\n",
    "            y_label = individual_labels[i]\n",
    "\n",
    "            _, target_preds, feature_map_list = maml_system.model.net_forward_feature_extractor(x=input_image,\n",
    "                                                             y=y_label, weights=names_weights_copy,\n",
    "                                                             backup_running_statistics=False, training=True,\n",
    "                                                             num_step=num_step, training_phase='test',\n",
    "                                                             epoch=0)\n",
    "            \n",
    "            feature_map = feature_map_list[3]\n",
    "            \n",
    "            # 선택한 클래스에 대한 스칼라 출력값을 기준으로 gradient 계산\n",
    "            target_class = y_label\n",
    "            output = target_preds\n",
    "\n",
    "            class_score = output[:, target_class].sum()\n",
    "            gradients = torch.autograd.grad(outputs=class_score, inputs=feature_map, grad_outputs=torch.ones_like(class_score))\n",
    "\n",
    "            # gradients를 사용하여 feature map의 가중치를 계산\n",
    "            weights = torch.mean(gradients[0], dim=(2, 3), keepdim=True)\n",
    "            grad_cam = F.relu(torch.sum(weights * feature_map, dim=1, keepdim=True))\n",
    "\n",
    "            # grad_cam을 이미지 크기로 리사이즈\n",
    "            grad_cam = F.interpolate(grad_cam, size=(input_image.shape[2], input_image.shape[3]), mode='bilinear', align_corners=False)\n",
    "            grad_cam = grad_cam.squeeze().cpu().detach().numpy()\n",
    "\n",
    "            visualize_and_save_grad_cam(\n",
    "                input_image=input_image,\n",
    "                grad_cam=grad_cam,\n",
    "                grad_cam_save_path=grad_cam_save_path + str(i) + \".png\",\n",
    "                original_save_path=original_save_path + str(i) + \".png\",\n",
    "                datasets=datasets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
