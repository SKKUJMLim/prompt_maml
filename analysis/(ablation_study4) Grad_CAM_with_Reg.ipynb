{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bd87b33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pytorchcv.model_provider import get_model as ptcv_get_model # model\n",
    "import sys, os\n",
    "import easydict\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline \n",
    "\n",
    "# enable cuda devices\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from meta_neural_network_architectures import VGGReLUNormNetwork, ResNet12\n",
    "from utils.parser_utils import get_args\n",
    "from data import MetaLearningSystemDataLoader\n",
    "from experiment_builder import ExperimentBuilder\n",
    "from utils.basic import show_batch\n",
    "from few_shot_learning_system import MAMLFewShotClassifier\n",
    "import prompters\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from data_augmentation import mixup_data, cutmix_data, random_flip, random_flip_like_torchvision\n",
    "from utils.grad_cam import visualize_and_save_grad_cam, make_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6c638561",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = \"mini_imagenet\"\n",
    "# datasets = \"tiered_imagenet\"\n",
    "# datasets = \"CIFAR_FS\"\n",
    "# datasets = \"CUB\"\n",
    "\n",
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "# os.environ['TEST_DATASET'] = \"tiered_imagenet\" # https://mtl.yyliu.net/download/Lmzjm9tX.html\n",
    "# os.environ['TEST_DATASET'] = \"CIFAR_FS\" # https://drive.google.com/file/d/1pTsCCMDj45kzFYgrnO67BWVbKs48Q3NI/view\n",
    "# os.environ['TEST_DATASET'] = \"CUB\" # https://data.caltech.edu/records/65de6-vp158"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f8daf114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.environ['DATASET_DIR'] ===  C:/Users/JM/PycharmProjects/MAML/datasets\n"
     ]
    }
   ],
   "source": [
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "print(\"os.environ['DATASET_DIR'] === \", os.environ['DATASET_DIR'])\n",
    "\n",
    "args = easydict.EasyDict(\n",
    "{\n",
    "  \"batch_size\":2,\n",
    "  \"image_height\":84,\n",
    "  \"image_width\":84,\n",
    "  \"image_channels\":3,\n",
    "  \"gpu_to_use\":0,\n",
    "  \"num_dataprovider_workers\":4,\n",
    "  \"max_models_to_save\":5,\n",
    "  \"dataset_name\":\"mini_imagenet_full_size\",\n",
    "  \"dataset_path\":\"mini_imagenet_full_size\",\n",
    "  \"reset_stored_paths\":False,\n",
    "  \"experiment_name\":\"../MAML_Prompt_padding_5way_5shot_filter128_miniImagenet\",\n",
    "  \"train_seed\": 0, \"val_seed\": 0,\n",
    "  \"indexes_of_folders_indicating_class\": [-3, -2],\n",
    "  \"sets_are_pre_split\": True,\n",
    "  \"train_val_test_split\": [0.64, 0.16, 0.20],\n",
    "  \"evaluate_on_test_set_only\": False,\n",
    "\n",
    "  \"total_epochs\": 100,\n",
    "  \"total_iter_per_epoch\":500, \"continue_from_epoch\": -2,\n",
    "  \"num_evaluation_tasks\":600,\n",
    "  \"multi_step_loss_num_epochs\": 15,\n",
    "  \"minimum_per_task_contribution\": 0.01,\n",
    "  \"learnable_per_layer_per_step_inner_loop_learning_rate\": True,\n",
    "  \"enable_inner_loop_optimizable_bn_params\": False,\n",
    "  \"evalute_on_test_set_only\": False,\n",
    "\n",
    "  \"max_pooling\": True,\n",
    "  \"per_step_bn_statistics\": False,\n",
    "  \"learnable_batch_norm_momentum\": False,\n",
    "  \"load_into_memory\": False,\n",
    "  \"init_inner_loop_learning_rate\": 0.01,\n",
    "  \"init_inner_loop_weight_decay\": 0.0005,\n",
    "  \"learnable_bn_gamma\": True,\n",
    "  \"learnable_bn_beta\": True,\n",
    "\n",
    "  \"dropout_rate_value\":0.0,\n",
    "  \"min_learning_rate\":0.001,\n",
    "  \"meta_learning_rate\":0.001,   \"total_epochs_before_pause\": 100,\n",
    "  \"first_order_to_second_order_epoch\":-1,\n",
    "  \"weight_decay\": 0.0,\n",
    "\n",
    "  \"norm_layer\":\"batch_norm\",\n",
    "  \"cnn_num_filters\":128,\n",
    "  \"num_stages\":4,\n",
    "  \"conv_padding\": True,\n",
    "  \"number_of_training_steps_per_iter\":5,\n",
    "  \"number_of_evaluation_steps_per_iter\":5,\n",
    "  \"cnn_blocks_per_stage\":1,\n",
    "  \"num_classes_per_set\":5,\n",
    "  \"num_samples_per_class\":5,\n",
    "  \"num_target_samples\": 15,\n",
    "  \"samples_per_iter\" : 1,\n",
    "\n",
    "  \"second_order\": True,\n",
    "  \"use_multi_step_loss_optimization\":False,\n",
    "  \"backbone\": \"4-CONV\",\n",
    "  \"arbiter\": False,\n",
    "  \"use_bias\": True,\n",
    "  \"prompter\": True,\n",
    "  \"prompt_engineering\": 'padding',\n",
    "  \"prompt_size\" : 5,\n",
    "  \"image_size\" : 84,\n",
    "  \"prompt_random_init\": False,\n",
    "  \"outer_prompt_learning_rate\": 0.001,\n",
    "  \"inner_prompt_learning_rate\": 0.01,\n",
    "  \"data_aug\" : \"mixup\",\n",
    "  \"ablation_record\": False,\n",
    "  \"random_prompt_init\": False\n",
    "}\n",
    ")\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "args.im_shape = (2, 3, args.image_height, args.image_width)\n",
    "\n",
    "args.use_cuda = torch.cuda.is_available()\n",
    "args.seed = 104\n",
    "args.reverse_channels=False\n",
    "args.labels_as_int=False\n",
    "args.reset_stored_filepaths=False\n",
    "args.num_of_gpus=1\n",
    "\n",
    "args.continue_from_epoch='latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dba8492c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using max pooling\n",
      "meta network params\n",
      "0.01\n",
      "Inner Loop parameters\n",
      "prompt_learning_rates_dict.prompt_weight_learning_rate torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv0-conv-weight torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv0-conv-bias torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv1-conv-weight torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv1-conv-bias torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv2-conv-weight torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv2-conv-bias torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv3-conv-weight torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv3-conv-bias torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-linear-weights torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-linear-bias torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv0-conv-weight torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv0-conv-bias torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv1-conv-weight torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv1-conv-bias torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv2-conv-weight torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv2-conv-bias torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv3-conv-weight torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv3-conv-bias torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-linear-weights torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-linear-bias torch.Size([6])\n",
      "Outer Loop parameters\n",
      "classifier.layer_dict.conv0.conv.weight torch.Size([128, 3, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv0.conv.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv0.norm_layer.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv0.norm_layer.weight torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv1.conv.weight torch.Size([128, 128, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv1.conv.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv1.norm_layer.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv1.norm_layer.weight torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv2.conv.weight torch.Size([128, 128, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv2.conv.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv2.norm_layer.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv2.norm_layer.weight torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv3.conv.weight torch.Size([128, 128, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv3.conv.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv3.norm_layer.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv3.norm_layer.weight torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.linear.weights torch.Size([5, 3200]) cuda:0 True\n",
      "classifier.layer_dict.linear.bias torch.Size([5]) cuda:0 True\n",
      "classifier.prompt.prompt_dict.pad_up torch.Size([3, 5, 84]) cuda:0 True\n",
      "classifier.prompt.prompt_dict.pad_down torch.Size([3, 5, 84]) cuda:0 True\n",
      "classifier.prompt.prompt_dict.pad_left torch.Size([3, 74, 5]) cuda:0 True\n",
      "classifier.prompt.prompt_dict.pad_right torch.Size([3, 74, 5]) cuda:0 True\n",
      "inner_loop_optimizer.prompt_learning_rates_dict.prompt_weight_learning_rate torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv0-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv0-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv1-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv1-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv2-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv2-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv3-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv3-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-weights torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv0-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv0-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv1-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv1-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv2-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv2-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv3-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv3-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-linear-weights torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-linear-bias torch.Size([6]) cuda:0 True\n",
      "1\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\MAML_Prompt_padding_5way_5shot_filter128_miniImagenet\n",
      "attempting to find existing checkpoint\n",
      "data {'test': 12000, 'train': 38400, 'val': 9600}\n",
      "train_seed 985773, val_seed: 985773, at start time\n",
      "50000 50000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_val_acc': 0.6932222223281861,\n",
       " 'best_val_iter': 41000,\n",
       " 'current_iter': 50000,\n",
       " 'best_epoch': 82,\n",
       " 'train_loss_mean': 0.43817064625024793,\n",
       " 'train_loss_std': 0.1217346856813005,\n",
       " 'train_accuracy_mean': 0.8396400016546249,\n",
       " 'train_accuracy_std': 0.051980802043907134,\n",
       " 'train_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_0_std': 0.0,\n",
       " 'train_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_1_std': 0.0,\n",
       " 'train_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_2_std': 0.0,\n",
       " 'train_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_3_std': 0.0,\n",
       " 'train_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'train_loss_importance_vector_4_std': 0.0,\n",
       " 'train_learning_rate_mean': 0.0010000000000000005,\n",
       " 'train_learning_rate_std': 4.336808689942018e-19,\n",
       " 'val_loss_mean': 0.8721063232421875,\n",
       " 'val_loss_std': 0.14623145672221946,\n",
       " 'val_accuracy_mean': 0.6667333312829336,\n",
       " 'val_accuracy_std': 0.05995179695900531,\n",
       " 'val_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_0_std': 0.0,\n",
       " 'val_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_1_std': 0.0,\n",
       " 'val_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_2_std': 0.0,\n",
       " 'val_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_3_std': 0.0,\n",
       " 'val_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'val_loss_importance_vector_4_std': 0.0,\n",
       " 'network': OrderedDict([('classifier.layer_dict.conv0.conv.weight',\n",
       "               tensor([[[[ 9.3018e-03,  9.9261e-03,  8.9948e-03],\n",
       "                         [ 7.7023e-03,  9.0612e-03,  8.9371e-03],\n",
       "                         [ 6.5003e-03,  7.7316e-03,  8.0053e-03]],\n",
       "               \n",
       "                        [[-6.5279e-04,  1.1903e-03,  1.3400e-03],\n",
       "                         [-1.6318e-03,  5.3815e-04,  1.4565e-03],\n",
       "                         [-2.6473e-03, -5.7181e-04,  4.0202e-04]],\n",
       "               \n",
       "                        [[ 9.2620e-03,  1.0052e-02,  9.7169e-03],\n",
       "                         [ 6.3386e-03,  7.8983e-03,  8.3571e-03],\n",
       "                         [ 5.9784e-03,  7.5205e-03,  7.9782e-03]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.1364e-03, -9.2588e-04, -9.4827e-04],\n",
       "                         [-1.0199e-03, -8.4684e-04, -7.8958e-04],\n",
       "                         [-1.0026e-03, -8.8605e-04, -8.6963e-04]],\n",
       "               \n",
       "                        [[-2.1983e-03, -2.2178e-03, -2.1501e-03],\n",
       "                         [-1.9889e-03, -2.0301e-03, -1.8989e-03],\n",
       "                         [-1.7974e-03, -1.6833e-03, -1.5738e-03]],\n",
       "               \n",
       "                        [[-5.4085e-04, -4.1947e-04, -4.1666e-04],\n",
       "                         [-3.3661e-04, -2.1327e-04, -1.7824e-04],\n",
       "                         [-3.6721e-04, -1.6807e-04, -1.3176e-04]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 4.6236e-06,  4.7030e-06,  2.7742e-06],\n",
       "                         [ 5.3822e-06,  5.5468e-06,  3.2150e-06],\n",
       "                         [ 5.1821e-06,  5.0991e-06,  2.4030e-06]],\n",
       "               \n",
       "                        [[-2.6011e-06, -2.1986e-06, -4.3255e-06],\n",
       "                         [-1.6539e-06, -1.3141e-06, -3.8612e-06],\n",
       "                         [-2.3292e-06, -2.2936e-06, -4.9385e-06]],\n",
       "               \n",
       "                        [[ 9.6566e-06,  8.9852e-06,  6.4226e-06],\n",
       "                         [ 1.4807e-05,  1.3538e-05,  9.0483e-06],\n",
       "                         [ 1.9320e-05,  1.4944e-05,  8.7722e-06]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 6.2973e-07,  7.5989e-07,  7.4401e-07],\n",
       "                         [ 6.6136e-07,  7.2076e-07,  7.0601e-07],\n",
       "                         [ 5.9291e-07,  6.5411e-07,  6.5109e-07]],\n",
       "               \n",
       "                        [[ 2.0861e-07,  3.2194e-07,  3.3799e-07],\n",
       "                         [ 2.1163e-07,  2.6130e-07,  2.8017e-07],\n",
       "                         [ 1.4138e-07,  1.8156e-07,  2.0489e-07]],\n",
       "               \n",
       "                        [[ 3.0813e-07,  4.6648e-07,  4.9582e-07],\n",
       "                         [ 2.7127e-07,  3.4774e-07,  3.9006e-07],\n",
       "                         [ 2.0577e-07,  2.5601e-07,  3.0286e-07]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 3.2286e-03,  2.9779e-03,  2.5246e-03],\n",
       "                         [ 3.1380e-03,  2.9317e-03,  1.8542e-03],\n",
       "                         [ 1.6984e-03,  1.3428e-03, -5.2502e-04]],\n",
       "               \n",
       "                        [[ 6.6210e-03,  4.2021e-03,  3.4284e-03],\n",
       "                         [ 7.1422e-03,  4.7385e-03,  3.6305e-03],\n",
       "                         [ 6.6089e-03,  4.4175e-03,  2.3839e-03]],\n",
       "               \n",
       "                        [[ 1.7170e-02,  1.2219e-02,  1.4354e-02],\n",
       "                         [ 1.6105e-02,  1.2204e-02,  1.4137e-02],\n",
       "                         [ 1.6663e-02,  1.3038e-02,  1.2414e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 5.2205e-08,  6.3142e-08,  6.0402e-08],\n",
       "                         [ 5.3694e-08,  5.9867e-08,  5.7562e-08],\n",
       "                         [ 4.5786e-08,  5.3225e-08,  5.3005e-08]],\n",
       "               \n",
       "                        [[ 2.2215e-08,  2.9141e-08,  2.4040e-08],\n",
       "                         [ 1.7544e-08,  2.1609e-08,  1.7626e-08],\n",
       "                         [ 7.1534e-09,  1.1058e-08,  9.7690e-09]],\n",
       "               \n",
       "                        [[ 5.7313e-08,  6.1519e-08,  4.5763e-08],\n",
       "                         [ 4.0268e-08,  3.9471e-08,  2.9411e-08],\n",
       "                         [ 3.5345e-08,  3.1748e-08,  2.3416e-08]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.conv.bias',\n",
       "               tensor([ 3.1381e-09, -1.6650e-06,  5.8802e-06,  1.5086e-07, -3.0219e-05,\n",
       "                       -1.2279e-04,  4.3463e-05, -1.3167e-07, -1.6355e-11, -5.2697e-11,\n",
       "                       -7.6272e-09, -1.5429e-04, -7.9359e-17,  3.4820e-05,  4.3829e-04,\n",
       "                       -7.2178e-06, -2.0122e-05, -5.9731e-09, -1.8230e-06, -3.5195e-12,\n",
       "                       -1.9104e-05, -8.9899e-09, -5.1658e-06,  1.3674e-04, -6.0880e-06,\n",
       "                       -3.1127e-08,  2.3082e-05, -8.2168e-08,  3.6980e-13, -5.6786e-07,\n",
       "                        7.5855e-10,  1.0693e-13, -1.8274e-05, -1.7811e-04,  3.3860e-06,\n",
       "                        1.7679e-15, -6.8028e-07,  6.3688e-05,  3.3810e-07,  6.9491e-05,\n",
       "                        1.7934e-08, -1.3228e-04,  3.0132e-08, -4.6493e-07, -3.7574e-05,\n",
       "                       -7.1327e-05, -2.2384e-06,  3.2299e-06, -5.0560e-15, -1.7771e-08,\n",
       "                        9.6208e-08, -2.6480e-06,  5.4244e-07,  7.0507e-10, -8.8467e-05,\n",
       "                        2.3339e-15,  2.5556e-05,  4.1134e-09, -1.5186e-08, -6.9195e-07,\n",
       "                       -9.6460e-08,  1.7352e-08,  3.0448e-10,  7.1502e-14,  2.1458e-06,\n",
       "                        8.5801e-09, -1.7873e-08,  4.6356e-09, -1.5449e-09,  5.7275e-05,\n",
       "                       -9.8999e-07,  1.4119e-09, -1.4497e-05,  2.3665e-05, -5.3079e-09,\n",
       "                        6.1656e-06, -8.4943e-07, -2.1667e-05,  3.1277e-05, -1.6291e-07,\n",
       "                        4.2436e-12,  3.2992e-05, -8.4885e-06,  1.1746e-09, -4.5572e-08,\n",
       "                        1.1554e-05,  2.4366e-13, -1.2776e-05,  1.3839e-05, -9.3414e-11,\n",
       "                        1.9568e-07,  3.5097e-06, -4.8440e-07,  3.0063e-08, -3.7497e-08,\n",
       "                        4.0140e-13, -5.3219e-07, -2.5796e-06,  6.7674e-07, -1.2462e-09,\n",
       "                        2.4135e-07,  7.7633e-12, -1.8366e-07,  2.0169e-07,  1.5526e-07,\n",
       "                       -1.9030e-06, -2.8633e-06,  8.6871e-07,  1.5445e-05, -6.3200e-10,\n",
       "                       -5.5999e-05, -6.6579e-13,  1.4959e-07, -9.9312e-08,  1.5821e-06,\n",
       "                       -8.0309e-05,  3.8384e-06, -2.8766e-05,  5.7894e-09,  7.4981e-14,\n",
       "                       -1.0998e-04, -6.0118e-06,  1.0202e-04, -4.2606e-08,  1.7888e-08,\n",
       "                       -5.7536e-06,  8.4708e-09, -3.9646e-11], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.bias',\n",
       "               tensor([-8.2821e-03, -3.4999e-02, -3.2150e-03, -1.4053e-02,  6.7290e-02,\n",
       "                        2.2091e-01, -3.6796e-01, -3.0406e-02, -4.3463e-10, -1.7154e-07,\n",
       "                       -2.8519e-03, -3.0174e-02, -1.1891e-08, -6.1227e-01, -3.6891e-02,\n",
       "                       -3.8188e-02, -3.8468e-02, -3.5454e-03, -2.9399e-02, -5.2317e-09,\n",
       "                        1.2125e-02, -9.0292e-03, -3.0396e-01, -1.5439e-02, -7.6766e-03,\n",
       "                       -2.0792e-02, -2.2647e-03, -4.7612e-02, -2.1860e-06, -1.2745e-01,\n",
       "                       -1.0089e-03, -3.4708e-05, -9.4003e-02,  5.0182e-01, -3.3352e-02,\n",
       "                       -9.7754e-10,  4.7080e-02,  2.0245e-03, -1.8653e-02, -5.6220e-01,\n",
       "                       -2.6964e-02,  4.1474e-01, -3.9601e-02, -2.6461e-02, -5.0153e-02,\n",
       "                       -3.8735e-02, -2.8037e-09,  4.6310e-09, -5.3873e-07, -3.1983e-08,\n",
       "                       -3.2846e-02, -4.4010e-02, -7.0369e-02, -4.0580e-07, -2.3482e-02,\n",
       "                       -2.4273e-10, -1.1112e-01, -2.2935e-02, -5.4347e-03, -5.1959e-02,\n",
       "                       -2.3538e-02, -9.1582e-04, -6.9051e-05, -4.7902e-10, -3.9519e-02,\n",
       "                       -4.9502e-02, -4.8361e-02, -3.1848e-03, -4.6013e-04, -1.8136e-03,\n",
       "                       -5.2291e-05, -4.1271e-08, -6.2549e-02, -3.5995e-01, -4.2392e-03,\n",
       "                       -2.6225e-01, -9.9275e-03,  1.3766e-02, -9.0288e-03, -2.5528e-02,\n",
       "                       -4.5102e-08,  3.1141e-02, -3.0340e-02, -2.0832e-06, -4.3688e-02,\n",
       "                       -2.3010e-11, -2.4489e-08, -2.6035e-01, -1.0138e-06, -1.1744e-10,\n",
       "                       -5.9240e-02, -1.0214e-02,  1.5947e-02, -1.9955e-02, -1.9228e-02,\n",
       "                       -2.3828e-10, -7.8528e-05, -3.8115e-02, -5.5135e-03, -2.1430e-04,\n",
       "                       -1.5712e-02, -4.6037e-09, -2.5663e-03, -1.7773e-02, -4.6471e-02,\n",
       "                       -5.0699e-01, -2.1921e-01, -4.9576e-02, -1.5428e-02, -1.1604e-04,\n",
       "                       -8.6495e-02, -6.5069e-09, -3.3006e-02, -3.2617e-03,  4.4804e-01,\n",
       "                       -3.6383e-02, -6.7161e-03, -1.9478e-02, -4.0970e-10, -2.1929e-10,\n",
       "                       -3.3244e-02, -4.8981e-02,  2.3248e-02, -5.6073e-03, -7.5837e-05,\n",
       "                       -3.3321e-04, -5.2223e-02, -1.0062e-04], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.weight',\n",
       "               tensor([ 1.9660e-03, -1.0324e-02,  2.0045e-04,  4.3391e-03,  1.5459e-01,\n",
       "                        3.2895e-01,  3.5723e-01,  1.9479e-03, -3.3423e-23, -5.3216e-08,\n",
       "                        1.0433e-04,  5.7060e-01,  5.6290e-21,  2.9190e-01,  5.4844e-01,\n",
       "                        5.6409e-01,  9.4100e-03, -4.5286e-04, -2.2071e-04, -1.3160e-12,\n",
       "                        3.5287e-01, -1.4779e-04,  2.0321e-01,  5.5436e-01, -9.1606e-04,\n",
       "                       -6.1445e-04,  6.1826e-04, -9.3409e-03, -1.0070e-18,  6.2515e-02,\n",
       "                        9.3204e-05, -5.8218e-16,  2.0845e-01,  4.7326e-01,  1.3988e-04,\n",
       "                        7.6561e-22,  7.7036e-02,  2.3601e-01, -7.2008e-03,  3.6965e-01,\n",
       "                        3.4673e-03,  3.9446e-01, -8.9798e-03, -5.5233e-03,  4.6198e-01,\n",
       "                        4.6660e-01,  1.4360e-12, -7.4987e-18, -2.9114e-21,  7.9427e-15,\n",
       "                       -1.0758e-02,  6.0261e-03,  1.7899e-03,  1.0061e-06,  4.7189e-01,\n",
       "                        1.0534e-10,  4.9563e-01,  1.8934e-03,  3.6752e-04, -1.3500e-02,\n",
       "                        3.9937e-05,  5.5180e-04,  2.7114e-05, -4.3254e-16,  6.9115e-03,\n",
       "                       -3.7526e-03, -7.6123e-03,  8.7860e-04,  5.3915e-04,  4.1208e-01,\n",
       "                        2.0424e-04,  1.9843e-14,  2.3308e-01,  2.6699e-01, -5.7872e-04,\n",
       "                        1.4761e-01, -7.3447e-04,  5.5399e-01,  5.3309e-01,  1.0928e-02,\n",
       "                       -2.7468e-05,  2.6096e-01,  6.5495e-03, -1.2357e-05,  1.7030e-02,\n",
       "                        1.1050e-17,  3.2292e-07,  3.0666e-01, -2.7056e-04, -1.0298e-21,\n",
       "                        7.3859e-03,  3.4752e-03, -4.4361e-04,  1.8655e-03, -7.9593e-03,\n",
       "                        1.2887e-12, -1.5729e-04, -1.4517e-02,  2.0832e-03,  3.4599e-04,\n",
       "                       -8.2973e-04, -2.2930e-20, -4.6925e-04, -2.9285e-03,  1.5315e-02,\n",
       "                        2.9008e-01,  4.2198e-01,  7.7634e-03,  3.5685e-01, -3.7653e-04,\n",
       "                        2.3645e-01,  6.5590e-22,  6.6529e-03, -6.4757e-04,  3.2120e-03,\n",
       "                        6.7160e-01,  9.5385e-04, -5.4956e-03, -8.4390e-09, -7.8332e-13,\n",
       "                        5.9845e-01,  1.1416e-02,  3.7688e-01,  1.5277e-03, -1.5119e-04,\n",
       "                        1.0826e-04, -2.9149e-03,  2.5113e-05], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.weight',\n",
       "               tensor([[[[ 3.8547e-05,  3.2589e-05,  2.6897e-05],\n",
       "                         [ 3.8010e-05,  3.4433e-05,  3.4243e-05],\n",
       "                         [ 3.6273e-05,  2.6180e-05,  3.0921e-05]],\n",
       "               \n",
       "                        [[ 6.0842e-04,  4.6141e-04,  2.5874e-04],\n",
       "                         [ 5.0280e-04,  3.7265e-04,  1.9371e-04],\n",
       "                         [ 3.8232e-04,  2.5875e-04,  1.0642e-04]],\n",
       "               \n",
       "                        [[ 4.2103e-05, -2.0057e-05, -4.0168e-05],\n",
       "                         [ 6.7282e-05,  7.5904e-07, -2.3688e-05],\n",
       "                         [ 6.5419e-05,  4.5296e-06, -1.5000e-05]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 7.2759e-06, -8.1831e-07, -2.9928e-06],\n",
       "                         [ 8.5869e-06,  4.2578e-08, -2.5194e-06],\n",
       "                         [ 1.0040e-05,  2.2744e-06,  1.1926e-06]],\n",
       "               \n",
       "                        [[ 2.4586e-03,  2.4539e-03,  7.2760e-04],\n",
       "                         [ 2.2757e-03,  2.7268e-03,  7.1497e-04],\n",
       "                         [ 1.8286e-03,  1.8100e-03,  2.9327e-04]],\n",
       "               \n",
       "                        [[ 1.6068e-06, -6.0041e-07, -1.2927e-06],\n",
       "                         [ 2.2798e-06,  5.8657e-10, -7.8466e-07],\n",
       "                         [ 2.4446e-06,  2.3724e-07, -4.4790e-07]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 3.1035e-07,  1.5124e-07,  4.9163e-08],\n",
       "                         [ 1.1790e-07, -9.4460e-08, -1.1735e-07],\n",
       "                         [ 8.3552e-08, -7.9466e-08, -1.3066e-08]],\n",
       "               \n",
       "                        [[ 5.8729e-07,  8.3253e-07,  5.5929e-07],\n",
       "                         [ 7.5707e-07,  9.3228e-07,  5.9643e-07],\n",
       "                         [ 8.2391e-07,  1.0740e-06,  6.1259e-07]],\n",
       "               \n",
       "                        [[-3.6537e-08, -2.1112e-08,  1.0786e-07],\n",
       "                         [ 1.6771e-08,  2.6523e-09,  1.2653e-07],\n",
       "                         [ 8.2440e-09, -2.6841e-08,  2.2829e-08]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.3510e-08, -8.0556e-09, -3.1536e-08],\n",
       "                         [-5.9932e-09,  1.1204e-09, -3.6046e-08],\n",
       "                         [-3.9522e-08, -3.4267e-08, -4.6311e-08]],\n",
       "               \n",
       "                        [[ 6.4541e-07, -5.4390e-08,  2.3372e-06],\n",
       "                         [ 1.4467e-06,  9.6190e-07,  2.2410e-06],\n",
       "                         [-1.0693e-07,  8.3581e-08,  1.6307e-06]],\n",
       "               \n",
       "                        [[-2.8347e-09, -9.0607e-09, -9.3724e-09],\n",
       "                         [-7.4082e-09, -3.5943e-07, -1.0960e-09],\n",
       "                         [ 1.0695e-09, -3.9058e-09, -1.9955e-08]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 5.5373e-06,  2.1862e-05,  2.1140e-05],\n",
       "                         [ 6.4953e-06,  2.9462e-05,  4.0395e-05],\n",
       "                         [ 1.9066e-05,  4.6532e-05,  7.1595e-05]],\n",
       "               \n",
       "                        [[ 9.2104e-05,  1.1373e-04,  1.1906e-04],\n",
       "                         [ 9.1983e-05,  1.3775e-04,  1.6803e-04],\n",
       "                         [ 6.3717e-05,  6.1467e-06,  1.3323e-04]],\n",
       "               \n",
       "                        [[-4.2189e-06, -4.6423e-07, -5.0938e-06],\n",
       "                         [-1.1726e-06,  1.1606e-06, -3.7008e-06],\n",
       "                         [-7.4260e-06, -4.0342e-06, -8.0520e-06]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-8.8108e-06, -4.7629e-06, -8.8543e-06],\n",
       "                         [-4.0914e-06, -1.2280e-07, -4.7302e-06],\n",
       "                         [-4.7252e-06, -6.9564e-07, -5.0218e-06]],\n",
       "               \n",
       "                        [[ 5.2657e-03,  5.2950e-03,  3.0522e-03],\n",
       "                         [ 5.7150e-03,  6.1934e-03,  2.6734e-03],\n",
       "                         [ 4.6223e-03,  5.5366e-03,  3.4281e-03]],\n",
       "               \n",
       "                        [[-1.7004e-06, -1.0957e-06, -2.3285e-06],\n",
       "                         [-6.5824e-07, -2.6687e-10, -1.0327e-06],\n",
       "                         [-8.4686e-07, -1.8507e-07, -1.1158e-06]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 3.3559e-09, -3.6184e-09, -2.0141e-09],\n",
       "                         [-1.0235e-09, -2.7949e-09, -2.2259e-09],\n",
       "                         [ 2.9426e-09, -2.5915e-09,  1.9769e-09]],\n",
       "               \n",
       "                        [[-2.3763e-09, -6.0981e-11, -9.9857e-10],\n",
       "                         [ 1.1807e-09,  2.3093e-09,  1.0351e-08],\n",
       "                         [ 1.1202e-08,  1.1254e-08,  2.8714e-08]],\n",
       "               \n",
       "                        [[ 5.0763e-10,  8.4378e-07, -7.3053e-09],\n",
       "                         [ 3.5664e-08,  3.8391e-09,  5.1736e-07],\n",
       "                         [ 9.5842e-08,  5.3359e-06,  1.0183e-08]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-4.1607e-11, -3.2627e-11, -3.6771e-11],\n",
       "                         [ 8.0649e-12,  1.4821e-12, -1.0676e-11],\n",
       "                         [ 3.6557e-11,  1.1748e-11, -1.6708e-11]],\n",
       "               \n",
       "                        [[ 6.2792e-08,  3.7207e-08,  3.2251e-08],\n",
       "                         [ 6.9050e-08, -4.5645e-09,  1.3323e-08],\n",
       "                         [ 1.3404e-07,  4.6497e-08,  2.3293e-08]],\n",
       "               \n",
       "                        [[ 2.3451e-10, -1.9648e-09,  5.7040e-08],\n",
       "                         [ 7.1477e-07, -3.2542e-12,  1.8406e-08],\n",
       "                         [ 1.1020e-09, -3.2984e-05,  1.2861e-08]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.3309e-05, -1.6362e-05, -1.5762e-05],\n",
       "                         [-1.1923e-05, -1.3422e-05, -1.7395e-05],\n",
       "                         [-1.6474e-05, -1.4823e-05, -1.4576e-05]],\n",
       "               \n",
       "                        [[-8.7458e-05, -9.5698e-05, -1.1014e-04],\n",
       "                         [-6.6861e-05, -6.7789e-05, -1.3519e-04],\n",
       "                         [-1.2609e-04, -1.2417e-04, -1.8664e-04]],\n",
       "               \n",
       "                        [[ 4.9760e-06,  3.3430e-06,  4.0017e-06],\n",
       "                         [ 1.4351e-06, -2.0964e-07,  5.9081e-07],\n",
       "                         [-1.2618e-05, -1.4932e-05, -1.3586e-05]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-3.0076e-07, -3.6166e-07, -4.4680e-07],\n",
       "                         [-6.6606e-08,  7.0366e-09,  3.6474e-08],\n",
       "                         [ 9.1318e-07,  1.6142e-06,  1.6164e-06]],\n",
       "               \n",
       "                        [[ 3.7790e-05,  3.5872e-05,  4.1061e-05],\n",
       "                         [ 2.3052e-05,  2.2374e-05,  3.2290e-05],\n",
       "                         [-1.4527e-05, -4.8559e-05, -2.3907e-05]],\n",
       "               \n",
       "                        [[ 1.6136e-07,  1.7083e-07,  2.0627e-07],\n",
       "                         [ 2.4371e-08, -2.5595e-10,  4.0656e-08],\n",
       "                         [-2.8833e-07, -2.7953e-07, -3.1435e-07]]],\n",
       "               \n",
       "               \n",
       "                       [[[-5.1696e-05, -5.2610e-05, -4.8391e-05],\n",
       "                         [-5.2192e-05, -5.6817e-05, -5.3990e-05],\n",
       "                         [-4.9890e-05, -5.2457e-05, -5.4966e-05]],\n",
       "               \n",
       "                        [[-3.4600e-04, -2.8958e-04, -3.1656e-04],\n",
       "                         [-3.4305e-04, -3.1286e-04, -3.2174e-04],\n",
       "                         [-4.3592e-04, -4.3024e-04, -4.4840e-04]],\n",
       "               \n",
       "                        [[-6.4724e-06, -2.6471e-06, -2.2195e-06],\n",
       "                         [-3.3450e-06,  6.9922e-09, -2.8988e-07],\n",
       "                         [-6.2899e-06, -2.3781e-06, -3.7968e-06]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-3.1131e-07, -8.9678e-07, -4.8148e-07],\n",
       "                         [ 5.4928e-07, -8.5786e-08,  3.1783e-07],\n",
       "                         [ 4.6894e-07, -1.4098e-07,  1.1283e-07]],\n",
       "               \n",
       "                        [[ 1.7520e-04,  3.6692e-04,  8.7585e-04],\n",
       "                         [ 1.0624e-04,  6.9545e-04,  1.3997e-03],\n",
       "                         [ 2.8106e-04,  6.7216e-04,  1.3498e-03]],\n",
       "               \n",
       "                        [[-1.1694e-07, -2.5827e-07, -1.6695e-07],\n",
       "                         [ 1.1232e-07, -1.2796e-09,  1.1156e-07],\n",
       "                         [ 7.2879e-08, -2.4824e-08,  7.1835e-08]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.bias',\n",
       "               tensor([-1.6303e-05, -5.1128e-09, -2.8541e-05, -2.6561e-05,  3.0988e-09,\n",
       "                        2.2826e-05,  4.6308e-05, -2.3128e-09, -3.9931e-10,  1.5643e-08,\n",
       "                       -2.3550e-06,  5.1185e-07,  3.1979e-07, -5.6483e-08,  2.0082e-08,\n",
       "                        6.6995e-07,  2.7476e-06,  2.5044e-06,  2.0871e-05,  5.8457e-05,\n",
       "                        1.7162e-06, -2.1182e-09, -2.7820e-05, -1.4507e-06,  8.4476e-06,\n",
       "                        1.5790e-05,  8.2863e-06, -2.1746e-05, -1.0865e-07,  1.9214e-08,\n",
       "                       -1.4308e-09, -6.8531e-06,  1.2291e-09, -8.0952e-05,  8.8464e-09,\n",
       "                        1.2181e-07, -1.7597e-06, -4.4428e-05,  9.1151e-08,  3.0405e-05,\n",
       "                        5.3782e-06, -2.7720e-05, -6.2769e-05,  1.2960e-05, -1.5912e-09,\n",
       "                        8.0136e-05, -9.6721e-06, -2.6090e-05,  4.6594e-05, -1.4129e-05,\n",
       "                        9.7250e-06, -4.4873e-05, -7.7754e-06, -1.0425e-07,  1.7263e-05,\n",
       "                       -1.9699e-05,  5.8612e-05,  3.1771e-07,  2.2846e-07,  9.0143e-06,\n",
       "                       -4.9749e-09, -4.9209e-05,  3.6480e-08,  2.4557e-05,  6.0915e-06,\n",
       "                        3.3872e-12,  4.0669e-05, -7.9541e-08,  7.5153e-05,  3.7868e-08,\n",
       "                        3.5612e-05, -5.6484e-07,  6.0795e-05,  8.1468e-09, -8.4690e-05,\n",
       "                        5.1116e-06, -5.2020e-06, -4.7970e-05, -6.2792e-06,  4.1112e-06,\n",
       "                        2.3578e-06,  8.6791e-05, -6.4438e-05,  1.1722e-05, -1.1510e-05,\n",
       "                        7.5308e-07, -2.1830e-08,  2.0463e-05, -4.4716e-06,  1.2981e-05,\n",
       "                        4.7594e-05, -9.7786e-05, -1.7148e-05,  1.6315e-07,  3.0109e-07,\n",
       "                        2.8154e-05, -4.0533e-07,  5.4298e-08,  1.5009e-11, -2.6251e-05,\n",
       "                        1.3021e-11, -1.4854e-05, -1.1495e-07, -3.4680e-05, -3.3361e-05,\n",
       "                        2.1610e-15, -6.3573e-05,  4.8127e-05, -6.4951e-07, -2.2561e-05,\n",
       "                       -4.2496e-05, -1.4700e-06,  1.5607e-08, -2.9485e-05, -3.5864e-05,\n",
       "                       -2.7869e-05,  7.6058e-05,  4.5071e-06, -6.2424e-06, -2.7983e-06,\n",
       "                        6.8485e-08, -6.1983e-05,  6.7619e-07,  4.4877e-06, -3.9466e-09,\n",
       "                       -5.0148e-10, -5.9269e-06, -2.0008e-05], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.bias',\n",
       "               tensor([-2.2769e-01, -1.9131e-02, -2.0420e-01, -2.3360e-01, -7.9796e-05,\n",
       "                       -2.5925e-01, -1.2494e-01, -1.6463e-10, -1.5886e-07, -2.9934e-03,\n",
       "                       -3.7848e-01, -4.5747e-03, -5.5422e-04, -6.6401e-04, -4.2441e-02,\n",
       "                       -2.7473e-02, -1.7608e-03, -3.9087e-02, -1.3727e-01, -2.7530e-01,\n",
       "                       -3.3138e-04, -4.1605e-02, -3.2380e-01, -2.5043e-01, -1.6272e-01,\n",
       "                       -4.7557e-01,  4.4445e-03, -2.1888e-01, -5.6089e-02, -8.7615e-02,\n",
       "                       -3.8757e-08, -2.7974e-02, -2.7575e-05, -1.8743e-01, -6.4090e-02,\n",
       "                       -3.3229e-11, -1.5866e-06, -2.1429e-01, -9.8679e-11, -2.4792e-01,\n",
       "                       -2.6901e-01, -2.7317e-01, -2.5897e-01, -1.3081e-11, -3.0789e-10,\n",
       "                       -2.1094e-01, -6.2238e-08, -1.9612e-01, -1.9428e-01, -1.4582e-01,\n",
       "                       -1.6717e-01, -2.3631e-01, -1.2041e-01, -7.3957e-02, -2.1261e-01,\n",
       "                       -2.6654e-01, -1.0746e-01, -8.3488e-10, -4.1884e-02, -1.9960e-01,\n",
       "                       -7.0584e-03, -1.8545e-01, -9.3045e-04, -1.9567e-01, -1.0436e-01,\n",
       "                       -1.4882e-07, -3.4861e-01, -5.4135e-02, -1.6819e-01, -1.3549e-02,\n",
       "                       -1.4665e-01, -8.7082e-03, -2.5422e-01, -1.0811e-02, -1.8620e-01,\n",
       "                       -7.1467e-03, -5.2270e-02, -1.4035e-01, -2.3166e-01, -2.8701e-01,\n",
       "                       -1.9402e-01, -3.8022e-01, -2.2906e-01, -1.2789e-01, -1.6221e-01,\n",
       "                       -6.5939e-02, -6.3634e-03, -2.2951e-01, -4.4701e-02, -1.7388e-01,\n",
       "                       -2.6454e-01, -1.7354e-01, -8.5915e-02, -5.6391e-03, -5.1093e-03,\n",
       "                       -2.0754e-01, -3.0307e-01, -3.8346e-08, -2.9421e-06, -1.9613e-01,\n",
       "                       -2.1670e-04, -1.7603e-01, -8.2411e-03, -3.3274e-01, -2.4051e-01,\n",
       "                       -2.1290e-09, -2.5326e-01, -2.3065e-01, -2.9689e-01, -1.5860e-01,\n",
       "                       -1.8498e-01, -2.9798e-01, -2.5209e-11, -2.3312e-01, -2.7512e-01,\n",
       "                       -1.1630e-01, -4.7733e-02, -1.4790e-01, -2.2558e-01, -1.6008e-03,\n",
       "                       -1.8547e-04, -1.0459e-01, -7.2023e-02, -3.1130e-01, -8.8485e-04,\n",
       "                       -2.8188e-04, -3.0036e-01, -1.1775e-01], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.weight',\n",
       "               tensor([ 3.2584e-01, -3.7984e-03,  3.5728e-01,  2.4507e-01, -2.5721e-04,\n",
       "                        2.8337e-01,  2.8475e-01,  4.7873e-10, -2.4114e-11, -4.9908e-04,\n",
       "                        1.9553e-01, -7.2921e-04, -1.7950e-04,  4.9729e-04, -1.8589e-02,\n",
       "                        3.0621e-03,  4.6487e-04, -6.7235e-03,  3.3359e-01,  3.7005e-01,\n",
       "                        2.2071e-04,  1.1432e-02,  2.2668e-01,  4.5451e-01,  3.3918e-01,\n",
       "                        3.9571e-01,  2.1597e-01,  2.2504e-01, -1.5936e-02,  3.1374e-01,\n",
       "                        2.8637e-05,  3.5945e-03,  1.0273e-04,  3.5105e-01,  3.8663e-03,\n",
       "                       -1.5858e-07, -8.6681e-05,  2.8807e-01,  1.4822e-09,  2.9080e-01,\n",
       "                        1.7375e-01,  3.7129e-01,  4.4187e-01,  1.2907e-13,  2.4389e-06,\n",
       "                        3.8290e-01,  5.8761e-06,  3.9117e-01,  3.2846e-01,  4.1437e-01,\n",
       "                        3.4216e-01,  3.1424e-01,  2.2393e-01, -1.5676e-02,  3.6616e-01,\n",
       "                        3.0646e-01,  3.3130e-01, -8.2032e-07,  1.2433e-02,  3.3212e-01,\n",
       "                        8.8339e-04,  4.4151e-01,  4.2581e-04,  2.8420e-01,  3.0799e-01,\n",
       "                       -2.4359e-05,  1.8014e-01, -9.3840e-03,  4.3089e-01, -1.8103e-03,\n",
       "                        4.3153e-01, -1.3834e-03,  4.3448e-01, -1.6294e-03,  4.7230e-01,\n",
       "                       -1.4002e-03, -4.7799e-03,  2.6512e-01,  2.6016e-01,  4.0852e-01,\n",
       "                        2.8542e-01,  4.5516e-01,  4.6592e-01,  2.2457e-01,  2.9814e-02,\n",
       "                       -1.0540e-02,  7.3128e-04,  5.9904e-01, -2.8555e-03,  3.2934e-01,\n",
       "                        4.8382e-01,  3.8467e-01,  3.1700e-01, -7.4661e-04,  5.9049e-04,\n",
       "                        3.3199e-01,  2.4054e-01, -1.4722e-05,  2.0421e-04,  3.1069e-01,\n",
       "                       -1.5618e-07,  2.9069e-01, -1.2414e-03,  3.5934e-01,  3.7713e-01,\n",
       "                       -1.7792e-12,  3.3577e-01,  4.0863e-01,  3.8418e-01,  2.7417e-01,\n",
       "                        2.5388e-01,  5.4505e-01,  2.6677e-08,  3.3145e-01,  3.8831e-01,\n",
       "                        2.7293e-01,  2.9139e-01,  3.6025e-01,  3.6824e-01,  3.3869e-04,\n",
       "                        3.0907e-04,  5.1359e-01, -1.9838e-02,  4.5833e-01,  3.4161e-04,\n",
       "                       -2.3771e-04,  8.4130e-02,  2.1765e-01], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.weight',\n",
       "               tensor([[[[-1.7805e-02, -2.3567e-02,  3.4177e-02],\n",
       "                         [ 9.5435e-03, -5.0115e-02, -4.4294e-02],\n",
       "                         [-3.8450e-02, -8.4363e-02, -1.1270e-01]],\n",
       "               \n",
       "                        [[ 1.2839e-02,  2.4906e-03, -4.3676e-03],\n",
       "                         [ 2.1032e-03,  5.9384e-03, -3.8271e-03],\n",
       "                         [-2.4950e-03, -1.4305e-02, -1.8876e-02]],\n",
       "               \n",
       "                        [[ 2.9242e-02, -5.6489e-02, -6.8640e-02],\n",
       "                         [ 1.0822e-01, -8.6555e-02, -1.2931e-01],\n",
       "                         [-5.9366e-03, -8.3475e-02,  2.4769e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 7.0685e-06,  2.3329e-06,  5.7643e-06],\n",
       "                         [ 4.1049e-06,  3.3490e-07,  3.5153e-06],\n",
       "                         [ 7.6851e-06,  3.9439e-06,  7.2009e-06]],\n",
       "               \n",
       "                        [[-1.9398e-02, -2.6853e-02, -3.2269e-02],\n",
       "                         [-1.2862e-02, -6.4126e-03, -5.7623e-03],\n",
       "                         [-1.7570e-02,  1.0164e-02,  2.0040e-02]],\n",
       "               \n",
       "                        [[ 2.5073e-02,  3.4286e-02, -7.8595e-03],\n",
       "                         [ 2.8178e-02,  4.5182e-02, -1.7008e-02],\n",
       "                         [ 5.7708e-03,  1.6389e-02,  1.3503e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 6.2222e-02,  5.2245e-03,  4.3422e-02],\n",
       "                         [ 4.6190e-02,  1.3397e-02,  3.1902e-03],\n",
       "                         [ 3.5072e-02,  1.9839e-02, -1.3466e-02]],\n",
       "               \n",
       "                        [[ 7.6285e-03,  1.0548e-02,  9.3497e-03],\n",
       "                         [ 1.6751e-02,  8.2273e-03,  1.1602e-02],\n",
       "                         [ 1.3699e-02,  1.5806e-02,  2.0784e-02]],\n",
       "               \n",
       "                        [[ 1.8983e-02, -7.1690e-02, -6.6719e-02],\n",
       "                         [-7.1914e-03, -6.6986e-02, -5.9084e-02],\n",
       "                         [ 8.7258e-02, -1.8117e-02, -8.1956e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.6814e-05,  1.4037e-05,  2.8521e-05],\n",
       "                         [ 7.1864e-06,  4.0599e-06,  1.9332e-05],\n",
       "                         [ 1.7547e-05,  1.4902e-05,  2.9980e-05]],\n",
       "               \n",
       "                        [[-5.4388e-03, -3.3011e-02, -1.5601e-02],\n",
       "                         [-2.8598e-03, -1.7455e-02, -2.0422e-02],\n",
       "                         [-1.1163e-03,  3.8118e-03, -6.9983e-03]],\n",
       "               \n",
       "                        [[-1.7622e-02, -1.8633e-03,  4.1766e-02],\n",
       "                         [-2.1409e-03, -2.9483e-02,  1.8233e-02],\n",
       "                         [ 1.6563e-02, -1.4199e-06,  3.4296e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.0844e-01, -9.1183e-02, -3.2124e-02],\n",
       "                         [ 1.5623e-03, -4.3138e-02, -9.2280e-03],\n",
       "                         [ 2.4907e-02,  1.1610e-02, -1.9557e-02]],\n",
       "               \n",
       "                        [[ 2.9430e-02,  2.6610e-02,  2.0865e-02],\n",
       "                         [ 1.9161e-02,  1.4693e-02,  1.9567e-02],\n",
       "                         [ 1.3455e-02,  3.1704e-03,  6.8985e-03]],\n",
       "               \n",
       "                        [[-4.2732e-03, -5.4636e-02,  6.2420e-02],\n",
       "                         [-3.2313e-02,  2.6687e-02, -6.1589e-03],\n",
       "                         [ 1.8372e-02,  6.1489e-02,  1.4887e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.7595e-05, -3.7449e-06, -9.6741e-06],\n",
       "                         [-2.3768e-05,  1.6394e-07, -7.0250e-06],\n",
       "                         [-2.9124e-05, -7.4890e-06, -1.3749e-05]],\n",
       "               \n",
       "                        [[-9.7505e-03, -1.2167e-02, -3.7298e-03],\n",
       "                         [-1.9423e-02, -3.1098e-03,  5.2735e-03],\n",
       "                         [ 1.0384e-03,  9.9344e-04,  6.3946e-03]],\n",
       "               \n",
       "                        [[-3.9909e-02, -8.0172e-02, -2.5939e-03],\n",
       "                         [-4.5772e-02, -5.0620e-02,  6.7588e-02],\n",
       "                         [-1.2237e-02, -2.9639e-02,  4.3640e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 2.5902e-02,  1.9115e-02, -1.9921e-02],\n",
       "                         [-7.6751e-03,  3.7279e-02, -1.1880e-02],\n",
       "                         [ 4.1319e-02,  3.4105e-02,  1.9067e-02]],\n",
       "               \n",
       "                        [[-2.5259e-02, -2.0434e-02, -2.0977e-02],\n",
       "                         [-2.2259e-02, -1.5950e-02, -2.1065e-02],\n",
       "                         [-2.1598e-02, -2.0502e-02, -1.8948e-02]],\n",
       "               \n",
       "                        [[-9.4955e-03, -2.4970e-02,  3.1058e-02],\n",
       "                         [ 1.9259e-02, -4.3285e-03, -3.5079e-02],\n",
       "                         [ 6.2932e-02,  8.0265e-03, -6.0785e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.1166e-05, -3.9340e-06, -9.8954e-06],\n",
       "                         [-8.8661e-06, -9.0584e-08, -6.5330e-06],\n",
       "                         [-1.0625e-05, -2.5318e-06, -8.0200e-06]],\n",
       "               \n",
       "                        [[ 2.8111e-03, -3.1905e-04,  1.2496e-02],\n",
       "                         [-7.0775e-03, -1.2988e-02, -1.4902e-02],\n",
       "                         [ 2.5028e-05, -7.6291e-03, -2.1722e-04]],\n",
       "               \n",
       "                        [[ 1.1763e-02,  3.7370e-03,  9.7870e-03],\n",
       "                         [-9.4656e-03,  4.3951e-02,  3.7472e-02],\n",
       "                         [-2.0808e-02,  6.3519e-02,  3.6683e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-2.8984e-02, -1.9040e-02, -4.0359e-02],\n",
       "                         [-8.8461e-02, -3.9368e-02, -3.0081e-02],\n",
       "                         [-8.0479e-03, -3.8868e-02,  4.4733e-02]],\n",
       "               \n",
       "                        [[ 6.5805e-05, -6.3807e-03, -7.5570e-03],\n",
       "                         [-7.9323e-03, -9.4828e-03, -1.2599e-02],\n",
       "                         [-5.4125e-03, -1.2455e-03, -3.1494e-03]],\n",
       "               \n",
       "                        [[-3.3132e-02, -2.9067e-02,  4.4539e-02],\n",
       "                         [-9.0833e-04, -1.4854e-01, -3.5855e-02],\n",
       "                         [ 2.8863e-02, -1.1821e-01, -2.2332e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.8317e-07,  6.5322e-06, -1.2837e-06],\n",
       "                         [-6.6257e-06, -5.4731e-07, -6.5274e-06],\n",
       "                         [-1.5329e-05, -1.2052e-05, -1.5597e-05]],\n",
       "               \n",
       "                        [[-1.0786e-02, -5.5640e-03,  9.5107e-03],\n",
       "                         [-7.8641e-04, -3.4871e-03,  4.6203e-04],\n",
       "                         [ 1.5915e-03, -2.6801e-04, -1.0571e-02]],\n",
       "               \n",
       "                        [[ 1.0458e-02,  1.1844e-02, -1.1779e-02],\n",
       "                         [-3.6311e-02, -2.0479e-02, -4.0481e-02],\n",
       "                         [-7.9255e-03, -1.9693e-02,  9.2187e-03]]],\n",
       "               \n",
       "               \n",
       "                       [[[-3.7229e-03, -1.9444e-02, -3.8370e-02],\n",
       "                         [-3.4629e-03, -1.0855e-02,  2.3715e-03],\n",
       "                         [ 2.4536e-02, -1.0427e-02,  1.0607e-02]],\n",
       "               \n",
       "                        [[ 3.4105e-03,  5.7686e-03,  8.8549e-03],\n",
       "                         [ 7.3088e-03,  6.5165e-03,  7.8834e-03],\n",
       "                         [ 1.2535e-02,  9.1503e-03,  7.9230e-03]],\n",
       "               \n",
       "                        [[ 9.3825e-02,  2.4780e-02, -2.7560e-02],\n",
       "                         [-6.4922e-02, -2.2074e-03,  4.8170e-02],\n",
       "                         [ 6.8926e-02,  8.3273e-03,  8.7049e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 3.6965e-06, -4.0812e-07,  3.6129e-06],\n",
       "                         [ 3.9839e-06,  4.2074e-07,  5.1755e-06],\n",
       "                         [ 4.7790e-06,  2.1006e-06,  5.7972e-06]],\n",
       "               \n",
       "                        [[ 2.6784e-03,  7.4123e-05, -5.1383e-04],\n",
       "                         [ 1.0617e-02,  9.8418e-03,  1.0068e-02],\n",
       "                         [ 1.5105e-02,  1.3646e-02,  2.1659e-02]],\n",
       "               \n",
       "                        [[-4.0362e-02, -6.7892e-02,  1.7399e-02],\n",
       "                         [-1.4630e-02, -1.7994e-03,  3.9865e-02],\n",
       "                         [-3.9558e-02,  1.5174e-02,  1.9351e-02]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.bias',\n",
       "               tensor([-1.8208e-05, -1.0396e-04, -2.8913e-05, -7.8916e-06, -3.1262e-05,\n",
       "                       -3.3753e-05,  2.9477e-05, -2.2321e-05, -9.2517e-05, -5.9117e-06,\n",
       "                        5.2392e-06, -4.8109e-05, -3.4734e-06,  3.5183e-05,  4.4498e-05,\n",
       "                        1.2913e-06, -5.1331e-06, -6.5822e-11,  5.9218e-05,  4.8989e-05,\n",
       "                       -2.8702e-05, -3.2478e-05,  7.3273e-06, -1.6411e-05, -7.4112e-05,\n",
       "                       -4.1196e-05,  1.4434e-09,  2.4271e-05, -5.8769e-05, -7.2782e-05,\n",
       "                        5.8996e-05,  6.7580e-06, -6.5089e-06, -2.8308e-06,  4.6519e-05,\n",
       "                       -1.0265e-05, -1.5014e-05, -4.1392e-06, -1.6126e-05,  1.3937e-05,\n",
       "                       -5.7140e-12, -2.7447e-05,  2.9637e-05, -1.6603e-05,  5.8332e-05,\n",
       "                       -2.2030e-05, -3.2991e-05, -7.4267e-06, -5.7908e-06,  4.3377e-05,\n",
       "                       -2.7376e-05, -1.2972e-05,  3.4380e-05, -5.0382e-05,  2.3824e-08,\n",
       "                       -1.2126e-05,  6.6856e-05, -1.3211e-05,  3.5883e-05,  3.9238e-05,\n",
       "                        2.6550e-05, -3.8389e-09, -6.1410e-05, -1.6252e-05, -4.8999e-05,\n",
       "                        1.0977e-04,  2.8789e-05, -2.1556e-05, -4.1825e-05,  2.6489e-05,\n",
       "                        1.5730e-05,  1.7668e-05, -5.6449e-06, -1.8598e-05,  3.7395e-06,\n",
       "                        6.2955e-06,  5.4754e-06, -3.1619e-06, -2.1215e-05,  1.3460e-05,\n",
       "                        3.1846e-05, -2.0688e-05,  4.2179e-06, -8.2105e-06, -1.8479e-05,\n",
       "                       -9.8774e-06,  9.5801e-05, -1.2861e-05, -3.8331e-06, -2.1849e-05,\n",
       "                       -8.5207e-05, -1.7626e-05, -1.9945e-05, -3.2626e-05, -5.6113e-05,\n",
       "                       -3.6950e-05,  5.8606e-06, -3.3114e-07, -2.3150e-05,  6.9649e-05,\n",
       "                        4.6941e-05,  7.2158e-05, -4.1657e-05,  1.1137e-05, -7.3242e-07,\n",
       "                        1.2530e-06, -2.9192e-05, -3.6018e-05, -8.6459e-06,  3.2824e-05,\n",
       "                        1.4452e-05, -5.1999e-05,  6.3840e-05, -1.3391e-05, -1.2859e-04,\n",
       "                        5.9952e-05,  1.6078e-05, -6.8993e-05,  3.9462e-05, -2.3026e-05,\n",
       "                       -8.1156e-06,  1.5962e-05, -5.7149e-05,  6.8483e-05,  2.3759e-05,\n",
       "                       -4.7581e-05, -4.4981e-05, -3.3261e-05], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.bias',\n",
       "               tensor([-3.9446e-01, -4.0204e-01, -3.6696e-01, -3.1121e-01, -3.1843e-01,\n",
       "                       -3.3512e-01, -2.8285e-01, -5.7964e-01, -5.0168e-01, -3.3600e-01,\n",
       "                       -2.9564e-01, -2.7963e-01, -2.8756e-01, -3.0930e-01, -2.8046e-01,\n",
       "                       -2.4438e-02, -2.8426e-01, -1.3621e-11, -3.0425e-01, -2.8422e-01,\n",
       "                       -4.0312e-01, -2.7318e-01, -3.7911e-01, -3.6069e-01, -2.8961e-01,\n",
       "                       -3.2310e-01, -9.2493e-13, -5.4637e-01, -3.4660e-01, -4.0397e-01,\n",
       "                       -4.0528e-01, -3.0217e-01, -4.6397e-01, -2.7030e-01, -3.6639e-01,\n",
       "                       -4.5506e-01, -2.7978e-01, -3.4262e-01, -1.9733e-01, -3.0077e-01,\n",
       "                       -4.2801e-09, -3.3583e-01, -2.9188e-01, -2.8343e-01, -2.6830e-01,\n",
       "                       -3.3190e-01, -4.5938e-01, -3.6728e-01, -3.6084e-01, -2.7206e-01,\n",
       "                       -3.5170e-01, -3.4234e-01, -3.5654e-01, -4.1906e-01, -4.6524e-12,\n",
       "                       -3.3082e-01, -3.6985e-01, -3.7771e-01, -3.2812e-01, -2.3666e-01,\n",
       "                       -3.3911e-01, -5.9982e-12, -2.7646e-01, -2.8257e-01, -2.9941e-01,\n",
       "                       -2.7864e-01, -2.1250e-01, -4.6911e-01, -2.5145e-01, -4.3961e-01,\n",
       "                       -3.0543e-01, -4.5035e-01, -4.4404e-01, -4.0903e-01, -2.6329e-01,\n",
       "                       -3.9828e-01, -2.6105e-01, -3.4279e-01, -3.0917e-01, -3.5608e-01,\n",
       "                       -2.7160e-01, -4.4081e-01, -3.7631e-01, -3.2866e-01, -3.2098e-02,\n",
       "                       -4.8890e-01, -2.8798e-01, -3.3046e-01, -4.4609e-01, -2.7030e-01,\n",
       "                       -3.7413e-01, -3.8471e-01, -4.5699e-01, -4.1709e-01, -4.2017e-01,\n",
       "                       -3.7523e-01, -4.2386e-01, -4.3267e-02, -2.5360e-01, -4.9090e-01,\n",
       "                       -3.2443e-01, -4.4190e-01, -3.4477e-01, -3.2860e-01, -3.1936e-01,\n",
       "                       -5.4131e-01, -4.2900e-01, -3.7387e-01, -4.7125e-01, -3.0236e-01,\n",
       "                       -3.2909e-01, -3.1409e-01, -3.1003e-01, -4.1589e-01, -2.8092e-01,\n",
       "                       -3.2156e-01, -2.7347e-01, -4.2905e-01, -3.5059e-01, -3.7225e-01,\n",
       "                       -4.1216e-01, -3.1898e-01, -3.6812e-01, -3.2729e-01, -4.5678e-01,\n",
       "                       -3.2171e-01, -4.3653e-01, -3.9312e-01], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.weight',\n",
       "               tensor([ 3.1814e-01,  3.1250e-01,  3.1517e-01,  2.4553e-01,  2.8348e-01,\n",
       "                        3.3462e-01,  2.4065e-01,  3.4641e-01,  3.7975e-01,  2.7711e-01,\n",
       "                        2.6602e-01,  2.6128e-01,  2.3569e-01,  3.5392e-01,  3.3132e-01,\n",
       "                       -4.5138e-03,  2.7768e-01,  1.9355e-08,  3.0685e-01,  2.4238e-01,\n",
       "                        1.9522e-01,  2.7744e-01,  3.0635e-01,  3.5684e-01,  2.8592e-01,\n",
       "                        3.0796e-01,  1.1880e-10,  3.6277e-01,  3.6927e-01,  2.5881e-01,\n",
       "                        3.9829e-01,  2.2860e-01,  3.8400e-01,  2.5188e-01,  2.8840e-01,\n",
       "                        3.1211e-01,  2.4169e-01,  3.1383e-01,  2.9004e-01,  2.6557e-01,\n",
       "                       -4.2091e-06,  3.1911e-01,  2.6912e-01,  2.9136e-01,  2.7513e-01,\n",
       "                        2.2973e-01,  4.0733e-01,  3.2901e-01,  2.5709e-01,  1.8109e-01,\n",
       "                        3.2340e-01,  2.5807e-01,  3.0937e-01,  2.8161e-01, -5.4220e-12,\n",
       "                        3.0251e-01,  2.7406e-01,  2.6561e-01,  2.7584e-01,  2.3588e-01,\n",
       "                        2.4293e-01, -9.9790e-10,  2.2411e-01,  2.8331e-01,  2.0468e-01,\n",
       "                        3.4841e-01,  2.2776e-01,  3.5210e-01,  2.4217e-01,  3.0620e-01,\n",
       "                        3.1609e-01,  2.8808e-01,  3.7674e-01,  2.5318e-01,  2.5729e-01,\n",
       "                        2.6781e-01,  2.4074e-01,  2.6333e-01,  2.7812e-01,  3.1169e-01,\n",
       "                        2.1386e-01,  3.7540e-01,  3.7231e-01,  2.6442e-01, -7.6075e-05,\n",
       "                        3.9442e-01,  1.8882e-01,  3.0407e-01,  3.4979e-01,  2.3638e-01,\n",
       "                        2.6018e-01,  3.3277e-01,  3.8044e-01,  3.4058e-01,  3.3441e-01,\n",
       "                        3.1769e-01,  3.5993e-01, -1.0504e-03,  2.9092e-01,  4.6947e-01,\n",
       "                        2.8052e-01,  4.0117e-01,  2.7448e-01,  2.7855e-01,  2.9875e-01,\n",
       "                        2.6699e-01,  3.1093e-01,  3.1589e-01,  3.9714e-01,  3.0969e-01,\n",
       "                        2.5710e-01,  2.5771e-01,  2.4432e-01,  3.4654e-01,  2.8857e-01,\n",
       "                        4.0970e-01,  2.7122e-01,  3.1739e-01,  3.6768e-01,  3.0644e-01,\n",
       "                        3.2320e-01,  2.3014e-01,  2.6223e-01,  2.9897e-01,  3.6378e-01,\n",
       "                        2.5678e-01,  3.2590e-01,  2.7122e-01], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.weight',\n",
       "               tensor([[[[-5.6493e-02, -5.2819e-02, -5.8452e-02],\n",
       "                         [-2.7304e-02, -5.6236e-02, -7.3187e-02],\n",
       "                         [-8.8901e-02, -7.5306e-02, -5.1938e-02]],\n",
       "               \n",
       "                        [[ 1.3873e-02, -1.5200e-02,  3.9169e-03],\n",
       "                         [-1.1717e-02,  2.0926e-02, -1.2470e-02],\n",
       "                         [ 3.2656e-02,  2.3377e-02,  2.1408e-02]],\n",
       "               \n",
       "                        [[-1.3046e-02,  1.2260e-02, -2.0560e-02],\n",
       "                         [ 1.1446e-02,  1.8557e-02,  2.8611e-02],\n",
       "                         [ 4.9427e-03,  2.7927e-02,  1.2284e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-3.4825e-02, -8.4421e-03,  1.9446e-02],\n",
       "                         [-1.3701e-02, -8.3861e-03, -1.1227e-02],\n",
       "                         [-3.0469e-02, -2.3698e-02,  8.5342e-03]],\n",
       "               \n",
       "                        [[ 4.0011e-02, -1.4467e-02,  4.8771e-02],\n",
       "                         [ 6.1509e-03, -6.8148e-02, -3.0248e-02],\n",
       "                         [-5.7707e-02, -1.3635e-02, -1.4956e-02]],\n",
       "               \n",
       "                        [[-3.3706e-02, -4.8342e-02, -3.1262e-02],\n",
       "                         [ 1.2380e-02,  8.0294e-03, -4.1556e-03],\n",
       "                         [-4.1642e-02, -2.5452e-02, -2.4698e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 7.6590e-03,  5.2958e-03,  1.9268e-02],\n",
       "                         [ 2.4861e-02,  7.9854e-02,  9.5852e-02],\n",
       "                         [ 2.5702e-02,  8.3393e-02,  7.4655e-02]],\n",
       "               \n",
       "                        [[-6.6004e-02, -4.0618e-02, -1.1457e-01],\n",
       "                         [-8.3568e-03, -6.4397e-02, -4.3781e-02],\n",
       "                         [-7.0837e-02, -1.1358e-01, -8.4195e-02]],\n",
       "               \n",
       "                        [[ 4.3466e-02,  1.0044e-01,  9.9276e-02],\n",
       "                         [ 4.3349e-02, -1.1305e-02,  4.5252e-02],\n",
       "                         [-1.7134e-02, -3.9026e-02, -5.7114e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.8451e-02, -4.7956e-02, -5.5820e-02],\n",
       "                         [-2.9244e-02, -7.3069e-02, -7.4070e-02],\n",
       "                         [-1.2485e-02, -1.4830e-02,  1.2467e-02]],\n",
       "               \n",
       "                        [[-3.5609e-02,  2.2497e-02, -5.5127e-02],\n",
       "                         [ 2.9282e-02,  4.8550e-02, -3.8091e-02],\n",
       "                         [-3.3344e-02,  6.6352e-02, -4.0040e-02]],\n",
       "               \n",
       "                        [[ 1.0740e-01,  8.7027e-02, -5.0004e-03],\n",
       "                         [ 1.0148e-01,  7.8241e-02,  7.2096e-02],\n",
       "                         [ 1.2592e-01,  7.0445e-02, -8.3987e-03]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.0192e-01,  1.0369e-01,  7.4000e-02],\n",
       "                         [ 2.5026e-02,  8.5860e-03, -3.0364e-02],\n",
       "                         [ 8.5919e-02,  2.3635e-02, -2.2018e-02]],\n",
       "               \n",
       "                        [[-4.4178e-02,  1.5501e-02, -1.1812e-03],\n",
       "                         [-3.9728e-02, -2.6008e-02, -4.9776e-02],\n",
       "                         [-1.2470e-02,  4.7611e-02,  2.3207e-02]],\n",
       "               \n",
       "                        [[-2.5830e-02,  1.8252e-02,  9.2033e-03],\n",
       "                         [ 3.7863e-02,  9.6290e-02,  5.2129e-02],\n",
       "                         [ 3.2000e-02,  4.2314e-02,  2.2884e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-5.7980e-02, -8.4724e-02, -2.3577e-02],\n",
       "                         [-3.6224e-02, -9.4820e-02, -8.8001e-02],\n",
       "                         [-5.6261e-02, -5.2729e-02, -1.7055e-02]],\n",
       "               \n",
       "                        [[ 2.7372e-02,  3.3430e-02,  4.4391e-02],\n",
       "                         [ 6.4522e-02,  5.1695e-02,  1.9713e-03],\n",
       "                         [ 1.8476e-02,  6.9570e-02,  3.5313e-02]],\n",
       "               \n",
       "                        [[-4.4074e-02, -4.3891e-02, -3.0083e-02],\n",
       "                         [-3.8428e-02, -7.5706e-02, -5.5784e-02],\n",
       "                         [ 2.6392e-02, -1.1897e-02, -6.4235e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 8.6382e-41,  4.6180e-41,  3.6487e-41],\n",
       "                         [ 5.9817e-41,  9.5134e-42,  2.9239e-41],\n",
       "                         [ 2.8409e-41,  8.8072e-41, -3.5814e-41]],\n",
       "               \n",
       "                        [[-9.3021e-41, -5.4340e-41,  9.4969e-41],\n",
       "                         [-3.1550e-41, -6.3406e-41,  3.5910e-41],\n",
       "                         [ 3.7639e-41, -9.5120e-41,  6.1427e-41]],\n",
       "               \n",
       "                        [[ 7.6776e-41,  5.8105e-41, -6.8072e-41],\n",
       "                         [ 3.2823e-41,  4.0957e-41, -1.7949e-41],\n",
       "                         [-8.1193e-41,  6.4484e-41, -7.3386e-42]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.0264e-40,  9.3772e-41,  8.4966e-41],\n",
       "                         [-4.2856e-41, -5.3476e-41,  3.2461e-41],\n",
       "                         [ 6.4963e-41,  4.3876e-41, -1.0203e-40]],\n",
       "               \n",
       "                        [[ 4.1316e-41,  5.1065e-41, -1.5653e-41],\n",
       "                         [ 9.6954e-41, -2.2005e-41, -9.1645e-41],\n",
       "                         [-1.0672e-40,  1.0650e-40, -6.3371e-41]],\n",
       "               \n",
       "                        [[ 6.7480e-41, -1.0908e-41, -2.3430e-41],\n",
       "                         [ 3.1854e-41,  6.6972e-41, -3.9369e-41],\n",
       "                         [-2.6674e-41, -3.2552e-41, -9.0249e-41]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 3.4034e-02, -2.0209e-02,  1.8817e-02],\n",
       "                         [ 3.7021e-02,  6.3864e-02,  6.3841e-02],\n",
       "                         [ 4.0422e-02,  6.3083e-02,  8.6636e-02]],\n",
       "               \n",
       "                        [[ 2.2778e-02, -3.2130e-02,  7.5329e-03],\n",
       "                         [-9.8920e-03, -4.8971e-02,  6.1809e-02],\n",
       "                         [-3.6022e-04, -9.2267e-03,  3.6360e-02]],\n",
       "               \n",
       "                        [[-2.9668e-02, -4.7815e-02, -3.2151e-02],\n",
       "                         [-1.6108e-02, -1.2173e-01, -4.4689e-03],\n",
       "                         [-4.3490e-02, -7.7332e-02, -5.2833e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 9.1326e-03, -9.5109e-04, -2.5212e-02],\n",
       "                         [ 1.0187e-02, -1.0055e-02,  6.9192e-03],\n",
       "                         [-6.7484e-02, -2.3608e-02, -1.6817e-02]],\n",
       "               \n",
       "                        [[ 3.1678e-02,  7.6592e-02,  6.7088e-02],\n",
       "                         [ 8.1835e-02,  1.0917e-01,  8.4107e-02],\n",
       "                         [ 4.3125e-02,  4.7159e-02,  7.8229e-02]],\n",
       "               \n",
       "                        [[ 1.8537e-02, -6.8667e-02, -7.9621e-02],\n",
       "                         [ 9.3532e-02, -4.9015e-03, -8.0446e-02],\n",
       "                         [-2.7581e-02, -5.3793e-02, -3.6549e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-4.6716e-02,  2.0428e-02,  1.3194e-01],\n",
       "                         [-3.4882e-02, -3.7550e-02,  9.8097e-02],\n",
       "                         [-4.0949e-02, -2.4712e-02,  7.1155e-02]],\n",
       "               \n",
       "                        [[-6.8312e-02,  1.5085e-03,  5.9356e-02],\n",
       "                         [-9.9608e-02, -2.0292e-02, -2.5800e-03],\n",
       "                         [-5.8407e-02,  3.3079e-03, -5.8115e-02]],\n",
       "               \n",
       "                        [[ 1.7599e-02, -3.2252e-02, -1.4794e-02],\n",
       "                         [ 4.8241e-03,  6.4265e-04, -2.7330e-03],\n",
       "                         [ 2.1238e-02, -1.4842e-02, -1.9287e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 7.1591e-02,  3.5314e-02, -4.6350e-02],\n",
       "                         [ 1.0566e-01,  2.0328e-02, -3.8883e-02],\n",
       "                         [ 1.0232e-01, -9.5740e-05, -1.2696e-01]],\n",
       "               \n",
       "                        [[ 7.0752e-03,  1.1803e-02,  1.1607e-02],\n",
       "                         [-1.6683e-02,  2.9059e-02,  5.1630e-02],\n",
       "                         [ 2.0730e-02,  2.9859e-02, -3.2905e-02]],\n",
       "               \n",
       "                        [[-2.1340e-02, -3.0131e-03,  1.4636e-02],\n",
       "                         [-7.9548e-02, -6.8567e-02, -4.3449e-02],\n",
       "                         [-4.1944e-02, -1.8841e-02, -3.8099e-02]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.bias',\n",
       "               tensor([ 8.8197e-05,  1.7611e-04,  5.2709e-05, -2.9567e-05,  4.2591e-13,\n",
       "                        3.1085e-05,  3.1276e-05, -3.9083e-05, -4.9238e-08, -2.5409e-05,\n",
       "                        2.4432e-05,  2.4205e-12,  8.1547e-05,  3.3890e-05, -4.3970e-05,\n",
       "                        1.1351e-05,  2.1437e-06, -8.6770e-07, -3.3877e-15,  8.1770e-05,\n",
       "                        1.4308e-05,  1.5430e-05,  2.7058e-05, -5.5837e-07, -3.2088e-05,\n",
       "                        3.1651e-05,  5.3402e-06,  7.0709e-05, -2.3947e-05, -4.1778e-05,\n",
       "                        3.1264e-05,  3.3522e-05, -4.7877e-06, -1.2296e-05,  5.8926e-05,\n",
       "                        3.3304e-11,  4.5101e-05,  4.4228e-05, -4.2635e-05,  1.2956e-06,\n",
       "                       -2.2192e-05, -1.2338e-10, -8.8040e-07, -1.4594e-05, -4.8801e-06,\n",
       "                       -4.7495e-05, -3.4297e-08, -1.9105e-05, -1.6965e-16, -2.8133e-05,\n",
       "                       -3.6368e-05,  7.4572e-06, -4.4086e-05, -9.3457e-06, -2.6907e-05,\n",
       "                       -6.5329e-05, -1.1920e-04, -3.9765e-06,  6.8209e-05,  3.6231e-05,\n",
       "                        3.5507e-16, -1.1661e-08,  4.4020e-06,  7.0575e-06, -6.8438e-05,\n",
       "                        3.8895e-05,  9.0476e-05,  1.3272e-05,  1.0380e-04, -2.0941e-10,\n",
       "                       -1.5433e-05,  4.8178e-11, -1.8469e-05, -6.3452e-06, -1.5128e-06,\n",
       "                       -3.4119e-10,  1.8696e-04,  2.1366e-05,  5.3618e-07, -2.8837e-05,\n",
       "                       -4.0265e-05, -2.1184e-12, -4.3234e-05, -4.9661e-05,  1.8459e-07,\n",
       "                       -3.1076e-08,  8.0951e-07,  2.8130e-05,  9.1416e-09, -3.0472e-05,\n",
       "                       -3.1140e-06,  3.8111e-05, -1.1637e-05,  6.2929e-06, -1.8943e-13,\n",
       "                       -4.1240e-05,  5.1365e-15, -2.2939e-05, -2.8196e-05,  9.2089e-05,\n",
       "                       -5.0984e-05,  9.9956e-05,  2.8233e-05, -2.3620e-06, -1.1397e-06,\n",
       "                        1.4392e-05, -6.1828e-05, -5.1258e-05,  7.3558e-05, -1.9319e-05,\n",
       "                       -8.1564e-06, -2.2633e-05,  1.1418e-05, -3.7497e-05, -4.8424e-05,\n",
       "                       -4.5802e-05,  6.8949e-05,  2.6802e-05, -3.4893e-16,  3.9315e-05,\n",
       "                       -6.6685e-06, -1.3706e-05,  5.4457e-05, -2.0136e-05,  2.4750e-15,\n",
       "                        4.8029e-15, -4.0854e-05,  3.1987e-06], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.bias',\n",
       "               tensor([-2.1105e-01, -4.8360e-01, -2.5551e-01, -3.7419e-01, -4.8131e-10,\n",
       "                       -2.3585e-01, -3.5467e-01, -2.9406e-01, -5.9318e-14, -2.7018e-01,\n",
       "                       -2.4300e-01, -1.5253e-20, -2.8372e-01, -1.8035e-01, -2.4085e-01,\n",
       "                       -5.3349e-01, -3.7404e-01, -2.9141e-01, -6.8099e-17, -3.5749e-01,\n",
       "                       -8.0989e-01, -3.0418e-01, -2.7074e-01, -3.1362e-01, -3.9297e-01,\n",
       "                       -6.8815e-02, -2.0312e-01, -3.4881e-01, -2.3731e-01, -2.2689e-01,\n",
       "                       -4.1194e-01, -3.5225e-01, -3.0742e-01, -2.4959e-01, -5.1196e-01,\n",
       "                       -3.0969e-24, -2.5375e-01, -3.1315e-01, -3.5726e-01, -1.8417e-01,\n",
       "                       -2.4326e-01, -4.0058e-11, -3.2204e-01, -1.9977e-01, -3.8840e-01,\n",
       "                       -2.5367e-01, -5.4280e-03, -2.1171e-01, -4.0766e-15, -1.0654e-01,\n",
       "                       -2.7014e-01, -2.2091e-01, -3.5437e-01, -2.9638e-01, -2.0415e-01,\n",
       "                       -3.3050e-01, -8.3107e-02, -2.7628e-01, -2.2991e-01, -2.0679e-01,\n",
       "                       -2.5604e-17, -3.4685e-03, -2.9153e-01, -4.8578e-01, -4.0855e-01,\n",
       "                       -4.0533e-01, -2.9203e-01, -6.2929e-01, -4.3898e-01, -1.9358e-14,\n",
       "                       -3.4924e-01, -7.9128e-07, -2.3063e-01,  1.7045e-01, -2.8279e-01,\n",
       "                        1.1345e-04, -3.4716e-01, -2.7422e-23, -2.3012e-01, -6.2627e-01,\n",
       "                       -3.1338e-01, -2.6658e-37, -5.7998e-01, -3.4196e-01, -4.9730e-02,\n",
       "                        8.4829e-17, -1.9149e-01, -2.4177e-01, -2.5768e-02, -4.7937e-01,\n",
       "                       -2.4567e-01, -2.4967e-01, -5.5108e-01, -1.5384e-01, -3.3705e-23,\n",
       "                       -3.2359e-01, -6.5856e-08, -2.9193e-01, -1.6978e-01, -1.2025e-01,\n",
       "                       -4.5889e-01, -3.6219e-01, -3.4273e-01, -4.8740e-01, -4.5649e-06,\n",
       "                       -2.5947e-18, -2.5467e-01, -6.4756e-01, -6.2178e-01, -4.9933e-01,\n",
       "                       -4.9825e-01, -3.7825e-01, -2.5345e-01, -4.3999e-01, -5.2751e-01,\n",
       "                       -1.5333e-01, -3.2037e-01, -3.9978e-01, -1.7009e-05, -2.4273e-01,\n",
       "                       -5.0533e-02, -3.4984e-01, -2.1820e-01, -2.3101e-01,  1.0038e-39,\n",
       "                       -2.0488e-24, -3.7612e-01, -4.5117e-01], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.weight',\n",
       "               tensor([ 2.4809e-01,  4.8370e-01,  3.1430e-01,  3.1535e-01, -2.3062e-06,\n",
       "                        2.3984e-01,  2.8252e-01,  1.9189e-01,  6.1575e-20,  2.3869e-01,\n",
       "                        2.6349e-01, -2.8878e-28,  2.7616e-01,  2.7463e-01,  2.2814e-01,\n",
       "                        3.7519e-01,  3.4780e-01,  3.1301e-01, -7.2444e-15,  3.2133e-01,\n",
       "                        5.5093e-01,  2.7933e-01,  2.3631e-01,  2.9227e-01,  3.6441e-01,\n",
       "                        1.9251e-01,  1.3644e-01,  3.6063e-01,  2.5785e-01,  2.4553e-01,\n",
       "                        3.5464e-01,  2.9820e-01,  2.0016e-01,  3.0496e-01,  3.8011e-01,\n",
       "                        1.5948e-32,  2.6193e-01,  2.5691e-01,  3.1298e-01,  9.7388e-02,\n",
       "                        2.2945e-01,  1.4179e-07,  2.6229e-01,  2.4915e-01,  3.3146e-01,\n",
       "                        2.6434e-01, -3.4223e-03,  2.8146e-01, -2.4948e-22,  1.8788e-01,\n",
       "                        2.8792e-01,  2.7144e-01,  3.5006e-01,  2.8066e-01,  2.2827e-01,\n",
       "                        3.3882e-01,  2.2805e-01,  2.7003e-01,  3.2154e-01,  2.5066e-01,\n",
       "                        2.4200e-11,  1.2681e-02,  2.1988e-01,  3.9849e-01,  3.4569e-01,\n",
       "                        4.1874e-01,  2.5860e-01,  4.8495e-01,  4.1823e-01,  5.4649e-11,\n",
       "                        2.5012e-01,  4.8165e-05,  2.0985e-01,  1.4875e-01,  2.6421e-01,\n",
       "                        1.3114e-10,  3.1844e-01,  1.8922e-24,  1.0271e-01,  4.6217e-01,\n",
       "                        2.6840e-01,  5.6458e-42,  4.8933e-01,  2.7875e-01,  1.1412e-02,\n",
       "                        1.6095e-12,  2.4751e-01,  1.9143e-01,  5.1199e-03,  3.9879e-01,\n",
       "                        2.5766e-01,  2.6225e-01,  4.5242e-01,  2.7418e-01,  3.6218e-18,\n",
       "                        3.2769e-01, -1.8524e-06,  3.0242e-01,  2.3736e-01,  1.7575e-01,\n",
       "                        3.6879e-01,  3.3297e-01,  2.8854e-01,  3.8234e-01,  1.0201e-05,\n",
       "                        5.0186e-15,  2.4384e-01,  5.3131e-01,  4.4623e-01,  3.6832e-01,\n",
       "                        4.4060e-01,  3.4849e-01,  2.4420e-01,  4.1531e-01,  4.2609e-01,\n",
       "                        2.4116e-01,  2.9685e-01,  3.6600e-01,  2.6081e-10,  1.6624e-01,\n",
       "                        6.4506e-02,  3.5733e-01,  2.5590e-01,  2.8445e-01, -1.3490e-41,\n",
       "                        3.2908e-31,  3.5414e-01,  3.9184e-01], device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.weights',\n",
       "               tensor([[-0.0130,  0.0218,  0.0248,  ...,  0.0218,  0.0112,  0.0050],\n",
       "                       [-0.0043, -0.0182, -0.0021,  ..., -0.0092,  0.0130,  0.0280],\n",
       "                       [ 0.0116, -0.0080,  0.0072,  ..., -0.0071, -0.0088, -0.0237],\n",
       "                       [ 0.0058, -0.0011, -0.0234,  ..., -0.0164,  0.0220,  0.0011],\n",
       "                       [ 0.0007,  0.0038, -0.0072,  ...,  0.0101, -0.0346, -0.0111]],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.bias',\n",
       "               tensor([-0.0082,  0.0263,  0.0032,  0.0128, -0.0059], device='cuda:0')),\n",
       "              ('classifier.prompt.prompt_dict.pad_up',\n",
       "               tensor([[[ 0.0165,  0.0180, -0.0048,  ..., -0.0492, -0.0508, -0.0325],\n",
       "                        [-0.0068, -0.0260,  0.0304,  ...,  0.0027,  0.0373,  0.0110],\n",
       "                        [ 0.0037, -0.0095, -0.0083,  ..., -0.0012, -0.0223, -0.0209],\n",
       "                        [-0.0387,  0.0251, -0.0200,  ...,  0.0788,  0.0105, -0.0272],\n",
       "                        [-0.0085,  0.0298,  0.0285,  ...,  0.0149,  0.0232,  0.0159]],\n",
       "               \n",
       "                       [[ 0.0392, -0.0322, -0.0700,  ..., -0.0372, -0.0431, -0.0221],\n",
       "                        [ 0.0215, -0.0172,  0.0077,  ...,  0.0093,  0.0624,  0.0139],\n",
       "                        [ 0.0284,  0.0010, -0.0034,  ..., -0.0253,  0.0377, -0.0085],\n",
       "                        [-0.0191,  0.0126, -0.0132,  ..., -0.0314, -0.0348, -0.0140],\n",
       "                        [-0.0442, -0.0084, -0.0064,  ..., -0.0370, -0.0410,  0.0136]],\n",
       "               \n",
       "                       [[ 0.0074,  0.0185, -0.0052,  ..., -0.0241, -0.0022,  0.0019],\n",
       "                        [ 0.0153,  0.0236,  0.0068,  ..., -0.0178,  0.0225, -0.0238],\n",
       "                        [-0.0136,  0.0548,  0.0131,  ..., -0.0233,  0.0424, -0.0179],\n",
       "                        [-0.0216,  0.0503, -0.0313,  ...,  0.0179,  0.0179,  0.0162],\n",
       "                        [ 0.0045,  0.0514,  0.0212,  ..., -0.0214, -0.0087, -0.0053]]],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.prompt.prompt_dict.pad_down',\n",
       "               tensor([[[-0.0494,  0.0307, -0.0013,  ...,  0.0119,  0.0106, -0.0052],\n",
       "                        [-0.0426,  0.0403, -0.0031,  ..., -0.0172,  0.0108, -0.0181],\n",
       "                        [-0.0155,  0.0176,  0.0058,  ..., -0.0342,  0.0215, -0.0158],\n",
       "                        [-0.0034,  0.0327,  0.0307,  ...,  0.0032,  0.0503, -0.0308],\n",
       "                        [-0.0114, -0.0292, -0.0390,  ...,  0.0144,  0.0190, -0.0432]],\n",
       "               \n",
       "                       [[ 0.0073,  0.0038, -0.0178,  ...,  0.0241,  0.0109,  0.0304],\n",
       "                        [-0.0081, -0.0377,  0.0374,  ...,  0.0057,  0.0026,  0.0248],\n",
       "                        [-0.0221, -0.0423, -0.0143,  ...,  0.0184,  0.0072, -0.0129],\n",
       "                        [-0.0208,  0.0232, -0.0112,  ...,  0.0365,  0.0060, -0.0332],\n",
       "                        [-0.0242, -0.0139, -0.0317,  ...,  0.0022, -0.0580, -0.0286]],\n",
       "               \n",
       "                       [[-0.0319,  0.0338,  0.0119,  ..., -0.0019,  0.0204,  0.0064],\n",
       "                        [-0.0017,  0.0184,  0.0169,  ..., -0.0093, -0.0137, -0.0211],\n",
       "                        [ 0.0245,  0.0553,  0.0092,  ..., -0.0050,  0.0265, -0.0220],\n",
       "                        [-0.0079,  0.0505, -0.0063,  ...,  0.0198,  0.0319,  0.0123],\n",
       "                        [ 0.0110,  0.0294, -0.0293,  ...,  0.0035, -0.0112, -0.0001]]],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.prompt.prompt_dict.pad_left',\n",
       "               tensor([[[ 0.0041,  0.0435, -0.0136,  0.0005, -0.0232],\n",
       "                        [-0.0108,  0.0008, -0.0292,  0.0348, -0.0303],\n",
       "                        [-0.0445, -0.0119, -0.0177,  0.0410, -0.0045],\n",
       "                        ...,\n",
       "                        [-0.0433,  0.0012,  0.0191, -0.0038,  0.0166],\n",
       "                        [ 0.0028,  0.0397,  0.0317,  0.0157,  0.0078],\n",
       "                        [-0.0118, -0.0016, -0.0260,  0.0253, -0.0061]],\n",
       "               \n",
       "                       [[-0.0584,  0.0117, -0.0114,  0.0059,  0.0124],\n",
       "                        [ 0.0186,  0.0162, -0.0241,  0.0396,  0.0027],\n",
       "                        [ 0.0042, -0.0039,  0.0177,  0.0095,  0.0367],\n",
       "                        ...,\n",
       "                        [-0.0038, -0.0152,  0.0098, -0.0032,  0.0275],\n",
       "                        [-0.0311, -0.0251, -0.0012, -0.0068,  0.0076],\n",
       "                        [-0.0134,  0.0092, -0.0184,  0.0061,  0.0073]],\n",
       "               \n",
       "                       [[-0.0095,  0.0046, -0.0072,  0.0209,  0.0150],\n",
       "                        [-0.0057, -0.0008, -0.0279,  0.0456, -0.0377],\n",
       "                        [ 0.0357, -0.0005,  0.0136, -0.0011, -0.0490],\n",
       "                        ...,\n",
       "                        [-0.0232, -0.0277,  0.0045, -0.0496, -0.0273],\n",
       "                        [ 0.0188,  0.0073,  0.0216,  0.0135, -0.0184],\n",
       "                        [ 0.0349, -0.0031, -0.0081,  0.0144, -0.0350]]], device='cuda:0')),\n",
       "              ('classifier.prompt.prompt_dict.pad_right',\n",
       "               tensor([[[-4.9766e-03,  1.6174e-02,  2.3082e-02, -1.7477e-02,  4.4506e-03],\n",
       "                        [ 4.6541e-02,  2.0551e-03,  1.5671e-02,  3.5107e-03,  3.1445e-03],\n",
       "                        [-6.0935e-03, -3.3772e-02,  2.3914e-02,  5.9967e-02,  5.9848e-03],\n",
       "                        ...,\n",
       "                        [ 2.3061e-02, -8.1392e-03,  4.1973e-02, -5.8299e-02, -7.4491e-02],\n",
       "                        [ 5.8435e-03, -1.6776e-02, -3.3613e-02, -4.4131e-03, -6.7435e-03],\n",
       "                        [ 2.7115e-02, -2.0748e-02, -3.8655e-02, -2.0290e-02, -4.5693e-02]],\n",
       "               \n",
       "                       [[-5.1869e-02,  1.1210e-02,  7.8248e-04, -1.4036e-02,  1.1135e-02],\n",
       "                        [ 1.1027e-02,  2.3755e-02,  2.1027e-02,  7.7971e-03,  5.7890e-03],\n",
       "                        [ 3.2935e-02,  1.2798e-03,  1.6193e-02, -1.7308e-02, -2.8065e-02],\n",
       "                        ...,\n",
       "                        [-2.9979e-03,  3.2814e-02,  3.9594e-02, -2.7325e-02, -1.6479e-02],\n",
       "                        [-3.9568e-02, -1.8452e-02,  3.0135e-02,  3.2085e-02,  4.7346e-02],\n",
       "                        [-8.1407e-03, -4.5112e-02,  3.4026e-03,  1.7324e-02, -2.9222e-02]],\n",
       "               \n",
       "                       [[-3.7439e-02, -7.4868e-03,  2.1709e-02,  1.4908e-02, -1.9136e-02],\n",
       "                        [ 2.4273e-02, -1.5803e-02,  8.7874e-03,  1.7101e-02, -1.1995e-02],\n",
       "                        [-5.9885e-04, -3.4585e-02, -5.8995e-03, -2.6212e-04, -3.5985e-02],\n",
       "                        ...,\n",
       "                        [ 2.9098e-02,  1.2480e-02,  3.0382e-02, -7.2367e-04,  1.5416e-02],\n",
       "                        [ 1.4330e-02,  2.5508e-02,  2.5987e-02,  5.6592e-02,  3.5449e-02],\n",
       "                        [-7.5961e-03, -3.0121e-02,  4.0476e-05, -4.2225e-03, -1.0821e-02]]],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.prompt_learning_rates_dict.prompt_weight_learning_rate',\n",
       "               tensor([-2.5127e-01, -3.8855e-01, -5.5575e-01, -4.1115e-01,  4.7584e-01,\n",
       "                       -1.1517e-14], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv0-conv-weight',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv0-conv-bias',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv1-conv-weight',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv1-conv-bias',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv2-conv-weight',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv2-conv-bias',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv3-conv-weight',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv3-conv-bias',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-weights',\n",
       "               tensor([ 2.7542e-01,  4.1222e-01,  6.2804e-01,  9.4693e-01,  9.9613e-01,\n",
       "                       -1.1517e-14], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-bias',\n",
       "               tensor([ 7.7784e-02,  2.1001e-02, -9.9159e-02, -1.4604e-01,  8.0976e-02,\n",
       "                       -1.1517e-14], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv0-conv-weight',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv0-conv-bias',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv1-conv-weight',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv1-conv-bias',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv2-conv-weight',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv2-conv-bias',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv3-conv-weight',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv3-conv-bias',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-linear-weights',\n",
       "               tensor([ 9.8530e-01, -1.6464e-01, -6.2858e-01, -1.0163e+00, -1.8705e+00,\n",
       "                       -1.7893e-16], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-linear-bias',\n",
       "               tensor([ 1.5738e-01,  1.5528e-01,  1.6393e-01,  1.0478e-03, -2.3904e-02,\n",
       "                       -1.7893e-16], device='cuda:0'))]),\n",
       " 'per_epoch_statistics': {'train_loss_mean': [1.4976510803699494,\n",
       "   1.2723990025520324,\n",
       "   1.149865487933159,\n",
       "   1.0803588899374008,\n",
       "   1.0244843945503235,\n",
       "   0.9830393017530441,\n",
       "   0.9662152271270752,\n",
       "   0.9316681063175202,\n",
       "   0.9006312277317047,\n",
       "   0.8990076937675476,\n",
       "   0.8551296761035919,\n",
       "   0.835764122724533,\n",
       "   0.8214213171601296,\n",
       "   0.7810179371833801,\n",
       "   0.7642036426067352,\n",
       "   0.7454685520529747,\n",
       "   0.7263362491726876,\n",
       "   0.7124306520223618,\n",
       "   0.7121403040289879,\n",
       "   0.691262623667717,\n",
       "   0.6682949922680855,\n",
       "   0.6550501520633698,\n",
       "   0.6466851117610931,\n",
       "   0.6390114841461182,\n",
       "   0.628437025487423,\n",
       "   0.612734661579132,\n",
       "   0.5955614692568779,\n",
       "   0.593463381767273,\n",
       "   0.5860638425946235,\n",
       "   0.5900722116827964,\n",
       "   0.5682616020441056,\n",
       "   0.5652016410827637,\n",
       "   0.5647661657035351,\n",
       "   0.5525010676681995,\n",
       "   0.5628233806490898,\n",
       "   0.5334728328883648,\n",
       "   0.5348228128552437,\n",
       "   0.5347045143842697,\n",
       "   0.5377095646858215,\n",
       "   0.5232856628000736,\n",
       "   0.5354481998383999,\n",
       "   0.5195477747619152,\n",
       "   0.5174692838490009,\n",
       "   0.5224674625992775,\n",
       "   0.5118796863257885,\n",
       "   0.5086849994957447,\n",
       "   0.5078493676185608,\n",
       "   0.5080788137316704,\n",
       "   0.49459001812338826,\n",
       "   0.49935533314943314,\n",
       "   0.5025602996349334,\n",
       "   0.4895026246905327,\n",
       "   0.5084436446726323,\n",
       "   0.48725895869731906,\n",
       "   0.4992180797755718,\n",
       "   0.49882310223579407,\n",
       "   0.48936404660344124,\n",
       "   0.4856445203125477,\n",
       "   0.48712728586792947,\n",
       "   0.49343866991996765,\n",
       "   0.49181216350197793,\n",
       "   0.48035408902168275,\n",
       "   0.4760382098555565,\n",
       "   0.4711251770555973,\n",
       "   0.4764563598632813,\n",
       "   0.4850903322696686,\n",
       "   0.487377430409193,\n",
       "   0.46825909623503686,\n",
       "   0.47290335229039193,\n",
       "   0.4807812477350235,\n",
       "   0.4729124873280525,\n",
       "   0.47127995762228964,\n",
       "   0.4824578228592873,\n",
       "   0.46721055611968043,\n",
       "   0.4755796495974064,\n",
       "   0.467760968118906,\n",
       "   0.4735359154343605,\n",
       "   0.4697905479669571,\n",
       "   0.4638237407505512,\n",
       "   0.4714756619930267,\n",
       "   0.456724684625864,\n",
       "   0.46350153523683546,\n",
       "   0.46717258313298227,\n",
       "   0.4691076202690601,\n",
       "   0.4571463318169117,\n",
       "   0.4593167600631714,\n",
       "   0.45908885437250135,\n",
       "   0.46374440282583235,\n",
       "   0.4533725802898407,\n",
       "   0.4509115675985813,\n",
       "   0.4587677549123764,\n",
       "   0.458233254134655,\n",
       "   0.4523784237205982,\n",
       "   0.4488631829917431,\n",
       "   0.4608990956544876,\n",
       "   0.46040881767868996,\n",
       "   0.46530788305401805,\n",
       "   0.4628433865010738,\n",
       "   0.44454613837599755],\n",
       "  'train_loss_std': [0.4557614689448639,\n",
       "   0.13141793115607747,\n",
       "   0.14353214666842817,\n",
       "   0.1453511363731022,\n",
       "   0.1505240785359527,\n",
       "   0.14539962636200524,\n",
       "   0.1465876421076358,\n",
       "   0.1463633869530955,\n",
       "   0.15081325877759155,\n",
       "   0.14640548127378814,\n",
       "   0.15068942437926747,\n",
       "   0.1534095778659748,\n",
       "   0.14623897280551157,\n",
       "   0.15040416244634827,\n",
       "   0.14010202097010335,\n",
       "   0.14969468774852246,\n",
       "   0.1338893413499347,\n",
       "   0.14395724654384137,\n",
       "   0.1481195636540352,\n",
       "   0.14470117564065035,\n",
       "   0.14488152441412205,\n",
       "   0.14099616109464,\n",
       "   0.14830460005267465,\n",
       "   0.14555378193200585,\n",
       "   0.13774962150215164,\n",
       "   0.1340889678438233,\n",
       "   0.13645389094622612,\n",
       "   0.14346907459339603,\n",
       "   0.1402821936516029,\n",
       "   0.1462924378784989,\n",
       "   0.14354406999892141,\n",
       "   0.1369803569872649,\n",
       "   0.13711575075255134,\n",
       "   0.14539195861629253,\n",
       "   0.13641060887007667,\n",
       "   0.13822739721448077,\n",
       "   0.13044201516241818,\n",
       "   0.1411436723280037,\n",
       "   0.14141449908006548,\n",
       "   0.1428723756921391,\n",
       "   0.1343359679795103,\n",
       "   0.13187057889489207,\n",
       "   0.1361499739225651,\n",
       "   0.1412018892340872,\n",
       "   0.13543019194339032,\n",
       "   0.12783082353611722,\n",
       "   0.13673238993853734,\n",
       "   0.13307726698897862,\n",
       "   0.13435151710376328,\n",
       "   0.133546613401235,\n",
       "   0.13106237541976964,\n",
       "   0.12611257819183866,\n",
       "   0.1332148996094292,\n",
       "   0.14021241422931727,\n",
       "   0.14076087835856077,\n",
       "   0.13232529925631537,\n",
       "   0.12680809177677385,\n",
       "   0.1341987123021176,\n",
       "   0.13263745819089867,\n",
       "   0.12633715882474944,\n",
       "   0.13607887608109007,\n",
       "   0.13673666444235513,\n",
       "   0.13148259098929488,\n",
       "   0.1301478593673482,\n",
       "   0.13229677637252918,\n",
       "   0.1246804085310673,\n",
       "   0.1336029180733379,\n",
       "   0.12503012701433106,\n",
       "   0.13258908870383354,\n",
       "   0.13928106383359287,\n",
       "   0.13188868992661906,\n",
       "   0.12932905731290714,\n",
       "   0.13738170849957218,\n",
       "   0.1289249440984549,\n",
       "   0.13397389613781138,\n",
       "   0.12557493186922522,\n",
       "   0.13297716751025915,\n",
       "   0.12672518850690512,\n",
       "   0.12883678937517593,\n",
       "   0.13836813358388866,\n",
       "   0.12787666111569024,\n",
       "   0.13038139411164903,\n",
       "   0.1336224569252491,\n",
       "   0.13538920929336093,\n",
       "   0.12817808676989878,\n",
       "   0.12355501083483358,\n",
       "   0.13141152302920853,\n",
       "   0.1328793271999396,\n",
       "   0.12821537073131864,\n",
       "   0.12244610353398684,\n",
       "   0.12474765784015097,\n",
       "   0.13229400302763125,\n",
       "   0.12381411205768089,\n",
       "   0.1253342712750095,\n",
       "   0.12968928367431173,\n",
       "   0.13858509660762877,\n",
       "   0.12226221173097379,\n",
       "   0.12912182169094588,\n",
       "   0.12762271032617384],\n",
       "  'train_accuracy_mean': [0.39316000017523767,\n",
       "   0.47659999883174897,\n",
       "   0.540666666507721,\n",
       "   0.5744933313727378,\n",
       "   0.5994666655659675,\n",
       "   0.6161733310818672,\n",
       "   0.6253599992990494,\n",
       "   0.6389199983477593,\n",
       "   0.6514133330583572,\n",
       "   0.6525866664648056,\n",
       "   0.670546666264534,\n",
       "   0.6788399993777275,\n",
       "   0.684880000114441,\n",
       "   0.7039600009322167,\n",
       "   0.7083333337306976,\n",
       "   0.7189466663002968,\n",
       "   0.7264533333778381,\n",
       "   0.733720000743866,\n",
       "   0.7304533348083496,\n",
       "   0.7406666666269303,\n",
       "   0.7489200000166893,\n",
       "   0.7546399990320206,\n",
       "   0.7581199994087219,\n",
       "   0.7605066665410996,\n",
       "   0.7649866669178009,\n",
       "   0.770506667137146,\n",
       "   0.778533332824707,\n",
       "   0.7785866671800613,\n",
       "   0.7805199996232987,\n",
       "   0.7791733338832855,\n",
       "   0.7875066665410996,\n",
       "   0.7902933324575424,\n",
       "   0.7892399990558624,\n",
       "   0.7933733334541321,\n",
       "   0.7888400001525879,\n",
       "   0.8014933344125748,\n",
       "   0.7989333343505859,\n",
       "   0.800533333659172,\n",
       "   0.8021600004434586,\n",
       "   0.8054933331012726,\n",
       "   0.8011599993705749,\n",
       "   0.8062533338069916,\n",
       "   0.8090399990081787,\n",
       "   0.8059066656827927,\n",
       "   0.8087199988365174,\n",
       "   0.8113066675662994,\n",
       "   0.8115200008153916,\n",
       "   0.8121333335638046,\n",
       "   0.8165733336210251,\n",
       "   0.8155199987888336,\n",
       "   0.8131466668844223,\n",
       "   0.8198133335113525,\n",
       "   0.8107466651201248,\n",
       "   0.8180666679143905,\n",
       "   0.815120000064373,\n",
       "   0.8158933329582214,\n",
       "   0.8181999993324279,\n",
       "   0.8208933335542679,\n",
       "   0.8183333337306976,\n",
       "   0.8180800001621247,\n",
       "   0.8192000005245209,\n",
       "   0.8223066666126251,\n",
       "   0.8238133320808411,\n",
       "   0.8244933340549468,\n",
       "   0.8245466668605804,\n",
       "   0.8198799991607666,\n",
       "   0.8195999989509583,\n",
       "   0.8276400003433227,\n",
       "   0.8248800007104874,\n",
       "   0.8224400004148483,\n",
       "   0.8254400005340576,\n",
       "   0.8259066679477691,\n",
       "   0.8233066667318344,\n",
       "   0.8288266662359237,\n",
       "   0.8239600012302398,\n",
       "   0.8262799997329712,\n",
       "   0.8240400000810623,\n",
       "   0.8258133325576782,\n",
       "   0.8281466664075852,\n",
       "   0.8263866666555405,\n",
       "   0.8320666675567627,\n",
       "   0.8292533344030381,\n",
       "   0.8279733341932297,\n",
       "   0.8259466677904129,\n",
       "   0.8320933346748352,\n",
       "   0.8287866683006286,\n",
       "   0.8304800000190735,\n",
       "   0.8293066685199737,\n",
       "   0.833600001335144,\n",
       "   0.8347866669893265,\n",
       "   0.8317866678237915,\n",
       "   0.8299199992418289,\n",
       "   0.833546666264534,\n",
       "   0.8346533342599869,\n",
       "   0.8292266670465469,\n",
       "   0.8302133334875107,\n",
       "   0.8272666653394699,\n",
       "   0.8297466659545898,\n",
       "   0.8362533336877823],\n",
       "  'train_accuracy_std': [0.08023225348198769,\n",
       "   0.07302051326229277,\n",
       "   0.07262016937209596,\n",
       "   0.07444721312637284,\n",
       "   0.07494875332466754,\n",
       "   0.07075482655707768,\n",
       "   0.07100847685605005,\n",
       "   0.06971043498521075,\n",
       "   0.07110463567031251,\n",
       "   0.07122155040581685,\n",
       "   0.07095766472608227,\n",
       "   0.07184171594039666,\n",
       "   0.06953565919107497,\n",
       "   0.0680970436103885,\n",
       "   0.06372841674997183,\n",
       "   0.06674296761757868,\n",
       "   0.06021848210005573,\n",
       "   0.0626732743681445,\n",
       "   0.06851613762411828,\n",
       "   0.06530407588391789,\n",
       "   0.06428262416885545,\n",
       "   0.06300672970274701,\n",
       "   0.06376710171270475,\n",
       "   0.06309013816569561,\n",
       "   0.06153029751186062,\n",
       "   0.059973411134197185,\n",
       "   0.05967303449898159,\n",
       "   0.061714415322417546,\n",
       "   0.06134634336630458,\n",
       "   0.0616818449785077,\n",
       "   0.061480303317675455,\n",
       "   0.05911798528100751,\n",
       "   0.0596020144535523,\n",
       "   0.06362737624490016,\n",
       "   0.05769487780789282,\n",
       "   0.0596515164570011,\n",
       "   0.05812109888258728,\n",
       "   0.06034036790465073,\n",
       "   0.05856013957233421,\n",
       "   0.05884103915816611,\n",
       "   0.05825908520619749,\n",
       "   0.056075408466552686,\n",
       "   0.05632022055637328,\n",
       "   0.06090466384123776,\n",
       "   0.05681671766018708,\n",
       "   0.054793179995031846,\n",
       "   0.05751058447403682,\n",
       "   0.05675506872680595,\n",
       "   0.05724248788260089,\n",
       "   0.055404941242533215,\n",
       "   0.05638408595971426,\n",
       "   0.05206287622863755,\n",
       "   0.05603548811249614,\n",
       "   0.05918629740533593,\n",
       "   0.059409006524613776,\n",
       "   0.05680553447562239,\n",
       "   0.0543736470241109,\n",
       "   0.057452606469965556,\n",
       "   0.05557437430846981,\n",
       "   0.05305806395856451,\n",
       "   0.057141384717101826,\n",
       "   0.058107290562327704,\n",
       "   0.05514136123187972,\n",
       "   0.05614889959320956,\n",
       "   0.05449276462845608,\n",
       "   0.052875399792642684,\n",
       "   0.05636090029915113,\n",
       "   0.05194214236191864,\n",
       "   0.057244962971260506,\n",
       "   0.05820425651947633,\n",
       "   0.05495296981023136,\n",
       "   0.055547780017900075,\n",
       "   0.056000789191203826,\n",
       "   0.05464350153535402,\n",
       "   0.05541647214433491,\n",
       "   0.05369818364793352,\n",
       "   0.05522912511493956,\n",
       "   0.05269772013128363,\n",
       "   0.05374703019447509,\n",
       "   0.05812180205825361,\n",
       "   0.05237552776757926,\n",
       "   0.056940302139116536,\n",
       "   0.05833908140921911,\n",
       "   0.05755011024822498,\n",
       "   0.05318287382833015,\n",
       "   0.05180170709072269,\n",
       "   0.05609548254147893,\n",
       "   0.05557104928637563,\n",
       "   0.05411855391879418,\n",
       "   0.051045285758313776,\n",
       "   0.05179368382490814,\n",
       "   0.05505143353180604,\n",
       "   0.051427176637063,\n",
       "   0.0526180353838264,\n",
       "   0.05510900220491995,\n",
       "   0.05736161223351689,\n",
       "   0.05194779566955311,\n",
       "   0.05370560133165593,\n",
       "   0.05325751265243395],\n",
       "  'train_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'train_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_learning_rate_mean': [0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005],\n",
       "  'train_learning_rate_std': [4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19],\n",
       "  'val_loss_mean': [1.399358289639155,\n",
       "   1.317028591632843,\n",
       "   1.219091570774714,\n",
       "   1.178765530983607,\n",
       "   1.1397278900941212,\n",
       "   1.1636886370182038,\n",
       "   1.1303802051146825,\n",
       "   1.0896931898593902,\n",
       "   1.0576848671833674,\n",
       "   1.0448989081382751,\n",
       "   0.9949173585573832,\n",
       "   1.0238481760025024,\n",
       "   0.9973815912008286,\n",
       "   0.9911275043090184,\n",
       "   0.9789812447627385,\n",
       "   0.9644921278953552,\n",
       "   0.9865651035308838,\n",
       "   0.9274450318018596,\n",
       "   0.9327361806233724,\n",
       "   0.9273810668786366,\n",
       "   0.905531950990359,\n",
       "   0.886098193526268,\n",
       "   0.907103154361248,\n",
       "   0.8984948728481928,\n",
       "   0.907102237145106,\n",
       "   0.8785417771339417,\n",
       "   0.8695186493794124,\n",
       "   0.8769688312212626,\n",
       "   0.8478982033332189,\n",
       "   0.8692993028958639,\n",
       "   0.8394914522767067,\n",
       "   0.862136828104655,\n",
       "   0.8490982344746589,\n",
       "   0.8681392121315002,\n",
       "   0.8393419290582339,\n",
       "   0.8626659582058589,\n",
       "   0.8417435982823371,\n",
       "   0.8861657543977102,\n",
       "   0.8534678193926811,\n",
       "   0.8443567719062169,\n",
       "   0.8678972451885542,\n",
       "   0.8465119983752568,\n",
       "   0.8437698143720627,\n",
       "   0.8204824555913607,\n",
       "   0.8613307428359985,\n",
       "   0.8821955943107604,\n",
       "   0.8179875301321348,\n",
       "   0.8504859594504038,\n",
       "   0.8282445120811462,\n",
       "   0.8113874783118565,\n",
       "   0.8416965268055598,\n",
       "   0.8614080572128295,\n",
       "   0.837695182065169,\n",
       "   0.8255114630858104,\n",
       "   0.8645178507765134,\n",
       "   0.845224871635437,\n",
       "   0.8324020796020826,\n",
       "   0.8280846043427785,\n",
       "   0.8556665020187696,\n",
       "   0.8323904758691788,\n",
       "   0.8670672326286634,\n",
       "   0.8263543403148651,\n",
       "   0.8496879295508066,\n",
       "   0.8339524973432223,\n",
       "   0.8324207928776741,\n",
       "   0.8385552179813385,\n",
       "   0.8594498284657797,\n",
       "   0.8487109392881393,\n",
       "   0.8405134704709053,\n",
       "   0.8336752869685491,\n",
       "   0.8329970435301463,\n",
       "   0.841186953385671,\n",
       "   0.8573515146970749,\n",
       "   0.8350729414820671,\n",
       "   0.8648404810825984,\n",
       "   0.8354417650898298,\n",
       "   0.8369058627883593,\n",
       "   0.8404220134019852,\n",
       "   0.879786081413428,\n",
       "   0.831205385128657,\n",
       "   0.8266532322764397,\n",
       "   0.8091307077805201,\n",
       "   0.8397306536634763,\n",
       "   0.8255975524584452,\n",
       "   0.85192979991436,\n",
       "   0.8473085675636928,\n",
       "   0.8335419381658237,\n",
       "   0.8578109556436538,\n",
       "   0.8443240795532863,\n",
       "   0.8441021972894669,\n",
       "   0.8512386427323023,\n",
       "   0.8380284085869789,\n",
       "   0.8553869905074437,\n",
       "   0.8313746950030327,\n",
       "   0.8336721170941989,\n",
       "   0.8436605433622996,\n",
       "   0.8465945690870285,\n",
       "   0.8498992280165354,\n",
       "   0.8539997784296671],\n",
       "  'val_loss_std': [0.09019181113900099,\n",
       "   0.10514647766467207,\n",
       "   0.11320548850909229,\n",
       "   0.12058872725560975,\n",
       "   0.1187063148387898,\n",
       "   0.12435671935181995,\n",
       "   0.1260872878415708,\n",
       "   0.12066601051589679,\n",
       "   0.1259879661245665,\n",
       "   0.11881021957146116,\n",
       "   0.1332103709065647,\n",
       "   0.12942368806833227,\n",
       "   0.128865349253737,\n",
       "   0.13188124181746202,\n",
       "   0.1288588706314955,\n",
       "   0.13933447399415,\n",
       "   0.1384007224362492,\n",
       "   0.1364295516867742,\n",
       "   0.13504631881245077,\n",
       "   0.13871749201270023,\n",
       "   0.12721847512172246,\n",
       "   0.13793917596296615,\n",
       "   0.13719912259226855,\n",
       "   0.14324592479305948,\n",
       "   0.13972009986446196,\n",
       "   0.13421520279834837,\n",
       "   0.13956319249213614,\n",
       "   0.13764866786111207,\n",
       "   0.13824145141428465,\n",
       "   0.142708924923466,\n",
       "   0.14298660926686316,\n",
       "   0.14382978911903932,\n",
       "   0.14826155927940238,\n",
       "   0.14201511887716523,\n",
       "   0.14096193070642982,\n",
       "   0.1479413369399458,\n",
       "   0.14179281920123835,\n",
       "   0.1493717798785757,\n",
       "   0.14180823658151492,\n",
       "   0.14407894896062517,\n",
       "   0.14575707490029108,\n",
       "   0.14504281465316457,\n",
       "   0.14195162220224378,\n",
       "   0.14080278124810097,\n",
       "   0.13892111517083752,\n",
       "   0.14356191855186282,\n",
       "   0.14326194492065827,\n",
       "   0.14539407062682455,\n",
       "   0.14330848560934484,\n",
       "   0.1367952361588392,\n",
       "   0.13982359672631894,\n",
       "   0.14523628280083917,\n",
       "   0.14118633214849702,\n",
       "   0.14044364760219763,\n",
       "   0.14117431896577187,\n",
       "   0.14531379530173916,\n",
       "   0.14234510980466591,\n",
       "   0.14315870082992962,\n",
       "   0.1444597263634845,\n",
       "   0.14680562107933387,\n",
       "   0.15065751022959775,\n",
       "   0.14647577432821737,\n",
       "   0.14087806309569034,\n",
       "   0.14509779872035938,\n",
       "   0.14539416046714937,\n",
       "   0.13603197114725615,\n",
       "   0.15395136309577825,\n",
       "   0.14545705897128927,\n",
       "   0.15221217631828474,\n",
       "   0.13789082901365438,\n",
       "   0.15322796572708774,\n",
       "   0.14362192049828634,\n",
       "   0.13992399924492613,\n",
       "   0.1503673191955944,\n",
       "   0.14665074715999143,\n",
       "   0.14323862967920506,\n",
       "   0.1511910983155227,\n",
       "   0.14525562768042083,\n",
       "   0.14939324653911323,\n",
       "   0.1462397200546254,\n",
       "   0.1495596534015839,\n",
       "   0.14860290998687167,\n",
       "   0.14003866751218466,\n",
       "   0.14386756594149855,\n",
       "   0.14267738091454893,\n",
       "   0.1424504309048611,\n",
       "   0.14072099659715367,\n",
       "   0.15601941370067354,\n",
       "   0.14648251276476223,\n",
       "   0.1449866978019328,\n",
       "   0.15301397843593575,\n",
       "   0.1452627684284929,\n",
       "   0.15565429068304112,\n",
       "   0.14773991652322785,\n",
       "   0.14965849342536525,\n",
       "   0.14291597065677464,\n",
       "   0.14476399282017927,\n",
       "   0.1485176793020998,\n",
       "   0.13681181949438181],\n",
       "  'val_accuracy_mean': [0.4078222224116325,\n",
       "   0.4602222233017286,\n",
       "   0.5078666654229164,\n",
       "   0.5294444447755814,\n",
       "   0.5463333323597908,\n",
       "   0.5310222207506498,\n",
       "   0.5492444443702698,\n",
       "   0.5683777751525243,\n",
       "   0.5813555560509364,\n",
       "   0.5876666647195816,\n",
       "   0.6072888882954915,\n",
       "   0.5976888883113861,\n",
       "   0.60813333183527,\n",
       "   0.6144222208857536,\n",
       "   0.6191111096739769,\n",
       "   0.622222220202287,\n",
       "   0.6181777774294217,\n",
       "   0.6412666655580203,\n",
       "   0.638155556221803,\n",
       "   0.6399333323041598,\n",
       "   0.6493999992807706,\n",
       "   0.6563333319624265,\n",
       "   0.6502222218116125,\n",
       "   0.6549111100037893,\n",
       "   0.6493333323796591,\n",
       "   0.6586444436510404,\n",
       "   0.6670666656891505,\n",
       "   0.662066666285197,\n",
       "   0.6768222230672837,\n",
       "   0.6656888883312543,\n",
       "   0.6831777787208557,\n",
       "   0.6728222215175629,\n",
       "   0.6755111093322436,\n",
       "   0.6684444427490235,\n",
       "   0.680933334628741,\n",
       "   0.6698444443941116,\n",
       "   0.6728666659196217,\n",
       "   0.663822222451369,\n",
       "   0.6745555545886358,\n",
       "   0.6778444441159567,\n",
       "   0.669733334183693,\n",
       "   0.6780888911088307,\n",
       "   0.6784666655460994,\n",
       "   0.6872444452842077,\n",
       "   0.6720222227772077,\n",
       "   0.6644222238659858,\n",
       "   0.6883777789274852,\n",
       "   0.6748888878027598,\n",
       "   0.6857999988396962,\n",
       "   0.6896444463729858,\n",
       "   0.677199999888738,\n",
       "   0.6738000017404556,\n",
       "   0.6814888906478882,\n",
       "   0.6842666667699814,\n",
       "   0.6732444457213084,\n",
       "   0.6776222229003906,\n",
       "   0.6805333338181178,\n",
       "   0.6851555541157722,\n",
       "   0.6787999997536341,\n",
       "   0.6794444432854653,\n",
       "   0.6740666665633519,\n",
       "   0.6857333328326544,\n",
       "   0.6742666671673457,\n",
       "   0.684511110385259,\n",
       "   0.6844000001748403,\n",
       "   0.6801111112038295,\n",
       "   0.674377776781718,\n",
       "   0.6765555576483409,\n",
       "   0.6858000010251999,\n",
       "   0.6834222229321798,\n",
       "   0.6848888911803563,\n",
       "   0.6774222226937612,\n",
       "   0.6737555555502573,\n",
       "   0.6871333339810372,\n",
       "   0.6752000009020169,\n",
       "   0.6801555554072062,\n",
       "   0.6880444441239039,\n",
       "   0.6799555569887161,\n",
       "   0.6663111113508542,\n",
       "   0.6842444451649984,\n",
       "   0.6878444437185923,\n",
       "   0.6932222223281861,\n",
       "   0.6812444450457891,\n",
       "   0.6852000011006991,\n",
       "   0.6790888886650404,\n",
       "   0.680066667397817,\n",
       "   0.683333332935969,\n",
       "   0.6829333331187566,\n",
       "   0.6787333327531815,\n",
       "   0.6754000002145767,\n",
       "   0.6803333326180776,\n",
       "   0.683088888724645,\n",
       "   0.6780444434285164,\n",
       "   0.6873333348830541,\n",
       "   0.6828666683038076,\n",
       "   0.6841777787605922,\n",
       "   0.6801777788996697,\n",
       "   0.6806444444259008,\n",
       "   0.674044446349144],\n",
       "  'val_accuracy_std': [0.053873634275849305,\n",
       "   0.05668844849135534,\n",
       "   0.05900443046896896,\n",
       "   0.062679568285379,\n",
       "   0.05689528983737677,\n",
       "   0.06359355511063156,\n",
       "   0.06030925909017131,\n",
       "   0.05763902500462656,\n",
       "   0.06238541689635694,\n",
       "   0.059282437652045224,\n",
       "   0.060952180364582026,\n",
       "   0.05935947861062383,\n",
       "   0.05854973212291565,\n",
       "   0.05810207764198442,\n",
       "   0.05773844742949389,\n",
       "   0.061317227733192176,\n",
       "   0.06103258025869311,\n",
       "   0.06341885590591913,\n",
       "   0.060332458732633244,\n",
       "   0.059857819611802476,\n",
       "   0.057958569858586634,\n",
       "   0.06028604748795994,\n",
       "   0.06068091099920101,\n",
       "   0.06090940150741033,\n",
       "   0.06145398935228629,\n",
       "   0.05943262163916177,\n",
       "   0.05943053129936992,\n",
       "   0.06000020975303737,\n",
       "   0.058173223998904094,\n",
       "   0.060647795825165544,\n",
       "   0.05864499177320622,\n",
       "   0.05817882771221386,\n",
       "   0.06053830908799791,\n",
       "   0.05784099339327864,\n",
       "   0.058671868975055254,\n",
       "   0.0589046143716412,\n",
       "   0.058061947912264524,\n",
       "   0.06041999676700709,\n",
       "   0.06092729848434051,\n",
       "   0.06219638859568255,\n",
       "   0.06084895832126768,\n",
       "   0.06055709921125015,\n",
       "   0.06197147503915563,\n",
       "   0.05763196893044985,\n",
       "   0.05903344726610633,\n",
       "   0.056901470086193534,\n",
       "   0.05840248105533408,\n",
       "   0.06133051627346454,\n",
       "   0.059121883331632354,\n",
       "   0.05781470833393,\n",
       "   0.06097368042288264,\n",
       "   0.05639481371381162,\n",
       "   0.05934711084763681,\n",
       "   0.05776468365549304,\n",
       "   0.059511526685282604,\n",
       "   0.0591605939135543,\n",
       "   0.06018992181402477,\n",
       "   0.06067348740899556,\n",
       "   0.057552911387992914,\n",
       "   0.06412593920904906,\n",
       "   0.06077690839584756,\n",
       "   0.05977503686047786,\n",
       "   0.06045521137302273,\n",
       "   0.061470423976309224,\n",
       "   0.06119951601756138,\n",
       "   0.05880906578997893,\n",
       "   0.06288100109935175,\n",
       "   0.05812746807491397,\n",
       "   0.05858824441999272,\n",
       "   0.05938068755758198,\n",
       "   0.06049262848896245,\n",
       "   0.05948812238985156,\n",
       "   0.057009156590112735,\n",
       "   0.05853207962783138,\n",
       "   0.05904735271395641,\n",
       "   0.058899584092887175,\n",
       "   0.05984818706857407,\n",
       "   0.06107431444231665,\n",
       "   0.06257575873523237,\n",
       "   0.05931884461579634,\n",
       "   0.05927417460704399,\n",
       "   0.06081047305478494,\n",
       "   0.05903112331234066,\n",
       "   0.060508317178602326,\n",
       "   0.060694008718822336,\n",
       "   0.05942062181227736,\n",
       "   0.05981824255109754,\n",
       "   0.06001718302572581,\n",
       "   0.05952176096264684,\n",
       "   0.05666296647419441,\n",
       "   0.06097024215441631,\n",
       "   0.05740899113902331,\n",
       "   0.061108509307035784,\n",
       "   0.06156838934107628,\n",
       "   0.06094263142553033,\n",
       "   0.05784293974406542,\n",
       "   0.05912420901699561,\n",
       "   0.06101146952516356,\n",
       "   0.05841180291508278],\n",
       "  'val_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'val_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0]}}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델을 구성한다\n",
    "model = MAMLFewShotClassifier(args=args, device=device,\n",
    "                              im_shape=(2, 3,\n",
    "                                        args.image_height, args.image_width))\n",
    "\n",
    "data = MetaLearningSystemDataLoader\n",
    "\n",
    "maml_system = ExperimentBuilder(model=model, data=data, args=args, device=device)\n",
    "maml_system.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3803d5ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MAMLFewShotClassifier(\n",
       "  (classifier): VGGReLUNormNetwork(\n",
       "    (layer_dict): ModuleDict(\n",
       "      (conv0): MetaConvNormLayerReLU(\n",
       "        (layer_dict): ModuleDict()\n",
       "        (conv): MetaConv2dLayer()\n",
       "        (norm_layer): MetaBatchNormLayer(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): MetaConvNormLayerReLU(\n",
       "        (layer_dict): ModuleDict()\n",
       "        (conv): MetaConv2dLayer()\n",
       "        (norm_layer): MetaBatchNormLayer(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): MetaConvNormLayerReLU(\n",
       "        (layer_dict): ModuleDict()\n",
       "        (conv): MetaConv2dLayer()\n",
       "        (norm_layer): MetaBatchNormLayer(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv3): MetaConvNormLayerReLU(\n",
       "        (layer_dict): ModuleDict()\n",
       "        (conv): MetaConv2dLayer()\n",
       "        (norm_layer): MetaBatchNormLayer(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (linear): MetaLinearLayer()\n",
       "    )\n",
       "    (prompt): PadPrompter(\n",
       "      (prompt_dict): ParameterDict(\n",
       "          (pad_up): Parameter containing: [torch.cuda.FloatTensor of size 3x5x84 (GPU 0)]\n",
       "          (pad_down): Parameter containing: [torch.cuda.FloatTensor of size 3x5x84 (GPU 0)]\n",
       "          (pad_left): Parameter containing: [torch.cuda.FloatTensor of size 3x74x5 (GPU 0)]\n",
       "          (pad_right): Parameter containing: [torch.cuda.FloatTensor of size 3x74x5 (GPU 0)]\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (inner_loop_optimizer): LSLRGradientDescentLearningRule(\n",
       "    (prompt_learning_rates_dict): ParameterDict(  (prompt_weight_learning_rate): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)])\n",
       "    (names_learning_rates_dict): ParameterDict(\n",
       "        (layer_dict-conv0-conv-weight): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv0-conv-bias): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv1-conv-weight): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv1-conv-bias): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv2-conv-weight): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv2-conv-bias): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv3-conv-weight): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv3-conv-bias): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-linear-weights): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-linear-bias): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "    )\n",
       "    (names_weight_decay_dict): ParameterDict(\n",
       "        (layer_dict-conv0-conv-weight): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv0-conv-bias): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv1-conv-weight): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv1-conv-bias): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv2-conv-weight): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv2-conv-bias): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv3-conv-weight): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv3-conv-bias): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-linear-weights): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-linear-bias): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_dir = maml_system.saved_models_filepath\n",
    "model_name = \"train_model\"\n",
    "model_idx = maml_system.state['best_epoch']\n",
    "\n",
    "state = maml_system.model.load_model(model_save_dir=model_save_dir,\n",
    "                                     model_name=model_name,\n",
    "                                     model_idx=model_idx+1)\n",
    "\n",
    "state_dict_loaded = state['network']\n",
    "\n",
    "maml_system.model.load_state_dict(state_dict=state_dict_loaded)\n",
    "\n",
    "# # 잘 불러왔는지 확인하는 코드\n",
    "# print(\"state_dict_loaded == \",state_dict_loaded)\n",
    "# print(\"=\"*10)\n",
    "# for key, value in maml_system.model.named_parameters():\n",
    "#     print(key)\n",
    "#     print(value)\n",
    "maml_system.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "db4855d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x00000133D106A1F8>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1466, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1424, in _shutdown_workers\n",
      "    if self._persistent_workers or self._workers_status[worker_id]:\n",
      "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\utils\\grad_cam.py:24: RuntimeWarning: invalid value encountered in true_divide\n",
      "  grad_cam = (grad_cam - np.min(grad_cam)) / (np.max(grad_cam) - np.min(grad_cam))\n"
     ]
    }
   ],
   "source": [
    "train_data = maml_system.data.get_train_batches(total_batches=int(600/2), augment_images=False)\n",
    "\n",
    "figure_idx = 0\n",
    "\n",
    "for sample_idx, train_sample in enumerate(train_data):\n",
    "    \n",
    "    x_support_set, x_target_set, y_support_set, y_target_set, seed = train_sample\n",
    "    \n",
    "    x_support_set = torch.Tensor(x_support_set).float().to(device=maml_system.model.device)\n",
    "    x_target_set = torch.Tensor(x_target_set).float().to(device=maml_system.model.device)\n",
    "    y_support_set = torch.Tensor(y_support_set).long().to(device=maml_system.model.device)\n",
    "    y_target_set = torch.Tensor(y_target_set).long().to(device=maml_system.model.device)\n",
    "    \n",
    "    for task_id, (x_support_set_task, y_support_set_task, x_target_set_task, y_target_set_task) in enumerate(zip(x_support_set,\n",
    "                              y_support_set,\n",
    "                              x_target_set,\n",
    "                              y_target_set)):\n",
    "        \n",
    "        names_weights_copy = maml_system.model.get_inner_loop_parameter_dict(maml_system.model.classifier.named_parameters())\n",
    "        \n",
    "        num_devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "\n",
    "        names_weights_copy = {\n",
    "            name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
    "                [num_devices] + [1 for i in range(len(value.shape))]) for\n",
    "            name, value in names_weights_copy.items()}\n",
    "        \n",
    "        prompted_weights_copy = {}\n",
    "        if args.prompter:\n",
    "            prompted_weights_copy = {key: value for key, value in names_weights_copy.items() if 'prompt' in key}\n",
    "\n",
    "        names_weights_copy = {key: value for key, value in names_weights_copy.items() if 'layer_dict' in key}\n",
    "        \n",
    "        n, s, c, h, w = x_target_set_task.shape\n",
    "\n",
    "        x_support_set_task = x_support_set_task.view(-1, c, h, w)\n",
    "        y_support_set_task = y_support_set_task.view(-1)\n",
    "        x_target_set_task = x_target_set_task.view(-1, c, h, w)\n",
    "        y_target_set_task = y_target_set_task.view(-1)\n",
    "        \n",
    "        num_steps=5\n",
    "        \n",
    "        for num_step in range(num_steps):            \n",
    "            \n",
    "            support_loss, support_preds = maml_system.model.net_forward(\n",
    "                                                               x=x_support_set_task,\n",
    "                                                               y=y_support_set_task,\n",
    "                                                               weights=names_weights_copy,\n",
    "                                                               prompted_weights=prompted_weights_copy,\n",
    "                                                               backup_running_statistics=num_step == 0,\n",
    "                                                               prepend_prompt=args.prompter,\n",
    "                                                               training=True,\n",
    "                                                               num_step=num_step,\n",
    "                                                               training_phase=False,\n",
    "                                                               epoch=0,\n",
    "                                                               inner_loop=True)\n",
    "        \n",
    "            names_weights_copy, prompted_weights_copy = maml_system.model.apply_inner_loop_update(\n",
    "                 loss=support_loss,\n",
    "                 names_weights_copy=names_weights_copy,\n",
    "                 prompted_weights_copy=prompted_weights_copy,\n",
    "                 use_second_order=True,\n",
    "                 current_step_idx=num_step,\n",
    "                 current_iter='test',\n",
    "                 training_phase=False)\n",
    "            \n",
    "        ##########Inner level 종료 #############\n",
    "        # Query set으로 Grad-CAM을 구한다\n",
    "        individual_images = torch.split(x_target_set_task, 1, dim=0)  # (25, 3, 84, 84) -> (1, 3, 84, 84) 텐서로 분리\n",
    "        individual_labels = torch.split(y_target_set_task, 1, dim=0)\n",
    "        \n",
    "        \n",
    "        original_save_path = \"Grad_CAM/\" + datasets + \"/Ours_with_reg/origin/\" + \"sample_idx_\" + str(sample_idx) + \"/\" + \"task_id_\" + str(task_id) + \"/\"\n",
    "        grad_cam_save_path = \"Grad_CAM/\" + datasets + \"/Ours_with_reg/gradcam/\"+ \"sample_idx_\" + str(sample_idx) + \"/\" + \"task_id_\" + str(task_id) + \"/\"\n",
    "        \n",
    "        make_folder(original_save_path)\n",
    "        make_folder(grad_cam_save_path)\n",
    "        \n",
    "        # 각 이미지 텐서 확인\n",
    "        for i in range(y_target_set_task.shape[0]):\n",
    "\n",
    "            input_image = individual_images[i]\n",
    "            y_label = individual_labels[i]\n",
    "\n",
    "            _, target_preds, feature_map_list = maml_system.model.net_forward_feature_extractor(x=input_image,\n",
    "                                                             y=y_label, weights=names_weights_copy,\n",
    "                                                             backup_running_statistics=False, training=True,\n",
    "                                                             num_step=num_step, training_phase='test',\n",
    "                                                             epoch=0)\n",
    "            \n",
    "            feature_map = feature_map_list[3]\n",
    "            \n",
    "            # 선택한 클래스에 대한 스칼라 출력값을 기준으로 gradient 계산\n",
    "            target_class = y_label\n",
    "            output = target_preds\n",
    "\n",
    "            class_score = output[:, target_class].sum()\n",
    "            gradients = torch.autograd.grad(outputs=class_score, inputs=feature_map, grad_outputs=torch.ones_like(class_score))\n",
    "\n",
    "            # gradients를 사용하여 feature map의 가중치를 계산\n",
    "            weights = torch.mean(gradients[0], dim=(2, 3), keepdim=True)\n",
    "            grad_cam = F.relu(torch.sum(weights * feature_map, dim=1, keepdim=True))\n",
    "\n",
    "            # grad_cam을 이미지 크기로 리사이즈\n",
    "            grad_cam = F.interpolate(grad_cam, size=(input_image.shape[2], input_image.shape[3]), mode='bilinear', align_corners=False)\n",
    "            grad_cam = grad_cam.squeeze().cpu().detach().numpy()\n",
    "\n",
    "            visualize_and_save_grad_cam(\n",
    "                input_image=input_image,\n",
    "                grad_cam=grad_cam,\n",
    "                grad_cam_save_path=grad_cam_save_path + str(i) + \".png\",\n",
    "                original_save_path=original_save_path + str(i) + \".png\",\n",
    "                datasets=datasets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
