{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd87b33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pytorchcv.model_provider import get_model as ptcv_get_model # model\n",
    "import sys, os\n",
    "import easydict\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline \n",
    "\n",
    "# enable cuda devices\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from meta_neural_network_architectures import VGGReLUNormNetwork, ResNet12\n",
    "from utils.parser_utils import get_args\n",
    "from data import MetaLearningSystemDataLoader\n",
    "from experiment_builder import ExperimentBuilder\n",
    "from utils.basic import show_batch\n",
    "from few_shot_learning_system import MAMLFewShotClassifier\n",
    "import prompters\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from data_augmentation import mixup_data, cutmix_data, random_flip, random_flip_like_torchvision\n",
    "from utils.grad_cam import visualize_and_save_grad_cam, make_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c638561",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = \"mini_imagenet\"\n",
    "# datasets = \"tiered_imagenet\"\n",
    "# datasets = \"CIFAR_FS\"\n",
    "# datasets = \"CUB\"\n",
    "\n",
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "# os.environ['TEST_DATASET'] = \"tiered_imagenet\" # https://mtl.yyliu.net/download/Lmzjm9tX.html\n",
    "# os.environ['TEST_DATASET'] = \"CIFAR_FS\" # https://drive.google.com/file/d/1pTsCCMDj45kzFYgrnO67BWVbKs48Q3NI/view\n",
    "# os.environ['TEST_DATASET'] = \"CUB\" # https://data.caltech.edu/records/65de6-vp158"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8daf114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.environ['DATASET_DIR'] ===  C:/Users/JM/PycharmProjects/MAML/datasets\n"
     ]
    }
   ],
   "source": [
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "print(\"os.environ['DATASET_DIR'] === \", os.environ['DATASET_DIR'])\n",
    "\n",
    "args = easydict.EasyDict(\n",
    "{\n",
    "  \"batch_size\":2,\n",
    "  \"image_height\":84,\n",
    "  \"image_width\":84,\n",
    "  \"image_channels\":3,\n",
    "  \"gpu_to_use\":0,\n",
    "  \"num_dataprovider_workers\":4,\n",
    "  \"max_models_to_save\":5,\n",
    "  \"dataset_name\":\"mini_imagenet_full_size\",\n",
    "  \"dataset_path\":\"mini_imagenet_full_size\",\n",
    "  \"reset_stored_paths\":False,\n",
    "  \"experiment_name\":\"../NoneDataAug_MAML_Prompt_padding_5way_5shot_filter128_miniImagenet\",\n",
    "  \"train_seed\": 0, \"val_seed\": 0,\n",
    "  \"indexes_of_folders_indicating_class\": [-3, -2],\n",
    "  \"sets_are_pre_split\": True,\n",
    "  \"train_val_test_split\": [0.64, 0.16, 0.20],\n",
    "  \"evaluate_on_test_set_only\": False,\n",
    "\n",
    "  \"total_epochs\": 100,\n",
    "  \"total_iter_per_epoch\":500, \"continue_from_epoch\": -2,\n",
    "  \"num_evaluation_tasks\":600,\n",
    "  \"multi_step_loss_num_epochs\": 15,\n",
    "  \"minimum_per_task_contribution\": 0.01,\n",
    "  \"learnable_per_layer_per_step_inner_loop_learning_rate\": True,\n",
    "  \"enable_inner_loop_optimizable_bn_params\": False,\n",
    "  \"evalute_on_test_set_only\": False,\n",
    "\n",
    "  \"max_pooling\": True,\n",
    "  \"per_step_bn_statistics\": False,\n",
    "  \"learnable_batch_norm_momentum\": False,\n",
    "  \"load_into_memory\": False,\n",
    "  \"init_inner_loop_learning_rate\": 0.01,\n",
    "  \"init_inner_loop_weight_decay\": 0.0005,\n",
    "  \"learnable_bn_gamma\": True,\n",
    "  \"learnable_bn_beta\": True,\n",
    "\n",
    "  \"dropout_rate_value\":0.0,\n",
    "  \"min_learning_rate\":0.001,\n",
    "  \"meta_learning_rate\":0.001,   \"total_epochs_before_pause\": 100,\n",
    "  \"first_order_to_second_order_epoch\":-1,\n",
    "  \"weight_decay\": 0.0,\n",
    "\n",
    "  \"norm_layer\":\"batch_norm\",\n",
    "  \"cnn_num_filters\":128,\n",
    "  \"num_stages\":4,\n",
    "  \"conv_padding\": True,\n",
    "  \"number_of_training_steps_per_iter\":5,\n",
    "  \"number_of_evaluation_steps_per_iter\":5,\n",
    "  \"cnn_blocks_per_stage\":1,\n",
    "  \"num_classes_per_set\":5,\n",
    "  \"num_samples_per_class\":5,\n",
    "  \"num_target_samples\": 15,\n",
    "  \"samples_per_iter\" : 1,\n",
    "\n",
    "  \"second_order\": True,\n",
    "  \"use_multi_step_loss_optimization\":False,\n",
    "  \"backbone\": \"4-CONV\",\n",
    "  \"arbiter\": False,\n",
    "  \"use_bias\": True,\n",
    "  \"prompter\": True,\n",
    "  \"prompt_engineering\": 'padding',\n",
    "  \"prompt_size\" : 5,\n",
    "  \"image_size\" : 84,\n",
    "  \"prompt_random_init\": False,\n",
    "  \"outer_prompt_learning_rate\": 0.001,\n",
    "  \"inner_prompt_learning_rate\": 0.01,\n",
    "  \"data_aug\" : \"none\",\n",
    "  \"ablation_record\": False,\n",
    "  \"random_prompt_init\": False\n",
    "}\n",
    ")\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "args.im_shape = (2, 3, args.image_height, args.image_width)\n",
    "\n",
    "args.use_cuda = torch.cuda.is_available()\n",
    "args.seed = 104\n",
    "args.reverse_channels=False\n",
    "args.labels_as_int=False\n",
    "args.reset_stored_filepaths=False\n",
    "args.num_of_gpus=1\n",
    "\n",
    "args.continue_from_epoch='latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dba8492c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using max pooling\n",
      "meta network params\n",
      "0.01\n",
      "Inner Loop parameters\n",
      "prompt_learning_rates_dict.prompt_weight_learning_rate torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv0-conv-weight torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv0-conv-bias torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv1-conv-weight torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv1-conv-bias torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv2-conv-weight torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv2-conv-bias torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv3-conv-weight torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv3-conv-bias torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-linear-weights torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-linear-bias torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv0-conv-weight torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv0-conv-bias torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv1-conv-weight torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv1-conv-bias torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv2-conv-weight torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv2-conv-bias torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv3-conv-weight torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv3-conv-bias torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-linear-weights torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-linear-bias torch.Size([6])\n",
      "Outer Loop parameters\n",
      "classifier.layer_dict.conv0.conv.weight torch.Size([128, 3, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv0.conv.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv0.norm_layer.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv0.norm_layer.weight torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv1.conv.weight torch.Size([128, 128, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv1.conv.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv1.norm_layer.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv1.norm_layer.weight torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv2.conv.weight torch.Size([128, 128, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv2.conv.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv2.norm_layer.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv2.norm_layer.weight torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv3.conv.weight torch.Size([128, 128, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv3.conv.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv3.norm_layer.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv3.norm_layer.weight torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.linear.weights torch.Size([5, 3200]) cuda:0 True\n",
      "classifier.layer_dict.linear.bias torch.Size([5]) cuda:0 True\n",
      "classifier.prompt.prompt_dict.pad_up torch.Size([3, 5, 84]) cuda:0 True\n",
      "classifier.prompt.prompt_dict.pad_down torch.Size([3, 5, 84]) cuda:0 True\n",
      "classifier.prompt.prompt_dict.pad_left torch.Size([3, 74, 5]) cuda:0 True\n",
      "classifier.prompt.prompt_dict.pad_right torch.Size([3, 74, 5]) cuda:0 True\n",
      "inner_loop_optimizer.prompt_learning_rates_dict.prompt_weight_learning_rate torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv0-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv0-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv1-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv1-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv2-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv2-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv3-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv3-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-weights torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv0-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv0-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv1-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv1-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv2-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv2-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv3-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv3-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-linear-weights torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-linear-bias torch.Size([6]) cuda:0 True\n",
      "1\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\NoneDataAug_MAML_Prompt_padding_5way_5shot_filter128_miniImagenet\n",
      "attempting to find existing checkpoint\n",
      "data {'test': 12000, 'train': 38400, 'val': 9600}\n",
      "train_seed 985773, val_seed: 985773, at start time\n",
      "50000 50000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_val_acc': 0.6914222224553426,\n",
       " 'best_val_iter': 48500,\n",
       " 'current_iter': 50000,\n",
       " 'best_epoch': 97,\n",
       " 'train_loss_mean': 0.4558107494413853,\n",
       " 'train_loss_std': 0.12472551710243478,\n",
       " 'train_accuracy_mean': 0.831879999756813,\n",
       " 'train_accuracy_std': 0.05344591432750241,\n",
       " 'train_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_0_std': 0.0,\n",
       " 'train_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_1_std': 0.0,\n",
       " 'train_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_2_std': 0.0,\n",
       " 'train_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_3_std': 0.0,\n",
       " 'train_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'train_loss_importance_vector_4_std': 0.0,\n",
       " 'train_learning_rate_mean': 0.0010000000000000005,\n",
       " 'train_learning_rate_std': 4.336808689942018e-19,\n",
       " 'val_loss_mean': 0.858727896908919,\n",
       " 'val_loss_std': 0.14251950136629574,\n",
       " 'val_accuracy_mean': 0.6694222216804823,\n",
       " 'val_accuracy_std': 0.06069809676102553,\n",
       " 'val_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_0_std': 0.0,\n",
       " 'val_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_1_std': 0.0,\n",
       " 'val_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_2_std': 0.0,\n",
       " 'val_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_3_std': 0.0,\n",
       " 'val_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'val_loss_importance_vector_4_std': 0.0,\n",
       " 'network': OrderedDict([('classifier.layer_dict.conv0.conv.weight',\n",
       "               tensor([[[[ 1.8264e-01, -3.4123e-01,  1.6868e-01],\n",
       "                         [-2.3988e-02,  1.1544e-02, -4.6118e-02],\n",
       "                         [-1.4109e-01,  3.4571e-01, -1.8268e-01]],\n",
       "               \n",
       "                        [[ 1.8969e-01, -3.6593e-01,  2.1905e-01],\n",
       "                         [-5.8537e-02,  2.3569e-02,  2.9728e-02],\n",
       "                         [-1.9031e-01,  2.8396e-01, -1.3257e-01]],\n",
       "               \n",
       "                        [[ 1.7054e-01, -2.8258e-01,  1.0856e-01],\n",
       "                         [ 4.3927e-03,  6.9484e-02, -5.0792e-02],\n",
       "                         [-1.2736e-01,  2.3112e-01, -1.1185e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 2.3347e-06,  2.9773e-06,  2.8892e-06],\n",
       "                         [ 2.6634e-06,  3.1239e-06,  3.2612e-06],\n",
       "                         [ 2.6561e-06,  2.8550e-06,  2.7209e-06]],\n",
       "               \n",
       "                        [[ 2.2801e-06,  3.4471e-06,  3.4746e-06],\n",
       "                         [ 2.5274e-06,  3.6474e-06,  3.7607e-06],\n",
       "                         [ 2.4243e-06,  3.3096e-06,  3.1139e-06]],\n",
       "               \n",
       "                        [[ 8.2387e-07,  1.5122e-06,  1.6795e-06],\n",
       "                         [ 7.6444e-07,  1.4183e-06,  1.6617e-06],\n",
       "                         [ 5.8203e-07,  1.1826e-06,  1.3111e-06]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 6.1776e-16,  8.7360e-16,  1.3650e-15],\n",
       "                         [ 8.1946e-16,  1.0318e-15,  1.7987e-15],\n",
       "                         [ 4.2895e-16,  1.3585e-16, -2.5193e-17]],\n",
       "               \n",
       "                        [[ 2.3811e-16,  9.5856e-17,  2.4457e-15],\n",
       "                         [-8.3371e-16,  1.1990e-15,  8.2776e-16],\n",
       "                         [-8.3197e-16, -2.8214e-16, -8.9781e-17]],\n",
       "               \n",
       "                        [[ 1.1360e-15,  1.0423e-15,  8.8379e-16],\n",
       "                         [ 1.2702e-15,  1.1410e-15,  1.0054e-15],\n",
       "                         [ 3.0206e-16, -2.1406e-16, -1.7582e-16]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-1.7166e-01, -2.2272e-01, -1.3081e-01],\n",
       "                         [-1.6447e-01, -1.8060e-01, -2.4964e-02],\n",
       "                         [-1.2623e-01, -4.1271e-02, -7.0582e-02]],\n",
       "               \n",
       "                        [[ 2.0478e-01,  1.3623e-01,  1.3643e-01],\n",
       "                         [ 1.9554e-01,  1.9379e-01,  2.0405e-01],\n",
       "                         [ 1.5102e-01,  2.4028e-01,  1.5895e-01]],\n",
       "               \n",
       "                        [[-1.0100e-01, -1.9436e-01, -3.8849e-02],\n",
       "                         [-1.2733e-01, -9.3208e-02,  5.4871e-02],\n",
       "                         [-6.9411e-02, -1.6722e-02,  1.0403e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.5618e-10, -1.6854e-10, -1.5809e-10],\n",
       "                         [-1.7383e-10, -1.8819e-10, -1.7920e-10],\n",
       "                         [-1.7628e-10, -1.8655e-10, -1.8704e-10]],\n",
       "               \n",
       "                        [[-2.4251e-10, -2.7209e-10, -2.6714e-10],\n",
       "                         [-2.6948e-10, -2.9737e-10, -2.8532e-10],\n",
       "                         [-2.6473e-10, -2.8450e-10, -2.8591e-10]],\n",
       "               \n",
       "                        [[-1.7776e-11, -2.6715e-11, -1.4386e-11],\n",
       "                         [-4.6659e-11, -5.6904e-11, -2.3606e-11],\n",
       "                         [-3.1630e-11, -4.4278e-11, -2.7615e-11]]],\n",
       "               \n",
       "               \n",
       "                       [[[-5.2911e-09, -4.7118e-09, -3.9005e-09],\n",
       "                         [-6.2234e-09, -6.1505e-09, -4.7127e-09],\n",
       "                         [-6.2987e-09, -6.3262e-09, -5.4557e-09]],\n",
       "               \n",
       "                        [[-7.3888e-09, -8.2033e-09, -7.2063e-09],\n",
       "                         [-7.7347e-09, -8.3405e-09, -7.7701e-09],\n",
       "                         [-7.3692e-09, -7.9585e-09, -7.8512e-09]],\n",
       "               \n",
       "                        [[-1.9949e-09, -2.5748e-09, -2.1373e-09],\n",
       "                         [-2.0470e-09, -2.3402e-09, -2.3161e-09],\n",
       "                         [-1.7154e-09, -2.0686e-09, -2.3748e-09]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.conv.bias',\n",
       "               tensor([ 5.0331e-05, -1.3242e-09,  1.4957e-07, -6.8118e-15, -5.1504e-08,\n",
       "                       -1.9975e-17, -7.7116e-06,  7.9871e-09, -1.7797e-09, -6.7891e-09,\n",
       "                        1.1266e-11, -1.6781e-04, -1.6397e-07,  2.4505e-05, -5.0401e-06,\n",
       "                        1.5937e-05,  2.6920e-06,  8.8282e-06, -3.5587e-10,  3.1035e-05,\n",
       "                       -3.0639e-09,  5.3258e-08, -1.6657e-07, -2.2532e-06,  1.2146e-08,\n",
       "                       -1.3891e-05, -9.7259e-11, -2.8762e-12, -4.0958e-10,  1.9177e-05,\n",
       "                       -5.1516e-05, -1.2857e-07, -1.1677e-05,  2.0367e-07, -1.8373e-05,\n",
       "                        9.6564e-08,  4.3953e-05, -6.6272e-05, -8.0554e-06,  4.9213e-06,\n",
       "                        2.3865e-11, -1.1698e-06, -1.2872e-05,  3.8897e-11, -2.5458e-06,\n",
       "                        2.0301e-07, -1.0001e-07, -9.0046e-16, -1.1003e-05, -4.4174e-08,\n",
       "                        6.4242e-08, -5.0219e-06, -6.1851e-09, -3.6711e-05, -7.0593e-08,\n",
       "                        1.3280e-07,  6.2422e-06, -7.1184e-08,  4.3597e-09, -4.4701e-05,\n",
       "                       -1.8041e-07,  6.5172e-11, -5.1235e-06, -3.6524e-06,  2.9077e-09,\n",
       "                       -3.9880e-14, -9.7972e-08,  4.3302e-05, -5.6275e-11,  2.6587e-08,\n",
       "                        6.4122e-05,  3.2357e-06, -3.7216e-06,  5.5160e-06,  2.9762e-11,\n",
       "                        2.4060e-05, -1.7379e-08, -7.5259e-05,  1.3983e-04, -1.4290e-08,\n",
       "                        3.5964e-12,  4.0149e-05,  6.1060e-06,  1.5237e-09, -2.0759e-06,\n",
       "                        1.7635e-05,  1.8698e-04,  1.0346e-04,  1.2983e-13,  7.0482e-06,\n",
       "                        4.2006e-12,  6.5522e-07, -1.3109e-04,  4.9840e-15, -1.4923e-05,\n",
       "                       -4.2116e-07, -1.6385e-09,  2.1360e-06,  4.9925e-06, -2.1206e-09,\n",
       "                       -1.3286e-08,  2.7433e-05,  7.7113e-09,  4.3667e-09,  8.2641e-05,\n",
       "                       -1.1617e-08, -1.0321e-05,  9.5001e-07,  6.7633e-14,  2.6065e-12,\n",
       "                        7.8318e-10,  3.1151e-06, -9.0836e-11, -3.4759e-04,  5.0219e-07,\n",
       "                        8.2023e-05,  3.0328e-06, -5.7309e-07,  8.6336e-06,  6.4345e-05,\n",
       "                       -2.0942e-07,  1.1008e-06,  1.6305e-04,  2.2817e-08, -1.3901e-05,\n",
       "                       -8.5807e-05,  1.7636e-08, -3.7211e-11], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.bias',\n",
       "               tensor([-1.8101e-02, -6.0610e-04, -3.9241e-05, -1.3005e-09, -9.5239e-03,\n",
       "                       -1.1476e-09, -3.3328e-01, -8.3283e-03, -1.0382e-03, -2.3968e-02,\n",
       "                       -2.5032e-10, -3.9061e-02, -1.4997e-02, -6.7410e-01, -2.3589e-10,\n",
       "                       -2.4695e-02, -5.8214e-03, -3.2672e-03, -9.3063e-11, -2.3803e-01,\n",
       "                       -6.3632e-03, -2.6151e-10, -7.1486e-04, -2.4765e-08, -7.5252e-03,\n",
       "                       -6.3792e-02, -1.1943e-05, -1.7220e-04, -7.3332e-05, -7.7014e-03,\n",
       "                       -1.8517e-02, -7.5453e-04, -3.8942e-02, -3.2227e-10, -3.1763e-01,\n",
       "                       -5.9703e-10, -7.2841e-01,  6.4795e-02, -3.4797e-10, -3.7729e-01,\n",
       "                       -8.4078e-11, -6.8534e-06, -8.5108e-05, -4.0034e-03, -5.3522e-02,\n",
       "                       -2.9660e-01, -1.4028e-03, -7.5994e-11, -8.6442e-04, -1.3488e-03,\n",
       "                       -6.1948e-03, -5.0825e-03, -1.3351e-09, -1.5428e-02, -3.0679e-04,\n",
       "                       -8.6060e-09, -2.8195e-01, -1.3123e-08, -2.3853e-03, -4.4790e-02,\n",
       "                       -9.9925e-04, -1.0758e-07, -3.5595e-02, -5.7457e-02, -2.7784e-09,\n",
       "                       -5.9420e-11, -5.1036e-08, -1.5902e-08, -3.9993e-10, -8.4629e-03,\n",
       "                       -1.9431e-02, -1.0094e-09, -4.5285e-02, -1.3735e-03, -1.1613e-10,\n",
       "                       -1.1846e-01, -4.4097e-04,  1.4700e-01,  1.2886e-02, -3.2645e-03,\n",
       "                       -8.9107e-10,  7.2812e-02, -1.0525e-08, -6.1916e-10, -8.3397e-05,\n",
       "                        2.8666e-01, -3.5832e-02,  1.1641e-01, -2.1777e-10, -5.0681e-02,\n",
       "                       -2.7577e-07, -1.3181e-02, -4.2999e-02, -3.0998e-09, -4.8151e-11,\n",
       "                       -1.2714e-02, -8.1131e-05, -4.9793e-03, -4.0384e-02, -2.9800e-07,\n",
       "                       -2.4832e-08, -2.2058e-02, -7.7476e-03, -2.6480e-02,  4.4663e-01,\n",
       "                       -5.1150e-02, -1.7486e-01, -2.0170e-02, -1.1342e-05, -8.1560e-10,\n",
       "                       -1.1657e-04, -7.3600e-04, -5.9924e-03,  5.0747e-01,  6.3891e-01,\n",
       "                       -2.7155e-02, -1.6015e-02, -4.7120e-03, -3.2158e-01, -1.4783e-02,\n",
       "                       -6.5048e-06, -2.0629e-09,  3.6475e-01,  3.9025e-10, -3.6481e-04,\n",
       "                        2.0408e-01, -9.6994e-07, -1.4223e-05], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.weight',\n",
       "               tensor([ 4.2753e-01,  1.6640e-04, -9.1107e-16,  4.2238e-08, -3.9797e-03,\n",
       "                       -2.2861e-25,  3.3458e-01,  1.7029e-03,  2.7142e-04,  6.1927e-03,\n",
       "                        2.3868e-13,  6.1731e-01,  3.0261e-03,  3.3632e-01,  6.4146e-10,\n",
       "                        4.1723e-01, -1.1910e-03, -5.1085e-05, -3.4827e-26,  1.5339e-01,\n",
       "                       -8.2465e-04,  1.7461e-07,  8.6981e-05,  8.7686e-06, -1.2279e-03,\n",
       "                       -2.5976e-02,  2.8080e-05,  5.4668e-05, -1.6288e-04,  8.0585e-02,\n",
       "                        3.2143e-01, -2.3860e-04,  1.1906e-01,  7.0496e-18,  2.2186e-01,\n",
       "                        1.3428e-19,  5.0033e-01,  2.1606e-01,  1.3355e-11,  3.6633e-01,\n",
       "                       -2.7136e-08, -5.3532e-05, -6.1920e-05, -1.7522e-03, -1.7107e-02,\n",
       "                        3.3870e-01, -3.5136e-04,  2.8180e-17, -4.1733e-04,  6.5098e-04,\n",
       "                        2.4127e-03,  1.0683e-03, -2.4434e-13,  5.3477e-01, -3.1451e-04,\n",
       "                       -3.5324e-16,  3.8843e-01, -4.4064e-07,  3.6944e-04,  6.5534e-01,\n",
       "                       -2.9184e-05, -7.5865e-09, -1.2743e-02, -5.4894e-03, -2.7362e-06,\n",
       "                        3.7972e-12,  5.8857e-09,  4.5163e-07,  1.3355e-09, -6.9552e-04,\n",
       "                        3.3396e-01, -3.0311e-08,  1.7009e-01, -3.7350e-04, -1.2853e-17,\n",
       "                        2.1802e-01, -1.1391e-04,  2.5096e-01,  5.9094e-01, -2.1492e-04,\n",
       "                       -5.0588e-06,  2.8099e-01, -7.8214e-12, -3.1285e-22, -1.3806e-05,\n",
       "                        3.8082e-01,  3.2148e-01,  3.5996e-01, -8.0971e-07,  1.0475e-01,\n",
       "                        6.5585e-05,  3.0844e-03,  4.6791e-01,  3.7638e-10,  1.2767e-08,\n",
       "                        4.5179e-03,  7.9443e-05,  1.0915e-03, -7.9085e-03, -4.9832e-07,\n",
       "                       -1.7239e-05,  3.2706e-01, -6.6213e-04, -3.3264e-04,  4.4477e-01,\n",
       "                       -1.9407e-02,  4.1109e-01, -7.4961e-03, -9.3775e-16,  7.4508e-14,\n",
       "                       -1.9898e-05, -5.7497e-04,  4.3055e-04,  4.1132e-01,  1.7682e-04,\n",
       "                        5.9744e-01, -2.3735e-03, -2.2710e-04,  2.4573e-01,  3.5764e-01,\n",
       "                        4.8945e-06,  1.5834e-09,  4.5418e-01,  6.4943e-15,  1.0988e-04,\n",
       "                        2.3620e-01, -4.0500e-06, -8.3805e-06], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.weight',\n",
       "               tensor([[[[-4.4255e-02, -4.5598e-02,  2.5903e-02],\n",
       "                         [-3.9667e-02, -3.7049e-02, -1.1833e-03],\n",
       "                         [-3.2736e-02, -3.7903e-02, -5.7883e-03]],\n",
       "               \n",
       "                        [[ 1.0436e-07, -2.7480e-07, -5.4336e-07],\n",
       "                         [ 2.7742e-07, -6.7250e-08,  4.7725e-07],\n",
       "                         [ 1.1954e-06,  2.2070e-07,  1.2978e-07]],\n",
       "               \n",
       "                        [[ 1.5283e-06,  2.5539e-07,  6.6579e-07],\n",
       "                         [ 1.3941e-06,  9.2027e-12,  4.4949e-07],\n",
       "                         [ 1.0799e-06, -1.0526e-07,  4.8938e-07]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.8054e-02,  1.0488e-02,  5.2972e-04],\n",
       "                         [ 2.1076e-02,  9.6170e-03, -2.8888e-03],\n",
       "                         [ 2.5211e-02,  1.2378e-02,  2.7501e-03]],\n",
       "               \n",
       "                        [[ 1.1577e-09,  2.5822e-10,  1.9240e-09],\n",
       "                         [ 1.3138e-09, -2.5615e-13,  2.0761e-09],\n",
       "                         [ 2.9217e-09,  1.4644e-09,  2.5413e-09]],\n",
       "               \n",
       "                        [[ 3.3820e-08,  1.1404e-08,  9.9550e-08],\n",
       "                         [ 3.1216e-08, -2.9627e-11,  8.7162e-08],\n",
       "                         [ 5.4201e-08,  3.8375e-08,  1.2642e-07]]],\n",
       "               \n",
       "               \n",
       "                       [[[-5.7182e-02,  2.3308e-02, -1.1892e-02],\n",
       "                         [-1.6797e-02, -7.9537e-03,  3.5896e-02],\n",
       "                         [-1.2693e-02,  2.0754e-02,  1.0634e-02]],\n",
       "               \n",
       "                        [[ 8.2958e-06,  5.3947e-06,  7.2564e-06],\n",
       "                         [ 2.4578e-06, -1.6669e-07,  3.3754e-06],\n",
       "                         [ 1.0543e-05,  8.1788e-06,  1.1044e-05]],\n",
       "               \n",
       "                        [[ 4.3686e-06,  9.4959e-06, -8.3941e-06],\n",
       "                         [-1.9901e-07,  9.7358e-11,  1.4170e-07],\n",
       "                         [ 3.4688e-06,  6.2342e-07, -1.4587e-05]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-4.9150e-02, -5.4249e-02, -6.0765e-02],\n",
       "                         [-1.4301e-02,  2.7697e-05,  9.3454e-03],\n",
       "                         [-7.4078e-03, -1.2900e-03,  1.2805e-02]],\n",
       "               \n",
       "                        [[ 1.1788e-08,  9.7264e-09, -7.4327e-09],\n",
       "                         [ 2.4001e-09, -9.1549e-13, -1.6212e-08],\n",
       "                         [-9.0738e-09, -1.1072e-08, -2.6211e-08]],\n",
       "               \n",
       "                        [[ 1.4043e-06,  7.7557e-07,  7.1043e-07],\n",
       "                         [ 6.6960e-07, -1.3623e-10,  1.0987e-07],\n",
       "                         [ 1.1751e-06,  5.3706e-07,  6.6462e-07]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 2.3343e-02,  4.6533e-02, -7.8059e-02],\n",
       "                         [ 3.0845e-02,  5.0510e-02,  4.9227e-02],\n",
       "                         [-4.8655e-02, -3.6788e-02,  4.3546e-02]],\n",
       "               \n",
       "                        [[ 9.8964e-07, -2.1728e-06,  3.4094e-06],\n",
       "                         [ 3.3589e-06, -2.2351e-07,  4.9329e-06],\n",
       "                         [ 3.0677e-06,  3.2737e-07,  5.5781e-06]],\n",
       "               \n",
       "                        [[-1.7595e-06,  1.6830e-08, -3.7281e-06],\n",
       "                         [-4.7743e-06, -1.5909e-10,  1.1517e-06],\n",
       "                         [-1.8289e-06, -3.4179e-07,  2.2974e-06]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 7.2786e-02,  3.9155e-02,  5.2090e-02],\n",
       "                         [-3.7956e-02, -2.3643e-02,  2.5437e-02],\n",
       "                         [ 1.9317e-03,  9.6407e-03,  1.2775e-03]],\n",
       "               \n",
       "                        [[-7.5434e-09, -4.5596e-09,  1.6807e-08],\n",
       "                         [-3.5467e-09,  2.4941e-13,  2.0680e-08],\n",
       "                         [-3.7263e-09,  4.0145e-10,  2.1489e-08]],\n",
       "               \n",
       "                        [[-1.4585e-07, -6.6668e-08,  2.5732e-07],\n",
       "                         [-1.1347e-07, -4.9103e-11,  3.1286e-07],\n",
       "                         [-1.2658e-07, -1.8025e-09,  3.1620e-07]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-1.0657e-01, -1.6583e-01, -5.9579e-02],\n",
       "                         [-2.1435e-01, -2.6060e-01, -2.3183e-01],\n",
       "                         [-1.4564e-01, -1.7418e-01, -8.1756e-02]],\n",
       "               \n",
       "                        [[-2.3261e-06, -2.2231e-07, -8.0831e-07],\n",
       "                         [-2.1716e-06,  2.6118e-07, -7.9483e-07],\n",
       "                         [-1.0143e-05, -7.9424e-06, -8.0384e-06]],\n",
       "               \n",
       "                        [[-1.9916e-06, -1.8033e-06,  3.9058e-06],\n",
       "                         [ 1.4936e-06,  9.9651e-11, -6.3570e-07],\n",
       "                         [-6.3359e-06, -3.6169e-06, -2.8780e-08]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 1.0318e-03,  1.8557e-02,  5.4548e-02],\n",
       "                         [-2.3491e-02, -3.6781e-02, -2.3097e-02],\n",
       "                         [-1.7477e-02, -7.3832e-03,  3.0173e-02]],\n",
       "               \n",
       "                        [[-7.0115e-08, -5.4335e-08, -5.8249e-08],\n",
       "                         [-1.6908e-08, -2.4503e-13, -6.8082e-09],\n",
       "                         [-4.8302e-08, -3.6602e-08, -4.4747e-08]],\n",
       "               \n",
       "                        [[-5.2721e-07, -3.6963e-07, -3.7799e-07],\n",
       "                         [-1.4306e-07,  5.7719e-11, -7.6710e-08],\n",
       "                         [-5.3383e-07, -4.1887e-07, -4.8994e-07]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.4009e-09, -1.1793e-09, -1.3490e-09],\n",
       "                         [-1.2163e-09, -1.2443e-09, -1.2318e-09],\n",
       "                         [-1.3734e-09, -1.1880e-09, -1.1504e-09]],\n",
       "               \n",
       "                        [[ 1.9409e-09, -1.7447e-05,  2.5317e-14],\n",
       "                         [ 1.3620e-14, -1.0587e-14, -9.7088e-15],\n",
       "                         [-1.5310e-14, -7.7838e-15,  9.0976e-17]],\n",
       "               \n",
       "                        [[ 3.0659e-09,  1.3014e-09,  5.5807e-10],\n",
       "                         [ 1.0982e-05,  1.3463e-15,  2.9449e-14],\n",
       "                         [-8.8897e-08,  8.5834e-07, -2.3705e-06]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 7.4910e-11,  1.7221e-10,  2.0551e-10],\n",
       "                         [ 6.7963e-11,  1.3430e-10,  1.8824e-10],\n",
       "                         [-4.4761e-11,  8.0484e-11,  1.6226e-10]],\n",
       "               \n",
       "                        [[-1.5107e-13, -4.4704e-13, -3.8230e-09],\n",
       "                         [ 1.4686e-11,  3.5763e-08,  3.6819e-14],\n",
       "                         [-3.7389e-08,  1.3941e-08,  3.6824e-07]],\n",
       "               \n",
       "                        [[-1.3592e-06,  7.5322e-16, -1.4722e-15],\n",
       "                         [ 3.9167e-16, -1.2064e-18, -8.7360e-17],\n",
       "                         [-2.5236e-16,  8.5182e-17,  2.0100e-16]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 6.1723e-03,  2.6897e-02,  1.8526e-02],\n",
       "                         [ 1.6771e-02,  4.7428e-02,  3.2654e-02],\n",
       "                         [ 1.8923e-02,  2.5564e-02,  3.8489e-02]],\n",
       "               \n",
       "                        [[ 2.0522e-07,  8.9316e-08,  1.6397e-07],\n",
       "                         [ 3.8320e-08, -5.2302e-08, -7.7341e-08],\n",
       "                         [ 1.0862e-07, -1.6188e-07, -5.6223e-09]],\n",
       "               \n",
       "                        [[-2.2631e-08,  4.0185e-08,  9.3342e-10],\n",
       "                         [-1.6815e-08,  3.2801e-12, -4.7584e-08],\n",
       "                         [-9.2429e-09, -1.1334e-08,  3.2859e-08]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.8842e-02, -1.9444e-02, -1.1925e-02],\n",
       "                         [-2.2614e-02, -1.1120e-02, -8.6427e-03],\n",
       "                         [-1.5668e-02, -8.3863e-03, -9.3880e-03]],\n",
       "               \n",
       "                        [[ 4.3143e-10,  5.9514e-10,  4.0439e-10],\n",
       "                         [ 5.2567e-06,  9.5132e-09,  1.6788e-09],\n",
       "                         [-1.5538e-09, -1.3363e-09, -1.3945e-09]],\n",
       "               \n",
       "                        [[-2.7147e-09,  4.6808e-07,  7.1004e-09],\n",
       "                         [-2.2972e-06,  4.8953e-12, -2.7085e-06],\n",
       "                         [-2.2584e-08,  8.3785e-07, -6.7587e-08]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.bias',\n",
       "               tensor([-8.6767e-06,  2.9239e-05, -5.2240e-05,  1.2281e-05,  1.4259e-04,\n",
       "                       -6.1112e-06,  3.1255e-05, -1.8677e-05, -1.2307e-05, -2.4727e-05,\n",
       "                       -2.4632e-05, -4.9155e-05,  1.4598e-05, -5.4578e-06,  1.2069e-15,\n",
       "                        1.4427e-06, -1.2737e-08,  1.4117e-10, -1.2441e-05, -3.1309e-05,\n",
       "                       -3.4007e-05,  4.9056e-06,  1.6945e-05,  9.9061e-08, -1.0870e-06,\n",
       "                        4.0993e-09, -3.5514e-05, -6.6471e-05,  4.6027e-05,  1.3214e-05,\n",
       "                        1.4234e-06,  4.5800e-05, -5.4418e-06, -5.6864e-09, -1.1643e-05,\n",
       "                       -5.9075e-05, -1.6497e-05,  1.1092e-05, -4.5710e-05, -5.9841e-05,\n",
       "                       -1.7468e-05,  2.7513e-06,  3.6095e-05, -2.1593e-08, -1.6455e-05,\n",
       "                       -3.7682e-08,  1.0771e-06,  6.1433e-06,  1.2833e-05,  1.1795e-06,\n",
       "                        4.7685e-06,  9.1093e-06, -3.3149e-05,  1.4599e-08,  1.6544e-07,\n",
       "                        3.6372e-05, -6.1998e-07, -1.4219e-05, -1.8446e-07,  4.1694e-12,\n",
       "                       -5.7221e-08,  6.0546e-07,  3.3061e-06,  8.6089e-05, -1.5797e-06,\n",
       "                        1.0388e-12,  1.1395e-07, -9.9715e-07, -6.8838e-11, -2.2561e-11,\n",
       "                        1.2185e-05, -3.3355e-05,  3.1837e-14,  2.3316e-06,  2.5518e-05,\n",
       "                       -7.4102e-08,  2.4352e-05,  3.9739e-07,  9.6731e-06, -4.9449e-07,\n",
       "                       -1.3401e-05,  3.9208e-05, -1.8984e-12,  1.8966e-05, -6.4817e-09,\n",
       "                       -3.3888e-05, -2.6979e-06, -2.5016e-14, -3.2229e-05,  1.8282e-09,\n",
       "                        8.9055e-05,  2.5469e-09,  6.3832e-05, -1.0651e-15,  1.3621e-05,\n",
       "                       -2.7263e-05,  2.2502e-05, -8.5253e-13,  1.7855e-05,  3.7604e-05,\n",
       "                       -1.2436e-04,  1.4723e-05, -1.9942e-05, -3.8050e-05, -4.9363e-05,\n",
       "                        5.7131e-07, -1.2422e-05, -6.2810e-05, -1.2802e-05, -2.6861e-13,\n",
       "                        2.0258e-06,  6.3851e-06, -1.2288e-11,  1.7734e-06,  1.6612e-07,\n",
       "                        1.5951e-05, -1.9663e-08,  4.0369e-05,  3.9662e-06, -1.3477e-06,\n",
       "                       -1.0274e-08,  9.9962e-06, -9.1879e-06, -2.1501e-05,  3.9598e-08,\n",
       "                       -4.3612e-05,  9.7481e-13,  6.9311e-08], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.bias',\n",
       "               tensor([-2.5159e-01, -2.5922e-01, -1.7230e-01, -3.1593e-01, -2.3954e-01,\n",
       "                       -2.0323e-01, -1.5706e-07, -4.2395e-09, -3.7350e-01, -3.0447e-01,\n",
       "                       -1.7779e-01, -1.9164e-01, -1.5168e-01, -1.9969e-01, -2.1585e-11,\n",
       "                       -3.0957e-02, -1.0886e-03, -1.2504e-05, -1.9420e-01, -2.8204e-01,\n",
       "                       -2.4635e-01, -2.4988e-01, -2.3866e-01, -3.6780e-02, -5.7773e-04,\n",
       "                       -7.5946e-03, -2.0139e-01, -2.6175e-01, -1.2533e-01, -1.0762e-08,\n",
       "                       -1.3125e-01, -1.2822e-01, -1.7667e-01, -8.0051e-03, -2.5452e-01,\n",
       "                       -1.4988e-01, -1.6705e-01, -2.7523e-01, -1.6943e-02, -1.4480e-01,\n",
       "                       -2.3240e-01, -2.2368e-01, -1.5545e-01, -3.5839e-02, -3.4146e-01,\n",
       "                       -3.1913e-03, -3.0698e-03, -2.8758e-01, -1.3656e-01, -5.3825e-06,\n",
       "                       -1.2661e-01, -2.4582e-01, -3.9132e-01, -3.0972e-03, -6.2475e-02,\n",
       "                       -2.3492e-01, -1.6789e-01, -3.1877e-01, -5.7746e-03, -3.2997e-07,\n",
       "                       -6.3398e-02, -5.1848e-07, -9.3735e-02, -1.9370e-01, -1.4386e-02,\n",
       "                       -6.9643e-06, -4.8764e-02, -3.4979e-02, -9.4713e-06, -2.1583e-05,\n",
       "                       -1.7324e-01, -1.3952e-01, -2.8335e-09, -3.3190e-02, -1.6360e-01,\n",
       "                       -4.9385e-10, -1.7912e-01, -1.0869e-02, -1.8189e-01, -6.8389e-05,\n",
       "                       -1.8238e-01, -2.6862e-01, -8.1472e-09, -2.0398e-01, -2.6014e-03,\n",
       "                        7.8874e-10, -2.4028e-01, -2.7706e-10, -2.0487e-01, -6.1729e-06,\n",
       "                       -2.3222e-01, -1.3733e-03, -1.6853e-01, -2.1503e-11, -1.3349e-01,\n",
       "                       -1.1794e-02, -1.3456e-01, -6.2095e-06, -1.3308e-01, -1.5293e-01,\n",
       "                       -2.1999e-01, -3.9964e-01, -1.2122e-01, -1.9735e-01, -2.2084e-01,\n",
       "                       -1.2229e-02, -1.7842e-01, -1.9416e-01, -3.0208e-01, -2.1517e-09,\n",
       "                       -2.5902e-03, -2.3985e-01, -9.6846e-06, -1.7772e-02, -2.3918e-02,\n",
       "                       -1.7226e-01, -2.7520e-02,  2.7002e-02, -2.0793e-01, -3.1913e-01,\n",
       "                       -7.6458e-04, -3.4487e-01, -2.5071e-02, -1.7792e-01, -4.8931e-08,\n",
       "                       -2.5008e-01, -3.9201e-06, -6.9668e-02], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.weight',\n",
       "               tensor([ 2.1870e-01,  2.8539e-01,  4.1345e-01,  3.2871e-01,  4.7845e-01,\n",
       "                        1.5855e-01,  4.9784e-07, -7.0535e-09,  1.7771e-01,  4.8527e-01,\n",
       "                        2.5452e-01,  4.6196e-01,  3.4008e-01,  3.4960e-01,  6.7082e-10,\n",
       "                       -2.7941e-03, -1.2687e-04,  5.3510e-05,  3.0366e-01,  2.5986e-01,\n",
       "                        3.1394e-01,  2.5385e-01,  2.8498e-01, -3.9277e-03, -2.8759e-04,\n",
       "                        1.0879e-03,  4.2178e-01,  6.3811e-01,  3.8345e-01, -5.6907e-06,\n",
       "                        3.9282e-01,  2.7951e-01,  3.4523e-01,  8.8137e-04,  2.8927e-01,\n",
       "                        2.8214e-01,  2.5085e-01,  3.9662e-01,  3.7719e-01,  2.7832e-01,\n",
       "                        3.7409e-01,  5.6509e-02,  3.7964e-01,  1.7991e-03,  1.8008e-01,\n",
       "                       -7.1141e-04,  6.8750e-04,  3.6592e-01,  3.3162e-01, -1.0307e-04,\n",
       "                        3.9236e-01,  2.1252e-01,  2.1165e-01, -6.4763e-04,  4.1049e-03,\n",
       "                        3.8958e-01,  1.1643e-01,  4.1533e-01, -1.0930e-03,  4.9867e-06,\n",
       "                        1.7439e-02, -5.3942e-06,  2.6740e-01,  5.0975e-01, -2.0582e-03,\n",
       "                        2.4918e-10, -2.0305e-03, -3.3035e-03, -7.8958e-05,  7.1474e-09,\n",
       "                        3.7743e-01,  3.2613e-01,  1.7867e-07, -4.3665e-03,  2.9528e-01,\n",
       "                        2.2244e-08,  2.6991e-01,  1.7842e-03,  3.2440e-01,  1.1201e-04,\n",
       "                        3.0006e-01,  4.0042e-01, -5.5365e-14,  2.6976e-01,  6.1369e-04,\n",
       "                       -6.9755e-08,  3.6521e-01, -4.6234e-08,  3.4778e-01,  7.0752e-05,\n",
       "                        3.6976e-01,  3.3413e-04,  3.6862e-01, -1.5574e-14,  2.1829e-01,\n",
       "                        1.6004e-03,  2.9586e-01,  6.9378e-06,  2.7198e-01,  3.0893e-01,\n",
       "                        3.4671e-01,  2.9555e-01,  2.8456e-01,  3.1424e-01,  3.8029e-01,\n",
       "                        1.6253e-03,  3.2267e-01,  4.7554e-01,  5.0420e-01, -9.7343e-09,\n",
       "                       -1.0677e-03,  4.3631e-01,  2.4581e-08,  3.0936e-03,  6.0710e-04,\n",
       "                        5.2542e-01,  9.2414e-03,  2.5298e-01,  3.1977e-01,  1.8695e-01,\n",
       "                       -3.5170e-04,  2.8573e-01, -3.7500e-03,  1.8963e-01,  4.2087e-12,\n",
       "                        5.1363e-01, -1.4195e-06, -2.3551e-02], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.weight',\n",
       "               tensor([[[[-1.6970e-04,  3.1762e-04,  1.6534e-04],\n",
       "                         [ 8.2762e-03, -2.8499e-03, -7.2604e-03],\n",
       "                         [ 7.1943e-03,  3.4308e-03, -1.2616e-02]],\n",
       "               \n",
       "                        [[-7.1759e-03, -2.8170e-02,  9.6621e-03],\n",
       "                         [-1.0627e-02,  1.6054e-02, -4.3077e-03],\n",
       "                         [ 3.3106e-03, -1.8386e-02, -1.6739e-02]],\n",
       "               \n",
       "                        [[ 2.4090e-02, -2.1569e-02,  2.5016e-02],\n",
       "                         [ 8.5830e-04, -1.2240e-01, -7.2211e-02],\n",
       "                         [-4.8498e-02, -2.9010e-02,  7.6830e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 6.6914e-03, -7.3969e-02, -3.7755e-02],\n",
       "                         [ 6.3952e-02, -3.5379e-02,  4.9947e-02],\n",
       "                         [ 8.1377e-02,  1.1575e-02,  4.9655e-02]],\n",
       "               \n",
       "                        [[-1.8339e-07, -1.0965e-07, -1.5752e-07],\n",
       "                         [-9.9299e-08,  9.9159e-13, -5.1128e-08],\n",
       "                         [-1.2230e-07, -2.5167e-08, -7.5518e-08]],\n",
       "               \n",
       "                        [[-1.8197e-02, -9.4516e-03, -3.4173e-03],\n",
       "                         [-1.3517e-02, -8.8869e-03, -5.0833e-03],\n",
       "                         [-1.0808e-02, -1.0663e-02, -8.5238e-03]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 2.5562e-03, -3.6861e-04,  1.3503e-02],\n",
       "                         [-2.5916e-03,  8.4937e-03,  3.1791e-02],\n",
       "                         [-1.2218e-03,  3.5139e-03,  3.5190e-02]],\n",
       "               \n",
       "                        [[ 7.1905e-02,  1.2813e-01,  1.0586e-01],\n",
       "                         [ 4.3821e-02,  4.1981e-02,  6.6620e-02],\n",
       "                         [ 6.6050e-02,  5.6239e-02,  1.2337e-01]],\n",
       "               \n",
       "                        [[ 4.5441e-02,  2.6505e-02,  8.4768e-02],\n",
       "                         [-3.1079e-02,  6.1001e-02,  3.5246e-02],\n",
       "                         [ 1.1609e-01,  5.2722e-02,  5.8262e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 3.6579e-02, -3.5011e-03,  4.7209e-03],\n",
       "                         [ 3.9457e-02, -5.0076e-04, -1.2588e-03],\n",
       "                         [-1.8341e-02, -4.0796e-02, -2.3768e-02]],\n",
       "               \n",
       "                        [[-7.7673e-08, -8.5386e-08, -8.3891e-08],\n",
       "                         [-1.7372e-08,  1.0686e-13,  4.9603e-09],\n",
       "                         [-1.7825e-08, -2.3266e-09,  4.1064e-09]],\n",
       "               \n",
       "                        [[-2.9998e-03, -6.9837e-03, -2.8203e-03],\n",
       "                         [ 1.2502e-03, -7.0977e-03,  1.0421e-03],\n",
       "                         [-2.2134e-03, -5.5637e-03,  2.2050e-03]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.1539e-02,  2.0754e-02,  2.6922e-02],\n",
       "                         [-3.6464e-03,  5.1847e-03,  1.4208e-02],\n",
       "                         [-4.8685e-03,  7.2312e-04,  9.0330e-03]],\n",
       "               \n",
       "                        [[ 1.0071e-01,  1.2348e-01,  8.6873e-02],\n",
       "                         [ 7.9846e-02,  1.0324e-01,  8.3039e-02],\n",
       "                         [ 6.2388e-02,  6.0131e-02,  1.0365e-02]],\n",
       "               \n",
       "                        [[ 5.2915e-02,  1.2989e-01,  1.4062e-01],\n",
       "                         [ 3.0117e-02,  3.5296e-02,  7.1985e-03],\n",
       "                         [ 7.2237e-02,  2.4093e-02,  2.3041e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.9054e-02,  4.1968e-03, -1.0212e-02],\n",
       "                         [-5.0842e-02, -1.6189e-02, -2.8433e-02],\n",
       "                         [-6.2787e-02, -3.6606e-02,  2.2035e-02]],\n",
       "               \n",
       "                        [[-6.5071e-08, -7.7951e-08, -1.5785e-07],\n",
       "                         [ 6.1326e-09, -1.4533e-12, -8.0591e-08],\n",
       "                         [-7.3976e-08, -8.6684e-08, -1.6985e-07]],\n",
       "               \n",
       "                        [[ 1.9860e-02,  1.4787e-02,  1.4448e-02],\n",
       "                         [ 1.8256e-02,  1.4320e-02,  2.3768e-02],\n",
       "                         [ 1.3748e-02,  1.1798e-02,  2.1900e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-1.3299e-02, -8.7031e-03,  2.5019e-03],\n",
       "                         [-6.6077e-03, -1.3803e-02, -6.2122e-04],\n",
       "                         [ 1.1145e-02, -6.5502e-03,  6.1038e-03]],\n",
       "               \n",
       "                        [[-6.3694e-02, -6.6055e-02,  3.4315e-03],\n",
       "                         [-8.0753e-02, -7.5652e-02, -7.9842e-02],\n",
       "                         [ 6.5897e-02,  3.1077e-02,  2.5797e-02]],\n",
       "               \n",
       "                        [[-9.4626e-02,  5.6251e-02,  8.9283e-02],\n",
       "                         [ 2.0990e-02, -4.4541e-03,  9.1657e-03],\n",
       "                         [ 1.3995e-01,  2.4260e-02, -7.5680e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-3.3050e-02, -3.2678e-02, -2.8423e-02],\n",
       "                         [-4.4740e-02,  6.8008e-03, -3.6385e-02],\n",
       "                         [ 3.9521e-02,  1.3140e-02, -1.2650e-02]],\n",
       "               \n",
       "                        [[-9.6409e-08, -9.1538e-08, -1.3384e-07],\n",
       "                         [-4.1393e-08,  7.6709e-13, -5.9819e-08],\n",
       "                         [-1.4366e-07, -1.1628e-07, -2.2094e-07]],\n",
       "               \n",
       "                        [[ 8.8149e-04, -1.4432e-02, -1.8203e-03],\n",
       "                         [-2.3191e-03, -7.8767e-03, -2.3146e-03],\n",
       "                         [-4.7223e-04, -8.2708e-03, -7.2888e-03]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 8.3825e-03,  1.8064e-02,  1.2759e-02],\n",
       "                         [ 1.4006e-02,  1.3197e-02,  1.2224e-02],\n",
       "                         [ 1.8033e-02,  1.0343e-02,  1.0669e-02]],\n",
       "               \n",
       "                        [[ 4.9500e-03,  2.1455e-02,  3.8174e-02],\n",
       "                         [ 1.9772e-02,  4.2454e-02,  1.3873e-02],\n",
       "                         [ 4.6868e-02,  7.8681e-02,  1.5910e-02]],\n",
       "               \n",
       "                        [[ 8.7816e-02,  1.2536e-01, -1.6786e-03],\n",
       "                         [ 3.2269e-02,  1.6684e-02, -3.2734e-03],\n",
       "                         [ 1.9118e-02,  6.6041e-04, -3.1566e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.5446e-02,  1.2263e-02,  5.4831e-03],\n",
       "                         [ 5.6087e-02,  8.4846e-02,  3.3022e-02],\n",
       "                         [ 7.2756e-02,  3.1521e-02,  2.9321e-02]],\n",
       "               \n",
       "                        [[-6.4859e-08, -4.4877e-08, -4.4047e-08],\n",
       "                         [-2.5428e-08,  3.9905e-13, -1.0006e-09],\n",
       "                         [-9.4110e-08, -7.0526e-08, -7.0832e-08]],\n",
       "               \n",
       "                        [[-3.9269e-03, -3.6194e-03, -4.4065e-03],\n",
       "                         [-3.5442e-03, -6.5107e-03, -5.6481e-03],\n",
       "                         [-3.6952e-03, -3.0102e-03,  4.9275e-04]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 4.1306e-03,  8.2004e-03,  1.8655e-02],\n",
       "                         [-1.1786e-03,  1.3578e-02,  2.8133e-02],\n",
       "                         [-2.0872e-03,  1.3525e-02,  2.9256e-02]],\n",
       "               \n",
       "                        [[-3.4689e-02,  1.2682e-02, -1.1218e-02],\n",
       "                         [-1.7881e-02,  1.3506e-02,  9.8848e-03],\n",
       "                         [ 6.1889e-03,  1.5999e-02,  5.7147e-03]],\n",
       "               \n",
       "                        [[-9.4065e-02, -5.3582e-02,  1.0526e-02],\n",
       "                         [ 1.1000e-02, -2.3898e-02,  5.9969e-03],\n",
       "                         [ 6.6573e-02,  1.7667e-01,  1.2066e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.1573e-02,  3.2453e-02,  4.5368e-02],\n",
       "                         [ 7.0838e-02,  7.6933e-02,  7.9041e-02],\n",
       "                         [ 3.6675e-02,  7.4033e-02,  3.7166e-02]],\n",
       "               \n",
       "                        [[ 1.8346e-07,  8.2220e-08,  1.3727e-07],\n",
       "                         [ 1.0400e-07, -1.3263e-12,  5.6903e-08],\n",
       "                         [ 1.2485e-07,  2.2136e-08,  8.1058e-08]],\n",
       "               \n",
       "                        [[ 2.3873e-03,  2.3938e-03,  1.2653e-03],\n",
       "                         [ 1.0583e-03,  7.3562e-04, -3.0333e-03],\n",
       "                         [ 7.1562e-03,  4.3025e-03,  6.6190e-04]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.bias',\n",
       "               tensor([ 4.1148e-06, -3.0719e-05,  7.4497e-06, -3.4848e-05, -2.3736e-05,\n",
       "                        2.7423e-05,  9.7458e-05,  2.2744e-05,  1.2797e-04,  3.3414e-06,\n",
       "                       -4.1116e-05,  2.5943e-05,  9.1436e-05, -4.1603e-05,  6.9108e-06,\n",
       "                       -1.9981e-05, -2.5792e-05,  7.1143e-05,  5.4551e-05,  4.0128e-05,\n",
       "                        1.7632e-05,  7.0416e-05, -4.1321e-06,  1.7419e-05, -1.5092e-05,\n",
       "                       -5.7090e-05,  7.6708e-05, -5.5327e-05,  7.3480e-05,  5.0790e-05,\n",
       "                        4.8549e-07,  8.0213e-06,  1.5264e-05,  7.6486e-05,  2.3384e-05,\n",
       "                        1.0855e-04,  3.6261e-05,  6.5156e-05,  3.5485e-05,  5.9375e-05,\n",
       "                        4.1000e-06,  2.7503e-05,  6.5003e-05,  2.0168e-05, -8.3550e-05,\n",
       "                       -9.7046e-05, -1.4699e-05, -9.3488e-06,  3.8215e-05,  2.4594e-05,\n",
       "                        3.4337e-05,  1.9131e-06,  4.2075e-05, -1.4390e-05, -4.2841e-05,\n",
       "                        7.4255e-07,  2.6968e-06,  2.9817e-05, -6.2324e-06,  6.0287e-05,\n",
       "                        2.5524e-05, -2.0588e-05,  4.2014e-05,  8.9406e-05, -1.3910e-05,\n",
       "                        1.4126e-05,  7.2740e-05,  1.9043e-05, -1.5898e-06, -4.6300e-05,\n",
       "                       -4.1803e-05,  5.5141e-05,  6.9545e-05, -2.9147e-13,  3.1673e-05,\n",
       "                        1.5363e-05, -6.4046e-05, -1.0479e-05, -4.5571e-06,  6.4764e-05,\n",
       "                        4.7045e-05,  8.2736e-06, -4.3851e-05,  2.0548e-05,  3.3424e-05,\n",
       "                       -7.4516e-09,  4.1237e-05, -3.5856e-05, -1.9378e-05,  8.5256e-06,\n",
       "                       -1.5111e-05, -3.6436e-05, -1.2282e-08, -4.6073e-05,  5.4436e-06,\n",
       "                       -2.6894e-05, -4.4803e-05, -3.7775e-05,  1.4704e-05,  5.5482e-05,\n",
       "                       -1.8633e-05,  3.1450e-06,  2.7742e-05,  8.6312e-06, -2.9363e-06,\n",
       "                       -2.8349e-05,  2.0668e-06,  6.1752e-05, -7.5442e-06,  5.2047e-05,\n",
       "                        1.5249e-06, -5.4357e-07,  1.3901e-05, -1.2682e-05,  1.9822e-05,\n",
       "                       -9.1055e-06,  4.3318e-05,  2.7723e-05,  2.5954e-08, -2.6200e-07,\n",
       "                       -2.5396e-05,  2.6903e-05,  2.6184e-05, -8.3174e-05, -2.4520e-05,\n",
       "                        6.4120e-05, -2.2401e-05,  5.5255e-05], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.bias',\n",
       "               tensor([-3.0563e-01, -3.1458e-01, -2.9772e-01, -2.4346e-01, -6.6835e-01,\n",
       "                       -1.9099e-01, -4.4044e-01, -3.9455e-01, -3.8735e-01, -3.1049e-01,\n",
       "                       -3.3268e-01, -2.2238e-01, -2.4641e-01, -2.2013e-01, -2.9882e-01,\n",
       "                       -3.9666e-01, -3.0288e-01, -4.4256e-01, -2.8804e-01, -3.0256e-01,\n",
       "                       -2.6832e-01, -4.1868e-01, -3.5976e-01, -3.1163e-01, -2.9641e-01,\n",
       "                       -3.2212e-01, -3.4523e-01, -3.6673e-01, -4.2566e-01, -3.6906e-01,\n",
       "                       -3.8152e-01, -3.3921e-01, -5.0597e-01, -3.5188e-01, -4.0000e-01,\n",
       "                       -3.5474e-01, -4.0755e-01, -3.6318e-01, -3.7759e-01, -3.7977e-01,\n",
       "                       -3.0839e-01, -2.9178e-01, -2.4657e-01, -2.7343e-01, -4.9589e-01,\n",
       "                       -3.5782e-01, -2.6987e-01, -3.1027e-01, -2.4167e-01, -2.8129e-01,\n",
       "                       -3.2476e-01, -5.4907e-02, -3.4402e-01, -2.3280e-01, -3.0943e-01,\n",
       "                       -3.1245e-01, -4.4543e-01, -4.3358e-01, -2.6492e-01, -3.1784e-01,\n",
       "                       -3.5520e-01, -5.6136e-02, -3.3466e-01, -3.4975e-01, -3.4201e-01,\n",
       "                       -2.9551e-01, -3.0021e-01, -3.4522e-01, -2.4248e-01, -3.4527e-01,\n",
       "                       -4.0757e-01, -4.3791e-01, -4.0889e-01, -1.5283e-06, -3.3880e-01,\n",
       "                       -3.4943e-01, -3.5251e-01, -3.6483e-01, -3.9116e-01, -3.6824e-01,\n",
       "                       -4.1225e-01, -3.5221e-01, -4.2016e-01, -3.7371e-01, -2.8595e-01,\n",
       "                       -3.2260e-09, -4.3658e-01, -6.1589e-01, -3.5254e-01, -4.1680e-01,\n",
       "                       -2.7020e-01, -3.9185e-01, -1.6627e-02, -3.3597e-01, -3.5498e-01,\n",
       "                       -2.6290e-01, -3.1257e-01, -3.0598e-01, -3.1074e-01, -4.0506e-01,\n",
       "                       -3.6151e-01, -2.6953e-01, -2.7332e-01, -2.8068e-01, -2.4352e-01,\n",
       "                       -4.6801e-01, -3.6620e-01, -4.0998e-01, -3.9226e-01, -3.9652e-01,\n",
       "                       -3.7766e-01, -1.2317e-02, -3.4483e-01, -3.2159e-01, -1.8135e-01,\n",
       "                       -4.3583e-01, -3.7318e-01, -3.6945e-01, -1.9208e-02, -4.4905e-02,\n",
       "                       -5.0897e-01, -3.7372e-01, -4.5999e-01, -2.7443e-01, -2.9310e-01,\n",
       "                       -3.8869e-01, -3.0887e-01, -3.0802e-01], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.weight',\n",
       "               tensor([ 2.3592e-01,  2.5663e-01,  2.7079e-01,  2.3522e-01,  4.1559e-01,\n",
       "                        3.1255e-01,  3.4737e-01,  3.1932e-01,  4.0211e-01,  2.3078e-01,\n",
       "                        3.2946e-01,  2.4021e-01,  3.0095e-01,  2.6415e-01,  2.6922e-01,\n",
       "                        3.0346e-01,  3.1151e-01,  2.8613e-01,  2.6763e-01,  2.2859e-01,\n",
       "                        3.5438e-01,  3.2578e-01,  4.0606e-01,  3.2513e-01,  2.1481e-01,\n",
       "                        3.2935e-01,  3.1585e-01,  2.9418e-01,  3.6476e-01,  3.2345e-01,\n",
       "                        3.3200e-01,  1.8931e-01,  3.6188e-01,  3.4013e-01,  3.7464e-01,\n",
       "                        3.1485e-01,  3.0583e-01,  2.8462e-01,  3.3134e-01,  3.1811e-01,\n",
       "                        3.1146e-01,  2.6502e-01,  3.8272e-01,  1.8106e-01,  4.3106e-01,\n",
       "                        3.8063e-01,  2.3874e-01,  2.6757e-01,  2.3639e-01,  2.8778e-01,\n",
       "                        3.3920e-01, -9.9439e-03,  2.7080e-01,  1.8937e-01,  2.7315e-01,\n",
       "                        2.0569e-01,  2.7809e-01,  3.9379e-01,  2.2394e-01,  2.2002e-01,\n",
       "                        3.0481e-01,  5.3630e-03,  2.6068e-01,  2.8310e-01,  2.8993e-01,\n",
       "                        2.2136e-01,  2.1598e-01,  2.8166e-01,  2.8554e-01,  3.1074e-01,\n",
       "                        3.6476e-01,  2.8530e-01,  3.5529e-01, -1.0906e-05,  2.6813e-01,\n",
       "                        1.9690e-01,  3.5162e-01,  2.4086e-01,  2.8262e-01,  3.1055e-01,\n",
       "                        3.6416e-01,  2.4663e-01,  2.5147e-01,  2.4771e-01,  3.1751e-01,\n",
       "                       -7.4019e-13,  3.3094e-01,  3.7604e-01,  3.5165e-01,  3.2362e-01,\n",
       "                        2.3468e-01,  3.1894e-01,  2.6266e-04,  3.1700e-01,  2.1215e-01,\n",
       "                        2.5553e-01,  2.4792e-01,  3.1999e-01,  2.7051e-01,  3.5401e-01,\n",
       "                        2.5810e-01,  2.5444e-01,  2.5836e-01,  2.7171e-01,  1.9829e-01,\n",
       "                        3.6425e-01,  3.3356e-01,  3.5483e-01,  3.7930e-01,  3.0989e-01,\n",
       "                        3.9350e-01, -1.2765e-03,  3.2671e-01,  2.8646e-01,  2.1674e-01,\n",
       "                        2.7782e-01,  3.6986e-01,  2.8355e-01,  1.0351e-03, -9.2582e-03,\n",
       "                        3.6544e-01,  3.2879e-01,  3.8032e-01,  3.1886e-01,  3.0557e-01,\n",
       "                        2.7602e-01,  2.1996e-01,  2.2006e-01], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.weight',\n",
       "               tensor([[[[ 5.3130e-03, -1.9780e-03,  5.6561e-02],\n",
       "                         [ 1.5122e-02, -1.5499e-02, -1.8957e-02],\n",
       "                         [ 3.3593e-02, -1.2456e-02,  1.8909e-02]],\n",
       "               \n",
       "                        [[ 1.7930e-02,  4.2224e-02,  2.5682e-02],\n",
       "                         [ 6.7252e-03,  2.3386e-02, -2.4938e-02],\n",
       "                         [-1.1179e-02,  1.2222e-02, -4.3314e-02]],\n",
       "               \n",
       "                        [[-6.5582e-02, -2.9642e-02, -1.3750e-02],\n",
       "                         [-2.8521e-02,  4.6883e-02,  1.5698e-02],\n",
       "                         [ 8.7255e-03,  1.7264e-03,  2.1632e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-7.2709e-03, -4.4612e-03,  6.5787e-02],\n",
       "                         [-8.8899e-03, -2.1925e-02,  4.3218e-02],\n",
       "                         [-1.2783e-02, -5.6096e-03,  1.6628e-02]],\n",
       "               \n",
       "                        [[-2.9000e-02, -4.3212e-02, -4.5758e-02],\n",
       "                         [-1.4740e-02, -3.3614e-02, -1.3895e-02],\n",
       "                         [-2.7585e-02, -3.2031e-02, -1.3259e-02]],\n",
       "               \n",
       "                        [[ 4.1170e-02, -3.4979e-02, -5.8073e-02],\n",
       "                         [ 4.7053e-02,  1.1501e-02, -1.7064e-03],\n",
       "                         [ 1.5702e-02,  2.2431e-02,  3.4584e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-2.0996e-02, -2.7753e-02, -5.0205e-02],\n",
       "                         [ 2.7860e-03, -5.7136e-02, -5.5309e-02],\n",
       "                         [ 5.3761e-03, -8.8264e-02, -8.8898e-02]],\n",
       "               \n",
       "                        [[-3.9438e-02, -2.1159e-02, -3.0146e-03],\n",
       "                         [-4.8190e-02, -7.5969e-03, -7.7427e-03],\n",
       "                         [-5.4145e-03, -1.7908e-02,  1.0635e-03]],\n",
       "               \n",
       "                        [[ 3.7948e-02,  4.2823e-02,  3.2218e-02],\n",
       "                         [-2.6161e-03,  5.6825e-02, -4.9147e-03],\n",
       "                         [-3.5823e-02,  5.3172e-02, -1.8067e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 3.5847e-02, -2.9904e-02, -8.6007e-03],\n",
       "                         [ 1.8160e-02, -1.9777e-02,  5.2063e-02],\n",
       "                         [-6.2782e-03,  1.2431e-02, -2.7841e-03]],\n",
       "               \n",
       "                        [[-3.4645e-02, -6.2690e-02, -1.2812e-02],\n",
       "                         [-4.5474e-02, -3.9181e-02, -4.6266e-02],\n",
       "                         [-6.6353e-02, -4.5572e-02, -8.3344e-02]],\n",
       "               \n",
       "                        [[-1.9088e-02,  1.4023e-02, -7.4619e-02],\n",
       "                         [ 4.1556e-02,  2.3961e-02, -1.0088e-01],\n",
       "                         [-3.1024e-02, -1.2478e-02, -7.9680e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.3228e-01,  4.2883e-03,  1.1995e-01],\n",
       "                         [ 2.0911e-02, -3.0739e-02,  7.1712e-02],\n",
       "                         [ 5.1864e-02, -4.4390e-02,  1.1230e-01]],\n",
       "               \n",
       "                        [[ 4.4259e-02,  6.6773e-04,  1.1222e-01],\n",
       "                         [-5.6907e-02, -9.5706e-02,  4.1929e-02],\n",
       "                         [-6.2271e-02, -1.2375e-01,  3.9232e-02]],\n",
       "               \n",
       "                        [[-1.9553e-02,  1.7628e-02,  1.0989e-02],\n",
       "                         [ 5.0570e-02, -5.2471e-02,  2.3417e-02],\n",
       "                         [ 6.4654e-02, -1.0896e-02, -1.4150e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 3.2401e-02,  3.9416e-02,  1.2624e-02],\n",
       "                         [ 3.9621e-02,  1.6973e-02, -4.9944e-02],\n",
       "                         [ 4.6996e-02, -3.7023e-03, -6.0787e-03]],\n",
       "               \n",
       "                        [[ 3.6207e-02,  5.2636e-02,  3.5395e-03],\n",
       "                         [-5.5915e-03, -2.5966e-02, -7.5678e-03],\n",
       "                         [ 2.8647e-02,  2.3489e-02,  2.1504e-02]],\n",
       "               \n",
       "                        [[ 6.7059e-02,  1.7840e-02, -9.6312e-03],\n",
       "                         [ 3.9628e-02, -3.5819e-03, -2.0748e-02],\n",
       "                         [-2.2450e-02, -3.1279e-02, -1.4217e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 4.2067e-42, -5.9231e-41,  1.5007e-41],\n",
       "                         [ 4.6845e-41,  4.1612e-41, -1.2927e-41],\n",
       "                         [-6.4398e-41,  7.2269e-41,  7.5236e-42]],\n",
       "               \n",
       "                        [[-3.1602e-41,  4.7783e-41, -3.6013e-43],\n",
       "                         [ 3.8847e-41,  5.7613e-41,  2.5303e-41],\n",
       "                         [-3.2413e-41, -1.2355e-41, -4.3217e-41]],\n",
       "               \n",
       "                        [[ 7.2126e-41,  5.9243e-41,  2.9685e-41],\n",
       "                         [ 7.0516e-41,  5.5908e-41,  5.3615e-41],\n",
       "                         [ 6.3185e-42,  4.0807e-41,  7.5506e-41]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-5.5276e-41,  2.7370e-41, -4.7288e-41],\n",
       "                         [ 1.6760e-41,  8.2061e-41,  8.0583e-41],\n",
       "                         [ 4.4357e-41, -2.8065e-41, -9.0309e-41]],\n",
       "               \n",
       "                        [[-5.5755e-41,  2.0371e-41, -4.9484e-41],\n",
       "                         [ 3.2621e-41, -5.9041e-41, -3.9738e-41],\n",
       "                         [-2.7670e-41, -1.2446e-41, -7.1159e-41]],\n",
       "               \n",
       "                        [[-4.4902e-41, -7.2994e-42,  4.5925e-41],\n",
       "                         [ 2.2233e-41,  3.1262e-41, -5.0919e-41],\n",
       "                         [ 5.5015e-41, -2.3916e-41, -4.4814e-42]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 2.0837e-02, -3.1339e-02,  5.9903e-02],\n",
       "                         [ 2.8330e-02, -4.8426e-03,  8.4465e-02],\n",
       "                         [ 2.9363e-02, -2.4854e-02,  6.6327e-02]],\n",
       "               \n",
       "                        [[-3.5870e-02, -9.2729e-03,  4.5005e-03],\n",
       "                         [ 1.3327e-02,  4.4002e-03,  3.6585e-03],\n",
       "                         [-1.7047e-02, -4.0070e-02, -4.9023e-02]],\n",
       "               \n",
       "                        [[-4.4597e-02, -5.5630e-02,  1.1121e-02],\n",
       "                         [-7.0841e-02, -7.6194e-03, -4.6218e-02],\n",
       "                         [-4.8051e-02, -4.2752e-02, -2.6553e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.4958e-02,  3.1096e-02, -3.3193e-03],\n",
       "                         [-4.2627e-04,  6.8605e-02, -2.2264e-02],\n",
       "                         [-5.2334e-02, -3.8526e-02, -8.1754e-02]],\n",
       "               \n",
       "                        [[ 3.1596e-02, -4.4737e-02, -4.3385e-02],\n",
       "                         [-2.7594e-02,  8.7404e-03,  2.6781e-02],\n",
       "                         [ 4.0714e-02,  5.5750e-02,  9.9914e-02]],\n",
       "               \n",
       "                        [[-5.7453e-02, -5.6769e-02, -5.7687e-02],\n",
       "                         [ 2.4897e-03, -8.6753e-02, -7.8637e-02],\n",
       "                         [ 2.3925e-02, -2.3921e-02,  3.2294e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-6.7210e-02, -9.3089e-03, -5.5546e-02],\n",
       "                         [-5.8678e-02, -2.4042e-02, -5.4562e-02],\n",
       "                         [-3.5017e-02, -1.9729e-02, -6.3944e-02]],\n",
       "               \n",
       "                        [[-6.7788e-02, -1.2684e-02, -2.2539e-03],\n",
       "                         [-4.8047e-02,  2.7606e-02,  2.9268e-02],\n",
       "                         [-1.0663e-02, -1.3603e-02, -2.5428e-02]],\n",
       "               \n",
       "                        [[-4.3759e-02, -3.6804e-03, -1.1538e-02],\n",
       "                         [-2.3065e-02, -2.5916e-02, -3.8411e-02],\n",
       "                         [ 3.3046e-02, -9.0393e-03,  2.3256e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.5838e-02,  2.5176e-02,  7.8847e-02],\n",
       "                         [ 3.7205e-02,  3.8724e-02,  4.4353e-02],\n",
       "                         [ 1.7558e-02,  2.5834e-02,  7.8250e-02]],\n",
       "               \n",
       "                        [[-1.7483e-03, -6.4069e-03,  5.4666e-02],\n",
       "                         [-1.4024e-02,  2.3173e-03,  4.7137e-05],\n",
       "                         [-4.3525e-02,  5.5488e-03, -1.4323e-02]],\n",
       "               \n",
       "                        [[-3.3331e-02, -2.1989e-02, -8.9064e-02],\n",
       "                         [-4.0411e-02, -9.6126e-02, -3.0313e-02],\n",
       "                         [-7.6919e-02, -4.3660e-02, -1.8066e-02]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.bias',\n",
       "               tensor([ 6.6720e-05, -2.9044e-05,  9.3644e-05, -2.6697e-05,  1.2022e-04,\n",
       "                       -4.4171e-05,  4.2583e-09, -3.6888e-05, -3.3932e-05, -1.7942e-05,\n",
       "                        2.3957e-07,  6.6745e-05,  1.5026e-05,  2.0148e-05,  1.2251e-05,\n",
       "                       -1.9049e-05, -3.4091e-05,  1.3866e-04,  2.0522e-12,  1.3810e-05,\n",
       "                        3.2157e-11, -4.2592e-05, -2.0559e-05,  5.9406e-11, -3.2642e-05,\n",
       "                        8.8354e-06, -4.9510e-05,  5.2061e-06,  5.9175e-06, -1.5176e-04,\n",
       "                       -6.9612e-05,  2.1542e-05, -7.9892e-05,  1.7976e-06,  2.5151e-05,\n",
       "                       -7.9762e-05,  9.3404e-06,  8.6768e-08,  1.1887e-05, -1.1735e-04,\n",
       "                        6.0666e-07,  1.8329e-05,  2.8508e-05,  1.2128e-17, -2.0750e-05,\n",
       "                        4.2234e-05,  4.5579e-05, -1.0751e-05,  4.9139e-05,  1.1984e-04,\n",
       "                       -6.0250e-05, -3.6171e-05, -4.7612e-06, -3.3149e-06, -1.0061e-05,\n",
       "                        3.6794e-05,  3.6426e-08, -2.4232e-06,  2.3655e-05, -4.7801e-05,\n",
       "                        1.7857e-05, -2.2519e-06, -1.2834e-06,  1.1839e-05,  1.9290e-05,\n",
       "                       -1.4093e-05, -1.5433e-05, -2.8619e-05, -2.7989e-05, -4.7456e-05,\n",
       "                       -2.8079e-05, -5.8285e-05,  2.3354e-05, -6.7080e-05,  7.2662e-11,\n",
       "                       -1.3584e-05, -4.8065e-05, -1.4097e-05,  3.5580e-05, -2.0654e-07,\n",
       "                        6.5457e-05,  1.9459e-05,  1.7677e-10, -1.4285e-08,  6.1659e-05,\n",
       "                        6.4544e-05,  5.6568e-05, -7.0586e-06, -2.2778e-05,  1.9640e-05,\n",
       "                       -3.0041e-06, -7.4548e-05, -3.3635e-05, -2.0906e-05,  2.7602e-07,\n",
       "                       -7.4361e-06,  2.4201e-10, -1.4499e-07, -2.0755e-05,  9.2620e-06,\n",
       "                       -1.3554e-05, -2.8774e-05,  2.2609e-14,  3.8223e-05,  4.2771e-06,\n",
       "                        1.2019e-05, -2.5601e-12, -6.2359e-05, -2.4408e-05,  1.1295e-05,\n",
       "                        2.7702e-06, -4.0171e-05, -5.0973e-05, -6.1408e-05,  1.9559e-05,\n",
       "                        5.5944e-05,  3.2600e-09, -1.7577e-05,  6.9647e-05, -4.1790e-06,\n",
       "                       -3.6063e-05, -1.4679e-05, -3.3627e-05, -3.0192e-05,  9.5059e-05,\n",
       "                       -6.7297e-06, -1.3410e-05, -2.6094e-05], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.bias',\n",
       "               tensor([-3.4368e-01, -3.7187e-01, -2.2315e-01, -4.0507e-01, -2.4545e-01,\n",
       "                       -2.3071e-01, -2.2333e-19, -5.2603e-01, -3.5572e-01, -2.7795e-01,\n",
       "                       -8.0811e-03, -3.2261e-01, -3.0220e-01, -3.7276e-01, -4.7557e-01,\n",
       "                       -2.0198e-01, -1.7460e-01, -4.7269e-01, -8.8359e-19, -3.9186e-01,\n",
       "                       -5.8312e-04, -2.4652e-01, -5.8680e-10, -3.4561e-05, -2.7710e-01,\n",
       "                       -3.4823e-01, -2.9655e-01, -2.1289e-01, -2.3053e-01, -2.4021e-01,\n",
       "                       -1.2193e-01, -3.9047e-04, -2.9979e-01, -1.7957e-01, -3.7876e-01,\n",
       "                       -1.9401e-01, -4.1630e-01, -4.3320e-30, -3.0751e-01, -3.3836e-01,\n",
       "                       -1.1883e-04, -7.4007e-02, -2.1467e-01, -1.1542e-23, -4.4861e-01,\n",
       "                       -1.6547e-01, -3.0349e-01, -2.4541e-01, -2.8078e-01, -4.4928e-01,\n",
       "                       -3.4207e-01, -3.9878e-01, -2.3932e-02, -2.9626e-01, -3.5668e-01,\n",
       "                       -5.7282e-01,  7.0207e-14, -4.6668e-35, -3.3035e-01, -3.0948e-01,\n",
       "                       -4.5024e-01, -1.1045e-01, -3.0781e-01, -2.7493e-01, -3.6285e-01,\n",
       "                       -3.4492e-01, -5.4243e-01, -1.1521e-01, -2.7794e-01, -2.1073e-01,\n",
       "                       -3.2689e-01, -2.2742e-01, -4.4304e-01, -3.6160e-01, -2.5321e-17,\n",
       "                        5.2988e-03, -7.0100e-01, -5.4747e-01, -4.2935e-01,  5.1127e-02,\n",
       "                       -5.8402e-01, -3.5517e-01, -1.2659e-19, -1.7307e-02, -2.2551e-04,\n",
       "                       -2.5738e-01, -4.5020e-01, -2.0826e-01, -3.9495e-01,  2.2534e-01,\n",
       "                       -2.0002e-01, -3.1215e-01, -3.6822e-01, -3.5747e-01, -4.8178e-14,\n",
       "                       -4.6489e-01, -1.6980e-03, -1.1321e-36, -1.1430e-01, -2.2211e-01,\n",
       "                       -1.6616e-01, -2.6964e-01, -1.1380e-21, -3.2552e-01, -3.3468e-01,\n",
       "                       -2.8501e-01, -8.6069e-11, -4.4103e-01, -5.4205e-01, -3.4410e-01,\n",
       "                       -2.6306e-01, -3.2150e-01, -2.9537e-01,  1.6975e-01, -1.4266e-01,\n",
       "                       -1.6252e-01, -2.8974e-34, -3.3296e-01, -4.6702e-01, -2.6048e-01,\n",
       "                       -1.1372e-01, -2.5747e-01, -3.9616e-01, -1.7037e-01, -3.8054e-01,\n",
       "                       -2.2645e-22, -3.1980e-01, -2.6032e-01], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.weight',\n",
       "               tensor([ 2.7914e-01,  3.7662e-01,  2.9605e-01,  4.3861e-01,  2.7963e-01,\n",
       "                        1.8722e-01, -1.1052e-14,  4.6149e-01,  2.9455e-01,  2.7036e-01,\n",
       "                        6.1319e-07,  2.7872e-01,  3.3582e-01,  3.8907e-01,  5.0032e-01,\n",
       "                        1.9406e-01,  2.2076e-01,  4.6753e-01,  3.3517e-13,  3.6937e-01,\n",
       "                        7.2512e-04,  2.9191e-01,  1.3915e-06,  7.5537e-04,  2.7153e-01,\n",
       "                        3.1079e-01,  3.3194e-01,  1.9593e-01,  2.5899e-01,  2.5754e-01,\n",
       "                        2.3295e-01, -6.7532e-04,  3.2951e-01,  1.6602e-01,  3.3806e-01,\n",
       "                        2.5146e-01,  3.5523e-01,  4.9269e-36,  3.3114e-01,  3.4983e-01,\n",
       "                       -3.2643e-04,  2.3549e-01,  2.6699e-01, -2.4157e-29,  3.6970e-01,\n",
       "                        2.8305e-01,  3.2000e-01,  2.5846e-01,  2.9285e-01,  4.0959e-01,\n",
       "                        3.5589e-01,  3.9481e-01,  2.3801e-01,  2.8281e-01,  3.6901e-01,\n",
       "                        5.2936e-01, -9.6289e-20,  3.2742e-39,  2.6744e-01,  2.8184e-01,\n",
       "                        3.3221e-01,  9.3814e-02,  1.4410e-01,  2.6860e-01,  3.2452e-01,\n",
       "                        3.1815e-01,  4.0799e-01,  2.0033e-01,  2.3466e-01,  2.8143e-01,\n",
       "                        2.6753e-01,  1.9631e-01,  3.1827e-01,  3.1739e-01,  8.6744e-25,\n",
       "                       -1.6480e-04,  5.0161e-01,  4.3257e-01,  3.8473e-01,  3.4408e-03,\n",
       "                        5.1626e-01,  3.3623e-01,  5.2002e-16, -3.5027e-03, -7.0735e-10,\n",
       "                        3.3562e-01,  3.9141e-01,  2.5099e-01,  3.5862e-01,  1.1734e-01,\n",
       "                        1.4150e-01,  2.6693e-01,  3.2581e-01,  3.2684e-01,  4.2521e-19,\n",
       "                        3.7670e-01,  2.3139e-04,  2.2274e-41,  2.3574e-01,  2.7498e-01,\n",
       "                        2.4075e-01,  2.5016e-01,  9.3971e-22,  2.9721e-01,  3.6312e-01,\n",
       "                        3.0917e-01, -4.2442e-07,  3.4753e-01,  4.6300e-01,  3.0619e-01,\n",
       "                        3.0461e-01,  3.1618e-01,  3.0656e-01,  2.1698e-01,  2.4138e-01,\n",
       "                        3.0562e-01,  1.8873e-35,  3.0071e-01,  4.1207e-01,  2.3553e-01,\n",
       "                        2.3931e-01,  2.3007e-01,  3.5393e-01,  2.3498e-01,  3.4553e-01,\n",
       "                       -8.9802e-26,  3.5373e-01,  2.7221e-01], device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.weights',\n",
       "               tensor([[-0.0020,  0.0004,  0.0020,  ...,  0.0007,  0.0030,  0.0012],\n",
       "                       [ 0.0012, -0.0015, -0.0032,  ...,  0.0037,  0.0028, -0.0002],\n",
       "                       [-0.0002, -0.0014, -0.0054,  ...,  0.0020,  0.0059, -0.0039],\n",
       "                       [ 0.0012,  0.0019,  0.0043,  ..., -0.0080, -0.0072,  0.0032],\n",
       "                       [-0.0001,  0.0008,  0.0021,  ...,  0.0026, -0.0030, -0.0003]],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.bias',\n",
       "               tensor([-0.0016,  0.0049,  0.0002,  0.0110, -0.0007], device='cuda:0')),\n",
       "              ('classifier.prompt.prompt_dict.pad_up',\n",
       "               tensor([[[ 7.5130e-03,  4.1744e-03,  1.2987e-02,  ...,  7.9473e-03,\n",
       "                         -9.5804e-03, -2.5830e-02],\n",
       "                        [-9.3412e-03,  1.1740e-02, -3.3205e-02,  ..., -1.2135e-02,\n",
       "                         -1.1630e-02, -1.9123e-02],\n",
       "                        [-1.4570e-02, -8.4153e-03,  3.9485e-03,  ...,  1.1257e-02,\n",
       "                         -2.4988e-02, -1.6318e-02],\n",
       "                        [-3.5516e-02,  8.0686e-04,  2.9344e-02,  ...,  1.3307e-02,\n",
       "                         -4.6928e-03, -1.7342e-02],\n",
       "                        [-3.7400e-02,  8.9368e-05,  2.6797e-02,  ..., -1.9179e-03,\n",
       "                          4.7594e-02,  8.5487e-03]],\n",
       "               \n",
       "                       [[-3.3756e-04, -1.5467e-02,  4.3531e-04,  ..., -3.4618e-02,\n",
       "                         -1.8572e-02, -1.8906e-02],\n",
       "                        [-9.3433e-03,  2.2227e-02,  1.2871e-02,  ..., -1.5152e-02,\n",
       "                          3.0331e-02,  1.5484e-02],\n",
       "                        [ 4.1033e-03,  1.8770e-02, -1.0899e-02,  ...,  2.4775e-02,\n",
       "                         -4.2437e-03,  1.2766e-02],\n",
       "                        [-2.1921e-03,  1.8347e-02,  1.0117e-03,  ..., -2.4762e-02,\n",
       "                         -4.7578e-03, -2.3316e-02],\n",
       "                        [ 1.5641e-03,  1.1520e-02,  4.4374e-03,  ..., -1.0685e-02,\n",
       "                         -1.7598e-02, -1.7871e-02]],\n",
       "               \n",
       "                       [[ 1.6623e-03, -3.4609e-03, -6.1229e-03,  ..., -1.5969e-02,\n",
       "                          1.0914e-02, -1.0821e-03],\n",
       "                        [-1.9814e-02,  1.1447e-02,  1.4043e-03,  ..., -1.1265e-02,\n",
       "                          3.4367e-02,  1.9642e-02],\n",
       "                        [-8.8546e-03,  3.9506e-03,  3.4624e-03,  ...,  2.6423e-03,\n",
       "                          1.2768e-02,  1.9800e-02],\n",
       "                        [-1.9184e-02,  3.7432e-03,  9.9160e-03,  ..., -5.1006e-04,\n",
       "                          2.8930e-02,  7.8193e-03],\n",
       "                        [ 8.8866e-04,  2.4372e-02,  3.2451e-02,  ...,  1.1503e-02,\n",
       "                          3.3820e-02,  1.0436e-02]]], device='cuda:0')),\n",
       "              ('classifier.prompt.prompt_dict.pad_down',\n",
       "               tensor([[[-0.0158, -0.0118,  0.0266,  ..., -0.0093,  0.0031,  0.0119],\n",
       "                        [-0.0008, -0.0208,  0.0160,  ..., -0.0287,  0.0036, -0.0218],\n",
       "                        [ 0.0078, -0.0181,  0.0018,  ...,  0.0179, -0.0033,  0.0094],\n",
       "                        [ 0.0056, -0.0073, -0.0342,  ...,  0.0162, -0.0080,  0.0151],\n",
       "                        [ 0.0017, -0.0159, -0.0012,  ...,  0.0122,  0.0037, -0.0044]],\n",
       "               \n",
       "                       [[-0.0100,  0.0078,  0.0272,  ...,  0.0257, -0.0323, -0.0075],\n",
       "                        [-0.0191, -0.0022,  0.0304,  ...,  0.0138,  0.0073, -0.0118],\n",
       "                        [-0.0072, -0.0214, -0.0019,  ...,  0.0136, -0.0242, -0.0085],\n",
       "                        [-0.0274, -0.0025, -0.0026,  ..., -0.0160, -0.0461, -0.0154],\n",
       "                        [-0.0076, -0.0066,  0.0145,  ..., -0.0162, -0.0203, -0.0062]],\n",
       "               \n",
       "                       [[ 0.0159,  0.0163,  0.0035,  ...,  0.0047,  0.0035,  0.0114],\n",
       "                        [-0.0050, -0.0012, -0.0153,  ..., -0.0128,  0.0186,  0.0045],\n",
       "                        [ 0.0112, -0.0050, -0.0056,  ..., -0.0021, -0.0073,  0.0128],\n",
       "                        [ 0.0055,  0.0414, -0.0042,  ..., -0.0039, -0.0053,  0.0255],\n",
       "                        [ 0.0138,  0.0240, -0.0092,  ...,  0.0249,  0.0232,  0.0174]]],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.prompt.prompt_dict.pad_left',\n",
       "               tensor([[[-0.0138, -0.0087,  0.0045, -0.0058, -0.0241],\n",
       "                        [ 0.0260,  0.0645,  0.0504,  0.0059, -0.0206],\n",
       "                        [-0.0339, -0.0138, -0.0014,  0.0054,  0.0027],\n",
       "                        ...,\n",
       "                        [-0.0238,  0.0054,  0.0190,  0.0540, -0.0342],\n",
       "                        [-0.0246,  0.0220,  0.0094,  0.0518, -0.0196],\n",
       "                        [-0.0286,  0.0041, -0.0207,  0.0210, -0.0347]],\n",
       "               \n",
       "                       [[ 0.0029, -0.0361, -0.0460, -0.0120, -0.0107],\n",
       "                        [ 0.0071,  0.0100, -0.0267, -0.0119, -0.0008],\n",
       "                        [-0.0427, -0.0105, -0.0343, -0.0076,  0.0173],\n",
       "                        ...,\n",
       "                        [-0.0418, -0.0109, -0.0026,  0.0201, -0.0222],\n",
       "                        [-0.0285,  0.0108, -0.0181,  0.0225, -0.0083],\n",
       "                        [-0.0333,  0.0083, -0.0107,  0.0199, -0.0062]],\n",
       "               \n",
       "                       [[ 0.0103, -0.0081, -0.0012,  0.0164, -0.0044],\n",
       "                        [ 0.0222,  0.0352, -0.0061, -0.0075, -0.0017],\n",
       "                        [ 0.0046,  0.0014, -0.0020,  0.0297, -0.0021],\n",
       "                        ...,\n",
       "                        [-0.0033,  0.0320, -0.0327, -0.0218, -0.0036],\n",
       "                        [-0.0002, -0.0048, -0.0381,  0.0075, -0.0148],\n",
       "                        [ 0.0091,  0.0049, -0.0349,  0.0224,  0.0107]]], device='cuda:0')),\n",
       "              ('classifier.prompt.prompt_dict.pad_right',\n",
       "               tensor([[[ 0.0032,  0.0208,  0.0083,  0.0121, -0.0175],\n",
       "                        [ 0.0034,  0.0263, -0.0344,  0.0286, -0.0049],\n",
       "                        [-0.0040, -0.0138, -0.0216,  0.0097,  0.0212],\n",
       "                        ...,\n",
       "                        [-0.0280,  0.0053, -0.0112,  0.0089, -0.0223],\n",
       "                        [ 0.0043, -0.0046, -0.0139, -0.0009,  0.0082],\n",
       "                        [ 0.0409, -0.0060,  0.0109,  0.0044,  0.0084]],\n",
       "               \n",
       "                       [[-0.0127, -0.0093,  0.0337, -0.0141, -0.0137],\n",
       "                        [ 0.0028,  0.0264, -0.0261,  0.0088,  0.0028],\n",
       "                        [ 0.0009,  0.0135, -0.0022, -0.0242, -0.0152],\n",
       "                        ...,\n",
       "                        [-0.0100,  0.0069,  0.0145,  0.0022, -0.0132],\n",
       "                        [ 0.0271, -0.0146,  0.0111,  0.0155, -0.0037],\n",
       "                        [ 0.0093, -0.0458,  0.0025, -0.0082, -0.0147]],\n",
       "               \n",
       "                       [[ 0.0003, -0.0042,  0.0193,  0.0058, -0.0051],\n",
       "                        [ 0.0209,  0.0111, -0.0158,  0.0022, -0.0148],\n",
       "                        [-0.0006, -0.0049,  0.0150, -0.0340,  0.0088],\n",
       "                        ...,\n",
       "                        [-0.0148,  0.0026, -0.0030, -0.0020, -0.0123],\n",
       "                        [ 0.0288, -0.0125,  0.0036,  0.0110,  0.0089],\n",
       "                        [ 0.0147,  0.0015, -0.0151,  0.0223, -0.0067]]], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.prompt_learning_rates_dict.prompt_weight_learning_rate',\n",
       "               tensor([ 3.4460e-03,  7.4616e-02,  1.0423e-01,  1.9819e-01,  2.4383e-01,\n",
       "                       -1.1517e-14], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv0-conv-weight',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv0-conv-bias',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv1-conv-weight',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv1-conv-bias',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv2-conv-weight',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv2-conv-bias',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv3-conv-weight',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv3-conv-bias',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-weights',\n",
       "               tensor([-2.3590e-02,  6.2843e-01,  3.0214e-01,  6.1373e-01,  8.3865e-01,\n",
       "                       -1.1517e-14], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-bias',\n",
       "               tensor([ 7.3374e-02,  3.4855e-02,  1.1948e-01, -2.7496e-02,  6.1903e-02,\n",
       "                       -1.1517e-14], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv0-conv-weight',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv0-conv-bias',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv1-conv-weight',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv1-conv-bias',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv2-conv-weight',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv2-conv-bias',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv3-conv-weight',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv3-conv-bias',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-linear-weights',\n",
       "               tensor([ 9.9746e-01,  1.3463e-01, -5.7126e-01, -1.0711e+00, -1.6469e+00,\n",
       "                       -1.7893e-16], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-linear-bias',\n",
       "               tensor([ 3.3659e-01,  2.0615e-01,  1.4065e-01, -1.3328e-02, -4.1551e-02,\n",
       "                       -1.7893e-16], device='cuda:0'))]),\n",
       " 'per_epoch_statistics': {'train_loss_mean': [1.496346093893051,\n",
       "   1.2924049600362777,\n",
       "   1.1642878410816193,\n",
       "   1.0810648844242097,\n",
       "   1.0233372339010238,\n",
       "   0.9850033233165741,\n",
       "   0.9642499526143073,\n",
       "   0.9238972016572953,\n",
       "   0.8975343232154847,\n",
       "   0.8889881691336632,\n",
       "   0.8410158300995827,\n",
       "   0.8209185247421265,\n",
       "   0.8072076091766357,\n",
       "   0.7662069013118744,\n",
       "   0.7520081683397293,\n",
       "   0.7411940937638283,\n",
       "   0.7190891927480698,\n",
       "   0.7086033374667168,\n",
       "   0.7035555218458176,\n",
       "   0.6873952871561051,\n",
       "   0.6689738253355026,\n",
       "   0.6525641787648201,\n",
       "   0.6411443006396293,\n",
       "   0.6340343555212021,\n",
       "   0.6246323977708816,\n",
       "   0.6081247877478599,\n",
       "   0.6043185717463493,\n",
       "   0.5986587316393852,\n",
       "   0.5938570529818535,\n",
       "   0.6046536201238633,\n",
       "   0.5834399861693382,\n",
       "   0.5790533012747765,\n",
       "   0.5808736301958561,\n",
       "   0.5702312004864216,\n",
       "   0.5807338356971741,\n",
       "   0.5494624493420124,\n",
       "   0.5511455799937248,\n",
       "   0.5529115287065506,\n",
       "   0.552762713432312,\n",
       "   0.5375264892876148,\n",
       "   0.5532640497386455,\n",
       "   0.5421308366060257,\n",
       "   0.5370814420580864,\n",
       "   0.5402184256911278,\n",
       "   0.5326841808855534,\n",
       "   0.5284627570807934,\n",
       "   0.5272563442587852,\n",
       "   0.5276968701481819,\n",
       "   0.5145571402013301,\n",
       "   0.5211905585825444,\n",
       "   0.5286516550779342,\n",
       "   0.5015884015262126,\n",
       "   0.5235008170008659,\n",
       "   0.5088444416821003,\n",
       "   0.5252730291783809,\n",
       "   0.5147478224337101,\n",
       "   0.5111212004721165,\n",
       "   0.5058917886912823,\n",
       "   0.5100892129838467,\n",
       "   0.51079413177073,\n",
       "   0.5105545319318772,\n",
       "   0.5039699096381665,\n",
       "   0.4942445837557316,\n",
       "   0.4886610935032368,\n",
       "   0.5016043248176575,\n",
       "   0.5048804458379745,\n",
       "   0.5062462021112442,\n",
       "   0.4883588737249374,\n",
       "   0.4954598996639252,\n",
       "   0.49659022119641305,\n",
       "   0.4929255698621273,\n",
       "   0.4891308481097221,\n",
       "   0.4969402396082878,\n",
       "   0.4898764785528183,\n",
       "   0.49228207167983057,\n",
       "   0.48442411866784096,\n",
       "   0.4901118133068085,\n",
       "   0.4907370929419994,\n",
       "   0.4786671579480171,\n",
       "   0.48844914343953133,\n",
       "   0.47831976661086084,\n",
       "   0.4887243137955666,\n",
       "   0.494289960116148,\n",
       "   0.4910734208226204,\n",
       "   0.4745271619260311,\n",
       "   0.4841573078930378,\n",
       "   0.4786517176926136,\n",
       "   0.48033083164691925,\n",
       "   0.4777427123486996,\n",
       "   0.46934881591796873,\n",
       "   0.4817459506094456,\n",
       "   0.4802328062355518,\n",
       "   0.4721788158118725,\n",
       "   0.4686367639005184,\n",
       "   0.47980103263258933,\n",
       "   0.4811181266903877,\n",
       "   0.48096853771805764,\n",
       "   0.475705917596817,\n",
       "   0.4598559148311615],\n",
       "  'train_loss_std': [0.4628824630744862,\n",
       "   0.13074887718867886,\n",
       "   0.14313705346043107,\n",
       "   0.14221093553454925,\n",
       "   0.14557086346442186,\n",
       "   0.14371881361290792,\n",
       "   0.15209507739084677,\n",
       "   0.144835170587049,\n",
       "   0.14713639395104355,\n",
       "   0.14277316242573487,\n",
       "   0.15169357279772938,\n",
       "   0.1506865384485588,\n",
       "   0.14223973153689515,\n",
       "   0.14500041318909251,\n",
       "   0.1378760496331313,\n",
       "   0.14023789334943787,\n",
       "   0.13844994997133891,\n",
       "   0.1431424506785462,\n",
       "   0.1488258117283837,\n",
       "   0.14322927669827293,\n",
       "   0.14706479080061527,\n",
       "   0.13575444068271647,\n",
       "   0.15219449643818167,\n",
       "   0.1475938180780873,\n",
       "   0.14329830981432398,\n",
       "   0.14070849370459326,\n",
       "   0.14204269781339304,\n",
       "   0.14781224836699963,\n",
       "   0.1408262876540963,\n",
       "   0.14465011357922147,\n",
       "   0.14296924317294077,\n",
       "   0.1384331633316693,\n",
       "   0.13989097032248982,\n",
       "   0.15259541700393314,\n",
       "   0.13555482186694537,\n",
       "   0.13515495272127762,\n",
       "   0.13546176695625925,\n",
       "   0.14035743247582735,\n",
       "   0.14697247682509426,\n",
       "   0.1382313088482783,\n",
       "   0.13904603342602778,\n",
       "   0.13861502835526235,\n",
       "   0.1426061025400776,\n",
       "   0.1393544971764454,\n",
       "   0.13478340803136318,\n",
       "   0.13097147610236431,\n",
       "   0.13846387125171558,\n",
       "   0.13684174227951815,\n",
       "   0.1346447606883688,\n",
       "   0.1356382401929446,\n",
       "   0.13435641156536568,\n",
       "   0.1279194453740445,\n",
       "   0.13182744911147473,\n",
       "   0.1369984545343593,\n",
       "   0.14259993091103765,\n",
       "   0.14193856920387105,\n",
       "   0.13637897219706258,\n",
       "   0.13562638739813812,\n",
       "   0.13762544659974074,\n",
       "   0.128730753768318,\n",
       "   0.13277939789568638,\n",
       "   0.142044275600681,\n",
       "   0.13159144513316465,\n",
       "   0.13073938514579364,\n",
       "   0.13337707015734598,\n",
       "   0.12900653451868968,\n",
       "   0.1363514672533703,\n",
       "   0.13269749087993823,\n",
       "   0.13976494719281857,\n",
       "   0.14114526855159196,\n",
       "   0.12969097834785936,\n",
       "   0.1350045656591339,\n",
       "   0.13978129451655283,\n",
       "   0.13237216783548436,\n",
       "   0.13490011655178277,\n",
       "   0.12925037117528934,\n",
       "   0.1318046146503244,\n",
       "   0.1323687265406117,\n",
       "   0.13026927136348135,\n",
       "   0.14293187468231805,\n",
       "   0.13409448195546572,\n",
       "   0.13571112327563403,\n",
       "   0.13737593071183332,\n",
       "   0.1398359490135198,\n",
       "   0.13174907737571495,\n",
       "   0.13012573107035344,\n",
       "   0.13645650589791491,\n",
       "   0.14030811943163085,\n",
       "   0.13302181704521812,\n",
       "   0.1261746148712798,\n",
       "   0.13320529324778418,\n",
       "   0.13285381566049503,\n",
       "   0.1254172558843821,\n",
       "   0.12998311153937567,\n",
       "   0.13048068026635695,\n",
       "   0.13961821010399195,\n",
       "   0.12385322688304462,\n",
       "   0.1355144454331911,\n",
       "   0.12418147846207225],\n",
       "  'train_accuracy_mean': [0.3937600003182888,\n",
       "   0.4648000004291534,\n",
       "   0.531799998819828,\n",
       "   0.5719599987268448,\n",
       "   0.5979466657042504,\n",
       "   0.6144133336544036,\n",
       "   0.6271600008606911,\n",
       "   0.6402400004267692,\n",
       "   0.6543733341097832,\n",
       "   0.6555600000619888,\n",
       "   0.6757866662144661,\n",
       "   0.6854666667580604,\n",
       "   0.6900266677141189,\n",
       "   0.7078800003528595,\n",
       "   0.7129466669559479,\n",
       "   0.7197200001478196,\n",
       "   0.7263600004315376,\n",
       "   0.7319066669940949,\n",
       "   0.7323199993371964,\n",
       "   0.7412133333683014,\n",
       "   0.7480800000429153,\n",
       "   0.7551866674423218,\n",
       "   0.7597333328723908,\n",
       "   0.7616266659498214,\n",
       "   0.7643066667318344,\n",
       "   0.7721866663694382,\n",
       "   0.7727999993562699,\n",
       "   0.7768266667127609,\n",
       "   0.7780800001621246,\n",
       "   0.7736266677379608,\n",
       "   0.7815866656303406,\n",
       "   0.783679998755455,\n",
       "   0.780813333272934,\n",
       "   0.7861733331680297,\n",
       "   0.7824666661024093,\n",
       "   0.7923866661787033,\n",
       "   0.7915733332633972,\n",
       "   0.7933999989032745,\n",
       "   0.7920266652107238,\n",
       "   0.8007599996328354,\n",
       "   0.7917866659164429,\n",
       "   0.7976399991512299,\n",
       "   0.8011866668462754,\n",
       "   0.7972133338451386,\n",
       "   0.8008800002336502,\n",
       "   0.8035466665029526,\n",
       "   0.8033333327770233,\n",
       "   0.802453332901001,\n",
       "   0.8078933337926865,\n",
       "   0.804600000500679,\n",
       "   0.8001333334445954,\n",
       "   0.8127866667509079,\n",
       "   0.8038666663169861,\n",
       "   0.8105466665029526,\n",
       "   0.802066666841507,\n",
       "   0.8083200005292892,\n",
       "   0.8096666655540466,\n",
       "   0.8118799995183945,\n",
       "   0.8098533335924148,\n",
       "   0.8082000000476837,\n",
       "   0.8087733328342438,\n",
       "   0.8130533345937729,\n",
       "   0.8154133331775665,\n",
       "   0.8188000010251999,\n",
       "   0.8152800000905991,\n",
       "   0.8124933331012726,\n",
       "   0.8107466665506363,\n",
       "   0.8185733330249786,\n",
       "   0.8161333339214325,\n",
       "   0.8145733332633972,\n",
       "   0.8166800006628037,\n",
       "   0.816986668229103,\n",
       "   0.8162666655778885,\n",
       "   0.8187733335494995,\n",
       "   0.8159199998378753,\n",
       "   0.8183333339691162,\n",
       "   0.8174266672134399,\n",
       "   0.8174266670942306,\n",
       "   0.8218133339881897,\n",
       "   0.8181466680765151,\n",
       "   0.8220933330059051,\n",
       "   0.8188266676664352,\n",
       "   0.8150666662454605,\n",
       "   0.8178400002717972,\n",
       "   0.8263733348846436,\n",
       "   0.8199466675519943,\n",
       "   0.8215199996232987,\n",
       "   0.8226133335828781,\n",
       "   0.8224000010490418,\n",
       "   0.8268666672706604,\n",
       "   0.8204000000953674,\n",
       "   0.8213733351230621,\n",
       "   0.8248000004291535,\n",
       "   0.8250666660070419,\n",
       "   0.8206133327484131,\n",
       "   0.8213599990606308,\n",
       "   0.8217599997520447,\n",
       "   0.8241199997663498,\n",
       "   0.8304666671752929],\n",
       "  'train_accuracy_std': [0.0762953055351448,\n",
       "   0.07227650574323434,\n",
       "   0.07558162503959025,\n",
       "   0.07384053802288292,\n",
       "   0.07092692322166999,\n",
       "   0.07129898206205043,\n",
       "   0.07486893051822904,\n",
       "   0.06847893608825141,\n",
       "   0.07018821379839135,\n",
       "   0.07029791983682347,\n",
       "   0.0719878318562535,\n",
       "   0.07090011688641316,\n",
       "   0.0650490359327253,\n",
       "   0.06778819261573539,\n",
       "   0.06635883350039962,\n",
       "   0.06439883829089693,\n",
       "   0.062989200288177,\n",
       "   0.06600074202671195,\n",
       "   0.06743717969600078,\n",
       "   0.06401002549189376,\n",
       "   0.06637370735306657,\n",
       "   0.06205256466172068,\n",
       "   0.06563007352534182,\n",
       "   0.06455745503674996,\n",
       "   0.06332866271356125,\n",
       "   0.06143412573582967,\n",
       "   0.06157401974097166,\n",
       "   0.0648599944102262,\n",
       "   0.05966761910477561,\n",
       "   0.06113084687204872,\n",
       "   0.06124390428936367,\n",
       "   0.061140020566506706,\n",
       "   0.059698172545579506,\n",
       "   0.06592538559868275,\n",
       "   0.05834955872240435,\n",
       "   0.05955906050923742,\n",
       "   0.05894415455245543,\n",
       "   0.06023136963269203,\n",
       "   0.06326156870392118,\n",
       "   0.05867613854455253,\n",
       "   0.05980196013299441,\n",
       "   0.05908964379671741,\n",
       "   0.059178944270379785,\n",
       "   0.06001602573466779,\n",
       "   0.05741179720381781,\n",
       "   0.05578250583314952,\n",
       "   0.05749357480201732,\n",
       "   0.057908960332235485,\n",
       "   0.05850969415165717,\n",
       "   0.058116320929510155,\n",
       "   0.05765784241258597,\n",
       "   0.05461756983068599,\n",
       "   0.05659038072197336,\n",
       "   0.05784242140885929,\n",
       "   0.061898089270666694,\n",
       "   0.06005719469841473,\n",
       "   0.058100106231053564,\n",
       "   0.05867214927148683,\n",
       "   0.059693111795163095,\n",
       "   0.05213704299637321,\n",
       "   0.05592063724940702,\n",
       "   0.05972091934505434,\n",
       "   0.05584528593909714,\n",
       "   0.05536449122094984,\n",
       "   0.057500043669465976,\n",
       "   0.055387976641338384,\n",
       "   0.05846383616223066,\n",
       "   0.05622344441977614,\n",
       "   0.05810626197754807,\n",
       "   0.06196752119345613,\n",
       "   0.05559456250558118,\n",
       "   0.059339960915227585,\n",
       "   0.05842008783227671,\n",
       "   0.05624001891037982,\n",
       "   0.05707128396177684,\n",
       "   0.05428791553950376,\n",
       "   0.05463942581200512,\n",
       "   0.056727420934303364,\n",
       "   0.05527065382023907,\n",
       "   0.06068524258725905,\n",
       "   0.05792846336778449,\n",
       "   0.056757192800680846,\n",
       "   0.05973586347448109,\n",
       "   0.059623645474930126,\n",
       "   0.05453727211529433,\n",
       "   0.05499734895477247,\n",
       "   0.05717809066853103,\n",
       "   0.059589089880439094,\n",
       "   0.05756480506780072,\n",
       "   0.05359315885233267,\n",
       "   0.05629146506698393,\n",
       "   0.05448243867690305,\n",
       "   0.051543984075911285,\n",
       "   0.05650030599963506,\n",
       "   0.056399975548871314,\n",
       "   0.05956971134304802,\n",
       "   0.054321800499778264,\n",
       "   0.0568303830581583,\n",
       "   0.05305808186859942],\n",
       "  'train_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'train_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_learning_rate_mean': [0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005],\n",
       "  'train_learning_rate_std': [4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19],\n",
       "  'val_loss_mean': [1.3855695275465647,\n",
       "   1.3313136502106984,\n",
       "   1.2110044300556182,\n",
       "   1.163428992231687,\n",
       "   1.1150872359673183,\n",
       "   1.1200141608715057,\n",
       "   1.0902823402484259,\n",
       "   1.0718771002689997,\n",
       "   1.0315382168690364,\n",
       "   1.0432236325740813,\n",
       "   0.984615732828776,\n",
       "   0.9823426645994187,\n",
       "   0.9669351762533188,\n",
       "   0.9526431081692378,\n",
       "   0.9401012845834096,\n",
       "   0.9229670967658361,\n",
       "   0.9713936672608058,\n",
       "   0.9225133486588796,\n",
       "   0.9376896009842555,\n",
       "   0.9004332739114761,\n",
       "   0.9023425255219142,\n",
       "   0.8850925326347351,\n",
       "   0.8787328711152077,\n",
       "   0.8781375086307526,\n",
       "   0.8887037482857704,\n",
       "   0.9015549502770106,\n",
       "   0.8930870233972867,\n",
       "   0.878707901040713,\n",
       "   0.8635358862082163,\n",
       "   0.8641655431191126,\n",
       "   0.8330710434913635,\n",
       "   0.8684516123930613,\n",
       "   0.8785140258073807,\n",
       "   0.8773742922147115,\n",
       "   0.8702817620833715,\n",
       "   0.8667421108484268,\n",
       "   0.8559070691466332,\n",
       "   0.8550054450829824,\n",
       "   0.8665093780557315,\n",
       "   0.8295639957984289,\n",
       "   0.8644967776536941,\n",
       "   0.843781838218371,\n",
       "   0.8409774909416835,\n",
       "   0.8275321811437607,\n",
       "   0.848246587117513,\n",
       "   0.875622916618983,\n",
       "   0.8214340731501579,\n",
       "   0.8733409424622853,\n",
       "   0.8311639568209648,\n",
       "   0.8238171725471815,\n",
       "   0.8475747946898142,\n",
       "   0.8541461092233658,\n",
       "   0.8797395451863607,\n",
       "   0.8258581519126892,\n",
       "   0.8496840089559555,\n",
       "   0.8549220836162568,\n",
       "   0.8245330474774043,\n",
       "   0.8514946871995925,\n",
       "   0.8517535161972046,\n",
       "   0.8454753261804581,\n",
       "   0.8492723941802979,\n",
       "   0.834957726597786,\n",
       "   0.84335165143013,\n",
       "   0.8389772958556811,\n",
       "   0.8200442336996396,\n",
       "   0.8384741989771525,\n",
       "   0.8505629183848699,\n",
       "   0.8694832553466161,\n",
       "   0.840363150437673,\n",
       "   0.856059288183848,\n",
       "   0.8277565697828929,\n",
       "   0.8411390109856923,\n",
       "   0.8161363436778386,\n",
       "   0.8364702330032985,\n",
       "   0.85490385333697,\n",
       "   0.8279813615481059,\n",
       "   0.8440004126230876,\n",
       "   0.8345594441890717,\n",
       "   0.8516289188464483,\n",
       "   0.8639714513222376,\n",
       "   0.8204352317253748,\n",
       "   0.8217419898509979,\n",
       "   0.8549242572983106,\n",
       "   0.8273665900031726,\n",
       "   0.8451677920420965,\n",
       "   0.8392119061946869,\n",
       "   0.82137531042099,\n",
       "   0.8271450742085775,\n",
       "   0.843922481139501,\n",
       "   0.8432069152593613,\n",
       "   0.8470339777072271,\n",
       "   0.8263056143124898,\n",
       "   0.8157795881231625,\n",
       "   0.8138158428668976,\n",
       "   0.8150241486231486,\n",
       "   0.8185139026244481,\n",
       "   0.8075438929597537,\n",
       "   0.8254609331488609,\n",
       "   0.8471215443809827],\n",
       "  'val_loss_std': [0.08969184169888339,\n",
       "   0.09483965384528881,\n",
       "   0.111301393474684,\n",
       "   0.11938114658113218,\n",
       "   0.1192477548698768,\n",
       "   0.12195690637436515,\n",
       "   0.1215984530052953,\n",
       "   0.12690071394298874,\n",
       "   0.12309151341409798,\n",
       "   0.11770860177049583,\n",
       "   0.13764689383309145,\n",
       "   0.13378596279871716,\n",
       "   0.13176748312018266,\n",
       "   0.1316443647295752,\n",
       "   0.1355623378843493,\n",
       "   0.13302411839473965,\n",
       "   0.13835643309389914,\n",
       "   0.13603384113075517,\n",
       "   0.14200997689609554,\n",
       "   0.13599231422475755,\n",
       "   0.12929532141741998,\n",
       "   0.1367913481216415,\n",
       "   0.14142186880308463,\n",
       "   0.13708737856011416,\n",
       "   0.13398098093715624,\n",
       "   0.1312327501430933,\n",
       "   0.1432426370915358,\n",
       "   0.13775711913187705,\n",
       "   0.13993274590312169,\n",
       "   0.14194716420036124,\n",
       "   0.13895186360419254,\n",
       "   0.14308541550567647,\n",
       "   0.1436232883516264,\n",
       "   0.139942079959465,\n",
       "   0.13921601001495554,\n",
       "   0.13515449456741568,\n",
       "   0.1369122949671256,\n",
       "   0.14852752112004977,\n",
       "   0.13929489338561513,\n",
       "   0.139393163803022,\n",
       "   0.14791771100759368,\n",
       "   0.13981881964407322,\n",
       "   0.13422618636880068,\n",
       "   0.1416856635842332,\n",
       "   0.13955410046349778,\n",
       "   0.14078270384441807,\n",
       "   0.14035434944108371,\n",
       "   0.14365413594810286,\n",
       "   0.1417745715825113,\n",
       "   0.14748113326644474,\n",
       "   0.1361929198379658,\n",
       "   0.14717131390264185,\n",
       "   0.1413837212866215,\n",
       "   0.1443378707202301,\n",
       "   0.13765822754941512,\n",
       "   0.14321737333654716,\n",
       "   0.135110142272776,\n",
       "   0.13882684792009953,\n",
       "   0.14411943947018005,\n",
       "   0.13938959560356745,\n",
       "   0.14091744952564692,\n",
       "   0.14743551998908824,\n",
       "   0.1456347550291971,\n",
       "   0.14262757066576046,\n",
       "   0.1461326187400414,\n",
       "   0.13668158657140436,\n",
       "   0.14546447214136848,\n",
       "   0.14833028792543565,\n",
       "   0.1453790801639148,\n",
       "   0.14545603559291054,\n",
       "   0.14860635952631635,\n",
       "   0.14360974584413838,\n",
       "   0.13430452680109686,\n",
       "   0.14347670840497362,\n",
       "   0.14464256476663068,\n",
       "   0.13815261897302625,\n",
       "   0.1534563183025094,\n",
       "   0.13633418670353606,\n",
       "   0.13839005533305618,\n",
       "   0.14863402821969826,\n",
       "   0.14827696712500765,\n",
       "   0.1436064948044575,\n",
       "   0.14621333813122298,\n",
       "   0.1484965346463041,\n",
       "   0.14492480443875322,\n",
       "   0.15050384913348197,\n",
       "   0.13865244228035503,\n",
       "   0.15192734683961653,\n",
       "   0.13918944199536562,\n",
       "   0.13436973077776035,\n",
       "   0.14771684831748477,\n",
       "   0.14175129272874426,\n",
       "   0.1401874085898844,\n",
       "   0.14373142826685642,\n",
       "   0.13832216263506725,\n",
       "   0.14348208518334712,\n",
       "   0.14237483398254597,\n",
       "   0.14879938524753183,\n",
       "   0.13436106344041276],\n",
       "  'val_accuracy_mean': [0.41364444464445116,\n",
       "   0.4523555561900139,\n",
       "   0.509711111287276,\n",
       "   0.5353777786095937,\n",
       "   0.5538666660586993,\n",
       "   0.5510444446404775,\n",
       "   0.5654666655262311,\n",
       "   0.5722888882954915,\n",
       "   0.5921333331863086,\n",
       "   0.5894222210844358,\n",
       "   0.6117111100753149,\n",
       "   0.6120444430907568,\n",
       "   0.6186444423596065,\n",
       "   0.6288444439570109,\n",
       "   0.6361777770519257,\n",
       "   0.6393333308895429,\n",
       "   0.6245777775843938,\n",
       "   0.6400444433093071,\n",
       "   0.6386666675408681,\n",
       "   0.6492000006635984,\n",
       "   0.6509333338340123,\n",
       "   0.6575333336989085,\n",
       "   0.6628444451093674,\n",
       "   0.6612888885537783,\n",
       "   0.6563111107548077,\n",
       "   0.6511555540561677,\n",
       "   0.6547333326935768,\n",
       "   0.662666667898496,\n",
       "   0.6692444461584092,\n",
       "   0.6666444429755211,\n",
       "   0.6789777778585752,\n",
       "   0.6676444448033969,\n",
       "   0.6626000010967255,\n",
       "   0.6623333323001862,\n",
       "   0.6652888884147008,\n",
       "   0.6683999982476234,\n",
       "   0.6690888887643814,\n",
       "   0.6733333353201548,\n",
       "   0.6690888878703117,\n",
       "   0.6806444456179936,\n",
       "   0.6677333333094915,\n",
       "   0.6761111090580623,\n",
       "   0.6762222236394883,\n",
       "   0.683044444322586,\n",
       "   0.6736666659514109,\n",
       "   0.6634888891379038,\n",
       "   0.684311112165451,\n",
       "   0.664044443766276,\n",
       "   0.6797777754068375,\n",
       "   0.6862444456418355,\n",
       "   0.6716666673620542,\n",
       "   0.6738444452484449,\n",
       "   0.6643333343664805,\n",
       "   0.6855333342154821,\n",
       "   0.6748666671911875,\n",
       "   0.6744888889789581,\n",
       "   0.680222219924132,\n",
       "   0.6746888889869054,\n",
       "   0.6765111110607783,\n",
       "   0.6720444458723068,\n",
       "   0.6755111090342204,\n",
       "   0.6785333336393038,\n",
       "   0.6765333346525828,\n",
       "   0.6788222214579582,\n",
       "   0.6872888882954915,\n",
       "   0.678244445224603,\n",
       "   0.6782444435358047,\n",
       "   0.666488889157772,\n",
       "   0.6797111112872759,\n",
       "   0.6747333333889644,\n",
       "   0.6831777763366699,\n",
       "   0.6801999992132187,\n",
       "   0.6868000000715255,\n",
       "   0.68217777689298,\n",
       "   0.6731999974449475,\n",
       "   0.6832222216327986,\n",
       "   0.6812222221493721,\n",
       "   0.677044444878896,\n",
       "   0.675822220047315,\n",
       "   0.6707333312431971,\n",
       "   0.6862888874610266,\n",
       "   0.6846444454789161,\n",
       "   0.6749333323041597,\n",
       "   0.6859555542469025,\n",
       "   0.6768888892730077,\n",
       "   0.6824222203095754,\n",
       "   0.6857777774333954,\n",
       "   0.6861777769525846,\n",
       "   0.6779999998211861,\n",
       "   0.6747555565834046,\n",
       "   0.6799111102024714,\n",
       "   0.6841777794559797,\n",
       "   0.6892666673660278,\n",
       "   0.6903333334128062,\n",
       "   0.6892666671673456,\n",
       "   0.6888888886570931,\n",
       "   0.6914222224553426,\n",
       "   0.6869999983906746,\n",
       "   0.6763555545608203],\n",
       "  'val_accuracy_std': [0.05563285277101294,\n",
       "   0.05536885991289118,\n",
       "   0.060899962800502985,\n",
       "   0.06076035023677977,\n",
       "   0.061874520158369806,\n",
       "   0.06298222689971152,\n",
       "   0.05930496232377687,\n",
       "   0.06096463582984421,\n",
       "   0.059469605215742766,\n",
       "   0.059887235116257764,\n",
       "   0.05972931301550421,\n",
       "   0.06080544208331747,\n",
       "   0.06261297566628482,\n",
       "   0.059424383437397275,\n",
       "   0.06101291633627845,\n",
       "   0.0595949290978602,\n",
       "   0.06073201185448173,\n",
       "   0.062015514643987746,\n",
       "   0.06329004418600842,\n",
       "   0.06194702676296361,\n",
       "   0.059540363622750156,\n",
       "   0.060523555288725515,\n",
       "   0.062196089811198285,\n",
       "   0.06029899311408035,\n",
       "   0.061975082500731464,\n",
       "   0.0593570332592185,\n",
       "   0.0618573892987554,\n",
       "   0.061559968497922564,\n",
       "   0.05847588724375368,\n",
       "   0.06110706591741987,\n",
       "   0.06041945694351134,\n",
       "   0.05910636736801637,\n",
       "   0.06120926002884899,\n",
       "   0.05782764992941842,\n",
       "   0.059786270372973065,\n",
       "   0.0601894795116956,\n",
       "   0.06080862338943336,\n",
       "   0.060850757385117436,\n",
       "   0.0614050264643897,\n",
       "   0.06074865750243035,\n",
       "   0.06333721825963078,\n",
       "   0.0602155607481557,\n",
       "   0.05801106883926321,\n",
       "   0.06193477693539975,\n",
       "   0.06076822982275024,\n",
       "   0.058866876875899796,\n",
       "   0.060391988411570834,\n",
       "   0.0643481126923043,\n",
       "   0.06101325674593542,\n",
       "   0.0629259379148372,\n",
       "   0.06075604188193723,\n",
       "   0.0584582638315099,\n",
       "   0.05985197922658782,\n",
       "   0.058924095068879905,\n",
       "   0.06005569182603697,\n",
       "   0.060483222675926554,\n",
       "   0.059999588323314555,\n",
       "   0.05889928335282385,\n",
       "   0.05906786813993783,\n",
       "   0.058753454541595085,\n",
       "   0.06136229853645938,\n",
       "   0.06152082576116199,\n",
       "   0.06378073638518394,\n",
       "   0.05922916894400112,\n",
       "   0.06144360427667025,\n",
       "   0.06011864302370585,\n",
       "   0.060794975888636346,\n",
       "   0.06342307955277703,\n",
       "   0.05992147517009757,\n",
       "   0.05865627538454403,\n",
       "   0.06027202208297474,\n",
       "   0.060338218995162604,\n",
       "   0.05914687375818656,\n",
       "   0.05797826889349464,\n",
       "   0.0597127461562968,\n",
       "   0.058980118783019696,\n",
       "   0.06133443927522686,\n",
       "   0.05664972065841848,\n",
       "   0.05893145704410608,\n",
       "   0.060498388788488254,\n",
       "   0.06343271489618699,\n",
       "   0.06166263870640382,\n",
       "   0.061232671647988825,\n",
       "   0.06151252356634809,\n",
       "   0.062204759180501966,\n",
       "   0.05905101117659598,\n",
       "   0.05972860488811939,\n",
       "   0.06251090241680121,\n",
       "   0.059375765240668774,\n",
       "   0.057958283432239445,\n",
       "   0.06302726878288246,\n",
       "   0.05841129702782655,\n",
       "   0.05921132359138228,\n",
       "   0.0611788510945266,\n",
       "   0.05950333963003416,\n",
       "   0.05993906435693583,\n",
       "   0.05883982724747126,\n",
       "   0.0624822922962142,\n",
       "   0.05901204728603214],\n",
       "  'val_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'val_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0]}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델을 구성한다\n",
    "model = MAMLFewShotClassifier(args=args, device=device,\n",
    "                              im_shape=(2, 3,\n",
    "                                        args.image_height, args.image_width))\n",
    "\n",
    "data = MetaLearningSystemDataLoader\n",
    "\n",
    "maml_system = ExperimentBuilder(model=model, data=data, args=args, device=device)\n",
    "maml_system.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3803d5ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MAMLFewShotClassifier(\n",
       "  (classifier): VGGReLUNormNetwork(\n",
       "    (layer_dict): ModuleDict(\n",
       "      (conv0): MetaConvNormLayerReLU(\n",
       "        (layer_dict): ModuleDict()\n",
       "        (conv): MetaConv2dLayer()\n",
       "        (norm_layer): MetaBatchNormLayer(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): MetaConvNormLayerReLU(\n",
       "        (layer_dict): ModuleDict()\n",
       "        (conv): MetaConv2dLayer()\n",
       "        (norm_layer): MetaBatchNormLayer(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv2): MetaConvNormLayerReLU(\n",
       "        (layer_dict): ModuleDict()\n",
       "        (conv): MetaConv2dLayer()\n",
       "        (norm_layer): MetaBatchNormLayer(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv3): MetaConvNormLayerReLU(\n",
       "        (layer_dict): ModuleDict()\n",
       "        (conv): MetaConv2dLayer()\n",
       "        (norm_layer): MetaBatchNormLayer(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (linear): MetaLinearLayer()\n",
       "    )\n",
       "    (prompt): PadPrompter(\n",
       "      (prompt_dict): ParameterDict(\n",
       "          (pad_up): Parameter containing: [torch.cuda.FloatTensor of size 3x5x84 (GPU 0)]\n",
       "          (pad_down): Parameter containing: [torch.cuda.FloatTensor of size 3x5x84 (GPU 0)]\n",
       "          (pad_left): Parameter containing: [torch.cuda.FloatTensor of size 3x74x5 (GPU 0)]\n",
       "          (pad_right): Parameter containing: [torch.cuda.FloatTensor of size 3x74x5 (GPU 0)]\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (inner_loop_optimizer): LSLRGradientDescentLearningRule(\n",
       "    (prompt_learning_rates_dict): ParameterDict(  (prompt_weight_learning_rate): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)])\n",
       "    (names_learning_rates_dict): ParameterDict(\n",
       "        (layer_dict-conv0-conv-weight): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv0-conv-bias): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv1-conv-weight): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv1-conv-bias): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv2-conv-weight): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv2-conv-bias): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv3-conv-weight): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv3-conv-bias): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-linear-weights): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-linear-bias): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "    )\n",
       "    (names_weight_decay_dict): ParameterDict(\n",
       "        (layer_dict-conv0-conv-weight): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv0-conv-bias): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv1-conv-weight): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv1-conv-bias): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv2-conv-weight): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv2-conv-bias): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv3-conv-weight): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-conv3-conv-bias): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-linear-weights): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "        (layer_dict-linear-bias): Parameter containing: [torch.cuda.FloatTensor of size 6 (GPU 0)]\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_dir = maml_system.saved_models_filepath\n",
    "model_name = \"train_model\"\n",
    "model_idx = maml_system.state['best_epoch']\n",
    "\n",
    "state = maml_system.model.load_model(model_save_dir=model_save_dir,\n",
    "                                     model_name=model_name,\n",
    "                                     model_idx=model_idx+1)\n",
    "\n",
    "state_dict_loaded = state['network']\n",
    "\n",
    "maml_system.model.load_state_dict(state_dict=state_dict_loaded)\n",
    "\n",
    "# # 잘 불러왔는지 확인하는 코드\n",
    "# print(\"state_dict_loaded == \",state_dict_loaded)\n",
    "# print(\"=\"*10)\n",
    "# for key, value in maml_system.model.named_parameters():\n",
    "#     print(key)\n",
    "#     print(value)\n",
    "maml_system.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db4855d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\utils\\grad_cam.py:24: RuntimeWarning: invalid value encountered in true_divide\n",
      "  grad_cam = (grad_cam - np.min(grad_cam)) / (np.max(grad_cam) - np.min(grad_cam))\n"
     ]
    }
   ],
   "source": [
    "train_data = maml_system.data.get_train_batches(total_batches=int(600/2), augment_images=False)\n",
    "\n",
    "figure_idx = 0\n",
    "\n",
    "for sample_idx, train_sample in enumerate(train_data):\n",
    "    \n",
    "    x_support_set, x_target_set, y_support_set, y_target_set, seed = train_sample\n",
    "    \n",
    "    x_support_set = torch.Tensor(x_support_set).float().to(device=maml_system.model.device)\n",
    "    x_target_set = torch.Tensor(x_target_set).float().to(device=maml_system.model.device)\n",
    "    y_support_set = torch.Tensor(y_support_set).long().to(device=maml_system.model.device)\n",
    "    y_target_set = torch.Tensor(y_target_set).long().to(device=maml_system.model.device)\n",
    "    \n",
    "    for task_id, (x_support_set_task, y_support_set_task, x_target_set_task, y_target_set_task) in enumerate(zip(x_support_set,\n",
    "                              y_support_set,\n",
    "                              x_target_set,\n",
    "                              y_target_set)):\n",
    "        \n",
    "        names_weights_copy = maml_system.model.get_inner_loop_parameter_dict(maml_system.model.classifier.named_parameters())\n",
    "        \n",
    "        num_devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "\n",
    "        names_weights_copy = {\n",
    "            name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
    "                [num_devices] + [1 for i in range(len(value.shape))]) for\n",
    "            name, value in names_weights_copy.items()}\n",
    "        \n",
    "        prompted_weights_copy = {}\n",
    "        if args.prompter:\n",
    "            prompted_weights_copy = {key: value for key, value in names_weights_copy.items() if 'prompt' in key}\n",
    "\n",
    "        names_weights_copy = {key: value for key, value in names_weights_copy.items() if 'layer_dict' in key}\n",
    "        \n",
    "        n, s, c, h, w = x_target_set_task.shape\n",
    "\n",
    "        x_support_set_task = x_support_set_task.view(-1, c, h, w)\n",
    "        y_support_set_task = y_support_set_task.view(-1)\n",
    "        x_target_set_task = x_target_set_task.view(-1, c, h, w)\n",
    "        y_target_set_task = y_target_set_task.view(-1)\n",
    "        \n",
    "        num_steps=5\n",
    "        \n",
    "        for num_step in range(num_steps):            \n",
    "            \n",
    "            support_loss, support_preds = maml_system.model.net_forward(\n",
    "                                                               x=x_support_set_task,\n",
    "                                                               y=y_support_set_task,\n",
    "                                                               weights=names_weights_copy,\n",
    "                                                               prompted_weights=prompted_weights_copy,\n",
    "                                                               backup_running_statistics=num_step == 0,\n",
    "                                                               prepend_prompt=args.prompter,\n",
    "                                                               training=True,\n",
    "                                                               num_step=num_step,\n",
    "                                                               training_phase=False,\n",
    "                                                               epoch=0,\n",
    "                                                               inner_loop=True)\n",
    "        \n",
    "            names_weights_copy, prompted_weights_copy = maml_system.model.apply_inner_loop_update(\n",
    "                 loss=support_loss,\n",
    "                 names_weights_copy=names_weights_copy,\n",
    "                 prompted_weights_copy=prompted_weights_copy,\n",
    "                 use_second_order=True,\n",
    "                 current_step_idx=num_step,\n",
    "                 current_iter='test',\n",
    "                 training_phase=False)\n",
    "            \n",
    "        ##########Inner level 종료 #############\n",
    "        # Query set으로 Grad-CAM을 구한다\n",
    "        individual_images = torch.split(x_target_set_task, 1, dim=0)  # (25, 3, 84, 84) -> (1, 3, 84, 84) 텐서로 분리\n",
    "        individual_labels = torch.split(y_target_set_task, 1, dim=0)\n",
    "        \n",
    "        \n",
    "        original_save_path = \"Grad_CAM/\" + datasets + \"/Ours_without_reg/origin/\" + \"sample_idx_\" + str(sample_idx) + \"/\" + \"task_id_\" + str(task_id) + \"/\"\n",
    "        grad_cam_save_path = \"Grad_CAM/\" + datasets + \"/Ours_without_reg/gradcam/\"+ \"sample_idx_\" + str(sample_idx) + \"/\" + \"task_id_\" + str(task_id) + \"/\"\n",
    "        \n",
    "        make_folder(original_save_path)\n",
    "        make_folder(grad_cam_save_path)\n",
    "        \n",
    "        # 각 이미지 텐서 확인\n",
    "        for i in range(y_target_set_task.shape[0]):\n",
    "\n",
    "            input_image = individual_images[i]\n",
    "            y_label = individual_labels[i]\n",
    "\n",
    "            _, target_preds, feature_map_list = maml_system.model.net_forward_feature_extractor(x=input_image,\n",
    "                                                             y=y_label, weights=names_weights_copy,\n",
    "                                                             backup_running_statistics=False, training=True,\n",
    "                                                             num_step=num_step, training_phase='test',\n",
    "                                                             epoch=0)\n",
    "            \n",
    "            feature_map = feature_map_list[3]\n",
    "            \n",
    "            # 선택한 클래스에 대한 스칼라 출력값을 기준으로 gradient 계산\n",
    "            target_class = y_label\n",
    "            output = target_preds\n",
    "\n",
    "            class_score = output[:, target_class].sum()\n",
    "            gradients = torch.autograd.grad(outputs=class_score, inputs=feature_map, grad_outputs=torch.ones_like(class_score))\n",
    "\n",
    "            # gradients를 사용하여 feature map의 가중치를 계산\n",
    "            weights = torch.mean(gradients[0], dim=(2, 3), keepdim=True)\n",
    "            grad_cam = F.relu(torch.sum(weights * feature_map, dim=1, keepdim=True))\n",
    "\n",
    "            # grad_cam을 이미지 크기로 리사이즈\n",
    "            grad_cam = F.interpolate(grad_cam, size=(input_image.shape[2], input_image.shape[3]), mode='bilinear', align_corners=False)\n",
    "            grad_cam = grad_cam.squeeze().cpu().detach().numpy()\n",
    "\n",
    "            visualize_and_save_grad_cam(\n",
    "                input_image=input_image,\n",
    "                grad_cam=grad_cam,\n",
    "                grad_cam_save_path=grad_cam_save_path + str(i) + \".png\",\n",
    "                original_save_path=original_save_path + str(i) + \".png\",\n",
    "                datasets=datasets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
