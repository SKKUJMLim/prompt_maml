{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00a643d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pytorchcv.model_provider import get_model as ptcv_get_model # model\n",
    "import sys, os\n",
    "import easydict\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline \n",
    "\n",
    "# enable cuda devices\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from meta_neural_network_architectures import VGGReLUNormNetwork, ResNet12\n",
    "from utils.parser_utils import get_args\n",
    "from data import MetaLearningSystemDataLoader\n",
    "from experiment_builder import ExperimentBuilder\n",
    "from utils import basic\n",
    "from few_shot_learning_system import MAMLFewShotClassifier\n",
    "import prompters\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e0281c",
   "metadata": {},
   "source": [
    "# 1. Dataset 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1418f5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choices=['padding', 'random_patch', 'fixed_patch'],\n",
    "method = 'padding'\n",
    "\n",
    "datasets = \"mini_imagenet\"\n",
    "# datasets = \"tiered_imagenet\"\n",
    "# datasets = \"CIFAR_FS\"\n",
    "# datasets = \"CUB\"\n",
    "\n",
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "# os.environ['TEST_DATASET'] = \"tiered_imagenet\" # https://mtl.yyliu.net/download/Lmzjm9tX.html\n",
    "# os.environ['TEST_DATASET'] = \"CIFAR_FS\" # https://drive.google.com/file/d/1pTsCCMDj45kzFYgrnO67BWVbKs48Q3NI/view\n",
    "# os.environ['TEST_DATASET'] = \"CUB\" # https://data.caltech.edu/records/65de6-vp158"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1855befa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.environ['DATASET_DIR'] ===  C:/Users/JM/PycharmProjects/MAML/datasets\n"
     ]
    }
   ],
   "source": [
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "print(\"os.environ['DATASET_DIR'] === \", os.environ['DATASET_DIR'])\n",
    "\n",
    "args = easydict.EasyDict(\n",
    "{\n",
    "  \"batch_size\":2,\n",
    "  \"image_height\":84,\n",
    "  \"image_width\":84,\n",
    "  \"image_channels\":3,\n",
    "  \"gpu_to_use\":0,\n",
    "  \"num_dataprovider_workers\":4,\n",
    "  \"max_models_to_save\":5,\n",
    "  \"dataset_name\":\"mini_imagenet_full_size\",\n",
    "  \"dataset_path\":\"mini_imagenet_full_size\",\n",
    "  \"reset_stored_paths\":False,\n",
    "  \"experiment_name\":\"../MAML_Prompt_padding_5way_5shot_filter64_miniImagenet\",\n",
    "  \"train_seed\": 0, \"val_seed\": 0,\n",
    "  \"indexes_of_folders_indicating_class\": [-3, -2],\n",
    "  \"sets_are_pre_split\": True,\n",
    "  \"train_val_test_split\": [0.64, 0.16, 0.20],\n",
    "  \"evaluate_on_test_set_only\": False,\n",
    "\n",
    "  \"total_epochs\": 100,\n",
    "  \"total_iter_per_epoch\":500, \"continue_from_epoch\": -2,\n",
    "  \"num_evaluation_tasks\":600,\n",
    "  \"multi_step_loss_num_epochs\": 15,\n",
    "  \"minimum_per_task_contribution\": 0.01,\n",
    "  \"learnable_per_layer_per_step_inner_loop_learning_rate\": True,\n",
    "  \"enable_inner_loop_optimizable_bn_params\": False,\n",
    "  \"evalute_on_test_set_only\": False,\n",
    "\n",
    "  \"max_pooling\": True,\n",
    "  \"per_step_bn_statistics\": False,\n",
    "  \"learnable_batch_norm_momentum\": False,\n",
    "  \"load_into_memory\": False,\n",
    "  \"init_inner_loop_learning_rate\": 0.01,\n",
    "  \"init_inner_loop_weight_decay\": 0.0005,\n",
    "  \"learnable_bn_gamma\": True,\n",
    "  \"learnable_bn_beta\": True,\n",
    "\n",
    "  \"dropout_rate_value\":0.0,\n",
    "  \"min_learning_rate\":0.001,\n",
    "  \"meta_learning_rate\":0.001,   \"total_epochs_before_pause\": 100,\n",
    "  \"first_order_to_second_order_epoch\":-1,\n",
    "  \"weight_decay\": 0.0,\n",
    "\n",
    "  \"norm_layer\":\"batch_norm\",\n",
    "  \"cnn_num_filters\":64,\n",
    "  \"num_stages\":4,\n",
    "  \"conv_padding\": True,\n",
    "  \"number_of_training_steps_per_iter\":5,\n",
    "  \"number_of_evaluation_steps_per_iter\":5,\n",
    "  \"cnn_blocks_per_stage\":1,\n",
    "  \"num_classes_per_set\":5,\n",
    "  \"num_samples_per_class\":5,\n",
    "  \"num_target_samples\": 15,\n",
    "  \"samples_per_iter\" : 1,\n",
    "\n",
    "  \"second_order\": True,\n",
    "  \"use_multi_step_loss_optimization\":False,\n",
    "  \"backbone\": \"4-CONV\",\n",
    "  \"arbiter\": False,\n",
    "  \"use_bias\": True,\n",
    "  \"prompter\": True,\n",
    "  \"prompt_engineering\": method,\n",
    "  \"prompt_size\" : 5,\n",
    "  \"image_size\" : 84,\n",
    "  \"prompt_random_init\": False,\n",
    "  \"outer_prompt_learning_rate\": 0.001,\n",
    "  \"inner_prompt_learning_rate\": 0.01\n",
    "}\n",
    ")\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "args.im_shape = (2, 3, args.image_height, args.image_width)\n",
    "\n",
    "args.use_cuda = torch.cuda.is_available()\n",
    "args.seed = 104\n",
    "args.reverse_channels=False\n",
    "args.labels_as_int=False\n",
    "args.reset_stored_filepaths=False\n",
    "args.num_of_gpus=1\n",
    "\n",
    "args.continue_from_epoch='latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc47251",
   "metadata": {},
   "source": [
    "# 2. 모델 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "197487d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using max pooling\n",
      "meta network params\n",
      "0.01\n",
      "Inner Loop parameters\n",
      "prompt_learning_rates_dict.prompt-prompt_dict-pad_up torch.Size([6])\n",
      "prompt_learning_rates_dict.prompt-prompt_dict-pad_down torch.Size([6])\n",
      "prompt_learning_rates_dict.prompt-prompt_dict-pad_left torch.Size([6])\n",
      "prompt_learning_rates_dict.prompt-prompt_dict-pad_right torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-linear-weights torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-linear-bias torch.Size([6])\n",
      "Outer Loop parameters\n",
      "classifier.prompt.prompt_dict.pad_up torch.Size([3, 5, 84]) cuda:0 True\n",
      "classifier.prompt.prompt_dict.pad_down torch.Size([3, 5, 84]) cuda:0 True\n",
      "classifier.prompt.prompt_dict.pad_left torch.Size([3, 74, 5]) cuda:0 True\n",
      "classifier.prompt.prompt_dict.pad_right torch.Size([3, 74, 5]) cuda:0 True\n",
      "classifier.layer_dict.conv0.conv.weight torch.Size([64, 3, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv0.conv.bias torch.Size([64]) cuda:0 True\n",
      "classifier.layer_dict.conv0.norm_layer.bias torch.Size([64]) cuda:0 True\n",
      "classifier.layer_dict.conv0.norm_layer.weight torch.Size([64]) cuda:0 True\n",
      "classifier.layer_dict.conv1.conv.weight torch.Size([64, 64, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv1.conv.bias torch.Size([64]) cuda:0 True\n",
      "classifier.layer_dict.conv1.norm_layer.bias torch.Size([64]) cuda:0 True\n",
      "classifier.layer_dict.conv1.norm_layer.weight torch.Size([64]) cuda:0 True\n",
      "classifier.layer_dict.conv2.conv.weight torch.Size([64, 64, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv2.conv.bias torch.Size([64]) cuda:0 True\n",
      "classifier.layer_dict.conv2.norm_layer.bias torch.Size([64]) cuda:0 True\n",
      "classifier.layer_dict.conv2.norm_layer.weight torch.Size([64]) cuda:0 True\n",
      "classifier.layer_dict.conv3.conv.weight torch.Size([64, 64, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv3.conv.bias torch.Size([64]) cuda:0 True\n",
      "classifier.layer_dict.conv3.norm_layer.bias torch.Size([64]) cuda:0 True\n",
      "classifier.layer_dict.conv3.norm_layer.weight torch.Size([64]) cuda:0 True\n",
      "classifier.layer_dict.linear.weights torch.Size([5, 1600]) cuda:0 True\n",
      "classifier.layer_dict.linear.bias torch.Size([5]) cuda:0 True\n",
      "inner_loop_optimizer.prompt_learning_rates_dict.prompt-prompt_dict-pad_up torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.prompt_learning_rates_dict.prompt-prompt_dict-pad_down torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.prompt_learning_rates_dict.prompt-prompt_dict-pad_left torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.prompt_learning_rates_dict.prompt-prompt_dict-pad_right torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-weights torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-bias torch.Size([6]) cuda:0 True\n",
      "1\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\MAML_Prompt_padding_5way_5shot_filter64_miniImagenet\n",
      "attempting to find existing checkpoint\n",
      "data {'test': 12000, 'train': 38400, 'val': 9600}\n",
      "train_seed 985773, val_seed: 985773, at start time\n",
      "50000 50000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_val_acc': 0.6696888887882233,\n",
       " 'best_val_iter': 19000,\n",
       " 'current_iter': 50000,\n",
       " 'best_epoch': 38,\n",
       " 'train_loss_mean': 0.3694986680448055,\n",
       " 'train_loss_std': 0.10750563424406782,\n",
       " 'train_accuracy_mean': 0.8620000009536744,\n",
       " 'train_accuracy_std': 0.046196683675607114,\n",
       " 'train_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_0_std': 0.0,\n",
       " 'train_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_1_std': 0.0,\n",
       " 'train_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_2_std': 0.0,\n",
       " 'train_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_3_std': 0.0,\n",
       " 'train_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'train_loss_importance_vector_4_std': 0.0,\n",
       " 'train_learning_rate_mean': 0.0010000000000000005,\n",
       " 'train_learning_rate_std': 4.336808689942018e-19,\n",
       " 'val_loss_mean': 0.9268281394243241,\n",
       " 'val_loss_std': 0.15553370869524177,\n",
       " 'val_accuracy_mean': 0.6556888891259829,\n",
       " 'val_accuracy_std': 0.057460515457355595,\n",
       " 'val_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_0_std': 0.0,\n",
       " 'val_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_1_std': 0.0,\n",
       " 'val_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_2_std': 0.0,\n",
       " 'val_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_3_std': 0.0,\n",
       " 'val_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'val_loss_importance_vector_4_std': 0.0,\n",
       " 'network': OrderedDict([('classifier.prompt.prompt_dict.pad_up',\n",
       "               tensor([[[-1.6968e-02, -1.6471e-02, -2.0967e-02,  ..., -6.7587e-03,\n",
       "                         -3.4279e-02, -7.6139e-03],\n",
       "                        [-6.5795e-03,  3.3126e-03,  3.5695e-03,  ...,  2.6936e-02,\n",
       "                          3.0140e-02,  3.2774e-02],\n",
       "                        [ 2.4537e-02, -2.8073e-02,  2.8505e-03,  ..., -2.7416e-02,\n",
       "                         -3.8427e-03, -1.9807e-02],\n",
       "                        [-5.2245e-02, -2.5130e-02,  1.8980e-02,  ..., -1.3586e-02,\n",
       "                         -2.6698e-02, -4.0954e-02],\n",
       "                        [ 2.2283e-02,  6.1350e-03, -9.6376e-04,  ...,  4.9632e-02,\n",
       "                          1.2547e-02,  9.3757e-03]],\n",
       "               \n",
       "                       [[-2.8598e-02, -2.4586e-02,  1.4320e-02,  ...,  9.5403e-03,\n",
       "                          1.7679e-03, -9.2087e-03],\n",
       "                        [-7.5751e-03,  7.7514e-03,  1.0693e-02,  ..., -1.2656e-02,\n",
       "                         -5.0279e-02, -2.2152e-02],\n",
       "                        [ 1.1165e-02,  1.1043e-02,  5.2658e-02,  ..., -1.0901e-02,\n",
       "                          2.4993e-02, -9.7013e-03],\n",
       "                        [-3.3659e-02,  2.5457e-02,  1.1826e-03,  ...,  7.5400e-03,\n",
       "                          8.4533e-03, -2.0342e-02],\n",
       "                        [ 3.1119e-02, -3.5392e-03, -3.4476e-02,  ..., -6.5637e-03,\n",
       "                         -2.6962e-02,  3.1711e-03]],\n",
       "               \n",
       "                       [[ 1.3845e-02, -6.3628e-04,  6.5032e-03,  ..., -9.4470e-03,\n",
       "                          2.0054e-02,  1.4276e-02],\n",
       "                        [ 1.5304e-02,  3.2988e-03,  1.4501e-02,  ...,  6.1417e-03,\n",
       "                          3.6878e-02,  2.6545e-02],\n",
       "                        [ 9.5897e-03, -5.5787e-02, -4.9623e-02,  ..., -1.9223e-02,\n",
       "                          4.0469e-02,  9.3717e-03],\n",
       "                        [ 3.5415e-02,  2.9471e-02, -9.9165e-03,  ...,  5.5648e-03,\n",
       "                          2.3288e-02, -7.3111e-03],\n",
       "                        [-1.2277e-03, -5.4861e-02,  5.5768e-02,  ...,  4.0130e-06,\n",
       "                         -1.9416e-02,  3.6077e-04]]], device='cuda:0')),\n",
       "              ('classifier.prompt.prompt_dict.pad_down',\n",
       "               tensor([[[-1.0938e-02,  4.6323e-03,  5.4035e-03,  ...,  1.2517e-02,\n",
       "                          3.5284e-03,  1.2952e-02],\n",
       "                        [-1.3834e-02, -2.2538e-02,  3.7634e-02,  ...,  1.4835e-02,\n",
       "                         -2.1915e-02, -3.1889e-02],\n",
       "                        [-5.2643e-03, -3.3077e-02,  2.2323e-02,  ..., -6.2290e-03,\n",
       "                         -4.1417e-02, -4.3972e-02],\n",
       "                        [ 1.7329e-02, -1.7554e-02, -2.5833e-02,  ...,  1.5114e-02,\n",
       "                          4.7297e-03,  1.1250e-02],\n",
       "                        [ 4.0671e-03, -3.0794e-02,  1.5884e-03,  ..., -3.4987e-02,\n",
       "                         -6.6133e-02, -4.3838e-03]],\n",
       "               \n",
       "                       [[ 1.5076e-02,  1.3082e-02,  1.1431e-03,  ...,  1.4575e-02,\n",
       "                          3.8853e-03, -9.2478e-05],\n",
       "                        [ 1.7756e-03,  3.1573e-03, -5.0878e-02,  ...,  3.1999e-02,\n",
       "                          4.5857e-02,  7.7324e-03],\n",
       "                        [-1.4933e-02, -1.3608e-02, -3.1834e-02,  ...,  5.6046e-03,\n",
       "                          3.1397e-02,  1.1456e-03],\n",
       "                        [ 1.6378e-02,  2.7489e-03, -6.9776e-02,  ..., -1.5419e-02,\n",
       "                          5.4450e-04, -3.8634e-02],\n",
       "                        [ 6.3225e-03,  3.4452e-02,  4.8371e-03,  ..., -3.2355e-02,\n",
       "                         -2.5867e-02, -1.1983e-02]],\n",
       "               \n",
       "                       [[-1.9614e-02, -1.1691e-02, -2.7453e-02,  ...,  1.8602e-02,\n",
       "                          1.2725e-02, -9.3363e-03],\n",
       "                        [ 2.3114e-02,  3.3845e-02, -1.1404e-02,  ...,  3.8537e-03,\n",
       "                          2.6920e-02,  7.2198e-03],\n",
       "                        [ 3.3394e-02,  2.6689e-03, -1.4988e-02,  ..., -2.7147e-03,\n",
       "                          1.7934e-02,  4.0711e-03],\n",
       "                        [ 1.8721e-02,  1.9741e-02, -6.2935e-02,  ...,  9.6565e-03,\n",
       "                          8.1037e-02, -1.6953e-02],\n",
       "                        [ 7.3075e-03,  2.6465e-02, -3.6956e-02,  ..., -8.1733e-03,\n",
       "                          3.0507e-02,  7.7455e-03]]], device='cuda:0')),\n",
       "              ('classifier.prompt.prompt_dict.pad_left',\n",
       "               tensor([[[ 0.0053,  0.0357, -0.0167,  0.0083,  0.0032],\n",
       "                        [-0.0226,  0.0435,  0.0056,  0.0112,  0.0028],\n",
       "                        [-0.0190,  0.0156,  0.0539, -0.0188, -0.0149],\n",
       "                        ...,\n",
       "                        [ 0.0117, -0.0119,  0.0464, -0.0091,  0.0156],\n",
       "                        [-0.0396,  0.0258,  0.0145, -0.0105, -0.0420],\n",
       "                        [-0.0351, -0.0400, -0.0574, -0.0694, -0.0284]],\n",
       "               \n",
       "                       [[-0.0234,  0.0144, -0.0259, -0.0069, -0.0555],\n",
       "                        [-0.0581, -0.0262, -0.0070,  0.0250, -0.0283],\n",
       "                        [-0.0232, -0.0279,  0.0182, -0.0131, -0.0109],\n",
       "                        ...,\n",
       "                        [-0.0120, -0.0761,  0.0021,  0.0042,  0.0217],\n",
       "                        [-0.0258,  0.0117,  0.0461,  0.0722,  0.0303],\n",
       "                        [ 0.0431,  0.0448,  0.0214,  0.0202, -0.0109]],\n",
       "               \n",
       "                       [[-0.0472, -0.0104, -0.0038,  0.0209, -0.0469],\n",
       "                        [ 0.0137,  0.0067,  0.0286,  0.0542, -0.0040],\n",
       "                        [ 0.0044, -0.0188,  0.0362,  0.0443, -0.0164],\n",
       "                        ...,\n",
       "                        [ 0.0489,  0.0244, -0.0337, -0.0185, -0.0107],\n",
       "                        [ 0.0503,  0.0654, -0.0160, -0.0190, -0.0169],\n",
       "                        [-0.0251, -0.0199, -0.0068,  0.0290,  0.0657]]], device='cuda:0')),\n",
       "              ('classifier.prompt.prompt_dict.pad_right',\n",
       "               tensor([[[ 0.0398, -0.0150,  0.0858,  0.0216,  0.0177],\n",
       "                        [-0.0799,  0.0041, -0.0286,  0.0039, -0.0033],\n",
       "                        [ 0.0320,  0.0292, -0.0082,  0.0398,  0.0168],\n",
       "                        ...,\n",
       "                        [-0.0600,  0.0094, -0.0459, -0.0279, -0.0500],\n",
       "                        [ 0.0036,  0.0700,  0.0737,  0.0244,  0.0409],\n",
       "                        [ 0.0173, -0.0048,  0.0092, -0.0039,  0.0201]],\n",
       "               \n",
       "                       [[-0.0257, -0.0742,  0.0332, -0.0329,  0.0003],\n",
       "                        [-0.0811,  0.0281, -0.0255,  0.0318, -0.0301],\n",
       "                        [ 0.0178, -0.0102, -0.0196,  0.0350,  0.0012],\n",
       "                        ...,\n",
       "                        [ 0.0051,  0.0200,  0.0498,  0.0830,  0.0159],\n",
       "                        [-0.0466, -0.0658, -0.0481, -0.0328, -0.0446],\n",
       "                        [-0.0098, -0.0013,  0.0244, -0.0109, -0.0235]],\n",
       "               \n",
       "                       [[ 0.0259, -0.0320,  0.0327, -0.0326,  0.0136],\n",
       "                        [-0.0320,  0.0450, -0.0310,  0.0256, -0.0159],\n",
       "                        [-0.0052, -0.0254, -0.0428, -0.0006,  0.0019],\n",
       "                        ...,\n",
       "                        [-0.0039,  0.0035,  0.0082,  0.0183,  0.0044],\n",
       "                        [ 0.0174,  0.0444, -0.0354, -0.0109,  0.0248],\n",
       "                        [ 0.0026, -0.0165, -0.0361, -0.0462, -0.0197]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.conv.weight',\n",
       "               tensor([[[[ 1.6224e-08,  1.8105e-08,  1.5143e-08],\n",
       "                         [ 2.3937e-08,  2.1502e-08,  1.7007e-08],\n",
       "                         [ 2.3922e-08,  2.1261e-08,  1.8408e-08]],\n",
       "               \n",
       "                        [[-1.0220e-08, -9.6817e-09, -1.4100e-08],\n",
       "                         [-5.8510e-09, -7.9343e-09, -1.3124e-08],\n",
       "                         [-6.1856e-09, -7.4995e-09, -1.2145e-08]],\n",
       "               \n",
       "                        [[ 2.6637e-08,  2.8993e-08,  2.7929e-08],\n",
       "                         [ 3.2847e-08,  3.2551e-08,  2.9908e-08],\n",
       "                         [ 3.7338e-08,  3.6182e-08,  3.5099e-08]]],\n",
       "               \n",
       "               \n",
       "                       [[[-3.2945e-02, -2.8342e-02, -4.6567e-02],\n",
       "                         [-5.4560e-02, -7.4464e-03, -3.0827e-02],\n",
       "                         [-8.1694e-02, -1.6677e-02, -4.3572e-02]],\n",
       "               \n",
       "                        [[ 1.1568e-01,  1.5484e-01,  5.4725e-02],\n",
       "                         [ 1.2905e-01,  1.8725e-01,  1.1791e-01],\n",
       "                         [ 9.0972e-02,  1.7459e-01,  8.9483e-02]],\n",
       "               \n",
       "                        [[-5.0241e-02,  1.8989e-02, -3.1840e-02],\n",
       "                         [-6.2208e-02,  4.1458e-02,  1.3188e-03],\n",
       "                         [-4.5856e-02,  8.0966e-03, -5.3355e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.9404e-03,  1.5087e-02,  4.6552e-02],\n",
       "                         [ 8.9231e-02,  1.0127e-01,  1.5492e-01],\n",
       "                         [ 8.4711e-02,  1.4776e-01,  2.2790e-01]],\n",
       "               \n",
       "                        [[-5.9421e-02, -4.2128e-02, -3.4087e-02],\n",
       "                         [ 2.0369e-02,  3.7029e-02,  9.1967e-02],\n",
       "                         [ 1.5875e-02,  6.5978e-02,  1.3588e-01]],\n",
       "               \n",
       "                        [[-9.9086e-02, -6.3992e-02, -4.2575e-02],\n",
       "                         [-5.0912e-02, -1.1640e-02,  3.3243e-02],\n",
       "                         [-2.7923e-02,  4.2622e-02,  9.5791e-02]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 1.1221e-01,  1.9502e-01, -2.8898e-01],\n",
       "                         [ 2.6082e-02, -4.7469e-02, -9.5667e-02],\n",
       "                         [-1.8596e-01, -2.0071e-01,  3.6727e-01]],\n",
       "               \n",
       "                        [[ 9.2301e-02,  1.8883e-01, -2.6177e-01],\n",
       "                         [ 1.0949e-01, -7.6632e-02,  3.6638e-02],\n",
       "                         [-1.7567e-01, -1.0784e-01,  3.1622e-01]],\n",
       "               \n",
       "                        [[-2.5262e-02,  2.0803e-01, -1.6317e-01],\n",
       "                         [ 1.2094e-01, -8.1231e-02, -7.5309e-02],\n",
       "                         [-1.0826e-01, -1.1119e-01,  2.7882e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 5.1456e-07,  3.2375e-07,  3.3945e-07],\n",
       "                         [ 6.8297e-07,  5.6730e-07,  6.0800e-07],\n",
       "                         [ 7.6211e-07,  7.5500e-07,  6.9542e-07]],\n",
       "               \n",
       "                        [[ 1.8311e-06,  1.5779e-06,  1.4444e-06],\n",
       "                         [ 1.8238e-06,  1.7053e-06,  1.4935e-06],\n",
       "                         [ 1.8098e-06,  1.8566e-06,  1.4186e-06]],\n",
       "               \n",
       "                        [[ 3.1281e-06,  2.9280e-06,  2.7498e-06],\n",
       "                         [ 3.2370e-06,  3.1347e-06,  3.0567e-06],\n",
       "                         [ 3.2846e-06,  3.3101e-06,  3.0588e-06]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.2606e-27, -1.2386e-27, -1.1110e-27],\n",
       "                         [-1.5699e-27, -1.6583e-27, -1.2738e-27],\n",
       "                         [-1.8414e-27, -1.9691e-27, -1.6101e-27]],\n",
       "               \n",
       "                        [[-1.0642e-27, -1.1494e-27, -1.0802e-27],\n",
       "                         [-1.2331e-27, -1.3342e-27, -1.1992e-27],\n",
       "                         [-1.5075e-27, -1.5447e-27, -1.5061e-27]],\n",
       "               \n",
       "                        [[-2.7728e-27, -2.7160e-27, -2.3719e-27],\n",
       "                         [-3.0047e-27, -3.1141e-27, -2.6896e-27],\n",
       "                         [-3.2396e-27, -3.3484e-27, -3.2942e-27]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.conv.bias',\n",
       "               tensor([-1.7707e-11,  8.9876e-05, -3.0082e-05, -1.0120e-04,  1.8996e-04,\n",
       "                        5.8252e-06,  5.6544e-05,  2.2140e-05,  1.3982e-09, -1.6008e-07,\n",
       "                       -2.8600e-05,  1.4515e-05, -9.7415e-11,  9.9619e-05,  4.1264e-07,\n",
       "                       -1.5813e-05, -6.0400e-05,  2.9719e-05, -1.0978e-04,  1.4587e-04,\n",
       "                       -3.2168e-08,  4.4284e-09,  4.6336e-05, -3.0039e-04, -2.1757e-07,\n",
       "                       -2.0472e-05, -1.2714e-07, -9.2991e-04,  3.5755e-04, -5.5223e-04,\n",
       "                       -1.6949e-04,  2.5507e-09, -6.8049e-06,  4.3060e-04,  9.5251e-06,\n",
       "                       -2.9923e-05,  5.7252e-06, -3.9120e-08, -4.2561e-05, -7.3432e-10,\n",
       "                        1.5668e-09,  8.3117e-15, -7.0311e-05,  1.4054e-06, -5.0826e-07,\n",
       "                        1.0072e-07,  6.8061e-05,  1.1279e-08, -1.9506e-09,  3.8489e-04,\n",
       "                        6.4340e-05,  1.6236e-04,  2.4471e-04, -4.1143e-05, -1.0179e-04,\n",
       "                        6.7666e-07,  9.6892e-06, -3.9437e-05,  6.0290e-05, -6.1548e-07,\n",
       "                        1.6796e-14,  2.2981e-04,  4.3411e-09,  4.2933e-13], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.bias',\n",
       "               tensor([-2.4168e-05, -3.1039e-03, -2.4768e-01, -1.3664e-01, -5.5015e-02,\n",
       "                       -2.7030e-01, -2.5485e-01, -2.7175e-01, -1.9649e-03, -2.0208e-03,\n",
       "                       -4.4490e-02, -2.6684e-01, -2.6154e-23, -2.3359e-01, -1.6773e-02,\n",
       "                       -9.0966e-02, -2.9606e-03, -1.6001e-02, -3.9294e-02,  1.4192e-02,\n",
       "                       -7.4924e-05, -8.8699e-29, -4.5436e-02,  3.0131e-02, -1.5471e-08,\n",
       "                       -5.1121e-01, -1.0542e-03,  3.6495e-02,  3.2922e-01,  1.0963e-01,\n",
       "                       -4.5855e-02, -2.0737e-15,  1.0172e+00, -2.0083e-01, -3.2318e-01,\n",
       "                       -6.8114e-01, -2.5924e-01, -3.4418e-03,  6.2725e-03, -4.2066e-02,\n",
       "                       -1.4365e-04, -1.3748e-37, -2.8343e-01, -2.8445e-03, -2.4799e-01,\n",
       "                       -2.6572e-02,  2.4946e-01, -5.0352e-02, -5.7715e-27,  9.4002e-02,\n",
       "                       -8.6568e-02, -5.4680e-02, -7.0503e-03, -3.2812e-01, -6.8621e-02,\n",
       "                       -6.1887e-18, -5.5009e-05,  3.1523e-02,  1.2754e-01, -4.5584e-16,\n",
       "                       -2.0890e-21,  1.2502e-01, -7.5847e-04, -8.2620e-21], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.weight',\n",
       "               tensor([-9.8103e-06,  1.3778e-01,  3.1895e-01,  2.9979e-01,  5.7340e-01,\n",
       "                        1.6414e-01,  3.1223e-01,  1.6769e-01,  1.1574e-04,  1.4641e-03,\n",
       "                        6.2856e-01,  2.7446e-01,  2.0365e-14,  2.9552e-01, -1.5227e-03,\n",
       "                        3.5194e-01,  3.9372e-01,  6.1984e-01,  5.4495e-01,  5.0275e-01,\n",
       "                       -4.1086e-05, -2.3918e-39,  1.3997e-01,  4.9592e-01, -6.9547e-08,\n",
       "                        4.9351e-01,  5.8776e-05,  4.3580e-01,  3.2364e-01,  3.8109e-01,\n",
       "                        4.5597e-01, -2.2465e-08,  1.9361e-03,  5.3985e-01,  2.4032e-01,\n",
       "                        5.0177e-01,  1.6917e-01, -6.4467e-04,  3.5954e-01, -3.9624e-03,\n",
       "                       -1.1768e-04,  9.8161e-42,  4.3397e-01, -4.4649e-05,  7.6479e-02,\n",
       "                       -6.3197e-03,  3.7037e-01,  1.1760e-02, -9.9941e-42,  4.5841e-01,\n",
       "                        3.1604e-01,  4.8836e-01,  3.9914e-01,  2.9339e-01,  2.3302e-01,\n",
       "                       -3.0246e-09,  1.0655e-04,  1.4005e-01,  3.0196e-01,  1.7796e-27,\n",
       "                       -1.0822e-17,  3.2463e-01, -1.0379e-04,  9.3834e-10], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.weight',\n",
       "               tensor([[[[-1.1297e-06, -2.1222e-08, -9.7013e-07],\n",
       "                         [-1.0813e-06, -8.8577e-10, -1.0083e-06],\n",
       "                         [-8.6740e-07, -1.5138e-08, -1.2405e-06]],\n",
       "               \n",
       "                        [[ 2.8488e-02,  2.1373e-02,  3.8033e-02],\n",
       "                         [ 2.2348e-02,  1.6500e-02,  2.2371e-02],\n",
       "                         [ 2.0462e-02,  9.7486e-03,  1.4102e-02]],\n",
       "               \n",
       "                        [[-1.2623e-02, -2.5824e-03,  1.3021e-02],\n",
       "                         [-2.0405e-02, -5.5607e-03,  1.3460e-02],\n",
       "                         [-3.3433e-02, -1.5014e-02,  1.7035e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-8.6532e-03, -2.6034e-02, -5.4687e-02],\n",
       "                         [ 1.5664e-03, -7.7497e-02, -7.8685e-02],\n",
       "                         [-4.6590e-02, -9.9191e-02, -9.3403e-02]],\n",
       "               \n",
       "                        [[-1.1783e-05, -7.4782e-06, -1.9557e-05],\n",
       "                         [-4.3428e-06, -2.5694e-08, -1.2348e-05],\n",
       "                         [-4.6030e-06, -6.2192e-07, -1.3880e-05]],\n",
       "               \n",
       "                        [[-4.2620e-22, -7.2269e-23, -4.4402e-22],\n",
       "                         [-3.1905e-22,  2.0634e-25, -3.8411e-22],\n",
       "                         [ 1.6500e-24,  3.8506e-22, -8.9030e-24]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 3.3098e-08,  4.6994e-08,  1.0476e-07],\n",
       "                         [-1.3654e-09, -3.0477e-10,  2.4225e-08],\n",
       "                         [-8.1175e-08, -9.4966e-08, -1.3089e-07]],\n",
       "               \n",
       "                        [[-8.2952e-02, -6.9786e-02, -3.9686e-02],\n",
       "                         [-8.1944e-02, -7.4598e-02, -4.9247e-02],\n",
       "                         [-7.4643e-02, -7.1794e-02, -1.5170e-02]],\n",
       "               \n",
       "                        [[ 1.4503e-03,  1.0328e-02,  5.0983e-02],\n",
       "                         [-2.0002e-02, -3.1323e-02,  4.4720e-02],\n",
       "                         [-6.9164e-03,  8.1975e-04,  1.2171e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-8.7188e-02, -7.7209e-02, -3.3634e-02],\n",
       "                         [-1.0472e-01,  2.0384e-02,  1.2918e-01],\n",
       "                         [-1.0888e-01, -5.3341e-02,  6.8077e-02]],\n",
       "               \n",
       "                        [[-6.2770e-06, -4.8467e-07, -9.5685e-08],\n",
       "                         [-6.8127e-06, -7.8011e-08,  4.1989e-07],\n",
       "                         [-5.0196e-06,  1.9164e-06,  2.5193e-06]],\n",
       "               \n",
       "                        [[ 1.9800e-22, -9.4886e-23, -8.7508e-23],\n",
       "                         [ 8.9123e-23, -1.4502e-26, -3.1717e-23],\n",
       "                         [ 1.4212e-22, -1.0779e-24, -3.0750e-23]]],\n",
       "               \n",
       "               \n",
       "                       [[[-9.8229e-09, -3.0580e-07, -2.2233e-07],\n",
       "                         [ 1.9204e-07, -1.4047e-10,  4.7116e-08],\n",
       "                         [ 2.7543e-08,  1.9022e-08, -1.2590e-07]],\n",
       "               \n",
       "                        [[-4.9234e-02, -5.8564e-02, -3.5989e-02],\n",
       "                         [-2.6571e-02, -5.1553e-02, -3.4810e-02],\n",
       "                         [-3.5842e-02, -1.7715e-02, -2.0004e-02]],\n",
       "               \n",
       "                        [[-4.6110e-05, -2.8516e-02, -6.0705e-02],\n",
       "                         [-2.2526e-02,  8.8856e-03, -3.8433e-02],\n",
       "                         [-3.7732e-02, -2.5347e-02, -1.9614e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 7.0371e-02, -4.1269e-02, -4.6698e-02],\n",
       "                         [ 1.0824e-01, -1.0389e-01, -8.7208e-02],\n",
       "                         [ 1.1460e-01,  1.2615e-02, -6.8662e-02]],\n",
       "               \n",
       "                        [[ 5.6854e-06, -2.6584e-06,  2.8362e-06],\n",
       "                         [ 8.1664e-06,  5.0015e-08,  5.6837e-06],\n",
       "                         [ 7.1603e-06, -9.9041e-07,  4.6821e-06]],\n",
       "               \n",
       "                        [[ 7.5539e-23, -9.3853e-23,  4.0448e-23],\n",
       "                         [ 7.8793e-23, -7.4297e-26,  1.7987e-22],\n",
       "                         [ 9.0928e-23, -5.5217e-25,  9.6397e-23]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-9.1692e-07, -8.3702e-07, -2.5219e-06],\n",
       "                         [-5.9325e-07, -9.2159e-11, -1.7636e-06],\n",
       "                         [-3.0321e-07,  1.6132e-07, -1.4152e-06]],\n",
       "               \n",
       "                        [[-5.7288e-03, -8.6668e-03, -1.6712e-02],\n",
       "                         [-1.1882e-02,  4.7921e-03, -1.6932e-02],\n",
       "                         [-2.5650e-02,  1.6501e-03, -6.7268e-03]],\n",
       "               \n",
       "                        [[ 5.7752e-02,  3.3074e-02, -1.6437e-02],\n",
       "                         [-5.7258e-03, -8.6047e-03, -2.7396e-02],\n",
       "                         [-2.4599e-02,  2.3872e-03,  3.3341e-04]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.1496e-01, -3.6173e-02,  1.2480e-02],\n",
       "                         [ 6.5602e-02, -6.5563e-03,  6.9133e-02],\n",
       "                         [ 9.4711e-04, -4.4159e-02,  4.1283e-03]],\n",
       "               \n",
       "                        [[-5.7638e-06,  3.0104e-07, -3.7954e-06],\n",
       "                         [-5.9173e-06, -1.5444e-07, -4.1606e-06],\n",
       "                         [-1.4914e-05, -1.0400e-05, -1.3962e-05]],\n",
       "               \n",
       "                        [[ 1.0201e-22, -1.7492e-22,  1.0097e-23],\n",
       "                         [-1.0308e-22,  3.5188e-25, -3.2020e-22],\n",
       "                         [-4.9157e-22, -4.9493e-22, -7.2882e-23]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.8556e-08, -4.2868e-08, -3.7959e-07],\n",
       "                         [-1.9434e-08, -5.3165e-09, -3.7899e-07],\n",
       "                         [-7.1429e-08, -6.0306e-08, -4.1464e-07]],\n",
       "               \n",
       "                        [[-5.9684e-02, -8.9428e-02, -9.3194e-02],\n",
       "                         [-4.9726e-02, -7.3852e-02, -6.2577e-02],\n",
       "                         [-3.6625e-02, -5.0940e-02, -5.5607e-02]],\n",
       "               \n",
       "                        [[-5.9833e-02, -5.4714e-02, -7.3145e-02],\n",
       "                         [-4.6024e-02, -2.8951e-02, -5.6457e-02],\n",
       "                         [-2.1546e-02,  2.1800e-02, -6.7723e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.3801e-02,  2.0239e-01, -1.6667e-01],\n",
       "                         [-1.3900e-01, -1.3309e-01, -1.4712e-01],\n",
       "                         [ 1.0590e-01, -8.2561e-02, -2.1066e-02]],\n",
       "               \n",
       "                        [[ 2.9483e-06,  1.2986e-06, -5.0673e-06],\n",
       "                         [ 1.5045e-06, -1.5237e-07, -6.3153e-06],\n",
       "                         [ 2.3791e-06,  6.7232e-07, -5.8343e-06]],\n",
       "               \n",
       "                        [[ 4.4089e-23, -1.2520e-23, -1.4817e-22],\n",
       "                         [ 3.6649e-23, -7.9440e-26, -3.1283e-23],\n",
       "                         [ 5.9279e-23, -3.2017e-23, -6.6440e-23]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.0159e-06, -5.7294e-07,  3.6449e-07],\n",
       "                         [ 1.0697e-06,  3.1908e-09,  1.8114e-07],\n",
       "                         [ 1.3106e-06,  7.9747e-09,  5.8168e-07]],\n",
       "               \n",
       "                        [[ 7.1548e-02,  5.7439e-02,  6.7546e-02],\n",
       "                         [ 6.4354e-02,  6.1488e-02,  5.7626e-02],\n",
       "                         [ 6.2462e-02,  5.4966e-02,  4.4859e-02]],\n",
       "               \n",
       "                        [[-4.8282e-02, -4.8979e-02, -1.6090e-02],\n",
       "                         [-3.6242e-02, -6.4028e-02, -9.3061e-03],\n",
       "                         [-3.8483e-02, -5.0702e-02, -6.9361e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-3.4555e-02,  3.8341e-02, -5.2693e-02],\n",
       "                         [ 5.8899e-02, -1.2672e-03,  5.0152e-02],\n",
       "                         [ 1.7531e-02, -6.2155e-02,  7.8694e-02]],\n",
       "               \n",
       "                        [[-1.2862e-05, -1.3391e-05, -2.1727e-05],\n",
       "                         [ 8.3108e-07, -4.3566e-08, -7.6131e-06],\n",
       "                         [ 2.3885e-06,  2.2248e-06, -4.0572e-06]],\n",
       "               \n",
       "                        [[ 1.1194e-22,  2.9343e-22, -9.5186e-22],\n",
       "                         [ 1.5778e-22, -8.4470e-25, -8.2072e-22],\n",
       "                         [ 7.7837e-23,  5.8090e-23,  1.3228e-22]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.bias',\n",
       "               tensor([-6.4968e-05,  3.1341e-08, -3.2238e-05, -1.9364e-05,  6.2042e-05,\n",
       "                       -1.4131e-05, -4.3138e-05, -6.0678e-05, -4.1105e-05,  1.9882e-04,\n",
       "                       -4.9348e-05, -1.1716e-04, -2.8955e-05,  1.8228e-05, -1.1887e-04,\n",
       "                       -8.1344e-05,  4.3719e-05, -1.6156e-05, -8.9213e-05,  8.6450e-06,\n",
       "                       -1.1155e-05, -6.5667e-05, -1.6839e-04, -7.9478e-05,  4.8515e-05,\n",
       "                        1.2961e-05,  6.0009e-05,  6.4162e-05,  3.6327e-05, -4.3035e-05,\n",
       "                       -1.3143e-04, -3.9599e-05, -6.9525e-05,  2.1345e-06,  4.2948e-05,\n",
       "                        6.4879e-05,  1.3066e-04, -1.2634e-04, -4.3658e-05,  6.6230e-05,\n",
       "                       -7.6136e-05, -1.3164e-04,  6.2111e-05, -9.2102e-05, -4.2901e-05,\n",
       "                        4.5293e-05, -8.6369e-05,  2.4803e-05,  3.1208e-06, -1.2281e-04,\n",
       "                        1.0196e-04, -2.2256e-04, -5.4649e-05,  1.4107e-04,  3.9590e-05,\n",
       "                       -2.0753e-05,  1.2595e-05, -6.0222e-05,  1.4356e-04, -4.2529e-05,\n",
       "                       -8.1487e-05, -1.7399e-04, -1.2796e-04,  1.5796e-04], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.bias',\n",
       "               tensor([-0.3197, -0.2808, -0.3904, -0.2301, -0.2448, -0.1654, -0.2436, -0.2689,\n",
       "                       -0.3047, -0.1466, -0.2075, -0.2108, -0.1868, -0.1603, -0.1887, -0.2618,\n",
       "                       -0.2754, -0.3380, -0.1318, -0.1823, -0.3780, -0.1337, -0.2498, -0.2832,\n",
       "                       -0.3246, -0.2034, -0.1665, -0.2474, -0.3457, -0.2517, -0.1731, -0.2216,\n",
       "                       -0.1760, -0.3204, -0.3673, -0.2948, -0.1804, -0.2376, -0.2137, -0.2049,\n",
       "                       -0.2007, -0.2205, -0.2535, -0.1461, -0.2567, -0.3329, -0.1672, -0.0423,\n",
       "                       -0.2923, -0.2525, -0.2325, -0.1446, -0.3936, -0.1715, -0.2585, -0.3324,\n",
       "                       -0.2091, -0.3785, -0.0732, -0.2126, -0.2618, -0.2248, -0.2354, -0.2533],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.weight',\n",
       "               tensor([0.4521, 0.3672, 0.3013, 0.3509, 0.4010, 0.4259, 0.3279, 0.4619, 0.3939,\n",
       "                       0.2766, 0.3568, 0.6207, 0.3359, 0.4652, 0.3152, 0.3936, 0.4732, 0.3141,\n",
       "                       0.2995, 0.3710, 0.3155, 0.3746, 0.4017, 0.4393, 0.4248, 0.3603, 0.3509,\n",
       "                       0.2727, 0.5122, 0.2861, 0.4795, 0.4484, 0.3478, 0.3060, 0.4651, 0.4069,\n",
       "                       0.4384, 0.3359, 0.3542, 0.3795, 0.3688, 0.4617, 0.3479, 0.3513, 0.4039,\n",
       "                       0.5069, 0.3961, 0.3254, 0.3226, 0.4253, 0.3740, 0.3679, 0.2652, 0.3297,\n",
       "                       0.4148, 0.3976, 0.3182, 0.4635, 0.3243, 0.3775, 0.3643, 0.3800, 0.3893,\n",
       "                       0.3406], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.weight',\n",
       "               tensor([[[[ 0.0598, -0.0579, -0.0675],\n",
       "                         [ 0.0023, -0.0345, -0.0155],\n",
       "                         [-0.0944, -0.0601, -0.0497]],\n",
       "               \n",
       "                        [[ 0.0649, -0.0124,  0.0935],\n",
       "                         [-0.0172,  0.0389, -0.0446],\n",
       "                         [-0.0063, -0.0738, -0.1346]],\n",
       "               \n",
       "                        [[-0.0628,  0.0088,  0.0181],\n",
       "                         [-0.1470, -0.1371,  0.0754],\n",
       "                         [-0.0242, -0.0308, -0.0071]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0735,  0.0682,  0.0252],\n",
       "                         [ 0.0248, -0.1581,  0.0056],\n",
       "                         [ 0.0388,  0.0489,  0.1067]],\n",
       "               \n",
       "                        [[-0.0054,  0.0882, -0.1115],\n",
       "                         [ 0.0525, -0.0711, -0.0173],\n",
       "                         [-0.0822, -0.0592, -0.0904]],\n",
       "               \n",
       "                        [[-0.0619,  0.0383,  0.0486],\n",
       "                         [ 0.0575,  0.1207, -0.0082],\n",
       "                         [ 0.0723,  0.0665, -0.1400]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0063,  0.0597,  0.0828],\n",
       "                         [-0.0561, -0.0379,  0.0469],\n",
       "                         [-0.1391, -0.0999, -0.0370]],\n",
       "               \n",
       "                        [[ 0.0314, -0.0296, -0.0891],\n",
       "                         [-0.0887,  0.0176,  0.0328],\n",
       "                         [ 0.0287,  0.1816,  0.1175]],\n",
       "               \n",
       "                        [[ 0.0440,  0.1217, -0.0536],\n",
       "                         [-0.0891, -0.1557, -0.0786],\n",
       "                         [-0.1615, -0.1282, -0.0618]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0596, -0.0708,  0.0198],\n",
       "                         [ 0.0872,  0.0375,  0.1086],\n",
       "                         [ 0.0162, -0.1064, -0.0900]],\n",
       "               \n",
       "                        [[-0.1478,  0.1116,  0.0007],\n",
       "                         [-0.0787, -0.0172,  0.0079],\n",
       "                         [-0.0927, -0.0780, -0.0855]],\n",
       "               \n",
       "                        [[ 0.0031,  0.0232, -0.0632],\n",
       "                         [ 0.0358, -0.0465, -0.0623],\n",
       "                         [ 0.0592,  0.0304,  0.0049]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0215, -0.0739,  0.0132],\n",
       "                         [-0.0150, -0.0170, -0.0408],\n",
       "                         [-0.0186, -0.0725, -0.0892]],\n",
       "               \n",
       "                        [[ 0.0021,  0.0687,  0.0617],\n",
       "                         [ 0.0866,  0.0535, -0.0564],\n",
       "                         [ 0.0454, -0.0129,  0.0582]],\n",
       "               \n",
       "                        [[-0.0703,  0.1731,  0.0483],\n",
       "                         [-0.0901, -0.0800, -0.1010],\n",
       "                         [-0.1121, -0.0069,  0.1722]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0585,  0.1458, -0.0156],\n",
       "                         [-0.0464,  0.0224,  0.1140],\n",
       "                         [ 0.0185,  0.0630,  0.0124]],\n",
       "               \n",
       "                        [[-0.0243,  0.1049, -0.0452],\n",
       "                         [-0.1083,  0.0693, -0.1266],\n",
       "                         [ 0.0268,  0.0052, -0.0858]],\n",
       "               \n",
       "                        [[-0.0230,  0.0871,  0.1087],\n",
       "                         [ 0.0112, -0.0031,  0.0072],\n",
       "                         [-0.0360, -0.0431, -0.0529]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-0.0079, -0.0505,  0.0513],\n",
       "                         [-0.0800, -0.0694, -0.0038],\n",
       "                         [ 0.0160, -0.1051, -0.0406]],\n",
       "               \n",
       "                        [[ 0.0711, -0.0305,  0.0020],\n",
       "                         [ 0.0249, -0.0505, -0.0370],\n",
       "                         [-0.1938, -0.0107, -0.0829]],\n",
       "               \n",
       "                        [[ 0.0169, -0.0547, -0.0649],\n",
       "                         [-0.0868, -0.1410, -0.0719],\n",
       "                         [-0.0642, -0.0529, -0.0583]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0558, -0.0874,  0.0486],\n",
       "                         [-0.0578, -0.1017, -0.0306],\n",
       "                         [-0.0221,  0.0026, -0.0445]],\n",
       "               \n",
       "                        [[ 0.1399,  0.0369, -0.0645],\n",
       "                         [-0.0055, -0.0190, -0.0876],\n",
       "                         [-0.1006, -0.0423, -0.0045]],\n",
       "               \n",
       "                        [[ 0.0384,  0.0107,  0.0166],\n",
       "                         [ 0.0440, -0.0186, -0.0547],\n",
       "                         [ 0.0579, -0.0494, -0.0103]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0445, -0.0307, -0.0021],\n",
       "                         [-0.0806, -0.0974, -0.0783],\n",
       "                         [-0.0068, -0.0617, -0.1224]],\n",
       "               \n",
       "                        [[-0.1030, -0.0205,  0.0432],\n",
       "                         [-0.0975, -0.0482, -0.0182],\n",
       "                         [-0.0908, -0.1242, -0.1555]],\n",
       "               \n",
       "                        [[-0.0860, -0.0185,  0.0361],\n",
       "                         [-0.0296,  0.0094,  0.0774],\n",
       "                         [-0.1044,  0.0246, -0.1343]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0312, -0.0896, -0.1101],\n",
       "                         [-0.0936, -0.0748, -0.0312],\n",
       "                         [ 0.1078,  0.0077, -0.0563]],\n",
       "               \n",
       "                        [[-0.0704, -0.1020, -0.1165],\n",
       "                         [ 0.0201,  0.0726,  0.0476],\n",
       "                         [ 0.1299,  0.1026,  0.1043]],\n",
       "               \n",
       "                        [[-0.0150, -0.0229,  0.0568],\n",
       "                         [-0.0088, -0.0664, -0.0283],\n",
       "                         [ 0.0253, -0.0359,  0.0106]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0599, -0.0291,  0.0032],\n",
       "                         [-0.1186, -0.0861, -0.0374],\n",
       "                         [-0.0809, -0.0932, -0.0705]],\n",
       "               \n",
       "                        [[ 0.0366, -0.0186,  0.0197],\n",
       "                         [-0.0720, -0.0505, -0.0610],\n",
       "                         [-0.0985,  0.0244, -0.1420]],\n",
       "               \n",
       "                        [[-0.0889, -0.0060, -0.0125],\n",
       "                         [-0.1363, -0.0497,  0.0412],\n",
       "                         [ 0.0018, -0.0500,  0.0070]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0164,  0.1183, -0.0256],\n",
       "                         [ 0.0061,  0.0288,  0.0548],\n",
       "                         [-0.0394, -0.0562,  0.0273]],\n",
       "               \n",
       "                        [[-0.0442, -0.0676,  0.0714],\n",
       "                         [ 0.0332, -0.0086,  0.0088],\n",
       "                         [ 0.1772,  0.0762,  0.0103]],\n",
       "               \n",
       "                        [[ 0.1423,  0.1384, -0.0096],\n",
       "                         [-0.1060,  0.0219,  0.0862],\n",
       "                         [-0.0369, -0.0388,  0.0768]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.bias',\n",
       "               tensor([-3.2984e-05, -3.8921e-05,  8.5863e-05, -6.5839e-05,  1.2763e-05,\n",
       "                       -4.6530e-05, -4.9722e-05, -1.2315e-05, -1.0639e-05, -1.8745e-05,\n",
       "                       -2.6135e-05, -5.8939e-05, -1.5278e-05,  9.7151e-05,  4.1404e-05,\n",
       "                       -8.6345e-05, -5.9699e-05,  6.0434e-05,  4.7675e-05, -5.8788e-05,\n",
       "                       -5.4301e-05,  4.9297e-05, -1.2928e-04, -5.2866e-05, -1.4622e-05,\n",
       "                        4.0202e-05, -4.6340e-05,  6.8734e-05, -6.7072e-05, -5.3482e-05,\n",
       "                        1.6451e-05, -5.3166e-05,  2.1882e-06,  9.0116e-05, -2.0880e-05,\n",
       "                       -1.8437e-04, -1.7182e-04,  8.3318e-05, -1.1335e-04, -8.7145e-05,\n",
       "                        1.1486e-04,  4.3593e-05, -1.9912e-05,  4.4711e-05,  4.3794e-05,\n",
       "                       -6.9820e-05, -4.9851e-05, -1.2849e-04, -1.5242e-04,  5.6376e-06,\n",
       "                       -1.8879e-04, -1.0126e-04, -5.5897e-05, -1.2304e-04, -1.1906e-04,\n",
       "                        1.7979e-04,  9.5599e-05,  9.4878e-05,  8.7391e-05,  7.6703e-05,\n",
       "                       -9.3875e-05, -1.0783e-04,  3.2539e-05, -1.5490e-04], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.bias',\n",
       "               tensor([-0.4588, -0.4316, -0.3908, -0.3534, -0.2448, -0.3402, -0.4067, -0.1987,\n",
       "                       -0.2813, -0.3425, -0.4603, -0.2529, -0.3493, -0.2710, -0.2831, -0.2820,\n",
       "                       -0.5607, -0.3239, -0.2898, -0.4460, -0.4491, -0.4668, -0.3496, -0.4171,\n",
       "                       -0.3733, -0.5069, -0.2644, -0.2899, -0.3202, -0.3718, -0.3722, -0.3339,\n",
       "                       -0.3960, -0.2619, -0.4136, -0.4626, -0.3719, -0.4949, -0.3954, -0.4362,\n",
       "                       -0.3879, -0.4008, -0.4086, -0.5398, -0.3425, -0.4327, -0.3899, -0.2765,\n",
       "                       -0.3682, -0.5932, -0.5280, -0.4233, -0.5560, -0.3050, -0.3875, -0.3971,\n",
       "                       -0.6073, -0.3397, -0.3229, -0.3636, -0.4727, -0.4085, -0.3743, -0.3127],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.weight',\n",
       "               tensor([0.4188, 0.4131, 0.3106, 0.4048, 0.3215, 0.4146, 0.4458, 0.2259, 0.3878,\n",
       "                       0.3418, 0.4220, 0.2214, 0.4446, 0.3285, 0.2932, 0.3098, 0.4109, 0.2821,\n",
       "                       0.3317, 0.4733, 0.3486, 0.4151, 0.3917, 0.4573, 0.3882, 0.4408, 0.2872,\n",
       "                       0.4240, 0.3586, 0.4021, 0.4494, 0.2896, 0.3648, 0.3599, 0.4510, 0.4264,\n",
       "                       0.4935, 0.4890, 0.3642, 0.4863, 0.4317, 0.3565, 0.4172, 0.4305, 0.3813,\n",
       "                       0.4650, 0.4140, 0.3625, 0.4606, 0.3596, 0.4575, 0.3782, 0.5823, 0.3627,\n",
       "                       0.3895, 0.3817, 0.5126, 0.3082, 0.3425, 0.3756, 0.3966, 0.3372, 0.3294,\n",
       "                       0.3363], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.weight',\n",
       "               tensor([[[[-0.0083, -0.0024,  0.0212],\n",
       "                         [ 0.0119, -0.0631,  0.0131],\n",
       "                         [-0.0053, -0.0540, -0.1109]],\n",
       "               \n",
       "                        [[-0.0055,  0.0290,  0.0473],\n",
       "                         [-0.0529,  0.0819,  0.0489],\n",
       "                         [ 0.0429,  0.0348, -0.0671]],\n",
       "               \n",
       "                        [[ 0.1209,  0.1345,  0.0264],\n",
       "                         [ 0.0069, -0.1140, -0.0053],\n",
       "                         [-0.0643,  0.0150,  0.0652]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0113,  0.0658,  0.0193],\n",
       "                         [ 0.0107, -0.0325, -0.0204],\n",
       "                         [-0.0923, -0.1000, -0.0460]],\n",
       "               \n",
       "                        [[ 0.0820,  0.0048, -0.0496],\n",
       "                         [ 0.0160,  0.0109,  0.0342],\n",
       "                         [-0.0002, -0.0614, -0.0789]],\n",
       "               \n",
       "                        [[ 0.1037,  0.0137, -0.0088],\n",
       "                         [ 0.1219, -0.0883,  0.0093],\n",
       "                         [ 0.0965, -0.0544,  0.0511]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0769,  0.0146,  0.0004],\n",
       "                         [ 0.0118,  0.0247,  0.0815],\n",
       "                         [ 0.0189,  0.0508, -0.0006]],\n",
       "               \n",
       "                        [[-0.0710, -0.0938, -0.0307],\n",
       "                         [-0.0042,  0.0207, -0.0085],\n",
       "                         [-0.0254, -0.0318, -0.0886]],\n",
       "               \n",
       "                        [[ 0.0295,  0.0774,  0.0237],\n",
       "                         [ 0.1244,  0.1166,  0.1039],\n",
       "                         [ 0.0970, -0.0110,  0.1009]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0242,  0.0004, -0.0260],\n",
       "                         [-0.0073, -0.0633, -0.0141],\n",
       "                         [ 0.1000, -0.0825,  0.0295]],\n",
       "               \n",
       "                        [[ 0.0414,  0.0331,  0.0492],\n",
       "                         [-0.0923, -0.0099, -0.0986],\n",
       "                         [-0.0278, -0.0072,  0.1125]],\n",
       "               \n",
       "                        [[-0.0098, -0.0283,  0.0284],\n",
       "                         [ 0.0550,  0.0976,  0.0867],\n",
       "                         [-0.0409, -0.0483, -0.0195]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0024,  0.0460,  0.0848],\n",
       "                         [ 0.0817,  0.0109, -0.0318],\n",
       "                         [-0.0478,  0.0127, -0.0475]],\n",
       "               \n",
       "                        [[ 0.0899,  0.0595,  0.0157],\n",
       "                         [-0.0925,  0.0320,  0.0659],\n",
       "                         [ 0.0075,  0.0009,  0.0592]],\n",
       "               \n",
       "                        [[-0.0027, -0.0375, -0.0604],\n",
       "                         [-0.0273,  0.0179, -0.0866],\n",
       "                         [ 0.0045, -0.0026, -0.0607]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 0.0845, -0.0455,  0.1307],\n",
       "                         [-0.0042,  0.0362,  0.0610],\n",
       "                         [-0.0121,  0.0162, -0.0334]],\n",
       "               \n",
       "                        [[-0.1358, -0.0359, -0.0672],\n",
       "                         [-0.0851, -0.0272, -0.0583],\n",
       "                         [-0.1358, -0.0824, -0.0760]],\n",
       "               \n",
       "                        [[-0.0254, -0.1019, -0.0531],\n",
       "                         [-0.1227, -0.1275, -0.0799],\n",
       "                         [-0.0914, -0.0397,  0.0091]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0081,  0.0428,  0.0504],\n",
       "                         [ 0.0179,  0.0132, -0.0888],\n",
       "                         [ 0.0694,  0.0032, -0.0037]],\n",
       "               \n",
       "                        [[ 0.0014,  0.0334,  0.0386],\n",
       "                         [ 0.0748,  0.0276,  0.0432],\n",
       "                         [-0.0180,  0.0178,  0.1415]],\n",
       "               \n",
       "                        [[ 0.0630,  0.0436,  0.0590],\n",
       "                         [ 0.0463,  0.0406,  0.0649],\n",
       "                         [-0.0363,  0.0137, -0.0269]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.1126, -0.0666, -0.0802],\n",
       "                         [-0.0964, -0.1300, -0.0376],\n",
       "                         [-0.1286, -0.1284, -0.0423]],\n",
       "               \n",
       "                        [[ 0.0396,  0.0209,  0.0027],\n",
       "                         [ 0.0085,  0.0060, -0.0215],\n",
       "                         [-0.0218,  0.0268, -0.0180]],\n",
       "               \n",
       "                        [[ 0.0690, -0.0023,  0.0307],\n",
       "                         [ 0.0373, -0.0023,  0.0527],\n",
       "                         [ 0.0444,  0.0144, -0.0690]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0019, -0.0722, -0.0325],\n",
       "                         [-0.0263, -0.0562,  0.0074],\n",
       "                         [-0.0460, -0.0328, -0.1271]],\n",
       "               \n",
       "                        [[-0.0286, -0.0432, -0.0888],\n",
       "                         [-0.0107,  0.0070,  0.0141],\n",
       "                         [-0.0567, -0.0326, -0.0643]],\n",
       "               \n",
       "                        [[ 0.0029, -0.1029, -0.0880],\n",
       "                         [-0.0627, -0.0093, -0.0479],\n",
       "                         [ 0.0241,  0.0527, -0.0472]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0673,  0.0097, -0.0313],\n",
       "                         [-0.0024,  0.0407, -0.0791],\n",
       "                         [ 0.0548,  0.0275,  0.0635]],\n",
       "               \n",
       "                        [[ 0.0447,  0.0275,  0.0139],\n",
       "                         [-0.0791,  0.0200, -0.0054],\n",
       "                         [ 0.0072,  0.0390, -0.0523]],\n",
       "               \n",
       "                        [[ 0.1353,  0.0759, -0.0087],\n",
       "                         [-0.0412,  0.0322,  0.0494],\n",
       "                         [ 0.1010,  0.0756,  0.0721]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0218,  0.0482,  0.0056],\n",
       "                         [ 0.0657, -0.0428,  0.0201],\n",
       "                         [ 0.0433, -0.0120, -0.0059]],\n",
       "               \n",
       "                        [[-0.1187, -0.1080, -0.0688],\n",
       "                         [-0.0739, -0.0606, -0.0524],\n",
       "                         [-0.0003, -0.0485, -0.0588]],\n",
       "               \n",
       "                        [[-0.0218, -0.0075, -0.0609],\n",
       "                         [ 0.0107, -0.0561, -0.0267],\n",
       "                         [ 0.0463,  0.0418, -0.0042]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-0.0368,  0.0486,  0.0472],\n",
       "                         [ 0.0189,  0.0488,  0.0454],\n",
       "                         [ 0.0644,  0.0493,  0.0343]],\n",
       "               \n",
       "                        [[-0.0284, -0.0331, -0.0082],\n",
       "                         [-0.0267, -0.0057, -0.0165],\n",
       "                         [-0.0849, -0.0727, -0.1275]],\n",
       "               \n",
       "                        [[ 0.0604, -0.0472, -0.0069],\n",
       "                         [-0.0607, -0.0019,  0.0031],\n",
       "                         [-0.0418, -0.0744,  0.0121]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.bias',\n",
       "               tensor([-9.3345e-05,  1.3024e-04, -1.1574e-04, -1.2292e-04, -1.6370e-04,\n",
       "                        3.0887e-04, -5.5996e-05,  4.4491e-05,  9.6102e-05, -1.1005e-04,\n",
       "                       -6.0894e-05, -5.6727e-05,  6.2190e-05,  6.0814e-05,  1.2964e-04,\n",
       "                        2.7015e-05, -6.6901e-05, -1.7066e-05, -9.3877e-05,  3.4541e-06,\n",
       "                        9.7838e-06, -5.4105e-05,  1.2633e-04, -7.5636e-05, -7.5941e-05,\n",
       "                       -2.2322e-04, -1.2663e-05,  5.6101e-05,  2.7109e-05, -1.0894e-04,\n",
       "                        2.0660e-05, -2.4451e-04, -2.0676e-05, -8.5743e-05, -6.3090e-05,\n",
       "                       -2.7364e-05,  7.3669e-05,  1.0281e-04,  7.0016e-05,  6.3110e-06,\n",
       "                       -1.0472e-04, -1.4651e-04, -1.2754e-06, -7.2649e-05,  1.3840e-04,\n",
       "                       -3.5273e-05, -1.9574e-04, -1.4541e-04, -1.3594e-04,  2.1333e-05,\n",
       "                       -7.5961e-05,  2.4097e-05,  1.0072e-04, -5.6227e-07, -2.2973e-04,\n",
       "                       -2.4489e-05,  2.5432e-06, -1.5903e-05,  5.3590e-05, -1.2271e-04,\n",
       "                       -1.0820e-04, -5.6820e-05, -4.7318e-05,  9.0739e-05], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.bias',\n",
       "               tensor([-0.8042, -0.2265, -0.3768, -0.4851, -0.5631, -0.5897, -0.3761, -0.0237,\n",
       "                       -0.4882, -0.2598, -0.3484, -0.6588, -0.5431, -0.6577, -0.8242, -0.5697,\n",
       "                       -0.4169, -0.3592, -0.6196, -0.2392, -0.4049, -0.4046, -0.6245, -0.4038,\n",
       "                       -0.2824, -0.5334, -0.5237, -0.5009, -0.8406, -0.4717, -0.6010, -0.6576,\n",
       "                       -0.5708, -0.4225, -0.3423, -0.3519, -0.5206, -0.3345, -0.6410, -0.0257,\n",
       "                       -0.3759, -0.4119, -0.3361, -0.4235, -0.6357, -0.4266, -0.4434, -0.3802,\n",
       "                       -0.4321, -0.3920, -0.2490, -0.3850, -0.2216, -0.5317, -0.7094, -0.7102,\n",
       "                       -0.4545, -0.3217, -0.6113, -0.3835, -0.4206, -0.5866, -0.0885, -0.3301],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.weight',\n",
       "               tensor([0.6704, 0.4191, 0.4174, 0.4444, 0.5798, 0.5131, 0.4007, 0.3543, 0.5347,\n",
       "                       0.4366, 0.3306, 0.6104, 0.4774, 0.4845, 0.8048, 0.5086, 0.4569, 0.3306,\n",
       "                       0.5889, 0.3313, 0.4562, 0.4720, 0.6020, 0.4407, 0.4260, 0.5097, 0.4737,\n",
       "                       0.5321, 0.7544, 0.4878, 0.5739, 0.5150, 0.4587, 0.4768, 0.4970, 0.3604,\n",
       "                       0.5399, 0.4639, 0.6623, 0.2567, 0.3943, 0.4788, 0.4375, 0.4963, 0.6319,\n",
       "                       0.4607, 0.4384, 0.5149, 0.5052, 0.4281, 0.4046, 0.4575, 0.3565, 0.5008,\n",
       "                       0.6412, 0.6293, 0.4564, 0.4335, 0.5839, 0.4044, 0.4579, 0.4808, 0.3473,\n",
       "                       0.4573], device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.weights',\n",
       "               tensor([[-0.0496, -0.0177, -0.0078,  ..., -0.0113, -0.0133, -0.0184],\n",
       "                       [-0.0154,  0.0013,  0.0151,  ...,  0.0116, -0.0099,  0.0146],\n",
       "                       [ 0.0223,  0.0046, -0.0262,  ...,  0.0254, -0.0135, -0.0252],\n",
       "                       [ 0.0143, -0.0242,  0.0149,  ..., -0.0430,  0.0064, -0.0038],\n",
       "                       [ 0.0275,  0.0306, -0.0010,  ...,  0.0126,  0.0315,  0.0349]],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.bias',\n",
       "               tensor([ 0.0222, -0.0053,  0.0075, -0.0253, -0.0118], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.prompt_learning_rates_dict.prompt-prompt_dict-pad_up',\n",
       "               tensor([-1.4980e-03, -2.0056e-02, -9.6170e-04, -2.5963e-04, -3.4091e-04,\n",
       "                       -9.8441e-42], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.prompt_learning_rates_dict.prompt-prompt_dict-pad_down',\n",
       "               tensor([-8.1682e-03,  2.1902e-02, -2.8892e-03,  3.2864e-04,  1.6797e-03,\n",
       "                       -9.8441e-42], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.prompt_learning_rates_dict.prompt-prompt_dict-pad_left',\n",
       "               tensor([ 1.5824e-03,  1.3268e-03,  5.8750e-03, -4.0909e-04, -3.5350e-05,\n",
       "                       -9.8441e-42], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.prompt_learning_rates_dict.prompt-prompt_dict-pad_right',\n",
       "               tensor([-7.8996e-03, -5.4425e-03,  2.6841e-03,  4.9899e-05,  6.5626e-04,\n",
       "                       -9.8441e-42], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-weights',\n",
       "               tensor([ 1.4977e-01,  1.2959e-01,  4.8240e+00,  7.0472e-01,  7.1629e-01,\n",
       "                       -9.8441e-42], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-bias',\n",
       "               tensor([-1.5626e-01,  1.2033e+00, -2.7153e-02,  3.4121e-02,  3.0905e-02,\n",
       "                       -9.8441e-42], device='cuda:0'))]),\n",
       " 'per_epoch_statistics': {'train_loss_mean': [1.3160445464849473,\n",
       "   1.106396884083748,\n",
       "   0.9860357806682587,\n",
       "   0.9131015764474869,\n",
       "   0.8637917112112046,\n",
       "   0.821464849948883,\n",
       "   0.7942467584013939,\n",
       "   0.7648510930538177,\n",
       "   0.7450599699020386,\n",
       "   0.7267218778729438,\n",
       "   0.6836605352759362,\n",
       "   0.672456706404686,\n",
       "   0.6629057773947715,\n",
       "   0.6528558265566826,\n",
       "   0.6341994258761406,\n",
       "   0.6237746843099594,\n",
       "   0.609835975766182,\n",
       "   0.6095672168135643,\n",
       "   0.5970894060134888,\n",
       "   0.5837510070204734,\n",
       "   0.5821724631786347,\n",
       "   0.5578820499777793,\n",
       "   0.5606756236851216,\n",
       "   0.5551093031466007,\n",
       "   0.5467784143090249,\n",
       "   0.5342186943292618,\n",
       "   0.5204739655852317,\n",
       "   0.5288931556046009,\n",
       "   0.5233785250782966,\n",
       "   0.5273634595274925,\n",
       "   0.5043366783559322,\n",
       "   0.5165498792827129,\n",
       "   0.5076219896376133,\n",
       "   0.4930650516748428,\n",
       "   0.5073383504152298,\n",
       "   0.48054526871442793,\n",
       "   0.48916585156321524,\n",
       "   0.4724599329531193,\n",
       "   0.47962649461627005,\n",
       "   0.4675383153557777,\n",
       "   0.477253717571497,\n",
       "   0.47056097686290743,\n",
       "   0.46487012737989425,\n",
       "   0.47072403752803804,\n",
       "   0.4629548645019531,\n",
       "   0.4570743176937103,\n",
       "   0.4545635378956795,\n",
       "   0.4456339888572693,\n",
       "   0.44834652096033095,\n",
       "   0.44938538658618926,\n",
       "   0.44969414550065995,\n",
       "   0.4290268030166626,\n",
       "   0.4386485442519188,\n",
       "   0.43669188991189,\n",
       "   0.44564403280615805,\n",
       "   0.44339534613490106,\n",
       "   0.43787645718455315,\n",
       "   0.42565498080849645,\n",
       "   0.429466683447361,\n",
       "   0.42710491171479226,\n",
       "   0.4297366060614586,\n",
       "   0.4237238437533379,\n",
       "   0.41944055134058,\n",
       "   0.4114208092689514,\n",
       "   0.4237313523888588,\n",
       "   0.42031281992793085,\n",
       "   0.42378030291199686,\n",
       "   0.41181282034516337,\n",
       "   0.41570329111814497,\n",
       "   0.4169016446173191,\n",
       "   0.4130225228667259,\n",
       "   0.4148272407054901,\n",
       "   0.4088503815233707,\n",
       "   0.4116770578622818,\n",
       "   0.4029316125512123,\n",
       "   0.40263688418269156,\n",
       "   0.40426160070300104,\n",
       "   0.40979480239748955,\n",
       "   0.4027254518568516,\n",
       "   0.3985153460800648,\n",
       "   0.3971044275164604,\n",
       "   0.4031112222671509,\n",
       "   0.40217092019319534,\n",
       "   0.40289022171497346,\n",
       "   0.3893948571681976,\n",
       "   0.40444845670461654,\n",
       "   0.3965265491604805,\n",
       "   0.39166100281476973,\n",
       "   0.3927401440143585,\n",
       "   0.3849285019040108,\n",
       "   0.386672621935606,\n",
       "   0.3958416793644428,\n",
       "   0.37655558878183365,\n",
       "   0.3886579092144966,\n",
       "   0.38493706327676774,\n",
       "   0.3910977513343096,\n",
       "   0.3874248753488064,\n",
       "   0.3884032698273659,\n",
       "   0.3769989092946053],\n",
       "  'train_loss_std': [0.17426562363843479,\n",
       "   0.14521306564758502,\n",
       "   0.16486727674919344,\n",
       "   0.14385107355700338,\n",
       "   0.15373981031071585,\n",
       "   0.15028814214669653,\n",
       "   0.14827021359947865,\n",
       "   0.1331597415197042,\n",
       "   0.13793265282746686,\n",
       "   0.14195659395370594,\n",
       "   0.1422195817869089,\n",
       "   0.14765801646523588,\n",
       "   0.13741469999744646,\n",
       "   0.14555672791466479,\n",
       "   0.1362152604310494,\n",
       "   0.1391482492927885,\n",
       "   0.13557368492896885,\n",
       "   0.13723027764131146,\n",
       "   0.13587406981124162,\n",
       "   0.13261452634128706,\n",
       "   0.1408511667245735,\n",
       "   0.131333095257273,\n",
       "   0.1415541091894704,\n",
       "   0.14142115625768714,\n",
       "   0.13369635541711458,\n",
       "   0.12900213653552414,\n",
       "   0.12650640844185462,\n",
       "   0.13241285907391778,\n",
       "   0.1327625685885847,\n",
       "   0.13135827100170736,\n",
       "   0.13047098185603387,\n",
       "   0.13035510062776734,\n",
       "   0.13005098436634902,\n",
       "   0.13127538635588618,\n",
       "   0.12808242957337473,\n",
       "   0.13203754311519894,\n",
       "   0.12369430664792501,\n",
       "   0.13236900083186104,\n",
       "   0.13284583399924943,\n",
       "   0.1289961499224474,\n",
       "   0.12810642486833013,\n",
       "   0.1253908333333386,\n",
       "   0.12804209263648234,\n",
       "   0.13538039515447148,\n",
       "   0.12510007945170765,\n",
       "   0.12353873903145342,\n",
       "   0.12913731652349833,\n",
       "   0.12458087407378352,\n",
       "   0.1238645488663649,\n",
       "   0.1279508638845226,\n",
       "   0.12152161029392249,\n",
       "   0.1164893035320653,\n",
       "   0.12621658977422973,\n",
       "   0.1215980552417574,\n",
       "   0.12824361456287348,\n",
       "   0.12771447880701894,\n",
       "   0.12018691131533764,\n",
       "   0.12571914239427764,\n",
       "   0.122262472391687,\n",
       "   0.1198831512999006,\n",
       "   0.12523548733761727,\n",
       "   0.1275698089078079,\n",
       "   0.12397200144302577,\n",
       "   0.11449525489037471,\n",
       "   0.12698391002871334,\n",
       "   0.12242081085250245,\n",
       "   0.12605612175192502,\n",
       "   0.12249913080557687,\n",
       "   0.12107335918513702,\n",
       "   0.12602726991917687,\n",
       "   0.1149889256023894,\n",
       "   0.11543232926473955,\n",
       "   0.11712647419530925,\n",
       "   0.11994160126679813,\n",
       "   0.12272243429610542,\n",
       "   0.11997026642780449,\n",
       "   0.12388627285308697,\n",
       "   0.1202556163764583,\n",
       "   0.12203030515335707,\n",
       "   0.1232489298689588,\n",
       "   0.1173355862225582,\n",
       "   0.12502517567838892,\n",
       "   0.12546427999203982,\n",
       "   0.12150128427837338,\n",
       "   0.11690065797012004,\n",
       "   0.11993698690791962,\n",
       "   0.11844979286341009,\n",
       "   0.1196754306852535,\n",
       "   0.1155078751442462,\n",
       "   0.11237699994648387,\n",
       "   0.11971862927379616,\n",
       "   0.12329881214455286,\n",
       "   0.11587176283292448,\n",
       "   0.11806802240058477,\n",
       "   0.11775161114008954,\n",
       "   0.12395026712325914,\n",
       "   0.11077170410729717,\n",
       "   0.11670014441347848,\n",
       "   0.10989382916347061],\n",
       "  'train_accuracy_mean': [0.45921333250403407,\n",
       "   0.5621733329296112,\n",
       "   0.6172266655564308,\n",
       "   0.6459466658234596,\n",
       "   0.6674666643738747,\n",
       "   0.6826933329701423,\n",
       "   0.6969600005149841,\n",
       "   0.7085466669797897,\n",
       "   0.7174533331394195,\n",
       "   0.725533332824707,\n",
       "   0.7405733331441879,\n",
       "   0.7470799998044968,\n",
       "   0.7491466664075851,\n",
       "   0.7529333325624465,\n",
       "   0.7591200007200241,\n",
       "   0.7641600002050399,\n",
       "   0.770426666378975,\n",
       "   0.7717333332300186,\n",
       "   0.7736666660308837,\n",
       "   0.7809999995231628,\n",
       "   0.7801733330488205,\n",
       "   0.7911600003242493,\n",
       "   0.7901466664075851,\n",
       "   0.7920799996852875,\n",
       "   0.7942800014019012,\n",
       "   0.7992799994945526,\n",
       "   0.8047333331108093,\n",
       "   0.8023866667747498,\n",
       "   0.8046933326721192,\n",
       "   0.8006533331871033,\n",
       "   0.8110400007963181,\n",
       "   0.8072799988985062,\n",
       "   0.8095333318710327,\n",
       "   0.8154133322238922,\n",
       "   0.810466666340828,\n",
       "   0.821400000333786,\n",
       "   0.8177199989557267,\n",
       "   0.8228000001907348,\n",
       "   0.8204800004959106,\n",
       "   0.8278266664743423,\n",
       "   0.8216666666269302,\n",
       "   0.8246000000238418,\n",
       "   0.8245200003385544,\n",
       "   0.8224000008106231,\n",
       "   0.8274533344507218,\n",
       "   0.829013335108757,\n",
       "   0.8314933326244355,\n",
       "   0.8346000015735626,\n",
       "   0.8327466682195663,\n",
       "   0.8320533337593079,\n",
       "   0.8322133339643478,\n",
       "   0.8402133333683014,\n",
       "   0.8360799993276596,\n",
       "   0.8372666659355164,\n",
       "   0.8349733347892762,\n",
       "   0.834,\n",
       "   0.8355066686868667,\n",
       "   0.8413733344078064,\n",
       "   0.8398400017023087,\n",
       "   0.8399466670751572,\n",
       "   0.8383066679239273,\n",
       "   0.8426800000667573,\n",
       "   0.8446266658306122,\n",
       "   0.8461333340406418,\n",
       "   0.8419866678714752,\n",
       "   0.8433733338117599,\n",
       "   0.8409066677093506,\n",
       "   0.846893334031105,\n",
       "   0.8452400013208389,\n",
       "   0.8437066679000854,\n",
       "   0.8456133334636688,\n",
       "   0.8428799998760224,\n",
       "   0.846733335018158,\n",
       "   0.8457066664695739,\n",
       "   0.849053334236145,\n",
       "   0.8485733352899552,\n",
       "   0.8491333348751068,\n",
       "   0.8457600013017654,\n",
       "   0.8500266672372818,\n",
       "   0.851853333234787,\n",
       "   0.8526400004625321,\n",
       "   0.850626667380333,\n",
       "   0.8511466660499573,\n",
       "   0.8484800000190735,\n",
       "   0.8551333333253861,\n",
       "   0.8496133351325988,\n",
       "   0.8532266671657562,\n",
       "   0.8536533352136612,\n",
       "   0.8534000016450882,\n",
       "   0.8582400014400482,\n",
       "   0.8559333350658417,\n",
       "   0.8544266678094864,\n",
       "   0.8596266674995422,\n",
       "   0.8550266680717469,\n",
       "   0.8555066677331924,\n",
       "   0.854866667509079,\n",
       "   0.8557466663122177,\n",
       "   0.8550666677951813,\n",
       "   0.8607600011825561],\n",
       "  'train_accuracy_std': [0.08975623149171615,\n",
       "   0.06979325751528698,\n",
       "   0.07818693918123763,\n",
       "   0.06724559876462981,\n",
       "   0.06865229745538287,\n",
       "   0.06746481749229996,\n",
       "   0.06580917270819338,\n",
       "   0.060061627345615466,\n",
       "   0.06298080077966177,\n",
       "   0.06309115294831749,\n",
       "   0.06340928992596356,\n",
       "   0.06606601165839927,\n",
       "   0.06041692643581664,\n",
       "   0.06652531702473724,\n",
       "   0.058764530868212324,\n",
       "   0.0623675929355608,\n",
       "   0.059043452037792134,\n",
       "   0.06109751768049017,\n",
       "   0.05962382075372094,\n",
       "   0.05889029338605562,\n",
       "   0.06271251069441793,\n",
       "   0.05776262737579534,\n",
       "   0.059853718091203725,\n",
       "   0.061179754914370936,\n",
       "   0.05821715565609827,\n",
       "   0.05642707586698455,\n",
       "   0.05510954794424926,\n",
       "   0.05681542872900099,\n",
       "   0.05641764331147801,\n",
       "   0.05674285013066238,\n",
       "   0.055849666287572375,\n",
       "   0.05696920677736967,\n",
       "   0.05482136792530942,\n",
       "   0.056002641634378805,\n",
       "   0.05432785299645917,\n",
       "   0.055875218604144336,\n",
       "   0.05348355486498753,\n",
       "   0.056352698995701396,\n",
       "   0.057025071207151494,\n",
       "   0.05381438265591297,\n",
       "   0.055813779613973935,\n",
       "   0.05252127853147657,\n",
       "   0.056671497974066745,\n",
       "   0.05722252799907983,\n",
       "   0.05401402107429259,\n",
       "   0.053667328243526095,\n",
       "   0.054664970916747865,\n",
       "   0.053063443581209915,\n",
       "   0.05331510512223769,\n",
       "   0.05557423126356028,\n",
       "   0.051730832078336696,\n",
       "   0.05043234366106727,\n",
       "   0.05306024305073198,\n",
       "   0.05263117034293907,\n",
       "   0.05367618422734486,\n",
       "   0.05397530497915199,\n",
       "   0.052707885027945794,\n",
       "   0.052790389795974016,\n",
       "   0.052652076988025355,\n",
       "   0.05096924555893118,\n",
       "   0.05439668534769516,\n",
       "   0.054668455625997155,\n",
       "   0.05250200776078442,\n",
       "   0.04956593996677253,\n",
       "   0.0535633359392861,\n",
       "   0.05216979441569122,\n",
       "   0.05368612775476745,\n",
       "   0.052439529977268674,\n",
       "   0.0513256299621104,\n",
       "   0.053643624961238486,\n",
       "   0.04977863870079104,\n",
       "   0.05094719656484032,\n",
       "   0.051418070192767375,\n",
       "   0.05185589751255716,\n",
       "   0.052317762561272664,\n",
       "   0.05069832653545,\n",
       "   0.0522375363191821,\n",
       "   0.05149390876182745,\n",
       "   0.05076918913799061,\n",
       "   0.05160048091942806,\n",
       "   0.05024614495825147,\n",
       "   0.05274895088700036,\n",
       "   0.05260520520363766,\n",
       "   0.05124495415432784,\n",
       "   0.04902294404782647,\n",
       "   0.051120181433541724,\n",
       "   0.05000610928789501,\n",
       "   0.050145432320784215,\n",
       "   0.04829258210007939,\n",
       "   0.04664871453361758,\n",
       "   0.04926026260088669,\n",
       "   0.05176404005685755,\n",
       "   0.050077659077241314,\n",
       "   0.050256888759530334,\n",
       "   0.04997765163619533,\n",
       "   0.05098893103589201,\n",
       "   0.04648868738318437,\n",
       "   0.05099385667364485,\n",
       "   0.047842801987990964],\n",
       "  'train_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'train_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_learning_rate_mean': [0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005],\n",
       "  'train_learning_rate_std': [4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19],\n",
       "  'val_loss_mean': [1.3232923622926076,\n",
       "   1.1894809277852376,\n",
       "   1.1264097958803176,\n",
       "   1.0947148025035858,\n",
       "   1.0711330926418305,\n",
       "   1.0195216075579325,\n",
       "   0.9987130067745844,\n",
       "   0.9990162128210067,\n",
       "   0.9722443191210429,\n",
       "   0.9552822176615398,\n",
       "   0.941646837592125,\n",
       "   0.967976932922999,\n",
       "   0.9248254166046779,\n",
       "   0.9382028935352961,\n",
       "   0.9223483260472616,\n",
       "   0.9224592175086339,\n",
       "   0.9475555159648259,\n",
       "   0.9184496374924978,\n",
       "   0.8966768890619278,\n",
       "   0.8978578901290893,\n",
       "   0.8948789072036744,\n",
       "   0.873430781364441,\n",
       "   0.8916192205746969,\n",
       "   0.8957611544926961,\n",
       "   0.919644811352094,\n",
       "   0.9286870654424032,\n",
       "   0.9099276053905487,\n",
       "   0.8902266538143158,\n",
       "   0.9011410888036092,\n",
       "   0.8812775923808416,\n",
       "   0.8965886867046357,\n",
       "   0.8986134952306748,\n",
       "   0.9165736542145411,\n",
       "   0.917927629550298,\n",
       "   0.9003595381975174,\n",
       "   0.886435733238856,\n",
       "   0.9007083714008332,\n",
       "   0.8727547977368036,\n",
       "   0.8839639522631964,\n",
       "   0.8950304442644119,\n",
       "   0.9240653739372889,\n",
       "   0.900753464102745,\n",
       "   0.9139314967393876,\n",
       "   0.8902520169814427,\n",
       "   0.9232384953896204,\n",
       "   0.9331978501876196,\n",
       "   0.8843337351083755,\n",
       "   0.9172906408707301,\n",
       "   0.8851274408896764,\n",
       "   0.8950312892595927,\n",
       "   0.877679214477539,\n",
       "   0.9010367784897486,\n",
       "   0.9122519061962764,\n",
       "   0.9398732682069143,\n",
       "   0.9001236250003178,\n",
       "   0.9225976471106211,\n",
       "   0.9012036907672882,\n",
       "   0.9150048094987869,\n",
       "   0.9194931705792745,\n",
       "   0.9032438343763352,\n",
       "   0.9222749497493108,\n",
       "   0.8941874619325002,\n",
       "   0.9418920918305715,\n",
       "   0.9026945020755132,\n",
       "   0.9057124626636505,\n",
       "   0.9022782121102015,\n",
       "   0.9027903050184249,\n",
       "   0.9119100389877955,\n",
       "   0.919948629339536,\n",
       "   0.8973599525292715,\n",
       "   0.9351670092344284,\n",
       "   0.9055193974574407,\n",
       "   0.9168798408905665,\n",
       "   0.9310293561220169,\n",
       "   0.9179115853706996,\n",
       "   0.9264047704140346,\n",
       "   0.909256991147995,\n",
       "   0.8992313595612844,\n",
       "   0.9051800396045049,\n",
       "   0.926942921479543,\n",
       "   0.9139430715640386,\n",
       "   0.9007696221272151,\n",
       "   0.9428027387460073,\n",
       "   0.9305345159769058,\n",
       "   0.9291775912046433,\n",
       "   0.9231759574015935,\n",
       "   0.9153006803989411,\n",
       "   0.9333254780371983,\n",
       "   0.9294368229309717,\n",
       "   0.9514156107107798,\n",
       "   0.951606368223826,\n",
       "   0.9369155927499135,\n",
       "   0.9632789973417918,\n",
       "   0.8909803402423858,\n",
       "   0.9318386042118072,\n",
       "   0.9189117723703384,\n",
       "   0.9211500809590022,\n",
       "   0.9386493430534999,\n",
       "   0.9463090296586355],\n",
       "  'val_loss_std': [0.11647823748506167,\n",
       "   0.11747905011162843,\n",
       "   0.1242761086015747,\n",
       "   0.1275362972966492,\n",
       "   0.12610198860233282,\n",
       "   0.1307354937912151,\n",
       "   0.1282765159368321,\n",
       "   0.1311958637417166,\n",
       "   0.12266193794229616,\n",
       "   0.12412701905799661,\n",
       "   0.132111515144934,\n",
       "   0.1347089107859676,\n",
       "   0.13570576988789185,\n",
       "   0.13287022690978711,\n",
       "   0.13053924830304703,\n",
       "   0.13848172785951424,\n",
       "   0.13533903787611842,\n",
       "   0.13601427185317172,\n",
       "   0.13365907220342743,\n",
       "   0.13821109740020027,\n",
       "   0.13186867640737432,\n",
       "   0.141764302344976,\n",
       "   0.13848096879712724,\n",
       "   0.14450195898798487,\n",
       "   0.13296671489015044,\n",
       "   0.13885184405743206,\n",
       "   0.13905714937163627,\n",
       "   0.13791286282284845,\n",
       "   0.13376248554467499,\n",
       "   0.1289059696256356,\n",
       "   0.1427827751065115,\n",
       "   0.13641614568240815,\n",
       "   0.14557600186577105,\n",
       "   0.14125549206087112,\n",
       "   0.1334761507299216,\n",
       "   0.1349928070029457,\n",
       "   0.13158012450788475,\n",
       "   0.1389706257079557,\n",
       "   0.1360023208590566,\n",
       "   0.13651932762759442,\n",
       "   0.14126848258278132,\n",
       "   0.14160891078585794,\n",
       "   0.13790884260333594,\n",
       "   0.14329846119983838,\n",
       "   0.14008599931764748,\n",
       "   0.1464906427917261,\n",
       "   0.1483932291118359,\n",
       "   0.13559518368970103,\n",
       "   0.143542504131164,\n",
       "   0.14732625023344303,\n",
       "   0.13554746323889835,\n",
       "   0.144913035775442,\n",
       "   0.14474994494491228,\n",
       "   0.15154016072800036,\n",
       "   0.1473742412165844,\n",
       "   0.14439224626068686,\n",
       "   0.14740492676549868,\n",
       "   0.14009107208413057,\n",
       "   0.1392204417395206,\n",
       "   0.14869365298617915,\n",
       "   0.1498705586779277,\n",
       "   0.14353392853450317,\n",
       "   0.14962893219170684,\n",
       "   0.14719012428221961,\n",
       "   0.14791679199019772,\n",
       "   0.15047804567229067,\n",
       "   0.14265861125748663,\n",
       "   0.14707151015964035,\n",
       "   0.1516854213992948,\n",
       "   0.14786281169415127,\n",
       "   0.15026038769602731,\n",
       "   0.15041819889895128,\n",
       "   0.15011347633986377,\n",
       "   0.1517699537226213,\n",
       "   0.1468329917249777,\n",
       "   0.15203696132976596,\n",
       "   0.14643765356728194,\n",
       "   0.148580413362242,\n",
       "   0.14468776951297913,\n",
       "   0.14730068208188082,\n",
       "   0.15327738172959116,\n",
       "   0.14885395679734761,\n",
       "   0.15147604711066445,\n",
       "   0.14806858325100006,\n",
       "   0.1529742844157197,\n",
       "   0.1415720442006397,\n",
       "   0.15247994745654891,\n",
       "   0.14996043452308686,\n",
       "   0.15318938312009678,\n",
       "   0.14995983850092554,\n",
       "   0.15490561737395375,\n",
       "   0.1495272912650288,\n",
       "   0.1580372485666539,\n",
       "   0.14502473632795077,\n",
       "   0.1508011542750039,\n",
       "   0.14973175765621707,\n",
       "   0.15273693034795652,\n",
       "   0.1590119757581431,\n",
       "   0.15637888665320515],\n",
       "  'val_accuracy_mean': [0.46097777783870697,\n",
       "   0.5262666661540667,\n",
       "   0.5511999993522962,\n",
       "   0.5670888857046763,\n",
       "   0.5765999995668729,\n",
       "   0.5990666659673055,\n",
       "   0.6080222203334172,\n",
       "   0.6106000012159347,\n",
       "   0.6209555538495382,\n",
       "   0.6278000008066495,\n",
       "   0.6328222206234932,\n",
       "   0.6268888870875041,\n",
       "   0.6398222217957179,\n",
       "   0.6353555532296499,\n",
       "   0.6426888883113862,\n",
       "   0.640444443623225,\n",
       "   0.6310888862609864,\n",
       "   0.6427111109097798,\n",
       "   0.6532666661341985,\n",
       "   0.6524888897935549,\n",
       "   0.6540444469451905,\n",
       "   0.6628888896107674,\n",
       "   0.6565333334604899,\n",
       "   0.6602888870239257,\n",
       "   0.6441333322723707,\n",
       "   0.6452888890107473,\n",
       "   0.6524888875087103,\n",
       "   0.6566666679581007,\n",
       "   0.6563333316644033,\n",
       "   0.6586444437503814,\n",
       "   0.663711110552152,\n",
       "   0.6549111089110374,\n",
       "   0.6460222212473551,\n",
       "   0.6469777776797613,\n",
       "   0.6573777796824773,\n",
       "   0.6623555566867193,\n",
       "   0.6529999996225039,\n",
       "   0.6696888887882233,\n",
       "   0.6642444433768591,\n",
       "   0.6575777777036032,\n",
       "   0.6479777777194977,\n",
       "   0.657733332713445,\n",
       "   0.6490888889630636,\n",
       "   0.6623111112912496,\n",
       "   0.649977777004242,\n",
       "   0.6456222233176231,\n",
       "   0.6652666670084,\n",
       "   0.6512888892491658,\n",
       "   0.6628444437185923,\n",
       "   0.6626666676998139,\n",
       "   0.6603999974330267,\n",
       "   0.6596888914704323,\n",
       "   0.6589333332578341,\n",
       "   0.6455333350102107,\n",
       "   0.6597333325942357,\n",
       "   0.654711111287276,\n",
       "   0.659377778172493,\n",
       "   0.65417777856191,\n",
       "   0.6551111101110776,\n",
       "   0.6563333333532015,\n",
       "   0.6532000015179317,\n",
       "   0.6590666667620341,\n",
       "   0.6424888890981674,\n",
       "   0.6584444441397985,\n",
       "   0.6572666670878728,\n",
       "   0.6557333314418793,\n",
       "   0.6607333332300186,\n",
       "   0.6539555557568868,\n",
       "   0.6537111107508341,\n",
       "   0.6610888890425364,\n",
       "   0.6480222215255101,\n",
       "   0.657133332490921,\n",
       "   0.6554444448153178,\n",
       "   0.6540888889630636,\n",
       "   0.6494000012675921,\n",
       "   0.6521777774890264,\n",
       "   0.663222220937411,\n",
       "   0.6599555537104607,\n",
       "   0.6575555555025736,\n",
       "   0.6481999986370405,\n",
       "   0.6535555545488994,\n",
       "   0.6603777766227722,\n",
       "   0.6452666659156482,\n",
       "   0.646933332880338,\n",
       "   0.651244444946448,\n",
       "   0.6526888897021611,\n",
       "   0.6514444431662559,\n",
       "   0.6502000000079473,\n",
       "   0.6510444457332293,\n",
       "   0.642244443098704,\n",
       "   0.6476000003019968,\n",
       "   0.6528888881206513,\n",
       "   0.6406888885299364,\n",
       "   0.6619999996821085,\n",
       "   0.6503333316246669,\n",
       "   0.6560222214460373,\n",
       "   0.6557777770360311,\n",
       "   0.6523777763048808,\n",
       "   0.6485111127297084],\n",
       "  'val_accuracy_std': [0.05985853296059155,\n",
       "   0.0586855013777382,\n",
       "   0.061441063934576934,\n",
       "   0.059486439912075556,\n",
       "   0.05867294155640943,\n",
       "   0.06047726351935027,\n",
       "   0.06070279349631914,\n",
       "   0.05969374945283532,\n",
       "   0.05972881384025232,\n",
       "   0.05951200359208484,\n",
       "   0.0612405549974414,\n",
       "   0.05973480732320621,\n",
       "   0.061691932823956915,\n",
       "   0.06047392836081185,\n",
       "   0.06034247592806707,\n",
       "   0.06133896883982308,\n",
       "   0.05955265085969499,\n",
       "   0.059683595464606345,\n",
       "   0.05951031050848992,\n",
       "   0.06045896936861308,\n",
       "   0.05936277433012993,\n",
       "   0.06040562646560633,\n",
       "   0.05806366959374301,\n",
       "   0.05854082132915861,\n",
       "   0.05650392334651066,\n",
       "   0.05892265861477068,\n",
       "   0.058849698151879175,\n",
       "   0.06141902492433686,\n",
       "   0.05773534865639954,\n",
       "   0.05867499732980259,\n",
       "   0.060982130458056026,\n",
       "   0.05909966185276509,\n",
       "   0.06079254035075571,\n",
       "   0.05982115156569107,\n",
       "   0.057778103670856396,\n",
       "   0.05785876339598118,\n",
       "   0.056322158016277296,\n",
       "   0.05816496041797942,\n",
       "   0.0567272842194433,\n",
       "   0.0591187103591776,\n",
       "   0.06041755685374715,\n",
       "   0.059333733694440505,\n",
       "   0.058488869044153007,\n",
       "   0.060870168741005724,\n",
       "   0.0585263458208206,\n",
       "   0.05990565125848547,\n",
       "   0.06058208010666744,\n",
       "   0.05573188165782992,\n",
       "   0.05948592709543813,\n",
       "   0.05882554436492487,\n",
       "   0.05954509044094451,\n",
       "   0.05985828483167214,\n",
       "   0.05945470856294574,\n",
       "   0.062392224555203164,\n",
       "   0.05893061755404687,\n",
       "   0.05877160831562687,\n",
       "   0.05842803340201826,\n",
       "   0.057373611336510366,\n",
       "   0.0558286629902371,\n",
       "   0.06129739479541807,\n",
       "   0.06011825418171676,\n",
       "   0.05983448823374708,\n",
       "   0.0629216881746736,\n",
       "   0.06084061563132645,\n",
       "   0.06039937507961247,\n",
       "   0.06063750509507266,\n",
       "   0.05826296833573934,\n",
       "   0.05926136373827872,\n",
       "   0.061511863515007044,\n",
       "   0.059555139258118006,\n",
       "   0.059966161503881434,\n",
       "   0.060898859822547294,\n",
       "   0.05983052185512422,\n",
       "   0.05932227823300274,\n",
       "   0.059377724786622385,\n",
       "   0.06014671913732397,\n",
       "   0.05899267614628442,\n",
       "   0.060098667865920025,\n",
       "   0.057141367549193564,\n",
       "   0.06132745135488414,\n",
       "   0.0605917331704176,\n",
       "   0.05860354823670131,\n",
       "   0.06073106606482574,\n",
       "   0.05992528971902571,\n",
       "   0.061603928966282584,\n",
       "   0.05792485286130465,\n",
       "   0.06109727257945239,\n",
       "   0.06019810627813995,\n",
       "   0.060918449556552026,\n",
       "   0.058178523531489924,\n",
       "   0.057398052725302316,\n",
       "   0.058272245071041166,\n",
       "   0.058873119440210966,\n",
       "   0.05752294262375492,\n",
       "   0.05907465443620553,\n",
       "   0.05976769525979177,\n",
       "   0.058003406217382036,\n",
       "   0.06137539260399769,\n",
       "   0.05929716530394583],\n",
       "  'val_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'val_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0]}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델을 구성한다\n",
    "model = MAMLFewShotClassifier(args=args, device=device,\n",
    "                              im_shape=(2, 3,\n",
    "                                        args.image_height, args.image_width))\n",
    "\n",
    "data = MetaLearningSystemDataLoader\n",
    "\n",
    "maml_system = ExperimentBuilder(model=model, data=data, args=args, device=device)\n",
    "maml_system.state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd3d9cc",
   "metadata": {},
   "source": [
    "# 3. 가우시안 커널 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7d1d5c",
   "metadata": {},
   "source": [
    "## 3-1 먼저 training task에 대한 centroid를 저장한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56e5e812",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 23.99 GiB total capacity; 53.07 GiB already allocated; 0 bytes free; 53.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28364\\243023031.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     53\u001b[0m               \u001b[0mnum_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_step\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m               \u001b[0mtraining_phase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m               epoch=0)\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             names_weights_copy, prompted_weights_copy = maml_system.model.apply_inner_loop_update(\n",
      "\u001b[1;32m~\\PycharmProjects\\prompt_maml\\few_shot_learning_system.py\u001b[0m in \u001b[0;36mnet_forward_feature_extractor\u001b[1;34m(self, x, y, weights, backup_running_statistics, training, num_step, training_phase, epoch, prompted_weights, prepend_prompt)\u001b[0m\n\u001b[0;32m    452\u001b[0m         preds, feature_map_list = self.classifier.forward(x=x, params=weights, prompted_params=prompted_weights,\n\u001b[0;32m    453\u001b[0m                                         \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackup_running_statistics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbackup_running_statistics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m                                                      num_step=num_step, prepend_prompt=prepend_prompt)\n\u001b[0m\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, num_step, params, prompted_params, training, backup_running_statistics, task_embedding, prepend_prompt)\u001b[0m\n\u001b[0;32m    955\u001b[0m             out = self.layer_dict['conv{}'.format(i)](out, params=param_dict['conv{}'.format(i)], training=training,\n\u001b[0;32m    956\u001b[0m                                                       \u001b[0mbackup_running_statistics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbackup_running_statistics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 957\u001b[1;33m                                                       num_step=num_step)\n\u001b[0m\u001b[0;32m    958\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_pooling\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    959\u001b[0m                 \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\maml\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, num_step, params, training, backup_running_statistics)\u001b[0m\n\u001b[0;32m    715\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    716\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 717\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconv_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    718\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    719\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalization\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\maml\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, params)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m         out = F.conv2d(input=x, weight=weight, bias=bias, stride=self.stride,\n\u001b[1;32m--> 388\u001b[1;33m                        padding=self.padding, dilation=self.dilation_rate, groups=self.groups)\n\u001b[0m\u001b[0;32m    389\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 23.99 GiB total capacity; 53.07 GiB already allocated; 0 bytes free; 53.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "train_data = maml_system.data.get_train_batches(total_batches=int(600/1), augment_images=False)\n",
    "\n",
    "trained_vector = torch.FloatTensor().cuda()\n",
    "\n",
    "for sample_idx, train_sample in enumerate(train_data):\n",
    "    \n",
    "    x_support_set, x_target_set, y_support_set, y_target_set, seed = train_sample\n",
    "    \n",
    "    x_support_set = torch.Tensor(x_support_set).float().to(device=maml_system.model.device)\n",
    "    x_target_set = torch.Tensor(x_target_set).float().to(device=maml_system.model.device)\n",
    "    y_support_set = torch.Tensor(y_support_set).long().to(device=maml_system.model.device)\n",
    "    y_target_set = torch.Tensor(y_target_set).long().to(device=maml_system.model.device)\n",
    "    \n",
    "    for task_id, (x_support_set_task, y_support_set_task, x_target_set_task, y_target_set_task) in enumerate(zip(x_support_set,\n",
    "                              y_support_set,\n",
    "                              x_target_set,\n",
    "                              y_target_set)):\n",
    "        \n",
    "        \n",
    "        names_weights_copy = maml_system.model.get_inner_loop_parameter_dict(maml_system.model.classifier.named_parameters())\n",
    "\n",
    "        num_devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "\n",
    "        names_weights_copy = {\n",
    "                name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
    "                    [num_devices] + [1 for i in range(len(value.shape))]) for\n",
    "                name, value in names_weights_copy.items()}\n",
    "\n",
    "        prompted_weights_copy = {}\n",
    "        if args.prompter:\n",
    "            prompted_weights_copy = {key: value for key, value in names_weights_copy.items() if 'prompt' in key}\n",
    "\n",
    "        names_weights_copy = {key: value for key, value in names_weights_copy.items() if 'layer_dict' in key}\n",
    "        \n",
    "        \n",
    "        n, s, c, h, w = x_target_set_task.shape\n",
    "\n",
    "        x_support_set_task = x_support_set_task.view(-1, c, h, w)\n",
    "        y_support_set_task = y_support_set_task.view(-1)\n",
    "        x_target_set_task = x_target_set_task.view(-1, c, h, w)\n",
    "        y_target_set_task = y_target_set_task.view(-1)\n",
    "        \n",
    "        \n",
    "        for num_step in range(5):\n",
    "\n",
    "            support_loss, support_preds, _ = maml_system.model.net_forward_feature_extractor(\n",
    "              x=x_support_set_task,\n",
    "              y=y_support_set_task,\n",
    "              weights=names_weights_copy,\n",
    "              prompted_weights=prompted_weights_copy,\n",
    "              backup_running_statistics=num_step == 0,\n",
    "              training=True,\n",
    "              num_step=num_step,\n",
    "              training_phase=False,\n",
    "              epoch=0)\n",
    "            \n",
    "            names_weights_copy, prompted_weights_copy = maml_system.model.apply_inner_loop_update(\n",
    "             loss=support_loss,\n",
    "             names_weights_copy=names_weights_copy,\n",
    "             prompted_weights_copy=prompted_weights_copy,\n",
    "             use_second_order=True,\n",
    "             current_step_idx=num_step,\n",
    "             current_iter='test',\n",
    "             training_phase=False)\n",
    "            \n",
    "            if num_step == 4:\n",
    "                                \n",
    "                target_loss, target_preds, feature_map_list = maml_system.model.net_forward_feature_extractor(\n",
    "                    x=x_target_set_task,\n",
    "                    y=y_target_set_task,\n",
    "                    weights=names_weights_copy,\n",
    "                    prompted_weights=prompted_weights_copy,\n",
    "                    backup_running_statistics=False, training=True,\n",
    "                    num_step=num_step,\n",
    "                    training_phase=False,\n",
    "                    epoch=0)\n",
    "                \n",
    "                task_centroid  = feature_map_list[3].mean(dim=0)  # shape: (64,)\n",
    "                trained_vector = torch.cat((trained_vector, task_centroid), dim=0)\n",
    "                # task의 평균을 구해야함\n",
    "                \n",
    "trained_vector = torch.nn.functional.normalize(trained_vector.detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7194ab45",
   "metadata": {},
   "source": [
    "## 3-2 test tasks에 대한 centroid를 저장한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae03b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = maml_system.data.get_test_batches(total_batches=int(600/1), augment_images=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
