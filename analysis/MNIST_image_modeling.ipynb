{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCl-YbqPMcSw"
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Note:** Interested in JAX? Check out our [JAX+Flax version](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial11/NF_image_modeling.html) of this tutorial!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 414,
     "status": "ok",
     "timestamp": 1732455752078,
     "user": {
      "displayName": "임종민",
      "userId": "13437322968883113888"
     },
     "user_tz": -540
    },
    "id": "JZg0EjI_McSz",
    "outputId": "662892cd-4657-4a2a-fb72-3cae2569d72a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tabulate in c:\\users\\jm\\anaconda3\\envs\\metal\\lib\\site-packages (0.9.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing requirements for tensorflow-estimator: [Errno 2] No such file or directory: 'c:\\\\users\\\\jm\\\\appdata\\\\roaming\\\\python\\\\python37\\\\site-packages\\\\tensorflow_estimator-2.3.0.dist-info\\\\METADATA'\n",
      "C:\\Users\\JM\\anaconda3\\envs\\metal\\lib\\site-packages\\ipykernel_launcher.py:13: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  del sys.path[0]\n",
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n"
     ]
    }
   ],
   "source": [
    "!pip install tabulate\n",
    "\n",
    "## Standard libraries\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "\n",
    "## Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "# Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "# PyTorch Lightning\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
    "    !pip install --quiet pytorch-lightning>=1.4\n",
    "    import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. MNIST)\n",
    "DATASET_PATH = \"../data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"../saved_models/tutorial11\"\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Fetching the device that will be used throughout this notebook\n",
    "device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\n",
    "print(\"Using device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 967,
     "status": "ok",
     "timestamp": 1732455753340,
     "user": {
      "displayName": "임종민",
      "userId": "13437322968883113888"
     },
     "user_tz": -540
    },
    "id": "g3ICW1ySMcS2",
    "outputId": "27a29cc4-b683-4265-d3cd-efc756c6bd09"
   },
   "outputs": [],
   "source": [
    "# import urllib.request\n",
    "# from urllib.error import HTTPError\n",
    "# # Github URL where saved models are stored for this tutorial\n",
    "# base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial11/\"\n",
    "# # Files to download\n",
    "# pretrained_files = [\"MNISTFlow_simple.ckpt\", \"MNISTFlow_vardeq.ckpt\", \"MNISTFlow_multiscale.ckpt\"]\n",
    "# # Create checkpoint path if it doesn't exist yet\n",
    "# os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "# # For each file, check whether it already exists. If not, try downloading it.\n",
    "# for file_name in pretrained_files:\n",
    "#     file_path = os.path.join(CHECKPOINT_PATH, file_name)\n",
    "#     if not os.path.isfile(file_path):\n",
    "#         file_url = base_url + file_name\n",
    "#         print(f\"Downloading {file_url}...\")\n",
    "#         try:\n",
    "#             urllib.request.urlretrieve(file_url, file_path)\n",
    "#         except HTTPError as e:\n",
    "#             print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HEtc5MWTMcS3"
   },
   "source": [
    "Loading the MNIST dataset using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1732455753869,
     "user": {
      "displayName": "임종민",
      "userId": "13437322968883113888"
     },
     "user_tz": -540
    },
    "id": "8KkQkPh5McS3",
    "outputId": "c17b9643-1039-4fae-f316-3abf0811ae61"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\anaconda3\\envs\\metal\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:205.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n",
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# Convert images from 0-1 to 0-255 (integers)\n",
    "def discretize(sample):\n",
    "    return (sample * 255).to(torch.int32)\n",
    "\n",
    "# Transformations applied on each image => make them a tensor and discretize\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                discretize])\n",
    "\n",
    "# Loading the training dataset. We need to split it into a training and validation part\n",
    "train_dataset = MNIST(root=DATASET_PATH, train=True, transform=transform, download=True)\n",
    "pl.seed_everything(42)\n",
    "train_set, val_set = torch.utils.data.random_split(train_dataset, [50000, 10000])\n",
    "\n",
    "# Loading the test set\n",
    "test_set = MNIST(root=DATASET_PATH, train=False, transform=transform, download=True)\n",
    "\n",
    "# We define a set of data loaders that we can use for various purposes later.\n",
    "# Note that for actually training a model, we will use different data loaders\n",
    "# with a lower batch size.\n",
    "train_loader = data.DataLoader(train_set, batch_size=256, shuffle=False, drop_last=False)\n",
    "val_loader = data.DataLoader(val_set, batch_size=64, shuffle=False, drop_last=False, num_workers=4)\n",
    "test_loader = data.DataLoader(test_set, batch_size=64, shuffle=False, drop_last=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Sa8esKcMcS4"
   },
   "source": [
    "Some visualizations from the mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1732455753869,
     "user": {
      "displayName": "임종민",
      "userId": "13437322968883113888"
     },
     "user_tz": -540
    },
    "id": "1cj6kXRVMcS4",
    "outputId": "cf5f495c-a65e-461e-a87c-74c7d7c68d46"
   },
   "outputs": [
    {
     "data": {
      "application/pdf": "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\n",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"341.674839pt\" height=\"180.72pt\" viewBox=\"0 0 341.674839 180.72\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2024-11-24T23:20:09.694534</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.5.3, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 180.72 \n",
       "L 341.674839 180.72 \n",
       "L 341.674839 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g clip-path=\"url(#p91b04fd010)\">\n",
       "    <image xlink:href=\"data:image/png;base64,\n",
       "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\" id=\"image08e37b49ed\" transform=\"scale(1 -1)translate(0 -166.32)\" x=\"7.2\" y=\"-7.2\" width=\"327.6\" height=\"166.32\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p91b04fd010\">\n",
       "   <rect x=\"7.2\" y=\"7.2\" width=\"327.274839\" height=\"166.32\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 600x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_imgs(imgs, title=None, row_size=4):\n",
    "    # Form a grid of pictures (we use max. 8 columns)\n",
    "    num_imgs = imgs.shape[0] if isinstance(imgs, torch.Tensor) else len(imgs)\n",
    "    is_int = imgs.dtype==torch.int32 if isinstance(imgs, torch.Tensor) else imgs[0].dtype==torch.int32\n",
    "    nrow = min(num_imgs, row_size)\n",
    "    ncol = int(math.ceil(num_imgs/nrow))\n",
    "    imgs = torchvision.utils.make_grid(imgs, nrow=nrow, pad_value=128 if is_int else 0.5)\n",
    "    np_imgs = imgs.cpu().numpy()\n",
    "    # Plot the grid\n",
    "    plt.figure(figsize=(1.5*nrow, 1.5*ncol))\n",
    "    plt.imshow(np.transpose(np_imgs, (1,2,0)), interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "show_imgs([train_set[i][0] for i in range(8)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2sBHVlGQP6Pk"
   },
   "source": [
    "This is a class for a Normalizing Flow model applied to image data. The model maps a given input image to a latent space through a sequence of continuous transformations (flows) and performs both training and sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1732455753869,
     "user": {
      "displayName": "임종민",
      "userId": "13437322968883113888"
     },
     "user_tz": -540
    },
    "id": "jmdHM13gMcS6"
   },
   "outputs": [],
   "source": [
    "class ImageFlow(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, flows, import_samples=8):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            flows - A list of flows (each a nn.Module) that should be applied on the images.\n",
    "            import_samples - Number of importance samples to use during testing (see explanation below). Can be changed at any time\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.flows = nn.ModuleList(flows)\n",
    "        self.import_samples = import_samples\n",
    "        # Create prior distribution for final latent space\n",
    "        self.prior = torch.distributions.normal.Normal(loc=0.0, scale=1.0)\n",
    "        # Example input for visualizing the graph\n",
    "        self.example_input_array = train_set[0][0].unsqueeze(dim=0)\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        # The forward function is only used for visualizing the graph\n",
    "        return self._get_likelihood(imgs)\n",
    "\n",
    "    def encode(self, imgs):\n",
    "        # Given a batch of images, return the latent representation z and ldj of the transformations\n",
    "        z, ldj = imgs, torch.zeros(imgs.shape[0], device=self.device)\n",
    "        for flow in self.flows:\n",
    "            z, ldj = flow(z, ldj, reverse=False)\n",
    "        return z, ldj\n",
    "\n",
    "    def _get_likelihood(self, imgs, return_ll=False):\n",
    "        \"\"\"\n",
    "        Given a batch of images, return the likelihood of those.\n",
    "        If return_ll is True, this function returns the log likelihood of the input.\n",
    "        Otherwise, the ouptut metric is bits per dimension (scaled negative log likelihood)\n",
    "        \"\"\"\n",
    "        z, ldj = self.encode(imgs)\n",
    "        log_pz = self.prior.log_prob(z).sum(dim=[1,2,3])\n",
    "        log_px = ldj + log_pz\n",
    "        nll = -log_px\n",
    "        # Calculating bits per dimension\n",
    "        bpd = nll * np.log2(np.exp(1)) / np.prod(imgs.shape[1:])\n",
    "        return bpd.mean() if not return_ll else log_px\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, img_shape, z_init=None):\n",
    "        \"\"\"\n",
    "        Sample a batch of images from the flow.\n",
    "        \"\"\"\n",
    "        # Sample latent representation from prior\n",
    "        if z_init is None:\n",
    "            z = self.prior.sample(sample_shape=img_shape).to(device)\n",
    "        else:\n",
    "            z = z_init.to(device)\n",
    "\n",
    "        # Transform z to x by inverting the flows\n",
    "        ldj = torch.zeros(img_shape[0], device=device)\n",
    "        for flow in reversed(self.flows):\n",
    "            z, ldj = flow(z, ldj, reverse=True)\n",
    "        return z\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        # An scheduler is optional, but can help in flows to get the last bpd improvement\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.99)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Normalizing flows are trained by maximum likelihood => return bpd\n",
    "        loss = self._get_likelihood(batch[0])\n",
    "        self.log('train_bpd', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._get_likelihood(batch[0])\n",
    "        self.log('val_bpd', loss)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Perform importance sampling during testing => estimate likelihood M times for each image\n",
    "        samples = []\n",
    "        for _ in range(self.import_samples):\n",
    "            img_ll = self._get_likelihood(batch[0], return_ll=True)\n",
    "            samples.append(img_ll)\n",
    "        img_ll = torch.stack(samples, dim=-1)\n",
    "\n",
    "        # To average the probabilities, we need to go from log-space to exp, and back to log.\n",
    "        # Logsumexp provides us a stable implementation for this\n",
    "        img_ll = torch.logsumexp(img_ll, dim=-1) - np.log(self.import_samples)\n",
    "\n",
    "        # Calculate final bpd\n",
    "        bpd = -img_ll * np.log2(np.exp(1)) / np.prod(batch[0].shape[1:])\n",
    "        bpd = bpd.mean()\n",
    "\n",
    "        self.log('test_bpd', bpd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdgWriz_QVet"
   },
   "source": [
    "This code implements a Dequantization module, commonly used in Normalizing Flows. Dequantization is a process for converting discrete data, such as images, into continuous data. Pixel values in images are typically represented as discrete values ranging from 0 to 255 (8-bit), and this module transforms them to be handled within continuous probability distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1732455753869,
     "user": {
      "displayName": "임종민",
      "userId": "13437322968883113888"
     },
     "user_tz": -540
    },
    "id": "XuswLNwUMcS7"
   },
   "outputs": [],
   "source": [
    "class Dequantization(nn.Module):\n",
    "\n",
    "    def __init__(self, alpha=1e-5, quants=256):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            alpha - small constant that is used to scale the original input.\n",
    "                    Prevents dealing with values very close to 0 and 1 when inverting the sigmoid\n",
    "            quants - Number of possible discrete values (usually 256 for 8-bit image)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.quants = quants\n",
    "\n",
    "    def forward(self, z, ldj, reverse=False):\n",
    "        if not reverse:\n",
    "            z, ldj = self.dequant(z, ldj)\n",
    "            z, ldj = self.sigmoid(z, ldj, reverse=True)\n",
    "        else:\n",
    "            z, ldj = self.sigmoid(z, ldj, reverse=False)\n",
    "            z = z * self.quants\n",
    "            ldj += np.log(self.quants) * np.prod(z.shape[1:])\n",
    "            z = torch.floor(z).clamp(min=0, max=self.quants-1).to(torch.int32)\n",
    "        return z, ldj\n",
    "\n",
    "    def sigmoid(self, z, ldj, reverse=False):\n",
    "        # Applies an invertible sigmoid transformation\n",
    "        if not reverse:\n",
    "            ldj += (-z-2*F.softplus(-z)).sum(dim=[1,2,3])\n",
    "            z = torch.sigmoid(z)\n",
    "            # Reversing scaling for numerical stability\n",
    "            ldj -= np.log(1 - self.alpha) * np.prod(z.shape[1:])\n",
    "            z = (z - 0.5 * self.alpha) / (1 - self.alpha)\n",
    "        else:\n",
    "            z = z * (1 - self.alpha) + 0.5 * self.alpha  # Scale to prevent boundaries 0 and 1\n",
    "            ldj += np.log(1 - self.alpha) * np.prod(z.shape[1:])\n",
    "            ldj += (-torch.log(z) - torch.log(1-z)).sum(dim=[1,2,3])\n",
    "            z = torch.log(z) - torch.log(1-z)\n",
    "        return z, ldj\n",
    "\n",
    "    def dequant(self, z, ldj):\n",
    "        # Transform discrete values to continuous volumes\n",
    "        z = z.to(torch.float32)\n",
    "        z = z + torch.rand_like(z).detach()\n",
    "        z = z / self.quants\n",
    "        ldj -= np.log(self.quants) * np.prod(z.shape[1:])\n",
    "        return z, ldj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-YIX7AuMcS8"
   },
   "source": [
    "A good check whether a flow is correctly implemented or not, is to verify that it is invertible. Hence, we will dequantize a randomly chosen training image, and then quantize it again. We would expect that we would get the exact same image out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1732455753869,
     "user": {
      "displayName": "임종민",
      "userId": "13437322968883113888"
     },
     "user_tz": -540
    },
    "id": "7UH3bm3pMcS8",
    "outputId": "3f426d83-5f3b-4ea4-d80b-98b30060c58e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully inverted dequantization\n"
     ]
    }
   ],
   "source": [
    "## Testing invertibility of dequantization layer\n",
    "pl.seed_everything(42)\n",
    "orig_img = train_set[0][0].unsqueeze(dim=0)\n",
    "ldj = torch.zeros(1,)\n",
    "dequant_module = Dequantization()\n",
    "deq_img, ldj = dequant_module(orig_img, ldj, reverse=False)\n",
    "reconst_img, ldj = dequant_module(deq_img, ldj, reverse=True)\n",
    "\n",
    "d1, d2 = torch.where(orig_img.squeeze() != reconst_img.squeeze())\n",
    "if len(d1) != 0:\n",
    "    print(\"Dequantization was not invertible.\")\n",
    "    for i in range(d1.shape[0]):\n",
    "        print(\"Original value:\", orig_img[0,0,d1[i], d2[i]].item())\n",
    "        print(\"Reconstructed value:\", reconst_img[0,0,d1[i], d2[i]].item())\n",
    "else:\n",
    "    print(\"Successfully inverted dequantization\")\n",
    "\n",
    "# Layer is not strictly invertible due to float precision constraints\n",
    "# assert (orig_img == reconst_img).all().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYPQUPEcMcS9"
   },
   "source": [
    "The test passes as expected, though failures may occur due to numerical inaccuracies in the sigmoid inversion. While the input to the inverse sigmoid is scaled between 0 and 1, its output spans $-\\infty$ to $\\infty$, and repeated logarithmic operations with 32-bit floats can cause inaccuracies. These inaccuracies are not critical but should be noted; using double precision (float64) can improve stability. The dequantization process transforms discrete values, like image pixel intensities, into continuous probability distributions. Finally, the resulting distribution from dequantization can be visualized to better understand the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1732455753869,
     "user": {
      "displayName": "임종민",
      "userId": "13437322968883113888"
     },
     "user_tz": -540
    },
    "id": "H0d8VoZ4McS_"
   },
   "outputs": [],
   "source": [
    "class VariationalDequantization(Dequantization):\n",
    "\n",
    "    def __init__(self, var_flows, alpha=1e-5):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            var_flows - A list of flow transformations to use for modeling q(u|x)\n",
    "            alpha - Small constant, see Dequantization for details\n",
    "        \"\"\"\n",
    "        super().__init__(alpha=alpha)\n",
    "        self.flows = nn.ModuleList(var_flows)\n",
    "\n",
    "    def dequant(self, z, ldj):\n",
    "        z = z.to(torch.float32)\n",
    "        img = (z / 255.0) * 2 - 1 # We condition the flows on x, i.e. the original image\n",
    "\n",
    "        # Prior of u is a uniform distribution as before\n",
    "        # As most flow transformations are defined on [-infinity,+infinity], we apply an inverse sigmoid first.\n",
    "        deq_noise = torch.rand_like(z).detach()\n",
    "        deq_noise, ldj = self.sigmoid(deq_noise, ldj, reverse=True)\n",
    "        for flow in self.flows:\n",
    "            deq_noise, ldj = flow(deq_noise, ldj, reverse=False, orig_img=img)\n",
    "        deq_noise, ldj = self.sigmoid(deq_noise, ldj, reverse=False)\n",
    "\n",
    "        # After the flows, apply u as in standard dequantization\n",
    "        z = (z + deq_noise) / 256.0\n",
    "        ldj -= np.log(256.0) * np.prod(z.shape[1:])\n",
    "        return z, ldj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5dA82x9RMcS_"
   },
   "source": [
    "### Coupling layers\n",
    "<center width=\"100%\" style=\"padding: 10px\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial11/coupling_flow.svg?raw=1\" width=\"450px\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1732455753869,
     "user": {
      "displayName": "임종민",
      "userId": "13437322968883113888"
     },
     "user_tz": -540
    },
    "id": "8q3FQlJ3McTA"
   },
   "outputs": [],
   "source": [
    "class CouplingLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, network, mask, c_in):\n",
    "        \"\"\"\n",
    "        Coupling layer inside a normalizing flow.\n",
    "        Inputs:\n",
    "            network - A PyTorch nn.Module constituting the deep neural network for mu and sigma.\n",
    "                      Output shape should be twice the channel size as the input.\n",
    "            mask - Binary mask (0 or 1) where 0 denotes that the element should be transformed,\n",
    "                   while 1 means the latent will be used as input to the NN.\n",
    "            c_in - Number of input channels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.network = network\n",
    "        self.scaling_factor = nn.Parameter(torch.zeros(c_in))\n",
    "        # Register mask as buffer as it is a tensor which is not a parameter,\n",
    "        # but should be part of the modules state.\n",
    "        self.register_buffer('mask', mask)\n",
    "\n",
    "    def forward(self, z, ldj, reverse=False, orig_img=None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            z - Latent input to the flow\n",
    "            ldj - The current ldj of the previous flows.\n",
    "                  The ldj of this layer will be added to this tensor.\n",
    "            reverse - If True, we apply the inverse of the layer.\n",
    "            orig_img (optional) - Only needed in VarDeq. Allows external\n",
    "                                  input to condition the flow on (e.g. original image)\n",
    "        \"\"\"\n",
    "        # Apply network to masked input\n",
    "        z_in = z * self.mask\n",
    "        if orig_img is None:\n",
    "            nn_out = self.network(z_in)\n",
    "        else:\n",
    "            nn_out = self.network(torch.cat([z_in, orig_img], dim=1))\n",
    "        s, t = nn_out.chunk(2, dim=1)\n",
    "\n",
    "        # Stabilize scaling output\n",
    "        s_fac = self.scaling_factor.exp().view(1, -1, 1, 1)\n",
    "        s = torch.tanh(s / s_fac) * s_fac\n",
    "\n",
    "        # Mask outputs (only transform the second part)\n",
    "        s = s * (1 - self.mask)\n",
    "        t = t * (1 - self.mask)\n",
    "\n",
    "        # Affine transformation\n",
    "        if not reverse:\n",
    "            # Whether we first shift and then scale, or the other way round,\n",
    "            # is a design choice, and usually does not have a big impact\n",
    "            z = (z + t) * torch.exp(s)\n",
    "            ldj += s.sum(dim=[1,2,3])\n",
    "        else:\n",
    "            z = (z * torch.exp(-s)) - t\n",
    "            ldj -= s.sum(dim=[1,2,3])\n",
    "\n",
    "        return z, ldj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1732455753869,
     "user": {
      "displayName": "임종민",
      "userId": "13437322968883113888"
     },
     "user_tz": -540
    },
    "id": "R3sftPFMMcTB"
   },
   "outputs": [],
   "source": [
    "def create_checkerboard_mask(h, w, invert=False):\n",
    "    x, y = torch.arange(h, dtype=torch.int32), torch.arange(w, dtype=torch.int32)\n",
    "    xx, yy = torch.meshgrid(x, y, indexing='ij')\n",
    "    mask = torch.fmod(xx + yy, 2)\n",
    "    mask = mask.to(torch.float32).view(1, 1, h, w)\n",
    "    if invert:\n",
    "        mask = 1 - mask\n",
    "    return mask\n",
    "\n",
    "def create_channel_mask(c_in, invert=False):\n",
    "    mask = torch.cat([torch.ones(c_in//2, dtype=torch.float32),\n",
    "                      torch.zeros(c_in-c_in//2, dtype=torch.float32)])\n",
    "    mask = mask.view(1, c_in, 1, 1)\n",
    "    if invert:\n",
    "        mask = 1 - mask\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SliWLgaYMcTC"
   },
   "source": [
    "We can also visualize the corresponding masks for an image of size $8\\times 8\\times 2$ (2 channels):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fr1_5yyhMcTC"
   },
   "source": [
    "이 코드는 Gated Convolutional Neural Network를 정의한 PyTorch 모듈로, 주로 Normalizing Flows와 같은 모델에서 조건부 네트워크나 Coupling Layer의 비선형 변환을 구현하는 데 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 298,
     "status": "ok",
     "timestamp": 1732455754163,
     "user": {
      "displayName": "임종민",
      "userId": "13437322968883113888"
     },
     "user_tz": -540
    },
    "id": "1E0D8cotMcTD"
   },
   "outputs": [],
   "source": [
    "class ConcatELU(nn.Module):\n",
    "    \"\"\"\n",
    "    Activation function that applies ELU in both direction (inverted and plain).\n",
    "    Allows non-linearity while providing strong gradients for any input (important for final convolution)\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([F.elu(x), F.elu(-x)], dim=1)\n",
    "\n",
    "\n",
    "class LayerNormChannels(nn.Module):\n",
    "\n",
    "    def __init__(self, c_in, eps=1e-5):\n",
    "        \"\"\"\n",
    "        This module applies layer norm across channels in an image.\n",
    "        Inputs:\n",
    "            c_in - Number of channels of the input\n",
    "            eps - Small constant to stabilize std\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(1, c_in, 1, 1))\n",
    "        self.beta = nn.Parameter(torch.zeros(1, c_in, 1, 1))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=1, keepdim=True)\n",
    "        var = x.var(dim=1, unbiased=False, keepdim=True)\n",
    "        y = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        y = y * self.gamma + self.beta\n",
    "        return y\n",
    "\n",
    "\n",
    "class GatedConv(nn.Module):\n",
    "\n",
    "    def __init__(self, c_in, c_hidden):\n",
    "        \"\"\"\n",
    "        This module applies a two-layer convolutional ResNet block with input gate\n",
    "        Inputs:\n",
    "            c_in - Number of channels of the input\n",
    "            c_hidden - Number of hidden dimensions we want to model (usually similar to c_in)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            ConcatELU(),\n",
    "            nn.Conv2d(2*c_in, c_hidden, kernel_size=3, padding=1),\n",
    "            ConcatELU(),\n",
    "            nn.Conv2d(2*c_hidden, 2*c_in, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        val, gate = out.chunk(2, dim=1)\n",
    "        return x + val * torch.sigmoid(gate)\n",
    "\n",
    "\n",
    "class GatedConvNet(nn.Module):\n",
    "\n",
    "    def __init__(self, c_in, c_hidden=32, c_out=-1, num_layers=3):\n",
    "        \"\"\"\n",
    "        Module that summarizes the previous blocks to a full convolutional neural network.\n",
    "        Inputs:\n",
    "            c_in - Number of input channels\n",
    "            c_hidden - Number of hidden dimensions to use within the network\n",
    "            c_out - Number of output channels. If -1, 2 times the input channels are used (affine coupling)\n",
    "            num_layers - Number of gated ResNet blocks to apply\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_out = c_out if c_out > 0 else 2 * c_in\n",
    "        layers = []\n",
    "        layers += [nn.Conv2d(c_in, c_hidden, kernel_size=3, padding=1)]\n",
    "        for layer_index in range(num_layers):\n",
    "            layers += [GatedConv(c_hidden, c_hidden),\n",
    "                       LayerNormChannels(c_hidden)]\n",
    "        layers += [ConcatELU(),\n",
    "                   nn.Conv2d(2*c_hidden, c_out, kernel_size=3, padding=1)]\n",
    "        self.nn = nn.Sequential(*layers)\n",
    "\n",
    "        self.nn[-1].weight.data.zero_()\n",
    "        self.nn[-1].bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.nn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_WGZH6uMcTD"
   },
   "source": [
    "### Training loop\n",
    "\n",
    "이 코드는 PyTorch 기반의 간단한 Normalizing Flow 모델을 생성하는 함수입니다. 주어진 설정에 따라 Variational Dequantization 또는 단순 Dequantization을 적용하며, 이후 여러 Coupling Layer를 추가하여 이미지 데이터를 모델링할 수 있는 Flow 모델을 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1732455754163,
     "user": {
      "displayName": "임종민",
      "userId": "13437322968883113888"
     },
     "user_tz": -540
    },
    "id": "Dme6Zrp1McTD",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_simple_flow(use_vardeq=True):\n",
    "    flow_layers = []\n",
    "    if use_vardeq:\n",
    "        vardeq_layers = [CouplingLayer(network=GatedConvNet(c_in=2, c_out=2, c_hidden=16),\n",
    "                                       mask=create_checkerboard_mask(h=28, w=28, invert=(i%2==1)),\n",
    "                                       c_in=1) for i in range(4)]\n",
    "        flow_layers += [VariationalDequantization(var_flows=vardeq_layers)]\n",
    "    else:\n",
    "        flow_layers += [Dequantization()]\n",
    "\n",
    "    for i in range(8):\n",
    "        flow_layers += [CouplingLayer(network=GatedConvNet(c_in=1, c_hidden=32),\n",
    "                                      mask=create_checkerboard_mask(h=28, w=28, invert=(i%2==1)),\n",
    "                                      c_in=1)]\n",
    "\n",
    "    flow_model = ImageFlow(flow_layers).to(device)\n",
    "    return flow_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1732455754163,
     "user": {
      "displayName": "임종민",
      "userId": "13437322968883113888"
     },
     "user_tz": -540
    },
    "id": "k5vsIV1PMcTL"
   },
   "outputs": [],
   "source": [
    "def train_flow(flow, model_name=\"MNISTFlow\"):\n",
    "    # Create a PyTorch Lightning trainer\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, model_name),\n",
    "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
    "                         devices=1,\n",
    "                         max_epochs=200, # max_epochs=200\n",
    "                         gradient_clip_val=1.0,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"min\", monitor=\"val_bpd\"),\n",
    "                                    LearningRateMonitor(\"epoch\")],\n",
    "                         check_val_every_n_epoch=5)\n",
    "    trainer.logger._log_graph = True\n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "    train_data_loader = data.DataLoader(train_set, batch_size=128, shuffle=True, drop_last=True, pin_memory=True, num_workers=8)\n",
    "    result = None\n",
    "    \n",
    "    print(\"Start training\", model_name)\n",
    "    trainer.fit(flow, train_data_loader, val_loader)\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "#     pretrained_filename = os.path.join(CHECKPOINT_PATH, model_name + \".ckpt\")\n",
    "#     if os.path.isfile(pretrained_filename):\n",
    "#         print(\"Found pretrained model, loading...\")\n",
    "#         ckpt = torch.load(pretrained_filename, map_location=device)\n",
    "#         flow.load_state_dict(ckpt['state_dict'])\n",
    "#         result = ckpt.get(\"result\", None)\n",
    "#     else:\n",
    "    #print(\"Start training\", model_name)\n",
    "      \n",
    "\n",
    "    # Test best model on validation and test set if no result has been found\n",
    "    # Testing can be expensive due to the importance sampling.\n",
    "    if result is None:\n",
    "        val_result = trainer.test(flow, val_loader, verbose=False)\n",
    "        start_time = time.time()\n",
    "        test_result = trainer.test(flow, test_loader, verbose=False)\n",
    "        duration = time.time() - start_time\n",
    "        result = {\"test\": test_result, \"val\": val_result, \"time\": duration / len(test_loader) / flow.import_samples}\n",
    "\n",
    "    return flow, result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DOZ7GojMcTL"
   },
   "source": [
    "## Multi-scale architecture\n",
    "\n",
    "One disadvantage of normalizing flows is that they operate on the exact same dimensions as the input. If the input is high-dimensional, so is the latent space, which requires larger computational cost to learn suitable transformations. However, particularly in the image domain, many pixels contain less information in the sense that we could remove them without loosing the semantical information of the image.\n",
    "\n",
    "Based on this intuition, deep normalizing flows on images commonly apply a multi-scale architecture [1]. After the first $N$ flow transformations, we split off half of the latent dimensions and directly evaluate them on the prior. The other half is run through $N$ more flow transformations, and depending on the size of the input, we split it again in half or stop overall at this position. The two operations involved in this setup is `Squeeze` and `Split` which we will review more closely and implement below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1732455754163,
     "user": {
      "displayName": "임종민",
      "userId": "13437322968883113888"
     },
     "user_tz": -540
    },
    "id": "mfu3zebFMcTM"
   },
   "outputs": [],
   "source": [
    "class SqueezeFlow(nn.Module):\n",
    "\n",
    "    def forward(self, z, ldj, reverse=False):\n",
    "        B, C, H, W = z.shape\n",
    "        if not reverse:\n",
    "            # Forward direction: H x W x C => H/2 x W/2 x 4C\n",
    "            z = z.reshape(B, C, H//2, 2, W//2, 2)\n",
    "            z = z.permute(0, 1, 3, 5, 2, 4)\n",
    "            z = z.reshape(B, 4*C, H//2, W//2)\n",
    "        else:\n",
    "            # Reverse direction: H/2 x W/2 x 4C => H x W x C\n",
    "            z = z.reshape(B, C//4, 2, 2, H, W)\n",
    "            z = z.permute(0, 1, 4, 2, 5, 3)\n",
    "            z = z.reshape(B, C//4, H*2, W*2)\n",
    "        return z, ldj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_MI54EqMcTM"
   },
   "source": [
    "Before moving on, we can verify our implementation by comparing our output with the example figure above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1732455754163,
     "user": {
      "displayName": "임종민",
      "userId": "13437322968883113888"
     },
     "user_tz": -540
    },
    "id": "jnJkg2DgMcTM",
    "outputId": "ca63cbaf-6fb5-49af-bc66-249eec70531d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image (before)\n",
      " tensor([[[[ 1,  2,  3,  4],\n",
      "          [ 5,  6,  7,  8],\n",
      "          [ 9, 10, 11, 12],\n",
      "          [13, 14, 15, 16]]]])\n",
      "\n",
      "Image (forward)\n",
      " tensor([[[[ 1,  2,  5,  6],\n",
      "          [ 3,  4,  7,  8]],\n",
      "\n",
      "         [[ 9, 10, 13, 14],\n",
      "          [11, 12, 15, 16]]]])\n",
      "\n",
      "Image (reverse)\n",
      " tensor([[[[ 1,  2,  3,  4],\n",
      "          [ 5,  6,  7,  8],\n",
      "          [ 9, 10, 11, 12],\n",
      "          [13, 14, 15, 16]]]])\n"
     ]
    }
   ],
   "source": [
    "sq_flow = SqueezeFlow()\n",
    "rand_img = torch.arange(1,17).view(1, 1, 4, 4)\n",
    "print(\"Image (before)\\n\", rand_img)\n",
    "forward_img, _ = sq_flow(rand_img, ldj=None, reverse=False)\n",
    "print(\"\\nImage (forward)\\n\", forward_img.permute(0,2,3,1)) # Permute for readability\n",
    "reconst_img, _ = sq_flow(forward_img, ldj=None, reverse=True)\n",
    "print(\"\\nImage (reverse)\\n\", reconst_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zuwqHM7CMcTM"
   },
   "source": [
    "The split operation divides the input into two parts, and evaluates one part directly on the prior. So that our flow operation fits to the implementation of the previous layers, we will return the prior probability of the first part as the log determinant jacobian of the layer. It has the same effect as if we would combine all variable splits at the end of the flow, and evaluate them together on the prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1732455754163,
     "user": {
      "displayName": "임종민",
      "userId": "13437322968883113888"
     },
     "user_tz": -540
    },
    "id": "8Nrqg2QbMcTN"
   },
   "outputs": [],
   "source": [
    "class SplitFlow(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.prior = torch.distributions.normal.Normal(loc=0.0, scale=1.0)\n",
    "\n",
    "    def forward(self, z, ldj, reverse=False):\n",
    "        if not reverse:\n",
    "            z, z_split = z.chunk(2, dim=1)\n",
    "            ldj += self.prior.log_prob(z_split).sum(dim=[1,2,3])\n",
    "        else:\n",
    "            z_split = self.prior.sample(sample_shape=z.shape).to(device)\n",
    "            z = torch.cat([z, z_split], dim=1)\n",
    "            ldj -= self.prior.log_prob(z_split).sum(dim=[1,2,3])\n",
    "        return z, ldj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGeca8MWMcTN"
   },
   "source": [
    "### Building a multi-scale flow\n",
    "<center width=\"100%\" style=\"padding: 20px\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial11/multiscale_flow.svg?raw=1\" width=\"1100px\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1732455754163,
     "user": {
      "displayName": "임종민",
      "userId": "13437322968883113888"
     },
     "user_tz": -540
    },
    "id": "uBHh0KeLMcTN"
   },
   "outputs": [],
   "source": [
    "def create_multiscale_flow():\n",
    "    flow_layers = []\n",
    "\n",
    "    vardeq_layers = [CouplingLayer(network=GatedConvNet(c_in=2, c_out=2, c_hidden=16),\n",
    "                                   mask=create_checkerboard_mask(h=28, w=28, invert=(i%2==1)),\n",
    "                                   c_in=1) for i in range(4)]\n",
    "    flow_layers += [VariationalDequantization(vardeq_layers)]\n",
    "\n",
    "    flow_layers += [CouplingLayer(network=GatedConvNet(c_in=1, c_hidden=32),\n",
    "                                  mask=create_checkerboard_mask(h=28, w=28, invert=(i%2==1)),\n",
    "                                  c_in=1) for i in range(2)]\n",
    "    flow_layers += [SqueezeFlow()]\n",
    "    for i in range(2):\n",
    "        flow_layers += [CouplingLayer(network=GatedConvNet(c_in=4, c_hidden=48),\n",
    "                                      mask=create_channel_mask(c_in=4, invert=(i%2==1)),\n",
    "                                      c_in=4)]\n",
    "    flow_layers += [SplitFlow(),\n",
    "                    SqueezeFlow()]\n",
    "    for i in range(4):\n",
    "        flow_layers += [CouplingLayer(network=GatedConvNet(c_in=8, c_hidden=64),\n",
    "                                      mask=create_channel_mask(c_in=8, invert=(i%2==1)),\n",
    "                                      c_in=8)]\n",
    "\n",
    "    flow_model = ImageFlow(flow_layers).to(device)\n",
    "    return flow_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UzjLJXP9McTN"
   },
   "source": [
    "We can show the difference in number of parameters below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1732455754164,
     "user": {
      "displayName": "임종민",
      "userId": "13437322968883113888"
     },
     "user_tz": -540
    },
    "id": "HwDiIsixMcTO",
    "outputId": "315d78c5-32d3-4929-eb54-818b1fb6d5b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 556,312\n",
      "Number of parameters: 628,388\n",
      "Number of parameters: 1,711,818\n"
     ]
    }
   ],
   "source": [
    "def print_num_params(model):\n",
    "    num_params = sum([np.prod(p.shape) for p in model.parameters()])\n",
    "    print(\"Number of parameters: {:,}\".format(num_params))\n",
    "\n",
    "print_num_params(create_simple_flow(use_vardeq=False))\n",
    "print_num_params(create_simple_flow(use_vardeq=True))\n",
    "print_num_params(create_multiscale_flow())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8uMFNBtpMcTO"
   },
   "source": [
    "Although the multi-scale flow has almost 3 times the parameters of the single scale flow, it is not necessarily more computationally expensive than its counterpart. We will compare the runtime in the following experiments as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2c_39LwIMcTO"
   },
   "source": [
    "## Analysing the flows\n",
    "\n",
    "In the last part of the notebook, we will train all the models we have implemented above, and try to analyze the effect of the multi-scale architecture and variational dequantization.\n",
    "\n",
    "### Training flow variants\n",
    "\n",
    "Before we can analyse the flow models, we need to train them first. We provide pre-trained models that contain the validation and test performance, and run-time information. As flow models are computationally expensive, we advice you to rely on those pretrained models for a first run through the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 561,
     "status": "ok",
     "timestamp": 1732455754722,
     "user": {
      "displayName": "임종민",
      "userId": "13437322968883113888"
     },
     "user_tz": -540
    },
    "id": "mdWnWFN3McTO",
    "outputId": "c223a4d1-099e-4007-d542-204a5292d1a8",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training MNISTFlow_simple\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type       | Params | In sizes | Out sizes\n",
      "------------------------------------------------------------\n",
      "0 | flows | ModuleList | 556 K  | ?        | ?        \n",
      "------------------------------------------------------------\n",
      "556 K     Trainable params\n",
      "0         Non-trainable params\n",
      "556 K     Total params\n",
      "2.225     Total estimated model params size (MB)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Descriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9924\\3044067802.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mflow_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"simple\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"vardeq\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multiscale\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mflow_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"simple\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflow_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"simple\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"result\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_flow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcreate_simple_flow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muse_vardeq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"MNISTFlow_simple\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mflow_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"vardeq\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflow_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"vardeq\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"result\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_flow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcreate_simple_flow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muse_vardeq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"MNISTFlow_vardeq\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mflow_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"multiscale\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflow_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"multiscale\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"result\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_flow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcreate_multiscale_flow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"MNISTFlow_multiscale\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9924\\2477065696.py\u001b[0m in \u001b[0;36mtrain_flow\u001b[1;34m(flow, model_name)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Start training\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m# Check whether pretrained model exists. If yes, load it and skip training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\metal\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    607\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lightning_module\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m         call._call_and_handle_interrupt(\n\u001b[1;32m--> 609\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    610\u001b[0m         )\n\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\metal\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_TunerExitException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\metal\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    648\u001b[0m             \u001b[0mmodel_connected\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m         )\n\u001b[1;32m--> 650\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    651\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\metal\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m   1098\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_lightning_module_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"on_fit_start\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log_hyperparams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore_checkpoint_after_setup\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\metal\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_log_hyperparams\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1167\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhparams_initial\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1168\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_hyperparams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhparams_initial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1169\u001b[1;33m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1170\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\metal\\lib\\site-packages\\lightning_utilities\\core\\rank_zero.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The `rank_zero_only.rank` needs to be set before use\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrank\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\metal\\lib\\site-packages\\pytorch_lightning\\loggers\\tensorboard.py\u001b[0m in \u001b[0;36mlog_graph\u001b[1;34m(self, model, input_array)\u001b[0m\n\u001b[0;32m    203\u001b[0m             \u001b[0minput_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_batch_transfer_handler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mpl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_is_scripting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mrank_zero_only\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\metal\\lib\\site-packages\\lightning_fabric\\loggers\\logger.py\u001b[0m in \u001b[0;36mexperiment\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mget_experiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_DummyExperiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\metal\\lib\\site-packages\\lightning_utilities\\core\\rank_zero.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The `rank_zero_only.rank` needs to be set before use\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrank\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\metal\\lib\\site-packages\\lightning_fabric\\loggers\\logger.py\u001b[0m in \u001b[0;36mget_experiment\u001b[1;34m()\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;33m@\u001b[0m\u001b[0mrank_zero_only\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mget_experiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mget_experiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_DummyExperiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\metal\\lib\\site-packages\\lightning_fabric\\loggers\\tensorboard.py\u001b[0m in \u001b[0;36mexperiment\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_TENSORBOARD_AVAILABLE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m             \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensorboard\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[0mtensorboardX\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSummaryWriter\u001b[0m  \u001b[1;31m# type: ignore[no-redef]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\metal\\lib\\site-packages\\torch\\utils\\tensorboard\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFileWriter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSummaryWriter\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_writer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRecordWriter\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\metal\\lib\\site-packages\\torch\\utils\\tensorboard\\writer.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevent_pb2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSessionLog\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevent_pb2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEvent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mevent_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\proto\\event_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msummary_pb2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtensorboard_dot_compat_dot_proto_dot_summary__pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m   \u001b[0mtensorboard_dot_compat_dot_proto_dot_histogram__pb2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensorboard_dot_compat_dot_proto_dot_summary__pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensorboard_dot_compat_dot_proto_dot_histogram__pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\proto\\summary_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhistogram_pb2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtensorboard_dot_compat_dot_proto_dot_histogram__pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensor_pb2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtensorboard_dot_compat_dot_proto_dot_tensor__pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\proto\\histogram_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m       \u001b[0mmessage_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menum_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontaining_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m       \u001b[0mis_extension\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextension_scope\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m       serialized_options=None, file=DESCRIPTOR),\n\u001b[0m\u001b[0;32m     43\u001b[0m     _descriptor.FieldDescriptor(\n\u001b[0;32m     44\u001b[0m       \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'max'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'tensorboard.HistogramProto.max'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\metal\\lib\\site-packages\\google\\protobuf\\descriptor.py\u001b[0m in \u001b[0;36m__new__\u001b[1;34m(cls, name, full_name, index, number, type, cpp_type, label, default_value, message_type, enum_type, containing_type, is_extension, extension_scope, options, serialized_options, has_default_value, containing_oneof, json_name, file, create_key)\u001b[0m\n\u001b[0;32m    559\u001b[0m                 \u001b[0mhas_default_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontaining_oneof\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjson_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m                 file=None, create_key=None):  # pylint: disable=redefined-builtin\n\u001b[1;32m--> 561\u001b[1;33m       \u001b[0m_message\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMessage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_CheckCalledFromGeneratedFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mis_extension\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_pool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFindExtensionByName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Descriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates"
     ]
    }
   ],
   "source": [
    "flow_dict = {\"simple\": {}, \"vardeq\": {}, \"multiscale\": {}}\n",
    "flow_dict[\"simple\"][\"model\"], flow_dict[\"simple\"][\"result\"] = train_flow(create_simple_flow(use_vardeq=False), model_name=\"MNISTFlow_simple\")\n",
    "flow_dict[\"vardeq\"][\"model\"], flow_dict[\"vardeq\"][\"result\"] = train_flow(create_simple_flow(use_vardeq=True), model_name=\"MNISTFlow_vardeq\")\n",
    "flow_dict[\"multiscale\"][\"model\"], flow_dict[\"multiscale\"][\"result\"] = train_flow(create_multiscale_flow(), model_name=\"MNISTFlow_multiscale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1732455754723,
     "user": {
      "displayName": "임종민",
      "userId": "13437322968883113888"
     },
     "user_tz": -540
    },
    "id": "0Q-Zc-w_McTP",
    "outputId": "243e50aa-ec32-40fe-f953-e569d08be59a"
   },
   "outputs": [],
   "source": [
    "flow_dict[\"simple\"][\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I0PKup6_McTP"
   },
   "source": [
    "### Density modeling and sampling\n",
    "\n",
    "Firstly, we can compare the models on their quantitative results. The following table shows all important statistics. The inference time specifies the time needed to determine the probability for a batch of 64 images for each model, and the sampling time the duration it took to sample a batch of 64 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1732455754723,
     "user": {
      "displayName": "임종민",
      "userId": "13437322968883113888"
     },
     "user_tz": -540
    },
    "id": "FY_tM5dpMcTP",
    "outputId": "ff3b3597-af6d-4a7b-e9e4-413a5ca7cec3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<!-- Some HTML code to increase font size in the following table -->\n",
    "<style>\n",
    "th {font-size: 120%;}\n",
    "td {font-size: 120%;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1732455754723,
     "user": {
      "displayName": "임종민",
      "userId": "13437322968883113888"
     },
     "user_tz": -540
    },
    "id": "p4SwDRYIMcTP",
    "outputId": "0d045000-1384-438c-fabc-58c71940ee9c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tabulate\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "table = [[key,\n",
    "          \"%4.3f bpd\" % flow_dict[key][\"result\"][\"val\"][0][\"test_bpd\"],\n",
    "          \"%4.3f bpd\" % flow_dict[key][\"result\"][\"test\"][0][\"test_bpd\"],\n",
    "          \"%2.0f ms\" % (1000 * flow_dict[key][\"result\"][\"time\"]),\n",
    "          \"%2.0f ms\" % (1000 * flow_dict[key][\"result\"].get(\"samp_time\", 0)),\n",
    "          \"{:,}\".format(sum([np.prod(p.shape) for p in flow_dict[key][\"model\"].parameters()]))]\n",
    "         for key in flow_dict]\n",
    "display(HTML(tabulate.tabulate(table, tablefmt='html', headers=[\"Model\", \"Validation Bpd\", \"Test Bpd\", \"Inference time\", \"Sampling time\", \"Num Parameters\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "executionInfo": {
     "elapsed": 1085,
     "status": "ok",
     "timestamp": 1732455755802,
     "user": {
      "displayName": "임종민",
      "userId": "13437322968883113888"
     },
     "user_tz": -540
    },
    "id": "pWcg7VYUMcTQ",
    "outputId": "d15af2e9-d252-4a1b-baed-3e7bbcf7ac26"
   },
   "outputs": [],
   "source": [
    "pl.seed_everything(44)\n",
    "samples = flow_dict[\"vardeq\"][\"model\"].sample(img_shape=[16,1,28,28])\n",
    "show_imgs(samples.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "executionInfo": {
     "elapsed": 410,
     "status": "ok",
     "timestamp": 1732455756210,
     "user": {
      "displayName": "임종민",
      "userId": "13437322968883113888"
     },
     "user_tz": -540
    },
    "id": "Rf3D-qAUMcTQ",
    "outputId": "18893537-0313-49ea-dd09-7258c3d5cf47"
   },
   "outputs": [],
   "source": [
    "pl.seed_everything(42)\n",
    "samples = flow_dict[\"multiscale\"][\"model\"].sample(img_shape=[16,8,7,7])\n",
    "show_imgs(samples.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qrQK_VakMcTQ"
   },
   "source": [
    "From the few samples, we can see a clear difference between the simple and the multi-scale model. The single-scale model has only learned local, small correlations while the multi-scale model was able to learn full, global relations that form digits. This show-cases another benefit of the multi-scale model. In contrast to VAEs, the outputs are sharp as normalizing flows can naturally model complex, multi-modal distributions while VAEs have the independent decoder output noise. Nevertheless, the samples from this flow are far from perfect as not all samples show true digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ivaZvNrjMcTR"
   },
   "source": [
    "### Interpolation in latent space\n",
    "\n",
    "Another popular test for the smoothness of the latent space of generative models is to interpolate between two training examples. As normalizing flows are strictly invertible, we can guarantee that any image is represented in the latent space. We again compare the variational dequantization model with the multi-scale model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 304,
     "status": "ok",
     "timestamp": 1732455756512,
     "user": {
      "displayName": "임종민",
      "userId": "13437322968883113888"
     },
     "user_tz": -540
    },
    "id": "lxdca22GMcTR"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def interpolate(model, img1, img2, num_steps=8):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        model - object of ImageFlow class that represents the (trained) flow model\n",
    "        img1, img2 - Image tensors of shape [1, 28, 28]. Images between which should be interpolated.\n",
    "        num_steps - Number of interpolation steps. 8 interpolation steps mean 6 intermediate pictures besides img1 and img2\n",
    "    \"\"\"\n",
    "    imgs = torch.stack([img1, img2], dim=0).to(model.device)\n",
    "    z, _ = model.encode(imgs)\n",
    "    alpha = torch.linspace(0, 1, steps=num_steps, device=z.device).view(-1, 1, 1, 1)\n",
    "    interpolations = z[0:1] * alpha + z[1:2] * (1 - alpha)\n",
    "    interp_imgs = model.sample(interpolations.shape[:1] + imgs.shape[1:], z_init=interpolations)\n",
    "    show_imgs(interp_imgs, row_size=8)\n",
    "\n",
    "exmp_imgs, _ = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "executionInfo": {
     "elapsed": 1538,
     "status": "ok",
     "timestamp": 1732455758048,
     "user": {
      "displayName": "임종민",
      "userId": "13437322968883113888"
     },
     "user_tz": -540
    },
    "id": "-ahvdXafMcTR",
    "outputId": "9f5dc11a-1831-4ef4-972d-9966c16efb12"
   },
   "outputs": [],
   "source": [
    "pl.seed_everything(42)\n",
    "for i in range(2):\n",
    "    interpolate(flow_dict[\"vardeq\"][\"model\"], exmp_imgs[2*i], exmp_imgs[2*i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "executionInfo": {
     "elapsed": 1422,
     "status": "ok",
     "timestamp": 1732455759467,
     "user": {
      "displayName": "임종민",
      "userId": "13437322968883113888"
     },
     "user_tz": -540
    },
    "id": "nC1zNBh5McTR",
    "outputId": "19dd3cb9-eeb9-4136-ee70-dfc03481752c"
   },
   "outputs": [],
   "source": [
    "pl.seed_everything(42)\n",
    "for i in range(2):\n",
    "    interpolate(flow_dict[\"multiscale\"][\"model\"], exmp_imgs[2*i], exmp_imgs[2*i+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xaeDuvSMcTS"
   },
   "source": [
    "### Visualization of latents in different levels of multi-scale\n",
    "\n",
    "In the following we will focus more on the multi-scale flow. We want to analyse what information is being stored in the variables split at early layers, and what information for the final variables. For this, we sample 8 images where each of them share the same final latent variables, but differ in the other part of the latent variables. Below we visualize three examples of this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 767
    },
    "executionInfo": {
     "elapsed": 1159,
     "status": "ok",
     "timestamp": 1732455760623,
     "user": {
      "displayName": "임종민",
      "userId": "13437322968883113888"
     },
     "user_tz": -540
    },
    "id": "vu5G5otBMcTS",
    "outputId": "04dff1d4-5b8d-43c9-a441-3418762270e3",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pl.seed_everything(44)\n",
    "for _ in range(3):\n",
    "    z_init = flow_dict[\"multiscale\"][\"model\"].prior.sample(sample_shape=[1,8,7,7])\n",
    "    z_init = z_init.expand(8, -1, -1, -1)\n",
    "    samples = flow_dict[\"multiscale\"][\"model\"].sample(img_shape=z_init.shape, z_init=z_init)\n",
    "    show_imgs(samples.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67MXf-b9McTS"
   },
   "source": [
    "We see that the early split variables indeed have a smaller effect on the image. Still, small differences can be spot when we look carefully at the borders of the digits. For instance, in the middle, the top part of the 3 has different thicknesses for different samples although all of them represent the same coarse structure. This shows that the flow indeed learns to separate the higher-level information in the final variables, while the early split ones contain local noise patterns."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial11/NF_image_modeling.ipynb",
     "timestamp": 1732435553295
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
