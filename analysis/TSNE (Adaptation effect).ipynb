{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e84eed6",
   "metadata": {},
   "source": [
    "# Before Adaptation vs After Adatpation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "912c279b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pytorchcv.model_provider import get_model as ptcv_get_model # model\n",
    "import sys, os\n",
    "import easydict\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline \n",
    "\n",
    "# enable cuda devices\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from meta_neural_network_architectures import VGGReLUNormNetwork, ResNet12\n",
    "from utils.parser_utils import get_args\n",
    "from data import MetaLearningSystemDataLoader\n",
    "from experiment_builder import ExperimentBuilder\n",
    "from few_shot_learning_system import MAMLFewShotClassifier\n",
    "import prompters\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from data_augmentation import mixup_data, cutmix_data, random_flip, random_flip_like_torchvision\n",
    "from utils.basic import gap, flatten_feature_map, plot_support_query_before_after_fixed_axes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f032c4",
   "metadata": {},
   "source": [
    "# 1. Dataset 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5591885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choices=['padding', 'random_patch', 'fixed_patch'],\n",
    "method = 'padding'\n",
    "\n",
    "datasets = \"mini_imagenet\"\n",
    "# datasets = \"tiered_imagenet\"\n",
    "# datasets = \"CIFAR_FS\"\n",
    "# datasets = \"CUB\"\n",
    "\n",
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "# os.environ['TEST_DATASET'] = \"tiered_imagenet\" # https://mtl.yyliu.net/download/Lmzjm9tX.html\n",
    "# os.environ['TEST_DATASET'] = \"CIFAR_FS\" # https://drive.google.com/file/d/1pTsCCMDj45kzFYgrnO67BWVbKs48Q3NI/view\n",
    "# os.environ['TEST_DATASET'] = \"CUB\" # https://data.caltech.edu/records/65de6-vp158"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e33976d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.environ['DATASET_DIR'] ===  C:/Users/JM/PycharmProjects/MAML/datasets\n"
     ]
    }
   ],
   "source": [
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "print(\"os.environ['DATASET_DIR'] === \", os.environ['DATASET_DIR'])\n",
    "\n",
    "args = easydict.EasyDict(\n",
    "{\n",
    "  \"batch_size\":2,\n",
    "  \"image_height\":84,\n",
    "  \"image_width\":84,\n",
    "  \"image_channels\":3,\n",
    "  \"gpu_to_use\":0,\n",
    "  \"num_dataprovider_workers\":4,\n",
    "  \"max_models_to_save\":5,\n",
    "  \"dataset_name\":\"mini_imagenet_full_size\",\n",
    "  \"dataset_path\":\"mini_imagenet_full_size\",\n",
    "  \"reset_stored_paths\":False,\n",
    "  \"experiment_name\":\"../MAML_Prompt_padding_5way_5shot_filter128_miniImagenet\",\n",
    "  \"train_seed\": 0, \"val_seed\": 0,\n",
    "  \"indexes_of_folders_indicating_class\": [-3, -2],\n",
    "  \"sets_are_pre_split\": True,\n",
    "  \"train_val_test_split\": [0.64, 0.16, 0.20],\n",
    "  \"evaluate_on_test_set_only\": False,\n",
    "\n",
    "  \"total_epochs\": 100,\n",
    "  \"total_iter_per_epoch\":500, \"continue_from_epoch\": -2,\n",
    "  \"num_evaluation_tasks\":600,\n",
    "  \"multi_step_loss_num_epochs\": 15,\n",
    "  \"minimum_per_task_contribution\": 0.01,\n",
    "  \"learnable_per_layer_per_step_inner_loop_learning_rate\": True,\n",
    "  \"enable_inner_loop_optimizable_bn_params\": False,\n",
    "  \"evalute_on_test_set_only\": False,\n",
    "\n",
    "  \"max_pooling\": True,\n",
    "  \"per_step_bn_statistics\": False,\n",
    "  \"learnable_batch_norm_momentum\": False,\n",
    "  \"load_into_memory\": False,\n",
    "  \"init_inner_loop_learning_rate\": 0.01,\n",
    "  \"init_inner_loop_weight_decay\": 0.0005,\n",
    "  \"learnable_bn_gamma\": True,\n",
    "  \"learnable_bn_beta\": True,\n",
    "\n",
    "  \"dropout_rate_value\":0.0,\n",
    "  \"min_learning_rate\":0.001,\n",
    "  \"meta_learning_rate\":0.001,   \"total_epochs_before_pause\": 100,\n",
    "  \"first_order_to_second_order_epoch\":-1,\n",
    "  \"weight_decay\": 0.0,\n",
    "\n",
    "  \"norm_layer\":\"batch_norm\",\n",
    "  \"cnn_num_filters\":128,\n",
    "  \"num_stages\":4,\n",
    "  \"conv_padding\": True,\n",
    "  \"number_of_training_steps_per_iter\":5,\n",
    "  \"number_of_evaluation_steps_per_iter\":5,\n",
    "  \"cnn_blocks_per_stage\":1,\n",
    "  \"num_classes_per_set\":5,\n",
    "  \"num_samples_per_class\":5,\n",
    "  \"num_target_samples\": 15,\n",
    "  \"samples_per_iter\" : 1,\n",
    "\n",
    "  \"second_order\": True,\n",
    "  \"use_multi_step_loss_optimization\":False,\n",
    "  \"backbone\": \"4-CONV\",\n",
    "  \"arbiter\": False,\n",
    "  \"use_bias\": True,\n",
    "  \"prompter\": True,\n",
    "  \"prompt_engineering\": method,\n",
    "  \"prompt_size\" : 5,\n",
    "  \"image_size\" : 84,\n",
    "  \"prompt_random_init\": False,\n",
    "  \"outer_prompt_learning_rate\": 0.001,\n",
    "  \"inner_prompt_learning_rate\": 0.01,\n",
    "  \"data_aug\" : \"random\"\n",
    "}\n",
    ")\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "args.im_shape = (2, 3, args.image_height, args.image_width)\n",
    "\n",
    "args.use_cuda = torch.cuda.is_available()\n",
    "args.seed = 104\n",
    "args.reverse_channels=False\n",
    "args.labels_as_int=False\n",
    "args.reset_stored_filepaths=False\n",
    "args.num_of_gpus=1\n",
    "\n",
    "args.continue_from_epoch='latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b3c2f44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using max pooling\n",
      "meta network params\n",
      "0.01\n",
      "Inner Loop parameters\n",
      "prompt_learning_rates_dict.prompt_weight_learning_rate torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv0-conv-weight torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv0-conv-bias torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv1-conv-weight torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv1-conv-bias torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv2-conv-weight torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv2-conv-bias torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv3-conv-weight torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv3-conv-bias torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-linear-weights torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-linear-bias torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv0-conv-weight torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv0-conv-bias torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv1-conv-weight torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv1-conv-bias torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv2-conv-weight torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv2-conv-bias torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv3-conv-weight torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv3-conv-bias torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-linear-weights torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-linear-bias torch.Size([6])\n",
      "Outer Loop parameters\n",
      "classifier.layer_dict.conv0.conv.weight torch.Size([128, 3, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv0.conv.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv0.norm_layer.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv0.norm_layer.weight torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv1.conv.weight torch.Size([128, 128, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv1.conv.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv1.norm_layer.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv1.norm_layer.weight torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv2.conv.weight torch.Size([128, 128, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv2.conv.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv2.norm_layer.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv2.norm_layer.weight torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv3.conv.weight torch.Size([128, 128, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv3.conv.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv3.norm_layer.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv3.norm_layer.weight torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.linear.weights torch.Size([5, 3200]) cuda:0 True\n",
      "classifier.layer_dict.linear.bias torch.Size([5]) cuda:0 True\n",
      "classifier.prompt.prompt_dict.pad_up torch.Size([3, 5, 84]) cuda:0 True\n",
      "classifier.prompt.prompt_dict.pad_down torch.Size([3, 5, 84]) cuda:0 True\n",
      "classifier.prompt.prompt_dict.pad_left torch.Size([3, 74, 5]) cuda:0 True\n",
      "classifier.prompt.prompt_dict.pad_right torch.Size([3, 74, 5]) cuda:0 True\n",
      "inner_loop_optimizer.prompt_learning_rates_dict.prompt_weight_learning_rate torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv0-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv0-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv1-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv1-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv2-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv2-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv3-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv3-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-weights torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv0-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv0-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv1-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv1-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv2-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv2-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv3-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv3-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-linear-weights torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-linear-bias torch.Size([6]) cuda:0 True\n",
      "1\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\MAML_Prompt_padding_5way_5shot_filter128_miniImagenet\n",
      "attempting to find existing checkpoint\n",
      "data {'test': 12000, 'train': 38400, 'val': 9600}\n",
      "train_seed 985773, val_seed: 985773, at start time\n",
      "75000 50000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_val_acc': 0.699288889169693,\n",
       " 'best_val_iter': 69500,\n",
       " 'current_iter': 75000,\n",
       " 'best_epoch': 139,\n",
       " 'train_loss_mean': 0.44109563875198365,\n",
       " 'train_loss_std': 0.1291735863213422,\n",
       " 'train_accuracy_mean': 0.8391066664457321,\n",
       " 'train_accuracy_std': 0.053464857991797364,\n",
       " 'train_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_0_std': 0.0,\n",
       " 'train_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_1_std': 0.0,\n",
       " 'train_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_2_std': 0.0,\n",
       " 'train_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_3_std': 0.0,\n",
       " 'train_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'train_loss_importance_vector_4_std': 0.0,\n",
       " 'train_learning_rate_mean': 0.0010000000000000005,\n",
       " 'train_learning_rate_std': 4.336808689942018e-19,\n",
       " 'val_loss_mean': 0.8291508863369624,\n",
       " 'val_loss_std': 0.14279745482250403,\n",
       " 'val_accuracy_mean': 0.6856222219268481,\n",
       " 'val_accuracy_std': 0.05946382204087874,\n",
       " 'val_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_0_std': 0.0,\n",
       " 'val_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_1_std': 0.0,\n",
       " 'val_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_2_std': 0.0,\n",
       " 'val_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_3_std': 0.0,\n",
       " 'val_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'val_loss_importance_vector_4_std': 0.0,\n",
       " 'network': OrderedDict([('classifier.layer_dict.conv0.conv.weight',\n",
       "               tensor([[[[ 1.9228e-01, -3.8295e-01,  2.1642e-01],\n",
       "                         [-5.5411e-04, -4.2862e-02,  1.0049e-02],\n",
       "                         [-1.9220e-01,  4.4129e-01, -2.6217e-01]],\n",
       "               \n",
       "                        [[ 2.0767e-01, -4.2443e-01,  2.3391e-01],\n",
       "                         [-1.8607e-03, -2.2443e-02,  5.8218e-02],\n",
       "                         [-2.1587e-01,  3.7522e-01, -1.9190e-01]],\n",
       "               \n",
       "                        [[ 1.6017e-01, -2.9681e-01,  1.1097e-01],\n",
       "                         [ 2.8712e-02,  7.0504e-03, -2.6183e-02],\n",
       "                         [-1.5920e-01,  3.3434e-01, -1.4752e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.3566e-06, -1.4712e-06, -1.6298e-06],\n",
       "                         [-1.3851e-06, -1.5410e-06, -1.6120e-06],\n",
       "                         [-1.3997e-06, -1.5809e-06, -1.5675e-06]],\n",
       "               \n",
       "                        [[-1.5547e-06, -1.6984e-06, -1.8430e-06],\n",
       "                         [-1.5758e-06, -1.7545e-06, -1.7947e-06],\n",
       "                         [-1.5013e-06, -1.7067e-06, -1.7273e-06]],\n",
       "               \n",
       "                        [[-1.2922e-06, -1.4896e-06, -1.6343e-06],\n",
       "                         [-1.3808e-06, -1.6499e-06, -1.6773e-06],\n",
       "                         [-1.3512e-06, -1.6254e-06, -1.6297e-06]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.0596e-05,  1.4122e-05,  2.1456e-05],\n",
       "                         [ 2.6158e-06,  1.3649e-06,  1.3392e-05],\n",
       "                         [ 5.6518e-06,  4.9864e-06,  1.2944e-05]],\n",
       "               \n",
       "                        [[ 1.3616e-06,  2.8715e-06,  6.9109e-06],\n",
       "                         [-5.8312e-06, -8.3366e-06, -5.3908e-09],\n",
       "                         [-2.9931e-06, -2.8658e-06,  1.9354e-06]],\n",
       "               \n",
       "                        [[-1.0209e-06, -6.1541e-06, -2.1187e-06],\n",
       "                         [-6.7038e-06, -1.5934e-05, -1.2052e-05],\n",
       "                         [-6.8605e-06, -1.0175e-05, -9.9732e-06]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-5.6669e-05, -4.3113e-05, -3.2973e-05],\n",
       "                         [-4.5658e-05, -3.8774e-05, -2.2090e-05],\n",
       "                         [-8.6215e-05, -7.2827e-05, -5.3452e-05]],\n",
       "               \n",
       "                        [[ 1.1640e-04,  1.3112e-04,  1.3284e-04],\n",
       "                         [ 1.2972e-04,  1.3209e-04,  1.4257e-04],\n",
       "                         [ 8.1345e-05,  8.7237e-05,  9.9537e-05]],\n",
       "               \n",
       "                        [[ 5.5699e-04,  5.8572e-04,  6.6027e-04],\n",
       "                         [ 6.2147e-04,  6.3539e-04,  7.0788e-04],\n",
       "                         [ 4.7796e-04,  5.0228e-04,  5.4027e-04]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 9.2844e-05,  9.9655e-05,  1.0707e-04],\n",
       "                         [ 1.0091e-04,  1.1212e-04,  1.2581e-04],\n",
       "                         [ 1.0796e-04,  1.1651e-04,  1.2718e-04]],\n",
       "               \n",
       "                        [[ 1.1589e-04,  1.3381e-04,  1.4426e-04],\n",
       "                         [ 1.2781e-04,  1.4793e-04,  1.5086e-04],\n",
       "                         [ 1.4000e-04,  1.4373e-04,  1.4288e-04]],\n",
       "               \n",
       "                        [[ 4.6410e-05,  5.2902e-05,  5.5448e-05],\n",
       "                         [ 5.8372e-05,  6.7719e-05,  6.9776e-05],\n",
       "                         [ 6.7931e-05,  7.0582e-05,  7.1054e-05]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.9477e-08, -1.9241e-08, -1.8357e-08],\n",
       "                         [-2.0274e-08, -2.0286e-08, -1.9949e-08],\n",
       "                         [-1.9327e-08, -1.9144e-08, -1.8706e-08]],\n",
       "               \n",
       "                        [[-1.2993e-08, -1.0423e-08, -9.2905e-09],\n",
       "                         [-1.1519e-08, -1.0288e-08, -8.8197e-09],\n",
       "                         [-1.1433e-08, -9.9111e-09, -9.7060e-09]],\n",
       "               \n",
       "                        [[-2.2416e-08, -2.1352e-08, -1.9316e-08],\n",
       "                         [-2.2470e-08, -2.1539e-08, -2.0338e-08],\n",
       "                         [-2.1251e-08, -2.0551e-08, -1.9282e-08]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.conv.bias',\n",
       "               tensor([-1.0766e-04, -1.8208e-06,  1.7318e-09,  1.8022e-09, -6.0413e-13,\n",
       "                       -1.7608e-11, -2.4008e-07, -2.0747e-06, -5.4608e-14,  3.0420e-05,\n",
       "                        2.2418e-08, -1.2807e-04, -4.8821e-05,  7.3929e-06,  1.6470e-06,\n",
       "                        5.5269e-08,  5.2318e-08,  8.3448e-08, -1.9499e-07,  1.8654e-14,\n",
       "                        2.8759e-09, -1.4524e-11,  2.0112e-09,  8.9400e-07, -1.2196e-13,\n",
       "                        1.3235e-08,  1.6770e-06,  5.5523e-09,  6.5927e-08,  3.5236e-05,\n",
       "                        1.2349e-08, -2.5207e-05,  3.8266e-05,  1.2119e-05,  8.8170e-07,\n",
       "                       -5.8973e-06, -3.4360e-05,  2.8971e-06,  5.7751e-08,  1.6981e-05,\n",
       "                       -6.3020e-06,  6.4398e-07,  4.9946e-09,  7.9443e-06, -1.0413e-04,\n",
       "                        2.7680e-05,  3.4991e-06,  3.1208e-08,  6.2563e-06, -5.0337e-06,\n",
       "                        2.2825e-13, -1.4672e-05, -5.3406e-10,  1.0271e-08,  5.3552e-09,\n",
       "                       -6.2026e-06,  7.4721e-07, -1.1659e-08, -3.4967e-08,  1.5412e-07,\n",
       "                       -2.6710e-10, -3.6212e-09, -8.7714e-06, -1.0849e-05, -4.3477e-12,\n",
       "                        3.1768e-08,  2.3553e-06, -3.2610e-13, -5.6527e-08, -6.3316e-05,\n",
       "                        1.1863e-06,  5.1931e-08, -2.5802e-07, -5.8583e-08,  1.4091e-06,\n",
       "                        3.8682e-12,  1.0926e-05, -1.6802e-12,  7.9895e-08,  2.9296e-06,\n",
       "                       -9.4223e-13,  1.1736e-09,  9.6172e-08,  2.8990e-08,  6.0627e-13,\n",
       "                        4.9836e-08,  8.6083e-12, -5.9371e-07, -1.6258e-06,  1.0432e-11,\n",
       "                       -2.4476e-05, -1.3638e-08,  3.2943e-04,  1.2705e-07, -1.2575e-06,\n",
       "                       -1.8062e-05,  9.7118e-09, -2.5870e-05,  3.8594e-14,  7.8743e-06,\n",
       "                       -2.7276e-04, -1.2596e-08, -1.1878e-08, -4.4890e-14, -3.3862e-06,\n",
       "                        1.5649e-08,  5.7829e-06,  2.6338e-07,  4.0213e-07,  3.3605e-05,\n",
       "                       -3.6644e-07,  2.0759e-06, -8.1007e-09, -8.7374e-05, -9.9383e-07,\n",
       "                        5.5006e-05,  6.2239e-15,  1.0627e-05, -9.5537e-14, -9.8418e-05,\n",
       "                       -5.9476e-05,  8.2735e-09, -5.9381e-06, -5.5690e-05, -1.0448e-05,\n",
       "                        1.9987e-06,  1.1278e-08, -7.9950e-11], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.bias',\n",
       "               tensor([-2.5809e-02, -5.1914e-04, -1.8808e-03, -2.3333e-03, -2.3490e-08,\n",
       "                       -9.3809e-08, -1.1433e-03, -4.1157e-09, -1.8247e-10, -2.9051e-01,\n",
       "                       -4.2983e-02, -2.9330e-02, -1.7057e-01, -7.6430e-01,  6.6548e-01,\n",
       "                       -1.9083e-08, -3.5998e-02, -4.9337e-02, -3.2940e-03, -8.5166e-10,\n",
       "                       -9.6635e-08, -7.7134e-05, -4.5418e-10, -4.1506e-10, -8.6609e-08,\n",
       "                       -2.8743e-02, -5.0643e-10, -6.2151e-06, -5.3311e-06,  1.2237e-01,\n",
       "                       -1.1510e-02, -2.4103e-01,  4.5868e-02, -7.2895e-02, -4.2280e-02,\n",
       "                       -2.8928e-02, -7.9263e-01,  7.6610e-02, -3.7418e-02, -3.8003e-01,\n",
       "                       -1.5165e-02, -1.8458e-02, -1.6674e-10, -4.3972e-02, -6.6921e-02,\n",
       "                       -4.8274e-02, -3.9454e-04, -4.3398e-02, -4.9393e-02, -4.2917e-02,\n",
       "                       -5.5716e-09,  1.2862e-01, -2.2174e-06, -6.0387e-03, -1.0611e-02,\n",
       "                       -3.3598e-02, -8.0976e-02, -1.3177e-08, -1.5636e-03, -4.8358e-04,\n",
       "                       -8.5369e-05, -3.6528e-02, -9.1975e-02, -4.6839e-02, -2.4339e-09,\n",
       "                       -3.2042e-02, -4.6208e-02, -3.9834e-07, -8.4504e-09, -1.8178e-02,\n",
       "                       -8.6329e-03, -8.8436e-11, -1.9233e-02, -2.4675e-02, -4.5663e-07,\n",
       "                       -1.3715e-09,  9.3442e-02, -4.4416e-09, -2.4440e-02, -5.0634e-02,\n",
       "                       -4.7305e-09, -1.8038e-10, -3.5714e-02, -9.7317e-06, -7.6755e-10,\n",
       "                       -3.1391e-02, -5.0279e-09, -4.1558e-02, -2.7369e-09, -3.5630e-07,\n",
       "                       -4.7416e-03, -2.3511e-07, -2.5465e-02, -1.7810e-02, -5.8317e-09,\n",
       "                       -5.7122e-02, -3.1276e-02, -2.7462e-01, -7.6875e-10, -2.3602e-01,\n",
       "                       -2.5703e-02, -2.2502e-02, -7.5949e-10,  3.6854e-08,  1.4450e-01,\n",
       "                       -1.0464e-04, -2.3220e-02, -2.7830e-02, -1.8998e-02, -3.7490e-05,\n",
       "                       -5.6322e-03,  2.9649e-01, -6.5495e-02, -4.2253e-02, -1.3606e-02,\n",
       "                       -3.1794e-02, -3.1002e-09,  1.4254e-02, -1.7801e-09,  6.8282e-02,\n",
       "                       -2.5634e-02, -4.2781e-02,  2.6376e-01, -6.9117e-02, -2.1164e-10,\n",
       "                       -1.2218e-02, -1.5941e-02, -4.4482e-06], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.weight',\n",
       "               tensor([ 3.3526e-01,  1.9167e-04, -6.8391e-04, -2.1747e-04, -4.9496e-07,\n",
       "                       -4.9566e-11,  2.7613e-04,  2.4229e-19, -9.6174e-20,  1.8506e-01,\n",
       "                       -4.1493e-03,  4.6442e-01,  2.2548e-01,  3.4757e-01,  1.4051e-03,\n",
       "                        2.9909e-06,  1.9145e-03,  1.8588e-02,  7.4195e-04, -8.3158e-24,\n",
       "                        6.3285e-07,  3.1460e-05,  6.7130e-21,  4.8129e-08,  6.7764e-08,\n",
       "                       -3.7241e-03,  1.1095e-10,  2.8907e-19,  1.3859e-06,  1.9952e-01,\n",
       "                        3.4571e-03,  3.2557e-01,  2.8908e-01,  3.1386e-01,  1.3068e-02,\n",
       "                        5.2425e-01,  4.9968e-01,  1.8781e-01,  8.5281e-03,  3.8761e-01,\n",
       "                        4.9566e-03,  4.0077e-03,  8.2336e-11,  1.9664e-03,  3.7786e-01,\n",
       "                        4.0160e-01,  1.1259e-04, -3.9127e-03,  2.2199e-02,  1.3936e-02,\n",
       "                        1.5354e-07,  4.5942e-01,  3.7167e-05,  4.4951e-04, -1.1669e-03,\n",
       "                        5.1375e-03,  3.1180e-01, -2.7176e-15, -5.3439e-04, -5.6643e-05,\n",
       "                       -1.0769e-04,  3.6468e-03,  2.6200e-01,  1.8098e-02, -1.1121e-05,\n",
       "                        5.5202e-03, -2.2146e-02,  1.5499e-06,  9.7797e-14,  3.4626e-01,\n",
       "                        5.6925e-04,  3.1853e-14,  1.1209e-03, -1.3584e-03, -3.7818e-04,\n",
       "                        2.3882e-10,  1.7380e-01,  4.6401e-13, -1.0196e-02, -1.4447e-02,\n",
       "                        5.7228e-17, -1.4056e-09, -9.1493e-03, -7.7048e-05, -1.1837e-07,\n",
       "                        8.8579e-03,  4.8816e-15, -5.7310e-03,  3.4566e-08,  4.5127e-04,\n",
       "                        8.4108e-04,  5.6366e-06,  3.7016e-01, -6.6159e-03,  2.5826e-07,\n",
       "                        1.3502e-02, -6.2362e-03,  3.9604e-01, -3.4727e-08,  2.5340e-01,\n",
       "                        5.0110e-01,  1.5501e-03,  3.3106e-09,  1.3406e-16,  1.0688e-03,\n",
       "                        1.2580e-04, -2.7252e-03,  3.6551e-04,  1.0066e-03,  3.5513e-06,\n",
       "                        8.6921e-04, -2.7740e-03, -1.4297e-02,  3.5214e-01,  4.1803e-03,\n",
       "                        5.4773e-01,  1.6884e-23, -4.2999e-02,  4.7924e-08,  4.5025e-01,\n",
       "                        4.7415e-01, -2.8217e-03,  4.0074e-01,  3.5637e-01,  2.1544e-09,\n",
       "                        1.6793e-03, -6.8909e-04,  1.9759e-05], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.weight',\n",
       "               tensor([[[[ 1.5695e-03, -5.7032e-02, -2.9868e-03],\n",
       "                         [-2.7996e-02, -8.3531e-02,  3.1111e-02],\n",
       "                         [ 1.0478e-01, -1.0303e-01,  3.3634e-02]],\n",
       "               \n",
       "                        [[ 4.3103e-07, -2.0037e-07,  1.2237e-06],\n",
       "                         [ 5.7432e-07, -1.1839e-07,  1.1313e-06],\n",
       "                         [-7.8870e-07, -1.6104e-06, -8.5676e-07]],\n",
       "               \n",
       "                        [[ 3.8306e-06,  4.1996e-06,  4.3505e-06],\n",
       "                         [-2.1818e-07,  6.6459e-08, -1.0587e-08],\n",
       "                         [-3.4418e-06, -3.3818e-06, -3.8774e-06]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 8.3234e-05,  6.4253e-05,  7.0436e-05],\n",
       "                         [ 3.5203e-05,  2.2897e-05,  3.4650e-05],\n",
       "                         [-4.8081e-05, -4.6398e-05, -2.7905e-05]],\n",
       "               \n",
       "                        [[ 2.7242e-05,  1.3766e-05,  3.8717e-05],\n",
       "                         [ 8.5378e-07, -1.7300e-05,  1.1540e-05],\n",
       "                         [-7.9276e-05, -9.7125e-05, -7.1933e-05]],\n",
       "               \n",
       "                        [[ 2.8196e-08,  1.2342e-08,  5.7852e-08],\n",
       "                         [ 1.3742e-08, -4.5810e-10,  3.9569e-08],\n",
       "                         [ 1.4135e-07,  1.2498e-07,  1.6029e-07]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.6531e-03, -7.5278e-02, -6.4949e-02],\n",
       "                         [ 3.6331e-02, -8.7467e-02, -9.6507e-02],\n",
       "                         [ 2.4241e-02, -3.9710e-02,  5.6739e-03]],\n",
       "               \n",
       "                        [[ 5.1084e-06,  5.4070e-06,  7.3495e-06],\n",
       "                         [-3.8266e-07,  2.6003e-07,  2.5452e-06],\n",
       "                         [-1.3988e-06, -9.1484e-07,  1.6825e-06]],\n",
       "               \n",
       "                        [[ 6.1651e-06,  7.1286e-06,  1.0248e-05],\n",
       "                         [-1.9111e-06, -7.7084e-07,  3.0698e-06],\n",
       "                         [-1.1638e-06,  5.4033e-07,  5.6840e-06]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.2410e-04,  2.1415e-04,  2.3240e-04],\n",
       "                         [ 1.0465e-04,  1.2986e-04,  1.6661e-04],\n",
       "                         [ 8.4319e-05,  1.1937e-04,  1.5559e-04]],\n",
       "               \n",
       "                        [[ 2.1665e-04,  2.2382e-04,  2.8856e-04],\n",
       "                         [ 4.1147e-05,  6.3914e-05,  1.2887e-04],\n",
       "                         [ 2.8632e-06,  3.2031e-05,  1.0273e-04]],\n",
       "               \n",
       "                        [[-1.5103e-08, -6.7282e-09, -2.5531e-08],\n",
       "                         [-4.8396e-09, -5.8739e-11, -1.8241e-08],\n",
       "                         [-2.1287e-08, -1.1913e-08, -2.4971e-08]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 4.0683e-03,  5.2381e-02, -3.0850e-03],\n",
       "                         [ 6.8705e-02, -8.1159e-02,  5.3797e-02],\n",
       "                         [-2.1517e-03,  2.6284e-02, -6.2020e-02]],\n",
       "               \n",
       "                        [[ 2.8706e-06,  1.2291e-06,  2.2474e-06],\n",
       "                         [ 1.4600e-06, -1.0326e-07,  9.4203e-07],\n",
       "                         [ 2.3155e-06,  5.9170e-07,  1.5742e-06]],\n",
       "               \n",
       "                        [[ 3.0357e-06,  1.1254e-06,  4.9354e-06],\n",
       "                         [ 1.5113e-06, -5.5967e-07,  3.1499e-06],\n",
       "                         [ 2.4232e-06,  1.0744e-07,  3.2646e-06]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-6.8424e-05, -1.0420e-04, -3.7301e-05],\n",
       "                         [-3.8869e-05, -9.7091e-05, -7.5316e-05],\n",
       "                         [ 1.4194e-05, -4.0148e-05, -4.1150e-05]],\n",
       "               \n",
       "                        [[ 4.1896e-05,  8.8409e-06,  5.2717e-05],\n",
       "                         [ 2.4159e-05, -1.3305e-05,  2.0680e-05],\n",
       "                         [ 3.7933e-05,  1.2007e-06,  2.8198e-05]],\n",
       "               \n",
       "                        [[ 6.2969e-08,  4.6771e-08,  8.1204e-08],\n",
       "                         [ 1.3417e-08,  1.9803e-11,  4.0851e-08],\n",
       "                         [ 2.6059e-08,  1.0058e-08,  5.2158e-08]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-8.8614e-14,  6.8446e-13, -5.4504e-13],\n",
       "                         [-2.7504e-13,  8.0959e-13, -7.1600e-13],\n",
       "                         [ 1.8672e-12, -7.7381e-13, -1.0437e-12]],\n",
       "               \n",
       "                        [[-1.7152e-09,  1.0296e-09,  1.6819e-08],\n",
       "                         [ 2.1325e-09, -2.4045e-11, -8.2644e-11],\n",
       "                         [ 1.4497e-12, -1.7644e-15,  8.1426e-08]],\n",
       "               \n",
       "                        [[-7.5469e-17, -1.1966e-17,  1.4466e-17],\n",
       "                         [-5.1931e-17, -7.0395e-18, -6.6036e-17],\n",
       "                         [-1.6349e-16, -1.2963e-16, -6.2481e-17]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.4536e-15, -2.2394e-15, -1.0331e-15],\n",
       "                         [-2.7920e-15, -2.9148e-15, -1.0163e-15],\n",
       "                         [-3.5155e-15, -2.8757e-15, -1.6031e-15]],\n",
       "               \n",
       "                        [[-1.2591e-15,  9.4592e-16,  3.3485e-15],\n",
       "                         [-6.5137e-16,  1.1505e-15,  3.5876e-15],\n",
       "                         [ 4.7514e-16,  3.1834e-15,  5.5867e-15]],\n",
       "               \n",
       "                        [[ 6.0253e-08,  1.3847e-12,  4.8175e-14],\n",
       "                         [-1.1157e-14,  1.1654e-16,  7.5787e-09],\n",
       "                         [ 7.6728e-15,  8.4598e-15,  1.4966e-11]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 6.0716e-02, -5.4222e-02, -6.5542e-02],\n",
       "                         [ 7.6453e-02,  7.7619e-02,  5.3585e-02],\n",
       "                         [-3.6023e-03,  1.2287e-01,  7.2876e-02]],\n",
       "               \n",
       "                        [[ 1.9270e-06,  3.1721e-08,  3.7762e-09],\n",
       "                         [ 2.1262e-06, -3.0081e-08, -7.6382e-08],\n",
       "                         [-9.4317e-07, -2.8849e-06, -2.8278e-06]],\n",
       "               \n",
       "                        [[-1.1461e-06, -4.9580e-07, -3.5916e-07],\n",
       "                         [-1.1484e-07,  1.3600e-08,  5.5160e-07],\n",
       "                         [-2.9553e-06, -2.3556e-06, -1.3710e-06]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.2907e-04, -1.3657e-04, -1.6962e-04],\n",
       "                         [-1.1038e-04, -1.0456e-04, -1.1740e-04],\n",
       "                         [-1.1941e-04, -1.1952e-04, -1.2450e-04]],\n",
       "               \n",
       "                        [[-1.7785e-05, -3.4181e-05, -5.0757e-05],\n",
       "                         [-2.6008e-05, -4.2766e-05, -4.4512e-05],\n",
       "                         [-4.8817e-05, -6.0582e-05, -5.3144e-05]],\n",
       "               \n",
       "                        [[ 4.1328e-08,  3.3762e-08,  2.2801e-08],\n",
       "                         [ 3.2353e-09,  7.2394e-10, -1.4439e-08],\n",
       "                         [-1.7319e-08, -6.1286e-09, -3.0143e-08]]],\n",
       "               \n",
       "               \n",
       "                       [[[-5.5801e-02, -3.9283e-03, -2.1383e-02],\n",
       "                         [ 1.5353e-02, -4.3544e-03,  1.8595e-02],\n",
       "                         [-6.8868e-03,  5.3788e-02,  3.0877e-03]],\n",
       "               \n",
       "                        [[ 6.4186e-06,  8.9440e-06,  1.4311e-06],\n",
       "                         [-3.0039e-06, -1.3032e-07, -8.4991e-06],\n",
       "                         [-8.7553e-06, -6.5315e-06, -1.5881e-05]],\n",
       "               \n",
       "                        [[ 2.6454e-05,  2.1426e-05,  1.8478e-05],\n",
       "                         [ 7.2922e-06,  6.1884e-07, -1.3302e-06],\n",
       "                         [ 1.5738e-05,  4.3340e-06,  3.2809e-06]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 4.0983e-05,  8.7041e-05, -5.7032e-05],\n",
       "                         [-1.8424e-04, -1.2311e-04, -2.8319e-04],\n",
       "                         [-2.2099e-04, -1.6492e-04, -2.9923e-04]],\n",
       "               \n",
       "                        [[ 1.8022e-04,  2.3264e-04,  6.1339e-06],\n",
       "                         [-1.0308e-04, -3.2971e-05, -2.7344e-04],\n",
       "                         [-1.2316e-04, -1.4013e-04, -2.9492e-04]],\n",
       "               \n",
       "                        [[-6.5505e-09,  6.0620e-08, -2.5865e-08],\n",
       "                         [-6.2518e-08,  1.4481e-10, -8.5896e-08],\n",
       "                         [-6.0947e-08, -5.6647e-08, -1.3193e-07]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.bias',\n",
       "               tensor([ 4.5168e-05, -3.0117e-05,  3.6113e-05, -2.1736e-06,  1.6835e-04,\n",
       "                        3.5484e-05, -2.1631e-09, -4.2750e-08, -1.2322e-13, -8.8280e-06,\n",
       "                       -6.3897e-07,  1.5514e-08, -8.1350e-08, -8.0250e-06, -2.0894e-09,\n",
       "                        9.3035e-06,  1.4602e-09,  1.1298e-05,  1.0218e-05, -5.4976e-08,\n",
       "                        7.9948e-05,  1.0569e-08,  9.7000e-08, -8.1266e-06,  2.6625e-05,\n",
       "                       -1.4714e-06, -1.8307e-05, -1.2225e-04, -1.4232e-04, -6.8579e-05,\n",
       "                        9.3456e-06, -3.2339e-05, -7.2536e-14, -8.0666e-06,  3.7663e-05,\n",
       "                       -2.3811e-05,  9.0170e-08, -6.1606e-05, -1.4049e-05, -4.0197e-05,\n",
       "                       -1.3548e-05, -7.0012e-05,  1.6610e-15,  2.3690e-06, -1.7877e-08,\n",
       "                        3.3214e-05, -2.0823e-05, -1.1988e-06, -6.3415e-05,  1.4345e-07,\n",
       "                       -7.0419e-09, -2.8471e-05, -4.2456e-05,  3.5109e-06,  1.7452e-06,\n",
       "                       -4.3338e-05, -3.4289e-06,  5.4746e-08, -2.9869e-06,  6.5749e-05,\n",
       "                       -1.6871e-08,  2.6363e-05, -8.5679e-11, -1.1450e-05,  1.9462e-05,\n",
       "                        1.6616e-10, -5.2157e-15, -4.8548e-05, -3.9929e-06,  2.0022e-05,\n",
       "                       -2.3773e-05,  6.1655e-07, -7.5857e-09,  9.8063e-06, -8.6595e-06,\n",
       "                       -2.4165e-05, -1.8007e-08,  4.4242e-05, -3.1743e-05, -1.0179e-05,\n",
       "                        6.2616e-07, -2.5623e-05, -4.8078e-08, -5.2821e-08, -1.3029e-05,\n",
       "                       -8.6953e-06, -5.9220e-05, -1.2131e-04,  2.0129e-06,  4.8198e-08,\n",
       "                        3.1398e-05,  1.1218e-05, -1.1826e-05, -7.7615e-08,  1.2837e-04,\n",
       "                       -1.5474e-08,  3.1306e-05,  1.5030e-06, -2.2974e-07, -5.1837e-06,\n",
       "                       -1.3173e-05, -3.7323e-06,  2.8273e-06,  9.2429e-16, -4.1643e-05,\n",
       "                       -7.9699e-06,  2.6537e-05, -1.0435e-05, -5.1954e-07,  2.3600e-05,\n",
       "                       -1.1240e-08, -3.6531e-05,  1.6629e-09, -1.4557e-05, -3.3385e-05,\n",
       "                       -2.9059e-05, -8.6512e-07,  1.2803e-04,  2.6218e-05,  2.8815e-05,\n",
       "                       -1.5188e-07,  1.7123e-06, -1.3313e-05,  4.6257e-05,  3.2781e-07,\n",
       "                        3.9636e-07, -2.7265e-05,  7.2250e-06], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.bias',\n",
       "               tensor([-1.5115e-01, -3.1402e-01, -1.8303e-01, -1.2133e-02, -2.1663e-01,\n",
       "                       -2.4162e-01, -2.2323e-07, -4.9754e-02, -4.7388e-11, -3.0910e-01,\n",
       "                       -6.8840e-06, -4.8328e-02, -5.1842e-02, -6.3425e-06, -6.8398e-10,\n",
       "                       -1.5040e-07, -2.7792e-05, -9.4731e-02, -2.7108e-01, -4.8928e-08,\n",
       "                       -1.2728e-01, -5.2919e-03, -2.3063e-02, -1.8212e-02, -1.5119e-01,\n",
       "                       -3.9287e-02, -2.3425e-01, -1.8593e-01, -1.5186e-02, -1.2504e-01,\n",
       "                       -1.2405e-01,  6.3092e-02, -4.3638e-08, -1.9626e-01, -1.7277e-01,\n",
       "                       -2.4427e-01, -4.6361e-02, -1.9265e-01, -2.4029e-01, -2.0945e-01,\n",
       "                       -1.7426e-01, -5.4928e-02, -3.3216e-08, -5.7906e-02, -1.8073e-02,\n",
       "                       -2.8147e-01, -2.1982e-01, -2.2724e-01, -2.0162e-01, -6.2892e-02,\n",
       "                       -1.4753e-03, -2.2835e-01, -1.6745e-01, -9.9206e-10, -6.7113e-02,\n",
       "                       -1.4650e-01, -1.1744e-04, -1.7754e-04, -1.4878e-01, -1.8586e-01,\n",
       "                       -3.8858e-04, -2.4775e-01, -4.7108e-06, -1.5611e-01, -1.2541e-01,\n",
       "                       -1.9238e-06, -4.8290e-11, -4.4985e-01, -8.4414e-02, -3.5834e-02,\n",
       "                       -1.1117e-01, -5.0948e-02, -7.1629e-03, -2.3098e-01, -1.8589e-01,\n",
       "                       -2.3255e-01, -8.5299e-03, -1.6975e-01, -1.9599e-01, -1.4893e-01,\n",
       "                       -4.0870e-02, -5.5576e-03, -7.6101e-03, -4.4184e-02, -1.5317e-01,\n",
       "                       -2.5087e-01, -3.1078e-01, -2.0881e-01, -1.5448e-01, -1.7251e-02,\n",
       "                       -2.1197e-01, -5.7133e-02, -1.3835e-01, -1.2130e-02, -2.4563e-01,\n",
       "                       -3.7382e-03, -2.8209e-01, -1.4660e-02, -5.3973e-02, -3.4547e-01,\n",
       "                       -1.9406e-01, -1.2694e-02, -6.6265e-05, -7.5035e-06, -1.8469e-01,\n",
       "                       -1.4090e-02, -1.3891e-01, -1.3155e-01, -4.4367e-02, -2.2380e-01,\n",
       "                       -3.6748e-02, -2.0704e-01, -1.1590e-02, -1.5942e-09, -2.1537e-01,\n",
       "                       -2.3270e-01, -3.5155e-02, -2.8855e-01, -1.6806e-01, -2.1776e-01,\n",
       "                       -5.2874e-02, -1.6782e-01, -2.1668e-01, -2.0899e-01, -4.4701e-02,\n",
       "                       -2.6233e-09, -2.3584e-01, -2.8385e-01], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.weight',\n",
       "               tensor([ 2.6532e-01,  2.0715e-01,  3.8209e-01, -1.8945e-03,  4.9843e-01,\n",
       "                        2.1708e-01, -5.8063e-05, -1.1586e-03, -4.9505e-08,  4.8084e-01,\n",
       "                       -7.6735e-11,  2.9084e-03, -2.4962e-02,  1.5918e-04, -2.5288e-10,\n",
       "                       -9.1117e-13, -3.2566e-04, -1.4554e-02,  1.8416e-01,  2.0516e-08,\n",
       "                        2.7979e-01, -8.6358e-04,  3.5524e-03,  2.2911e-03,  2.8209e-01,\n",
       "                       -6.7828e-03,  4.1100e-01,  5.7101e-01,  2.3564e-01,  2.3557e-01,\n",
       "                        1.8358e-01,  1.9113e-01,  1.4182e-12,  3.8140e-01,  2.8341e-01,\n",
       "                        1.7351e-01,  4.9462e-03,  3.1468e-01,  3.3420e-01,  3.5592e-01,\n",
       "                        2.0780e-01,  2.9089e-01,  6.7969e-10, -5.0611e-03, -2.5277e-03,\n",
       "                        3.5446e-01,  3.4840e-01,  3.6084e-01,  2.9067e-01,  4.0440e-03,\n",
       "                       -7.0053e-04,  1.4181e-01,  4.2094e-01, -2.2558e-06,  2.3988e-01,\n",
       "                        3.4501e-01,  1.3630e-04,  2.5436e-04,  8.7545e-02,  3.4499e-01,\n",
       "                       -2.2140e-04,  3.0961e-01,  3.6507e-05,  2.5331e-01,  2.8411e-01,\n",
       "                        1.4401e-04,  2.5930e-14,  3.6041e-01,  2.0267e-01,  4.7530e-03,\n",
       "                        3.8362e-01,  4.7990e-03,  1.3029e-03,  3.8613e-01,  3.8848e-01,\n",
       "                        2.9537e-01, -1.6289e-03,  3.0565e-01,  3.0061e-01,  2.0382e-01,\n",
       "                       -1.6124e-02, -1.1573e-03,  1.0887e-03,  6.8003e-03,  2.3247e-01,\n",
       "                        3.5745e-01,  4.6195e-01,  6.7543e-01,  3.1324e-01,  1.6888e-03,\n",
       "                        2.1650e-01,  2.1976e-02,  1.5640e-01, -3.0253e-04,  5.1016e-01,\n",
       "                        8.7720e-04,  2.2294e-01, -1.7185e-03, -1.6013e-02,  1.3848e-01,\n",
       "                        1.8158e-01, -2.0348e-03, -3.3019e-04, -5.6111e-10,  3.9194e-01,\n",
       "                       -1.7497e-03,  2.9699e-01,  2.6754e-01, -6.6517e-03,  2.5667e-01,\n",
       "                       -3.3140e-04,  4.9930e-01, -1.3358e-03,  7.7385e-11,  2.9451e-01,\n",
       "                        2.9847e-01, -2.9918e-03,  4.4894e-01,  2.6036e-01,  3.2710e-01,\n",
       "                       -6.9202e-03,  3.0018e-01,  3.7325e-01,  2.9502e-01,  2.5506e-02,\n",
       "                        1.6260e-06,  2.9978e-01,  3.3959e-01], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.weight',\n",
       "               tensor([[[[-1.7331e-02, -1.0295e-02,  6.0006e-02],\n",
       "                         [ 1.4751e-02,  1.1840e-02,  4.4171e-02],\n",
       "                         [-1.8700e-02,  9.9152e-03,  9.8307e-03]],\n",
       "               \n",
       "                        [[ 3.2548e-02, -3.1620e-03,  4.7042e-02],\n",
       "                         [ 8.7354e-03, -4.6184e-02, -2.6271e-02],\n",
       "                         [ 3.9488e-02, -1.0486e-02, -4.3416e-02]],\n",
       "               \n",
       "                        [[-3.5336e-03, -4.2004e-02,  9.4704e-02],\n",
       "                         [ 2.4011e-02,  6.0114e-02,  1.0110e-01],\n",
       "                         [-4.9922e-02,  1.9818e-02, -3.7397e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 8.4008e-12,  1.3610e-11,  1.9948e-11],\n",
       "                         [-2.4882e-10,  2.6633e-11,  2.2842e-11],\n",
       "                         [-1.2628e-10, -1.2588e-10, -1.1820e-10]],\n",
       "               \n",
       "                        [[-2.6030e-02, -2.2646e-02, -4.5673e-02],\n",
       "                         [-3.6530e-02,  4.4861e-02, -3.2543e-02],\n",
       "                         [-9.9535e-02, -7.1312e-02, -3.1628e-02]],\n",
       "               \n",
       "                        [[-5.0294e-02, -2.0907e-03, -5.2632e-02],\n",
       "                         [-5.4901e-02,  8.1871e-03, -2.9578e-02],\n",
       "                         [-2.3333e-02,  2.3127e-02,  5.3094e-03]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.3915e-08, -1.3539e-08, -1.0208e-08],\n",
       "                         [-7.6919e-09, -1.0022e-08, -1.1036e-08],\n",
       "                         [-1.6319e-08, -1.6095e-08, -1.8324e-08]],\n",
       "               \n",
       "                        [[-7.1355e-09, -1.8958e-09, -5.7762e-09],\n",
       "                         [-8.8858e-09, -1.0907e-08, -7.5267e-09],\n",
       "                         [-1.3050e-08, -4.8257e-09, -6.3485e-09]],\n",
       "               \n",
       "                        [[ 1.1903e-09, -9.5016e-10, -4.6731e-09],\n",
       "                         [ 3.1978e-09, -2.5720e-09, -5.3921e-09],\n",
       "                         [-1.1078e-09, -4.2107e-09,  1.2604e-09]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 3.5160e-17,  1.1911e-17,  2.7220e-17],\n",
       "                         [ 1.1128e-07, -1.7800e-20,  6.4114e-18],\n",
       "                         [ 1.8461e-07, -1.7758e-13, -2.3470e-07]],\n",
       "               \n",
       "                        [[-4.5536e-09,  3.8789e-09,  1.4197e-09],\n",
       "                         [-8.1445e-09, -7.2252e-09, -3.6486e-09],\n",
       "                         [-1.3546e-08, -8.2122e-10, -5.5370e-09]],\n",
       "               \n",
       "                        [[ 1.2165e-08,  1.5477e-08,  1.8301e-08],\n",
       "                         [ 1.1084e-08,  1.2940e-08,  1.1914e-08],\n",
       "                         [ 6.5109e-09,  1.2531e-08,  8.0403e-09]]],\n",
       "               \n",
       "               \n",
       "                       [[[-2.0525e-02,  1.5007e-04, -6.8799e-04],\n",
       "                         [ 1.3207e-02, -2.5649e-02, -4.2961e-02],\n",
       "                         [-4.4341e-03, -4.7235e-02, -5.5492e-02]],\n",
       "               \n",
       "                        [[-1.4054e-02,  5.3095e-02,  1.9296e-02],\n",
       "                         [ 7.4229e-03, -4.8933e-02,  7.1470e-03],\n",
       "                         [-1.2933e-02, -7.3876e-02, -9.2843e-02]],\n",
       "               \n",
       "                        [[-1.6037e-02,  3.1129e-02,  1.1733e-01],\n",
       "                         [-3.2431e-02,  5.7320e-02,  5.8305e-02],\n",
       "                         [-6.3027e-02, -1.6542e-02,  1.3459e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-3.2700e-10, -3.7454e-12, -1.6353e-10],\n",
       "                         [-3.0087e-10,  1.8615e-11, -1.4256e-10],\n",
       "                         [-4.9974e-10, -1.6924e-10, -3.0955e-10]],\n",
       "               \n",
       "                        [[-1.0961e-02,  8.5780e-02,  6.3631e-02],\n",
       "                         [ 2.8655e-02,  1.5317e-02,  5.1010e-03],\n",
       "                         [ 6.3905e-02,  7.3681e-02,  4.9896e-02]],\n",
       "               \n",
       "                        [[-6.6658e-02,  4.9188e-02, -6.8371e-05],\n",
       "                         [-2.6394e-02,  2.4509e-02, -6.3541e-02],\n",
       "                         [ 2.9110e-02, -8.6412e-03, -8.7618e-03]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-2.4340e-03, -7.6446e-02, -4.8478e-02],\n",
       "                         [-1.8862e-02, -5.6813e-02, -2.8551e-02],\n",
       "                         [-2.7547e-02,  9.6841e-03, -4.1929e-02]],\n",
       "               \n",
       "                        [[-9.8509e-02, -5.6164e-02, -1.8223e-02],\n",
       "                         [-9.2969e-02, -8.3309e-02, -4.7287e-02],\n",
       "                         [-6.3075e-02, -8.8742e-02, -5.0179e-02]],\n",
       "               \n",
       "                        [[-4.3223e-02, -9.3727e-02, -4.2817e-02],\n",
       "                         [ 9.1428e-03,  4.6011e-02, -9.3698e-03],\n",
       "                         [ 2.0409e-02,  6.4687e-02,  2.5330e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 4.8324e-10,  1.1324e-10,  3.6723e-10],\n",
       "                         [ 4.0323e-10, -3.5206e-11,  1.7468e-10],\n",
       "                         [ 6.3891e-10,  3.5280e-10,  8.3946e-10]],\n",
       "               \n",
       "                        [[ 3.2267e-02,  2.4582e-02,  8.6782e-02],\n",
       "                         [ 1.1874e-02, -5.1519e-02, -6.6258e-03],\n",
       "                         [ 9.2948e-02, -3.0583e-03, -4.3539e-02]],\n",
       "               \n",
       "                        [[-7.7873e-02,  4.6624e-02,  6.2669e-02],\n",
       "                         [-8.6713e-02,  1.7010e-03,  6.5482e-02],\n",
       "                         [-1.0964e-01, -7.4254e-03,  3.4417e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 3.8917e-02,  4.7500e-02,  5.0847e-02],\n",
       "                         [ 2.4168e-02, -1.4516e-02, -3.8104e-02],\n",
       "                         [-6.3570e-05, -1.7624e-02, -5.0674e-03]],\n",
       "               \n",
       "                        [[-4.4582e-02, -5.3816e-02, -5.7317e-02],\n",
       "                         [-8.4361e-02, -5.4290e-02, -4.6439e-02],\n",
       "                         [-1.1457e-01,  5.1760e-03, -2.5250e-02]],\n",
       "               \n",
       "                        [[-1.0224e-01, -6.3555e-02,  3.9176e-02],\n",
       "                         [-1.0838e-01, -6.0592e-02,  1.5014e-02],\n",
       "                         [ 8.9480e-02,  3.7615e-02, -1.7611e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.1091e-11,  8.6899e-11,  1.9585e-10],\n",
       "                         [-1.0885e-10, -2.4445e-11, -1.5381e-10],\n",
       "                         [-1.3987e-10, -2.5605e-11,  3.7611e-11]],\n",
       "               \n",
       "                        [[-2.4591e-02,  9.2847e-02,  7.3792e-02],\n",
       "                         [ 6.3681e-03, -3.6026e-02, -4.5709e-02],\n",
       "                         [-4.1956e-02, -5.0628e-03,  2.1480e-02]],\n",
       "               \n",
       "                        [[-4.2018e-02, -2.1273e-02, -2.1377e-02],\n",
       "                         [-1.0082e-02, -9.3999e-03,  4.4530e-03],\n",
       "                         [ 3.0255e-02,  4.7723e-02,  3.0611e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-4.0684e-09, -2.1106e-09, -2.1904e-09],\n",
       "                         [-2.0228e-09, -1.9443e-09, -2.7149e-09],\n",
       "                         [-4.5859e-09, -3.6660e-09, -3.6155e-09]],\n",
       "               \n",
       "                        [[-1.0328e-09, -2.3321e-09, -1.5114e-09],\n",
       "                         [-1.7891e-09, -2.2804e-09, -1.6883e-09],\n",
       "                         [-2.4500e-09, -2.4208e-09, -1.7160e-09]],\n",
       "               \n",
       "                        [[-4.3234e-10, -2.2020e-09, -1.0053e-10],\n",
       "                         [ 2.5660e-09,  4.9719e-10, -1.2553e-09],\n",
       "                         [-3.1708e-10, -1.1552e-09,  2.4696e-09]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.5557e-08,  1.5582e-10,  1.2349e-16],\n",
       "                         [-1.4798e-11, -1.9435e-13,  2.0571e-14],\n",
       "                         [-1.5178e-05,  1.5439e-06,  1.7371e-10]],\n",
       "               \n",
       "                        [[ 8.8018e-11,  1.8227e-09,  2.7502e-09],\n",
       "                         [-1.8966e-09, -4.7839e-09, -2.7987e-09],\n",
       "                         [-3.8434e-09, -3.6197e-09, -5.6636e-10]],\n",
       "               \n",
       "                        [[ 2.9312e-09,  4.4875e-09,  4.3292e-09],\n",
       "                         [ 3.1608e-09,  3.6188e-09,  3.8885e-09],\n",
       "                         [ 1.8508e-09,  2.5927e-09,  1.9351e-09]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.bias',\n",
       "               tensor([-5.3384e-05,  5.5572e-07,  1.3152e-05,  6.0222e-05,  4.7515e-06,\n",
       "                        2.8423e-05, -5.4325e-05,  3.5956e-05,  4.4897e-05, -3.2208e-05,\n",
       "                        5.3095e-08, -4.4170e-05, -2.3967e-05, -5.7456e-06,  1.9538e-05,\n",
       "                       -2.3510e-05, -8.4308e-06,  5.2713e-06,  3.5566e-05,  1.1082e-05,\n",
       "                        4.6182e-06,  1.6678e-05,  5.6850e-05,  1.0873e-05, -9.0663e-06,\n",
       "                        1.1822e-05,  4.7681e-05,  6.2435e-05, -1.2264e-05,  6.4167e-05,\n",
       "                       -2.3430e-05, -2.4236e-05, -2.1450e-06,  5.0586e-05, -5.4256e-06,\n",
       "                       -1.6500e-05, -2.5635e-05, -7.3973e-05, -2.7957e-05, -2.5788e-05,\n",
       "                       -2.3111e-05,  1.0915e-06, -4.1539e-05,  9.3469e-06,  6.3956e-07,\n",
       "                        1.0264e-05,  9.2556e-06,  2.4093e-05,  3.9909e-05, -7.1425e-05,\n",
       "                        1.1722e-05, -6.6994e-06,  1.2229e-05, -1.0588e-04, -3.7606e-05,\n",
       "                       -2.8187e-05,  9.3681e-05,  2.1290e-05,  2.6317e-05, -3.7702e-05,\n",
       "                        5.4658e-06, -8.9180e-06, -8.5006e-08, -2.4837e-05,  1.4544e-05,\n",
       "                       -5.0152e-06,  5.9595e-06,  1.0830e-05, -4.6397e-05,  8.7463e-05,\n",
       "                       -2.2675e-06,  4.3275e-05,  1.1086e-05,  1.6485e-05, -3.6259e-05,\n",
       "                        1.1657e-05, -2.8464e-05, -1.4426e-05, -3.0550e-07,  5.8997e-05,\n",
       "                       -1.2027e-04,  5.7587e-05, -6.8657e-05, -2.7564e-05,  2.2999e-05,\n",
       "                       -2.8612e-05, -1.4204e-04,  3.2883e-11,  9.0800e-06, -8.7231e-05,\n",
       "                        4.3347e-05,  6.1474e-05,  3.1957e-08, -1.7775e-05,  1.3384e-05,\n",
       "                        8.8116e-05, -3.2787e-05, -1.6790e-06,  3.0065e-05,  1.1317e-05,\n",
       "                       -6.2813e-05, -4.9765e-05,  4.6875e-06,  4.0287e-05,  8.7538e-06,\n",
       "                        1.2088e-05, -8.4215e-06,  2.6617e-05, -1.8096e-05, -8.3361e-06,\n",
       "                        6.2211e-06,  3.5478e-05, -1.2395e-07,  2.8924e-05,  8.4677e-05,\n",
       "                        2.6372e-05,  5.6342e-06, -1.9853e-05,  7.4075e-06, -1.1090e-05,\n",
       "                        3.6349e-05,  3.5301e-05,  1.7749e-05,  6.1878e-05, -9.1290e-06,\n",
       "                        3.6832e-06, -4.2189e-05, -2.8849e-08], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.bias',\n",
       "               tensor([-2.8877e-01, -2.5261e-06, -2.3193e-01, -3.2576e-01, -3.3149e-01,\n",
       "                       -2.6402e-01, -3.9454e-01, -3.4977e-01, -1.5933e-01, -2.4774e-01,\n",
       "                       -7.4630e-02, -2.6290e-01, -3.8616e-01, -3.6579e-01, -2.9344e-01,\n",
       "                       -2.4617e-01, -2.1937e-01, -4.2036e-01, -3.9470e-01, -4.7105e-01,\n",
       "                       -3.6036e-01, -2.9724e-01, -4.5556e-01, -2.4364e-01, -1.9501e-01,\n",
       "                       -3.0994e-01, -2.0532e-01, -3.4752e-01, -2.5188e-01, -2.1788e-01,\n",
       "                       -4.9082e-01, -4.9991e-01, -2.9614e-01, -2.6678e-01, -2.2864e-01,\n",
       "                       -2.9664e-01, -2.5535e-01, -3.1973e-01, -3.5434e-01, -2.7234e-01,\n",
       "                       -2.4243e-01, -3.8635e-01, -2.6127e-01, -2.3128e-01, -3.1675e-01,\n",
       "                       -3.2966e-01, -2.5050e-01, -3.6137e-01, -2.7179e-01, -3.7908e-01,\n",
       "                       -6.3067e-01, -2.7521e-01, -4.2928e-01, -2.6467e-01, -3.9084e-01,\n",
       "                       -3.4539e-01, -3.0123e-01, -3.4827e-01, -3.1838e-01, -2.7537e-01,\n",
       "                       -3.3725e-01, -2.7659e-01, -3.7621e-02, -4.2970e-01, -3.1315e-01,\n",
       "                       -1.9243e-01, -3.1630e-01, -3.1210e-01, -2.2573e-01, -3.5831e-01,\n",
       "                       -2.9017e-01, -3.8468e-01, -3.2735e-01, -2.7492e-01, -1.4545e-01,\n",
       "                       -2.6132e-01, -3.3288e-01, -2.8789e-01, -9.5447e-03, -2.3727e-01,\n",
       "                       -3.6506e-01, -3.1933e-01, -3.4813e-01, -2.3968e-01, -2.3048e-01,\n",
       "                       -3.2235e-01, -4.5712e-01, -8.2832e-11, -3.7387e-02, -4.2282e-01,\n",
       "                       -2.6802e-01, -4.1776e-01, -3.5843e-02, -2.3747e-01, -3.4734e-01,\n",
       "                       -3.4931e-01, -3.9029e-01, -2.8163e-01, -3.4240e-01, -6.5988e-01,\n",
       "                       -4.1117e-01, -3.7355e-01, -2.6374e-01, -2.5283e-01, -2.4225e-01,\n",
       "                       -3.8587e-01, -3.4695e-01, -1.7012e-01, -3.5898e-01, -2.2599e-01,\n",
       "                       -3.6073e-01, -3.1948e-01, -2.3852e-01, -3.0432e-01, -2.8615e-01,\n",
       "                       -4.8673e-01, -2.4185e-01, -2.8005e-01, -2.2292e-01, -3.3806e-01,\n",
       "                       -3.4269e-01, -2.2614e-01, -2.9892e-01, -2.8446e-01, -2.6000e-01,\n",
       "                       -2.8056e-01, -3.4189e-01, -6.5306e-07], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.weight',\n",
       "               tensor([ 2.0678e-01, -9.5074e-05,  2.4242e-01,  2.2641e-01,  3.3979e-01,\n",
       "                        2.6554e-01,  2.6870e-01,  3.4303e-01,  2.5907e-01,  2.5934e-01,\n",
       "                       -4.6513e-03,  2.5524e-01,  2.8665e-01,  3.4962e-01,  3.4227e-01,\n",
       "                        2.6426e-01,  2.6422e-01,  3.8013e-01,  2.7958e-01,  2.6097e-01,\n",
       "                        2.3944e-01,  2.8393e-01,  3.0921e-01,  1.5783e-01,  2.8303e-01,\n",
       "                        2.8895e-01,  2.6113e-01,  3.4879e-01,  1.7416e-01,  2.5073e-01,\n",
       "                        2.9264e-01,  5.9240e-01,  2.2539e-01,  2.4922e-01,  2.7103e-01,\n",
       "                        3.2336e-01,  2.5207e-01,  2.5241e-01,  2.6298e-01,  2.5556e-01,\n",
       "                        2.8258e-01,  2.3481e-01,  2.4579e-01,  2.1036e-01,  2.8397e-01,\n",
       "                        1.4527e-01,  2.1243e-01,  3.3618e-01,  2.9245e-01,  2.8892e-01,\n",
       "                        3.2394e-01,  3.4894e-01,  3.8505e-01,  2.3979e-01,  3.7395e-01,\n",
       "                        1.9660e-01,  2.6565e-01,  3.2480e-01,  2.3506e-01,  2.9744e-01,\n",
       "                        1.9628e-01,  2.0955e-01,  4.4421e-03,  2.8274e-01,  1.9789e-01,\n",
       "                        2.1098e-01,  2.7745e-01,  1.3352e-01,  2.7085e-01,  2.2034e-01,\n",
       "                        2.9111e-01,  3.1565e-01,  2.2906e-01,  1.7416e-01,  1.3961e-01,\n",
       "                        2.5378e-01,  3.2304e-01,  2.6205e-01, -9.1787e-04,  2.0702e-01,\n",
       "                        2.3118e-01,  3.5330e-01,  3.2838e-01,  2.5715e-01,  1.6050e-01,\n",
       "                        2.2387e-01,  2.7300e-01, -5.7141e-09, -3.0046e-03,  3.7713e-01,\n",
       "                        2.4274e-01,  3.8799e-01, -5.8702e-03,  2.5189e-01,  3.2774e-01,\n",
       "                        3.2763e-01,  2.8939e-01,  1.0957e-01,  3.5117e-01,  3.8153e-01,\n",
       "                        4.3353e-01,  4.1552e-01,  2.5881e-01,  1.9501e-01,  2.3573e-01,\n",
       "                        2.8571e-01,  2.6685e-01,  2.8701e-01,  3.0291e-01,  2.8970e-01,\n",
       "                        2.1864e-01,  3.0988e-01,  2.4018e-01,  3.0244e-01,  2.7472e-01,\n",
       "                        3.7712e-01,  2.5809e-01,  2.9308e-01,  3.1725e-01,  3.5354e-01,\n",
       "                        2.9664e-01,  1.9477e-01,  1.8307e-01,  3.1260e-01,  2.4936e-01,\n",
       "                        2.5207e-01,  3.4603e-01, -1.0371e-04], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.weight',\n",
       "               tensor([[[[ 7.0982e-02,  7.9410e-02,  3.0355e-03],\n",
       "                         [ 1.7107e-02,  6.1006e-02,  7.4279e-02],\n",
       "                         [ 1.7022e-02, -6.4248e-03,  5.5946e-02]],\n",
       "               \n",
       "                        [[ 9.8332e-09,  1.8531e-08, -8.8301e-09],\n",
       "                         [-1.0304e-08,  3.0064e-10, -2.3638e-08],\n",
       "                         [-5.9089e-09, -9.8482e-09,  1.6421e-08]],\n",
       "               \n",
       "                        [[ 5.0339e-02,  2.7086e-02, -5.3247e-02],\n",
       "                         [ 4.2114e-02,  1.0880e-02, -7.0717e-02],\n",
       "                         [-2.9786e-03,  2.6186e-03,  7.5392e-04]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.0683e-02,  2.1060e-03,  4.0872e-02],\n",
       "                         [-9.6171e-02, -6.6975e-02, -1.0878e-02],\n",
       "                         [-6.9846e-02, -3.2795e-02,  4.5723e-03]],\n",
       "               \n",
       "                        [[-4.9733e-02, -3.4573e-02, -4.4569e-02],\n",
       "                         [-6.5452e-02, -6.2136e-02, -4.1287e-02],\n",
       "                         [-7.9759e-02, -4.9359e-02, -9.6032e-02]],\n",
       "               \n",
       "                        [[ 2.7246e-09,  4.5237e-09, -2.5491e-09],\n",
       "                         [-2.8867e-09,  6.8706e-12, -5.1977e-09],\n",
       "                         [ 4.0512e-09,  5.1350e-09,  6.8290e-09]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 6.5778e-02,  5.8882e-02, -3.1478e-02],\n",
       "                         [ 1.6190e-02, -4.6832e-02, -3.2797e-02],\n",
       "                         [ 5.3279e-02, -2.5976e-03, -3.3902e-02]],\n",
       "               \n",
       "                        [[-4.1491e-08, -1.6706e-08, -7.9568e-08],\n",
       "                         [-4.4218e-08,  6.9352e-10, -9.3721e-08],\n",
       "                         [-1.6869e-07, -1.3960e-07, -2.2876e-07]],\n",
       "               \n",
       "                        [[ 5.7947e-02, -2.1033e-02,  7.8558e-02],\n",
       "                         [-7.4864e-02, -5.4443e-02, -8.8665e-03],\n",
       "                         [ 6.9208e-03,  4.1272e-02,  9.1094e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-5.9438e-02, -1.4834e-04, -1.8204e-02],\n",
       "                         [ 8.5289e-03, -3.5314e-02,  4.3399e-02],\n",
       "                         [-9.7253e-03, -6.2594e-02,  1.6519e-02]],\n",
       "               \n",
       "                        [[-5.3520e-02,  8.3235e-02,  5.9917e-02],\n",
       "                         [ 3.3629e-02,  4.4292e-02,  3.9087e-02],\n",
       "                         [-3.4302e-03,  8.4357e-02, -2.4349e-02]],\n",
       "               \n",
       "                        [[-2.3189e-08, -4.6250e-09, -2.7872e-08],\n",
       "                         [-1.4601e-08,  1.7770e-10, -2.8017e-08],\n",
       "                         [-4.6100e-08, -3.8590e-08, -6.2809e-08]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 6.8090e-03,  8.3769e-03,  2.6661e-03],\n",
       "                         [-1.6320e-02, -3.2914e-02, -5.5196e-03],\n",
       "                         [ 8.1040e-03, -2.2148e-02, -1.6673e-02]],\n",
       "               \n",
       "                        [[-1.2260e-07,  3.7368e-08, -4.7756e-08],\n",
       "                         [-2.0057e-07,  2.4082e-09, -1.2699e-07],\n",
       "                         [ 1.5508e-07,  1.7639e-08,  1.7988e-08]],\n",
       "               \n",
       "                        [[ 8.2727e-02,  6.7308e-02, -4.5194e-02],\n",
       "                         [ 5.2855e-02,  8.8404e-03, -1.0772e-01],\n",
       "                         [ 4.8017e-02,  5.9041e-02, -3.1666e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-5.0887e-02,  1.8263e-02, -5.6861e-02],\n",
       "                         [-1.1412e-02,  6.1381e-02,  1.9727e-02],\n",
       "                         [ 1.7353e-02,  8.5976e-02,  2.1617e-02]],\n",
       "               \n",
       "                        [[ 2.4995e-02,  8.3277e-03, -5.3684e-02],\n",
       "                         [ 1.2380e-01,  5.6386e-02,  2.0702e-02],\n",
       "                         [ 7.4589e-02,  4.5642e-02,  1.6249e-02]],\n",
       "               \n",
       "                        [[ 3.6409e-08,  1.4030e-08,  2.9305e-08],\n",
       "                         [ 3.6743e-08,  7.1305e-10,  1.5890e-08],\n",
       "                         [ 8.7513e-08,  4.9962e-08,  5.8368e-08]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-3.3791e-02, -4.3370e-03, -1.8219e-02],\n",
       "                         [-4.4120e-03,  2.7360e-02, -4.1297e-02],\n",
       "                         [ 3.9245e-02,  3.7069e-02, -4.2522e-02]],\n",
       "               \n",
       "                        [[-1.3034e-07, -1.2226e-07, -1.6549e-07],\n",
       "                         [-7.8189e-08,  2.8045e-09, -3.9921e-08],\n",
       "                         [-1.5518e-07, -1.4035e-07, -1.7120e-07]],\n",
       "               \n",
       "                        [[ 1.8813e-02,  1.0196e-01,  9.2164e-02],\n",
       "                         [ 1.0857e-01,  1.9857e-02,  8.9373e-02],\n",
       "                         [-2.3130e-02, -1.0688e-02, -8.9704e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-6.7933e-02, -7.3628e-02, -9.5557e-02],\n",
       "                         [-6.8063e-02, -4.6807e-02, -4.3664e-02],\n",
       "                         [-5.2275e-02, -7.1555e-02, -5.7333e-02]],\n",
       "               \n",
       "                        [[-2.4382e-02, -6.2317e-03,  6.2511e-03],\n",
       "                         [ 9.1717e-03, -5.0477e-02,  8.7841e-03],\n",
       "                         [-4.1638e-02, -7.3323e-02, -6.7270e-02]],\n",
       "               \n",
       "                        [[-5.5694e-08, -2.9984e-08, -4.0238e-08],\n",
       "                         [-1.9129e-08,  9.3745e-10, -1.0803e-08],\n",
       "                         [-7.9667e-08, -6.3053e-08, -5.6711e-08]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 5.2338e-02,  6.0799e-02,  3.1711e-02],\n",
       "                         [ 1.2327e-02,  1.6936e-02,  5.8571e-03],\n",
       "                         [ 1.7760e-02,  2.9862e-02,  5.8698e-03]],\n",
       "               \n",
       "                        [[ 3.5637e-08,  6.9852e-08,  3.5410e-08],\n",
       "                         [-2.6170e-08,  2.1785e-09, -4.0555e-08],\n",
       "                         [-1.0676e-07, -8.9558e-08, -1.2063e-07]],\n",
       "               \n",
       "                        [[ 1.5593e-02,  3.3697e-02, -1.8942e-02],\n",
       "                         [ 1.3374e-02,  2.6558e-03,  2.1688e-02],\n",
       "                         [ 1.6505e-02, -8.2735e-03,  2.9050e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 9.2746e-03,  6.8915e-02,  5.2035e-02],\n",
       "                         [ 3.4178e-03,  7.4335e-02,  6.6590e-02],\n",
       "                         [ 1.3377e-02,  5.1529e-02,  8.9151e-02]],\n",
       "               \n",
       "                        [[-1.8626e-03,  1.7149e-03, -1.9535e-02],\n",
       "                         [-6.0669e-02, -2.7301e-02, -5.9660e-03],\n",
       "                         [-5.5495e-03, -1.2876e-04, -2.1121e-02]],\n",
       "               \n",
       "                        [[ 1.1242e-08,  1.7249e-08,  9.2259e-09],\n",
       "                         [-3.0355e-09,  6.2973e-10, -1.0758e-08],\n",
       "                         [-2.2272e-08, -2.2997e-08, -3.0779e-08]]],\n",
       "               \n",
       "               \n",
       "                       [[[-8.2447e-02, -8.5969e-02, -4.0496e-02],\n",
       "                         [ 1.4534e-04,  4.7732e-03, -1.2706e-02],\n",
       "                         [ 7.4253e-03,  4.7436e-02, -1.9959e-02]],\n",
       "               \n",
       "                        [[-2.5602e-08, -1.0256e-07, -3.6268e-07],\n",
       "                         [ 1.0315e-07,  1.8022e-09, -2.0729e-07],\n",
       "                         [ 1.2862e-07,  9.3515e-08, -1.1672e-07]],\n",
       "               \n",
       "                        [[ 2.4610e-02,  3.8200e-02,  9.2711e-03],\n",
       "                         [ 4.5607e-02,  6.4954e-03, -1.6531e-02],\n",
       "                         [-3.7691e-02, -8.3063e-02, -5.9664e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 7.9180e-02,  1.4805e-01,  1.7302e-01],\n",
       "                         [ 2.1721e-02,  7.2818e-02,  9.3465e-02],\n",
       "                         [ 7.9417e-03, -1.2020e-02, -3.9898e-02]],\n",
       "               \n",
       "                        [[-9.3138e-03, -3.0494e-02, -4.3417e-02],\n",
       "                         [-1.4944e-02, -8.8690e-02, -1.0930e-02],\n",
       "                         [-1.4744e-02, -5.5430e-02, -2.9767e-02]],\n",
       "               \n",
       "                        [[ 8.2815e-08,  3.1840e-09, -6.1195e-08],\n",
       "                         [ 6.1240e-08,  1.0097e-09, -6.4833e-08],\n",
       "                         [ 9.6884e-09, -5.7379e-08, -3.8444e-08]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.bias',\n",
       "               tensor([-4.9818e-05, -4.9128e-05, -4.9026e-06,  6.0460e-05, -1.2960e-06,\n",
       "                       -2.6684e-05, -2.4007e-05, -8.2047e-06,  4.2040e-10,  1.1959e-05,\n",
       "                       -2.8109e-12, -1.9603e-09, -6.5203e-05,  3.6199e-05, -2.9295e-07,\n",
       "                        4.0757e-05,  5.3992e-06, -6.6394e-06, -3.2356e-05,  8.4213e-06,\n",
       "                        4.1439e-10, -5.6632e-05, -4.2017e-06,  4.5048e-09, -4.3890e-05,\n",
       "                       -1.8752e-05, -1.1911e-04, -8.0684e-05,  4.0105e-06,  2.6982e-05,\n",
       "                        5.7867e-06, -4.7395e-06,  4.6435e-05,  7.5639e-05,  4.8089e-08,\n",
       "                        3.0158e-05, -5.1838e-05,  4.7928e-05, -4.9714e-05, -1.5330e-07,\n",
       "                       -1.7552e-05,  4.0345e-09,  4.6622e-05, -3.5057e-05, -4.4895e-05,\n",
       "                       -3.8652e-12, -1.7395e-05, -1.3697e-05, -3.8390e-08,  1.0753e-11,\n",
       "                        5.2107e-05,  3.0900e-15,  2.4130e-05,  4.3603e-05, -1.3968e-05,\n",
       "                       -1.0691e-05,  1.7734e-05, -3.9420e-06, -4.3011e-05, -5.0145e-05,\n",
       "                       -5.9573e-07,  6.1723e-05,  8.4465e-05, -2.3809e-05, -1.2263e-05,\n",
       "                        4.9214e-05,  9.1012e-11,  2.8544e-05,  1.0471e-04, -4.6739e-06,\n",
       "                       -5.4063e-05, -1.4323e-05, -5.6934e-05, -3.9008e-06,  2.7981e-05,\n",
       "                        2.1995e-05, -1.0173e-15, -1.1939e-05,  7.1817e-13,  2.2785e-05,\n",
       "                        3.8789e-06, -1.9016e-05,  1.8728e-06,  4.1041e-05, -1.0123e-08,\n",
       "                       -7.0457e-05, -6.7310e-05, -3.2320e-05,  4.9395e-05, -7.2372e-05,\n",
       "                        2.2037e-05, -2.7530e-05, -6.8682e-05, -2.7555e-05, -7.6898e-05,\n",
       "                       -1.1269e-04, -2.7107e-06,  4.7947e-08,  2.1195e-05, -1.3600e-04,\n",
       "                       -9.9001e-07, -1.1858e-05, -2.6857e-05, -8.7576e-05,  8.7753e-14,\n",
       "                        5.7057e-05, -4.0086e-05,  6.4778e-05, -6.2133e-05, -4.0435e-05,\n",
       "                       -2.9529e-05,  5.2916e-07, -2.2138e-05,  4.9253e-05, -1.1146e-04,\n",
       "                       -1.4121e-05, -4.2397e-06, -4.1352e-05, -1.3730e-05,  3.7409e-06,\n",
       "                        2.3817e-05,  1.8746e-08,  1.2473e-12, -6.3390e-06, -3.4651e-05,\n",
       "                        6.8277e-05,  3.2506e-05,  1.6163e-04], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.bias',\n",
       "               tensor([-3.1727e-01, -2.1232e-01, -3.7658e-01, -6.2612e-01, -8.1702e-06,\n",
       "                       -4.9993e-01, -2.9261e-01, -1.4359e-01, -4.8696e-13, -1.2529e-04,\n",
       "                       -1.2306e-06, -1.9392e-03, -2.4268e-01, -3.7255e-01, -3.3178e-03,\n",
       "                       -2.7396e-01, -2.2390e-01, -2.3781e-01, -2.1436e-01, -2.9339e-01,\n",
       "                       -1.6199e-09, -1.1930e-01, -1.0736e-03, -1.2399e-04, -2.2485e-01,\n",
       "                       -3.9703e-01, -2.8769e-01, -2.7503e-01, -2.7306e-01, -2.5825e-01,\n",
       "                       -4.6269e-01, -5.9403e-01, -9.9626e-02, -4.3996e-01, -2.6369e-01,\n",
       "                       -4.0215e-01, -5.7431e-01, -2.6101e-01, -4.6382e-01, -3.6287e-14,\n",
       "                       -5.6752e-04, -5.9543e-03, -2.6546e-01, -1.7982e-01, -2.6815e-01,\n",
       "                       -3.3883e-08, -2.7374e-01, -4.2947e-01, -8.3639e-07, -6.9047e-05,\n",
       "                       -6.1529e-01, -2.4836e-09, -3.3099e-01, -4.2239e-01, -2.2350e-01,\n",
       "                       -1.6356e-01, -2.7682e-01, -2.5305e-01, -2.1428e-01, -3.6047e-01,\n",
       "                       -6.4510e-04, -5.7068e-02, -5.3824e-01,  1.0521e-01, -3.4511e-01,\n",
       "                       -2.1750e-01, -5.0276e-32, -1.9979e-01, -2.0614e-01, -2.0909e-01,\n",
       "                       -3.2428e-01, -4.5414e-01, -5.3133e-01, -1.3205e-28, -3.0260e-01,\n",
       "                       -4.0488e-01, -1.7121e-04, -1.4692e-01, -1.4711e-17, -1.3676e-01,\n",
       "                       -3.2487e-01, -2.0687e-01, -3.1969e-06, -2.5972e-01, -1.7248e-19,\n",
       "                       -4.9652e-02, -2.5657e-01, -2.9610e-01, -3.3691e-01, -3.6858e-01,\n",
       "                       -3.4705e-01, -2.2554e-01, -2.5759e-01, -3.2570e-01, -4.2677e-01,\n",
       "                       -3.5633e-01, -5.1244e-01, -2.7078e-11, -3.3285e-01, -3.3559e-01,\n",
       "                       -6.1424e-03, -1.8606e-01, -2.7248e-01, -3.6253e-01, -2.3002e-10,\n",
       "                       -2.0330e-01, -3.1376e-01, -3.8102e-01, -4.0893e-01, -3.1467e-01,\n",
       "                       -2.5545e-01, -5.6661e-01, -3.0435e-01, -4.0062e-01, -4.6427e-01,\n",
       "                       -6.8853e-02, -4.1148e-01, -3.3666e-01, -6.7905e-08, -2.3305e-01,\n",
       "                       -1.6369e-01, -9.9178e-23, -2.0268e-22, -3.3787e-14, -2.4897e-01,\n",
       "                       -3.3798e-01, -2.8520e-01, -4.5763e-01], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.weight',\n",
       "               tensor([ 2.7527e-01,  2.4636e-01,  2.6848e-01,  4.5465e-01,  1.0100e-08,\n",
       "                        4.1025e-01,  2.3395e-01,  2.1365e-01,  3.7873e-20, -1.3704e-03,\n",
       "                        9.1009e-04, -2.8100e-03,  1.7035e-01,  2.8924e-01, -2.9825e-03,\n",
       "                        2.5310e-01,  2.5050e-01,  1.8335e-01,  2.1503e-01,  2.7216e-01,\n",
       "                        2.5592e-08,  2.0786e-01,  1.1159e-07,  6.0587e-04,  1.6532e-01,\n",
       "                        3.6026e-01,  2.8183e-01,  2.8791e-01,  2.8258e-01,  2.2325e-01,\n",
       "                        3.9285e-01,  4.4712e-01,  1.6823e-01,  4.4531e-01,  2.3329e-01,\n",
       "                        3.6514e-01,  3.7981e-01,  2.5525e-01,  4.0061e-01,  3.0705e-21,\n",
       "                        1.7587e-03, -2.6958e-03,  2.9358e-01,  1.7412e-01,  2.8113e-01,\n",
       "                       -2.1205e-06,  3.1851e-01,  3.9157e-01, -1.6880e-04,  2.1628e-04,\n",
       "                        5.2849e-01, -3.3207e-07,  3.2059e-01,  3.9118e-01,  2.4038e-01,\n",
       "                        2.3104e-01,  2.5519e-01,  1.7289e-01,  1.9927e-01,  3.3068e-01,\n",
       "                        3.2032e-04,  1.4664e-01,  4.5794e-01,  2.1653e-01,  3.1212e-01,\n",
       "                        2.4167e-01,  1.0193e-37,  2.4424e-01,  2.1541e-01,  2.1907e-01,\n",
       "                        2.7524e-01,  3.5053e-01,  4.4657e-01,  9.8704e-31,  2.7408e-01,\n",
       "                        3.7310e-01, -1.2008e-10,  1.3076e-01, -1.2253e-12,  1.8733e-01,\n",
       "                        3.3040e-01,  1.8623e-01,  9.5487e-07,  2.0028e-01, -3.3073e-26,\n",
       "                        1.8495e-01,  2.6064e-01,  2.6635e-01,  3.0643e-01,  3.2801e-01,\n",
       "                        3.1806e-01,  2.1947e-01,  2.6013e-01,  2.4732e-01,  3.6779e-01,\n",
       "                        3.4801e-01,  3.9539e-01, -1.8914e-06,  3.0592e-01,  3.1476e-01,\n",
       "                       -2.8300e-03,  2.0191e-01,  2.2849e-01,  2.8560e-01, -6.5300e-16,\n",
       "                        2.5745e-01,  2.4548e-01,  2.9317e-01,  3.4889e-01,  2.5692e-01,\n",
       "                        2.7356e-01,  5.0862e-01,  2.1172e-01,  3.4361e-01,  3.8585e-01,\n",
       "                        1.8967e-01,  3.8773e-01,  3.6571e-01, -1.2569e-06,  2.3266e-01,\n",
       "                        2.1316e-01,  4.6101e-17,  5.5050e-17, -1.0791e-20,  2.1346e-01,\n",
       "                        2.9702e-01,  2.2017e-01,  4.3030e-01], device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.weights',\n",
       "               tensor([[-0.0041,  0.0071,  0.0054,  ...,  0.0027, -0.0167, -0.0264],\n",
       "                       [-0.0084, -0.0043, -0.0170,  ...,  0.0094,  0.0111,  0.0355],\n",
       "                       [ 0.0060, -0.0006,  0.0064,  ...,  0.0039,  0.0240,  0.0125],\n",
       "                       [-0.0038,  0.0021,  0.0006,  ...,  0.0204, -0.0059, -0.0324],\n",
       "                       [ 0.0109, -0.0025,  0.0067,  ..., -0.0347, -0.0119,  0.0107]],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.bias',\n",
       "               tensor([ 0.0234,  0.0147,  0.0096,  0.0109, -0.0048], device='cuda:0')),\n",
       "              ('classifier.prompt.prompt_dict.pad_up',\n",
       "               tensor([[[ 0.0211, -0.0003,  0.0142,  ...,  0.0298,  0.0178, -0.0118],\n",
       "                        [ 0.0163, -0.0321,  0.0165,  ...,  0.0308,  0.0045, -0.0201],\n",
       "                        [-0.0089, -0.0224, -0.0356,  ..., -0.0141, -0.0025, -0.0224],\n",
       "                        [ 0.0038, -0.0367,  0.0503,  ...,  0.0184,  0.0320,  0.0073],\n",
       "                        [ 0.0135, -0.0361,  0.0317,  ...,  0.0371,  0.0104, -0.0169]],\n",
       "               \n",
       "                       [[ 0.0166,  0.0089,  0.0254,  ..., -0.0175, -0.0008,  0.0017],\n",
       "                        [ 0.0201, -0.0236,  0.0310,  ..., -0.0140, -0.0252, -0.0071],\n",
       "                        [-0.0153,  0.0114, -0.0301,  ...,  0.0003,  0.0054, -0.0027],\n",
       "                        [ 0.0092,  0.0151,  0.0174,  ..., -0.0264, -0.0394,  0.0003],\n",
       "                        [ 0.0417,  0.0184, -0.0012,  ..., -0.0327, -0.0337, -0.0114]],\n",
       "               \n",
       "                       [[ 0.0013, -0.0131, -0.0136,  ...,  0.0044,  0.0124,  0.0199],\n",
       "                        [ 0.0162, -0.0046,  0.0227,  ...,  0.0100,  0.0043,  0.0168],\n",
       "                        [-0.0069,  0.0192, -0.0164,  ...,  0.0120,  0.0186, -0.0220],\n",
       "                        [ 0.0116,  0.0023,  0.0191,  ...,  0.0029,  0.0151, -0.0084],\n",
       "                        [ 0.0139,  0.0179,  0.0226,  ..., -0.0025,  0.0208, -0.0295]]],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.prompt.prompt_dict.pad_down',\n",
       "               tensor([[[-0.0021, -0.0166, -0.0048,  ..., -0.0183,  0.0040, -0.0075],\n",
       "                        [-0.0062, -0.0431,  0.0394,  ...,  0.0165, -0.0243, -0.0322],\n",
       "                        [-0.0035,  0.0125,  0.0212,  ...,  0.0195, -0.0094, -0.0054],\n",
       "                        [-0.0110, -0.0078, -0.0032,  ...,  0.0228,  0.0218,  0.0151],\n",
       "                        [ 0.0023, -0.0058,  0.0238,  ..., -0.0066, -0.0245, -0.0361]],\n",
       "               \n",
       "                       [[-0.0109,  0.0114, -0.0407,  ..., -0.0319,  0.0378,  0.0172],\n",
       "                        [ 0.0055, -0.0345,  0.0244,  ...,  0.0098, -0.0128,  0.0109],\n",
       "                        [ 0.0028,  0.0244,  0.0051,  ...,  0.0171, -0.0291,  0.0015],\n",
       "                        [ 0.0048,  0.0040, -0.0212,  ...,  0.0098,  0.0048, -0.0024],\n",
       "                        [ 0.0133,  0.0026, -0.0244,  ...,  0.0443,  0.0241, -0.0326]],\n",
       "               \n",
       "                       [[ 0.0263,  0.0506, -0.0234,  ..., -0.0246,  0.0141, -0.0027],\n",
       "                        [ 0.0105, -0.0207,  0.0207,  ...,  0.0034,  0.0051, -0.0077],\n",
       "                        [ 0.0102,  0.0172, -0.0223,  ...,  0.0109,  0.0097, -0.0096],\n",
       "                        [ 0.0043,  0.0120, -0.0279,  ...,  0.0250,  0.0142, -0.0224],\n",
       "                        [-0.0059,  0.0121,  0.0088,  ...,  0.0052,  0.0109, -0.0288]]],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.prompt.prompt_dict.pad_left',\n",
       "               tensor([[[-0.0361,  0.0181,  0.0015,  0.0414, -0.0141],\n",
       "                        [ 0.0207, -0.0074,  0.0291, -0.0509, -0.0065],\n",
       "                        [-0.0296,  0.0392,  0.0126, -0.0215, -0.0064],\n",
       "                        ...,\n",
       "                        [ 0.0038, -0.0160,  0.0226, -0.0025, -0.0211],\n",
       "                        [-0.0221, -0.0242, -0.0216, -0.0005, -0.0028],\n",
       "                        [-0.0528,  0.0071,  0.0391,  0.0320, -0.0060]],\n",
       "               \n",
       "                       [[-0.0190,  0.0345, -0.0503,  0.0181, -0.0090],\n",
       "                        [ 0.0334, -0.0115, -0.0136, -0.0334,  0.0404],\n",
       "                        [-0.0287, -0.0378, -0.0499,  0.0153,  0.0276],\n",
       "                        ...,\n",
       "                        [ 0.0309,  0.0224,  0.0627,  0.0258, -0.0048],\n",
       "                        [ 0.0132, -0.0021, -0.0401, -0.0358, -0.0237],\n",
       "                        [-0.0207,  0.0401, -0.0444, -0.0585, -0.0076]],\n",
       "               \n",
       "                       [[-0.0392,  0.0214, -0.0197,  0.0056,  0.0014],\n",
       "                        [ 0.0247,  0.0004,  0.0198, -0.0185,  0.0238],\n",
       "                        [ 0.0099,  0.0355,  0.0057, -0.0061, -0.0209],\n",
       "                        ...,\n",
       "                        [ 0.0073, -0.0171,  0.0015,  0.0137, -0.0128],\n",
       "                        [-0.0094,  0.0088, -0.0004,  0.0176,  0.0181],\n",
       "                        [-0.0053,  0.0537,  0.0093,  0.0063,  0.0025]]], device='cuda:0')),\n",
       "              ('classifier.prompt.prompt_dict.pad_right',\n",
       "               tensor([[[ 0.0407,  0.0253, -0.0056, -0.0073, -0.0219],\n",
       "                        [-0.0675, -0.0263, -0.0154,  0.0511, -0.0253],\n",
       "                        [-0.0079,  0.0215,  0.0188,  0.0133, -0.0017],\n",
       "                        ...,\n",
       "                        [ 0.0204, -0.0219, -0.0286,  0.0491, -0.0422],\n",
       "                        [-0.0405,  0.0354, -0.0159,  0.0053, -0.0128],\n",
       "                        [ 0.0209,  0.0060, -0.0118,  0.0188,  0.0153]],\n",
       "               \n",
       "                       [[ 0.0175, -0.0550, -0.0023,  0.0306,  0.0410],\n",
       "                        [-0.0162, -0.0179, -0.0118,  0.0253, -0.0090],\n",
       "                        [ 0.0116,  0.0313,  0.0170, -0.0135, -0.0171],\n",
       "                        ...,\n",
       "                        [ 0.0131, -0.0344,  0.0073,  0.0704, -0.0151],\n",
       "                        [-0.0130,  0.0210, -0.0191,  0.0039,  0.0313],\n",
       "                        [ 0.0195, -0.0412, -0.0130,  0.0064, -0.0097]],\n",
       "               \n",
       "                       [[ 0.0241,  0.0059, -0.0169, -0.0118, -0.0412],\n",
       "                        [-0.0177,  0.0270, -0.0232,  0.0192, -0.0059],\n",
       "                        [ 0.0152,  0.0016, -0.0157,  0.0317,  0.0526],\n",
       "                        ...,\n",
       "                        [-0.0072,  0.0053,  0.0063,  0.0082, -0.0206],\n",
       "                        [-0.0608,  0.0606, -0.0013, -0.0226,  0.0287],\n",
       "                        [ 0.0149, -0.0083, -0.0204, -0.0085, -0.0053]]], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.prompt_learning_rates_dict.prompt_weight_learning_rate',\n",
       "               tensor([ 6.7735e-03, -3.5076e-04,  1.3716e-03,  8.3016e-03, -7.6450e-04,\n",
       "                        9.6921e-12], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv0-conv-weight',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv0-conv-bias',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv1-conv-weight',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv1-conv-bias',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv2-conv-weight',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv2-conv-bias',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv3-conv-weight',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv3-conv-bias',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-weights',\n",
       "               tensor([3.9216e-01, 5.4648e-01, 9.2367e-01, 1.1267e+00, 1.0611e+00, 9.6921e-12],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-bias',\n",
       "               tensor([ 1.7182e-01,  1.7953e-01,  3.6042e-02, -1.0476e-01,  9.4527e-02,\n",
       "                        9.6921e-12], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv0-conv-weight',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv0-conv-bias',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv1-conv-weight',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv1-conv-bias',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv2-conv-weight',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv2-conv-bias',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv3-conv-weight',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv3-conv-bias',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-linear-weights',\n",
       "               tensor([ 9.9289e-01, -2.0374e-01, -8.4855e-01, -1.0434e+00, -1.7972e+00,\n",
       "                        5.5578e-10], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-linear-bias',\n",
       "               tensor([ 2.2165e-01,  1.8181e-01,  1.1143e-01,  9.3132e-03, -2.5020e-02,\n",
       "                        5.5578e-10], device='cuda:0'))]),\n",
       " 'per_epoch_statistics': {'train_loss_mean': [1.6207496290206909,\n",
       "   1.342897901058197,\n",
       "   1.2236537531614304,\n",
       "   1.1348049457073213,\n",
       "   1.0828544614315032,\n",
       "   1.0395694247484206,\n",
       "   1.0173706741333008,\n",
       "   0.9757553768157959,\n",
       "   0.9500979747772217,\n",
       "   0.942631136059761,\n",
       "   0.8980446484088898,\n",
       "   0.8778141911029815,\n",
       "   0.8616352691650391,\n",
       "   0.8257936012744903,\n",
       "   0.8036870549321175,\n",
       "   0.7834847896695137,\n",
       "   0.7570608896613121,\n",
       "   0.7447124595046043,\n",
       "   0.7367639369368553,\n",
       "   0.7155400152802467,\n",
       "   0.6868605269789696,\n",
       "   0.6682883580327034,\n",
       "   0.6575315053462982,\n",
       "   0.6474029502272606,\n",
       "   0.6359921804070473,\n",
       "   0.6178860854506493,\n",
       "   0.6051021536588669,\n",
       "   0.6006010538339615,\n",
       "   0.5974101521372795,\n",
       "   0.6038241744041443,\n",
       "   0.5774149975180626,\n",
       "   0.5732839403152465,\n",
       "   0.5677141354382038,\n",
       "   0.5605494692325592,\n",
       "   0.5702561058998108,\n",
       "   0.5429059826731681,\n",
       "   0.5425041063427926,\n",
       "   0.5426579396724701,\n",
       "   0.545066031217575,\n",
       "   0.5318128537237644,\n",
       "   0.542967883437872,\n",
       "   0.5228942370414734,\n",
       "   0.5206523779928685,\n",
       "   0.5279693941175938,\n",
       "   0.5169165155887604,\n",
       "   0.5126333491206169,\n",
       "   0.5150745753347874,\n",
       "   0.50938992780447,\n",
       "   0.5055655215382576,\n",
       "   0.5101301383376121,\n",
       "   0.5067368417978286,\n",
       "   0.49460212591290476,\n",
       "   0.5119811763763428,\n",
       "   0.4939535201489925,\n",
       "   0.5019407892227172,\n",
       "   0.49968585413694383,\n",
       "   0.4969970774650574,\n",
       "   0.491454145014286,\n",
       "   0.48856047829985616,\n",
       "   0.4957058700621128,\n",
       "   0.4985882677733898,\n",
       "   0.4857126070857048,\n",
       "   0.47704756781458857,\n",
       "   0.47438314494490624,\n",
       "   0.48193382316827776,\n",
       "   0.4861598019897938,\n",
       "   0.4924478581547737,\n",
       "   0.4772479425370693,\n",
       "   0.479242424249649,\n",
       "   0.4835649347305298,\n",
       "   0.47209443771839144,\n",
       "   0.47544128581881523,\n",
       "   0.47873051118850707,\n",
       "   0.4695740088224411,\n",
       "   0.4737606158554554,\n",
       "   0.46875581809878347,\n",
       "   0.47342330414056777,\n",
       "   0.4708072311580181,\n",
       "   0.46047626322507856,\n",
       "   0.47161584442853927,\n",
       "   0.45594624188542365,\n",
       "   0.46462134128808974,\n",
       "   0.47184071227908136,\n",
       "   0.47357726147770884,\n",
       "   0.45588243955373764,\n",
       "   0.4646154716908932,\n",
       "   0.4604516882300377,\n",
       "   0.46534774380922317,\n",
       "   0.4551590910255909,\n",
       "   0.4500463908016682,\n",
       "   0.45917791962623594,\n",
       "   0.459131173402071,\n",
       "   0.44881688073277476,\n",
       "   0.4561172934472561,\n",
       "   0.45814833241701125,\n",
       "   0.4632396776378155,\n",
       "   0.4641384927034378,\n",
       "   0.4592018865644932,\n",
       "   0.4424883471429348,\n",
       "   0.4613002304434776,\n",
       "   0.44902970623970034,\n",
       "   0.44778686910867693,\n",
       "   0.451509437084198,\n",
       "   0.45878856068849566,\n",
       "   0.4579270626604557,\n",
       "   0.45507100349664686,\n",
       "   0.4498344167768955,\n",
       "   0.4561372907757759,\n",
       "   0.4372186378240585,\n",
       "   0.44489673498272897,\n",
       "   0.4528974254131317,\n",
       "   0.45654128047823905,\n",
       "   0.4438097078204155,\n",
       "   0.4511192974150181,\n",
       "   0.45328653666377067,\n",
       "   0.4539843057394028,\n",
       "   0.4459153451323509,\n",
       "   0.45172879257798193,\n",
       "   0.44730655324459073,\n",
       "   0.44623310589790344,\n",
       "   0.4495633260756731,\n",
       "   0.4389980199337006,\n",
       "   0.43544453498721125,\n",
       "   0.4446828138232231,\n",
       "   0.4471737766265869,\n",
       "   0.44192572477459907,\n",
       "   0.43888657763600347,\n",
       "   0.44132227611541747,\n",
       "   0.449049838244915,\n",
       "   0.4389493317604065,\n",
       "   0.4434012931883335,\n",
       "   0.43817851746082304,\n",
       "   0.4529563322067261,\n",
       "   0.42974509581923487,\n",
       "   0.4370175579190254,\n",
       "   0.4377596211135387,\n",
       "   0.43504974791407586,\n",
       "   0.4370754491090775,\n",
       "   0.4292725393176079,\n",
       "   0.43267275288701057,\n",
       "   0.4363987884670496,\n",
       "   0.4385294496119022,\n",
       "   0.4368993066847324,\n",
       "   0.4347342394590378,\n",
       "   0.43551832109689714,\n",
       "   0.437297875225544,\n",
       "   0.44219688612222674,\n",
       "   0.4326752769649029],\n",
       "  'train_loss_std': [0.5701254183200988,\n",
       "   0.12204418895870366,\n",
       "   0.13654178128566352,\n",
       "   0.1394452266483158,\n",
       "   0.14787013789240025,\n",
       "   0.14352702334361342,\n",
       "   0.1484441045627974,\n",
       "   0.1411010248003142,\n",
       "   0.15023887267729344,\n",
       "   0.14550382871616763,\n",
       "   0.14543687451459517,\n",
       "   0.1533186619104157,\n",
       "   0.14503134537057402,\n",
       "   0.15272660449559586,\n",
       "   0.14370392298366733,\n",
       "   0.14452262102728078,\n",
       "   0.13459980500043883,\n",
       "   0.14842590944590012,\n",
       "   0.14847778699538403,\n",
       "   0.1422503432780089,\n",
       "   0.1457361564797551,\n",
       "   0.1356326876368041,\n",
       "   0.14790061711979013,\n",
       "   0.14672857239756942,\n",
       "   0.13790628125241755,\n",
       "   0.13893919572314395,\n",
       "   0.1356942288849327,\n",
       "   0.1433650498074844,\n",
       "   0.14029708338306462,\n",
       "   0.14516633660659176,\n",
       "   0.14148226968704816,\n",
       "   0.14199564265487985,\n",
       "   0.13954572530040107,\n",
       "   0.1443576075116581,\n",
       "   0.1325772000601414,\n",
       "   0.13710205022987237,\n",
       "   0.13343932875813497,\n",
       "   0.14218156967366177,\n",
       "   0.14509140935841988,\n",
       "   0.13884201570289215,\n",
       "   0.1351642432662727,\n",
       "   0.1324296468172866,\n",
       "   0.13487797075971694,\n",
       "   0.13961403656866092,\n",
       "   0.13500739149627528,\n",
       "   0.13026662193399802,\n",
       "   0.1346854347931988,\n",
       "   0.13430363328828093,\n",
       "   0.13297564137535733,\n",
       "   0.13281899434801708,\n",
       "   0.1300347310398624,\n",
       "   0.1285025379663272,\n",
       "   0.13188600132362968,\n",
       "   0.13872244343137616,\n",
       "   0.1402513253000202,\n",
       "   0.13764885295876433,\n",
       "   0.1282814757161406,\n",
       "   0.1368383172644055,\n",
       "   0.1349302926110465,\n",
       "   0.12513697363443496,\n",
       "   0.1341942470559963,\n",
       "   0.13328847898929827,\n",
       "   0.13291781760841875,\n",
       "   0.13250915293971743,\n",
       "   0.13216485612737144,\n",
       "   0.12718338366989676,\n",
       "   0.13344520652459296,\n",
       "   0.12563239944275117,\n",
       "   0.13156584074475075,\n",
       "   0.1323348927462172,\n",
       "   0.12805550861187437,\n",
       "   0.12995734120029864,\n",
       "   0.13299024982574817,\n",
       "   0.13163904226180906,\n",
       "   0.13295422377890548,\n",
       "   0.12675829087654908,\n",
       "   0.13513858886441532,\n",
       "   0.12590118695546917,\n",
       "   0.12836102730202964,\n",
       "   0.1353924073321222,\n",
       "   0.12863425040269832,\n",
       "   0.12908041914720247,\n",
       "   0.13131768644598865,\n",
       "   0.13704373762645836,\n",
       "   0.13030337971576283,\n",
       "   0.12393749938320081,\n",
       "   0.1334796990076111,\n",
       "   0.13049857121780112,\n",
       "   0.12854013784589202,\n",
       "   0.1201523726904664,\n",
       "   0.12363445052060348,\n",
       "   0.1276398798246761,\n",
       "   0.12194959397191421,\n",
       "   0.1317483326776292,\n",
       "   0.1274694094966332,\n",
       "   0.13576485502559868,\n",
       "   0.12230545359418532,\n",
       "   0.12869998890598253,\n",
       "   0.12331387418604615,\n",
       "   0.13564906005986924,\n",
       "   0.12207328912676288,\n",
       "   0.1253247660627108,\n",
       "   0.12592473123232742,\n",
       "   0.1283814534563065,\n",
       "   0.12494338166925605,\n",
       "   0.12209753479104135,\n",
       "   0.12937258143279895,\n",
       "   0.12676454523203956,\n",
       "   0.12337921290202537,\n",
       "   0.13055563762571498,\n",
       "   0.12912062605628952,\n",
       "   0.12363725571621653,\n",
       "   0.1268686687411449,\n",
       "   0.12848764933989457,\n",
       "   0.13816004826701317,\n",
       "   0.12713648000820718,\n",
       "   0.1251445863130046,\n",
       "   0.12244531196223317,\n",
       "   0.13033385312955148,\n",
       "   0.13438971736079341,\n",
       "   0.1251080528407045,\n",
       "   0.11887901056962903,\n",
       "   0.12880950877700797,\n",
       "   0.1256116553071109,\n",
       "   0.1234636277813903,\n",
       "   0.12561706390131513,\n",
       "   0.12255022233890762,\n",
       "   0.12864124751194267,\n",
       "   0.12823032946697369,\n",
       "   0.11920847120512101,\n",
       "   0.12594644869594754,\n",
       "   0.12363017848855123,\n",
       "   0.1337680445848671,\n",
       "   0.12287802379283538,\n",
       "   0.13042965297618891,\n",
       "   0.13061527374957646,\n",
       "   0.12412405369365882,\n",
       "   0.12754218351426516,\n",
       "   0.13043165788763725,\n",
       "   0.11964840289787076,\n",
       "   0.1324266893213183,\n",
       "   0.12278431974551458,\n",
       "   0.12525726822910374,\n",
       "   0.1282588288573014,\n",
       "   0.12811982907459965,\n",
       "   0.12457849275292185,\n",
       "   0.13230363488567773,\n",
       "   0.12522783357079997],\n",
       "  'train_accuracy_mean': [0.3587600005865097,\n",
       "   0.4357333332300186,\n",
       "   0.5032133337855339,\n",
       "   0.5472533318400383,\n",
       "   0.571399999320507,\n",
       "   0.593893332362175,\n",
       "   0.6042266663908958,\n",
       "   0.6191999998688698,\n",
       "   0.6327066656351089,\n",
       "   0.6334799995422363,\n",
       "   0.6535066657066345,\n",
       "   0.6624933336377143,\n",
       "   0.6679733315706253,\n",
       "   0.6855599996447563,\n",
       "   0.6945066661834717,\n",
       "   0.7024666675329209,\n",
       "   0.7150266677141189,\n",
       "   0.7215733343362808,\n",
       "   0.7229733327627182,\n",
       "   0.7317333317995072,\n",
       "   0.7422933328151703,\n",
       "   0.7495466660261154,\n",
       "   0.7542399997711182,\n",
       "   0.758426666021347,\n",
       "   0.7614533331394195,\n",
       "   0.7683200001716614,\n",
       "   0.7751066666841507,\n",
       "   0.7755333337783813,\n",
       "   0.7765733320713043,\n",
       "   0.7736399972438812,\n",
       "   0.7858133326768875,\n",
       "   0.7869466661214829,\n",
       "   0.7884266664981842,\n",
       "   0.791453332901001,\n",
       "   0.7866666649580002,\n",
       "   0.7965066654682159,\n",
       "   0.7974533327817916,\n",
       "   0.7982000002861023,\n",
       "   0.7988000000715256,\n",
       "   0.8027999988794327,\n",
       "   0.7995599987506866,\n",
       "   0.8055199995040894,\n",
       "   0.8085599981546402,\n",
       "   0.8037200003862381,\n",
       "   0.808186667561531,\n",
       "   0.8096400002241134,\n",
       "   0.8087866654396058,\n",
       "   0.8107866662740707,\n",
       "   0.8126666661500931,\n",
       "   0.8109199995994568,\n",
       "   0.811146666765213,\n",
       "   0.8176266663074493,\n",
       "   0.8105199999809265,\n",
       "   0.818413333773613,\n",
       "   0.8138933327794075,\n",
       "   0.814626669049263,\n",
       "   0.817880000114441,\n",
       "   0.8183066660165786,\n",
       "   0.819506667137146,\n",
       "   0.8162133350372315,\n",
       "   0.8150933333635331,\n",
       "   0.8209333342313766,\n",
       "   0.8227333347797394,\n",
       "   0.82548000061512,\n",
       "   0.8220533343553543,\n",
       "   0.8196800009012223,\n",
       "   0.8174000000953674,\n",
       "   0.8237999992370606,\n",
       "   0.8242533326148986,\n",
       "   0.8194266667366028,\n",
       "   0.8264266659021378,\n",
       "   0.8245466675758362,\n",
       "   0.8257600005865097,\n",
       "   0.8263866684436798,\n",
       "   0.8249999984502793,\n",
       "   0.8262133328914643,\n",
       "   0.824599999666214,\n",
       "   0.8249466671943665,\n",
       "   0.8299866679906845,\n",
       "   0.8254933342933655,\n",
       "   0.8314533327817917,\n",
       "   0.8282533339262008,\n",
       "   0.8262266675233841,\n",
       "   0.8241866672039032,\n",
       "   0.832653333067894,\n",
       "   0.8280266671180725,\n",
       "   0.8292800015211106,\n",
       "   0.8286666665077209,\n",
       "   0.8327599998712539,\n",
       "   0.8347866673469544,\n",
       "   0.8303600004911422,\n",
       "   0.8302266664505005,\n",
       "   0.8348666681051254,\n",
       "   0.8310533353090286,\n",
       "   0.8307466675043106,\n",
       "   0.8285333335399627,\n",
       "   0.8281466666460037,\n",
       "   0.8315466672182084,\n",
       "   0.8377999999523162,\n",
       "   0.8303333330154419,\n",
       "   0.8332400012016297,\n",
       "   0.8358000000715256,\n",
       "   0.8339866666793824,\n",
       "   0.8308933335542679,\n",
       "   0.8305600000619888,\n",
       "   0.831053334236145,\n",
       "   0.8346533335447311,\n",
       "   0.831373334288597,\n",
       "   0.8402933336496353,\n",
       "   0.83721333360672,\n",
       "   0.8329200013875961,\n",
       "   0.8313733339309692,\n",
       "   0.8363866653442383,\n",
       "   0.8330266667604447,\n",
       "   0.8343466680049896,\n",
       "   0.8319733335971832,\n",
       "   0.8361333329677582,\n",
       "   0.8343600002527237,\n",
       "   0.8350533336400986,\n",
       "   0.8376400002241134,\n",
       "   0.8349733330011367,\n",
       "   0.838133334159851,\n",
       "   0.8399200019836426,\n",
       "   0.8350400011539459,\n",
       "   0.8360400000810623,\n",
       "   0.8379333338737488,\n",
       "   0.8384533344507218,\n",
       "   0.8381333335638046,\n",
       "   0.8354533343315125,\n",
       "   0.8386400017738342,\n",
       "   0.8366400010585785,\n",
       "   0.8374133343696595,\n",
       "   0.8326533340215683,\n",
       "   0.8401866685152054,\n",
       "   0.8395599998235702,\n",
       "   0.8385733330249786,\n",
       "   0.8391200014352799,\n",
       "   0.8394266667366028,\n",
       "   0.842733335018158,\n",
       "   0.8404400013685226,\n",
       "   0.8389200006723404,\n",
       "   0.8380933347940445,\n",
       "   0.8400800006389618,\n",
       "   0.8404666664600372,\n",
       "   0.8406133335828782,\n",
       "   0.840453333735466,\n",
       "   0.8380666667222977,\n",
       "   0.8412666673660278],\n",
       "  'train_accuracy_std': [0.07886962537844523,\n",
       "   0.07026881631922058,\n",
       "   0.07330232009775471,\n",
       "   0.07297374009062686,\n",
       "   0.07374125775426174,\n",
       "   0.07013667415318214,\n",
       "   0.07360194555771413,\n",
       "   0.0674492726844266,\n",
       "   0.07194231125979224,\n",
       "   0.07217925747774732,\n",
       "   0.06875829708255225,\n",
       "   0.07468634485948317,\n",
       "   0.06871311909258222,\n",
       "   0.07036364279680934,\n",
       "   0.06571352669097906,\n",
       "   0.06772135695184836,\n",
       "   0.0625180800333156,\n",
       "   0.06668942994247003,\n",
       "   0.06823328765854518,\n",
       "   0.06638018416321761,\n",
       "   0.06521951413795153,\n",
       "   0.061826054684348916,\n",
       "   0.06356851262778161,\n",
       "   0.06519860386072111,\n",
       "   0.06123995502771815,\n",
       "   0.0619298860180819,\n",
       "   0.0595312213898618,\n",
       "   0.06356837048170232,\n",
       "   0.06057861904919525,\n",
       "   0.0626496020383926,\n",
       "   0.061153219922533135,\n",
       "   0.061382673998520175,\n",
       "   0.0589049337871019,\n",
       "   0.06310149832790016,\n",
       "   0.05626761562408467,\n",
       "   0.06064007499233582,\n",
       "   0.05950614495660266,\n",
       "   0.06046527049290608,\n",
       "   0.06058954762159572,\n",
       "   0.057523559102717225,\n",
       "   0.0576617115894456,\n",
       "   0.055564344323803215,\n",
       "   0.05635575279725951,\n",
       "   0.05870401838739842,\n",
       "   0.0565606911593917,\n",
       "   0.056610593554967316,\n",
       "   0.05693695369507426,\n",
       "   0.056966296151998726,\n",
       "   0.05755384057447954,\n",
       "   0.05718700659795122,\n",
       "   0.05654827602657838,\n",
       "   0.05487167498010173,\n",
       "   0.05597615178459887,\n",
       "   0.05861412797011499,\n",
       "   0.05948256361450368,\n",
       "   0.05878184514393767,\n",
       "   0.05338201121694022,\n",
       "   0.059189145998295,\n",
       "   0.05666922498080967,\n",
       "   0.05175041882320637,\n",
       "   0.057144769593400216,\n",
       "   0.055863484860378634,\n",
       "   0.05722854871472283,\n",
       "   0.05535755356703228,\n",
       "   0.055795315046277834,\n",
       "   0.05438369738595403,\n",
       "   0.05695452484391348,\n",
       "   0.05175888684473089,\n",
       "   0.054930647533018184,\n",
       "   0.05648150382326039,\n",
       "   0.05281359415915145,\n",
       "   0.056190501377405694,\n",
       "   0.055682235954727884,\n",
       "   0.05518463466485158,\n",
       "   0.05494866432011799,\n",
       "   0.054783363088130664,\n",
       "   0.05564885985549246,\n",
       "   0.05528891334796946,\n",
       "   0.05487359151122599,\n",
       "   0.05797203518515245,\n",
       "   0.053133993598267405,\n",
       "   0.05464119716156778,\n",
       "   0.05624140273046057,\n",
       "   0.05778797343368325,\n",
       "   0.054676259451465294,\n",
       "   0.0530535946961993,\n",
       "   0.05678119266468469,\n",
       "   0.056170377603075125,\n",
       "   0.055032357707590886,\n",
       "   0.05116008862968624,\n",
       "   0.053167696176842624,\n",
       "   0.05363450184135096,\n",
       "   0.050336249849537974,\n",
       "   0.05649976953003563,\n",
       "   0.053312269959653824,\n",
       "   0.056076773381767295,\n",
       "   0.054001120022498864,\n",
       "   0.053819523618640855,\n",
       "   0.050512530741718895,\n",
       "   0.054666464212285576,\n",
       "   0.05141111172816182,\n",
       "   0.05248538617963892,\n",
       "   0.05194715228238676,\n",
       "   0.05460486299774985,\n",
       "   0.052636677623179876,\n",
       "   0.052143834965649095,\n",
       "   0.05478474987677001,\n",
       "   0.05398582789440922,\n",
       "   0.05095273044046717,\n",
       "   0.053321364428849646,\n",
       "   0.054643148990610194,\n",
       "   0.05308925955818002,\n",
       "   0.05268448521384365,\n",
       "   0.05403204302644353,\n",
       "   0.05639735968503305,\n",
       "   0.05396228603702185,\n",
       "   0.052616686181192446,\n",
       "   0.05088823695804636,\n",
       "   0.05621365673258605,\n",
       "   0.055566851955246126,\n",
       "   0.05252956317102217,\n",
       "   0.050313507918074984,\n",
       "   0.053218150432856076,\n",
       "   0.05330101817545723,\n",
       "   0.05142466622917108,\n",
       "   0.05139040152399645,\n",
       "   0.05256707105863284,\n",
       "   0.053891084251328734,\n",
       "   0.05479532700711681,\n",
       "   0.0502165918443237,\n",
       "   0.05126575011649125,\n",
       "   0.05138848048703028,\n",
       "   0.055826556963001706,\n",
       "   0.05353097575109976,\n",
       "   0.054589232794169065,\n",
       "   0.05443863218677491,\n",
       "   0.05252156297252018,\n",
       "   0.05353072255669786,\n",
       "   0.05616242486653854,\n",
       "   0.04992868069300265,\n",
       "   0.05585368112060202,\n",
       "   0.05137928979311616,\n",
       "   0.05503286166889159,\n",
       "   0.053451994439947435,\n",
       "   0.05379778648410453,\n",
       "   0.05106134828647597,\n",
       "   0.05534775132884393,\n",
       "   0.0514958053792535],\n",
       "  'train_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'train_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_learning_rate_mean': [0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005],\n",
       "  'train_learning_rate_std': [4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19],\n",
       "  'val_loss_mean': [1.441416900952657,\n",
       "   1.3846008117993673,\n",
       "   1.2682440356413522,\n",
       "   1.2167465635140737,\n",
       "   1.1956471415360768,\n",
       "   1.2200581177075704,\n",
       "   1.123721994360288,\n",
       "   1.1467954057455063,\n",
       "   1.0952849153677622,\n",
       "   1.0884990980227789,\n",
       "   1.0175517588853835,\n",
       "   1.0389101243019103,\n",
       "   1.0180739078919092,\n",
       "   1.0213970828056336,\n",
       "   1.0133472857872645,\n",
       "   1.0012899972995122,\n",
       "   0.993230233391126,\n",
       "   0.9507336417833964,\n",
       "   0.9401851975917817,\n",
       "   0.9469722998142243,\n",
       "   0.9226946310202281,\n",
       "   0.8991351246833801,\n",
       "   0.8960789090394974,\n",
       "   0.8867152986923853,\n",
       "   0.9207024757067362,\n",
       "   0.8809591494003932,\n",
       "   0.8881822508573533,\n",
       "   0.8737424963712692,\n",
       "   0.8496209345261256,\n",
       "   0.8854538373152415,\n",
       "   0.8618759785095851,\n",
       "   0.8372607083121936,\n",
       "   0.8495802170038224,\n",
       "   0.8708902553717295,\n",
       "   0.8311816070477168,\n",
       "   0.8509714325269063,\n",
       "   0.8673336320122083,\n",
       "   0.864456162750721,\n",
       "   0.8687086872259776,\n",
       "   0.8249712866544724,\n",
       "   0.8644219722350438,\n",
       "   0.8492595298091571,\n",
       "   0.8691458024581273,\n",
       "   0.8481071758270263,\n",
       "   0.851428176065286,\n",
       "   0.8603093306223552,\n",
       "   0.826475417514642,\n",
       "   0.8466011018554369,\n",
       "   0.8335600717862447,\n",
       "   0.8274499620000522,\n",
       "   0.8514898576339086,\n",
       "   0.8453600164254507,\n",
       "   0.8455632005135219,\n",
       "   0.8338497508565584,\n",
       "   0.8739532963434855,\n",
       "   0.8486619993050893,\n",
       "   0.8377566576004029,\n",
       "   0.8347696493069331,\n",
       "   0.8405166812737783,\n",
       "   0.8327777141332626,\n",
       "   0.8658423807223637,\n",
       "   0.8161090305447578,\n",
       "   0.8334618045886357,\n",
       "   0.839156512717406,\n",
       "   0.8425728618105253,\n",
       "   0.8348103048404057,\n",
       "   0.8459403287371,\n",
       "   0.8230433669686318,\n",
       "   0.8238097157080968,\n",
       "   0.8433031636476517,\n",
       "   0.8589755072196325,\n",
       "   0.8365624426802,\n",
       "   0.8499731681744258,\n",
       "   0.835593406756719,\n",
       "   0.862349781692028,\n",
       "   0.8515472497542699,\n",
       "   0.8485699105262756,\n",
       "   0.8320024336377779,\n",
       "   0.8758211203416189,\n",
       "   0.8291878257195154,\n",
       "   0.8285033581654231,\n",
       "   0.820870799223582,\n",
       "   0.8549568528930346,\n",
       "   0.823812338411808,\n",
       "   0.8413575561841329,\n",
       "   0.8462733939290047,\n",
       "   0.8324114596843719,\n",
       "   0.854802497625351,\n",
       "   0.8304598619540532,\n",
       "   0.8593304971853892,\n",
       "   0.863206976254781,\n",
       "   0.8332255019744237,\n",
       "   0.8533081621925036,\n",
       "   0.8367525950074196,\n",
       "   0.8284652309616407,\n",
       "   0.8285233368476231,\n",
       "   0.8216201958060264,\n",
       "   0.8145164253314336,\n",
       "   0.8474739395578702,\n",
       "   0.8183140490452449,\n",
       "   0.854152193069458,\n",
       "   0.8536674376328787,\n",
       "   0.8347023249665896,\n",
       "   0.856138942639033,\n",
       "   0.8343078074852626,\n",
       "   0.8427321048577626,\n",
       "   0.8257475736737251,\n",
       "   0.820791670580705,\n",
       "   0.8235936591029167,\n",
       "   0.8187837612628937,\n",
       "   0.8498469489812851,\n",
       "   0.8366892975568772,\n",
       "   0.8144606240590413,\n",
       "   0.8152683785557747,\n",
       "   0.8499923638502757,\n",
       "   0.8362324607372283,\n",
       "   0.8268182643254598,\n",
       "   0.8390438640117646,\n",
       "   0.8169289497534434,\n",
       "   0.8296001875400543,\n",
       "   0.8160243553916613,\n",
       "   0.8450383215149244,\n",
       "   0.8173968028028806,\n",
       "   0.8636752602458,\n",
       "   0.8138732849558195,\n",
       "   0.8166079483429591,\n",
       "   0.8214769736925761,\n",
       "   0.8210546944538752,\n",
       "   0.8190888904531797,\n",
       "   0.8405789794524511,\n",
       "   0.8176964631676674,\n",
       "   0.843300825258096,\n",
       "   0.8165814751386642,\n",
       "   0.8334662779172262,\n",
       "   0.8223350204030673,\n",
       "   0.8276028515895207,\n",
       "   0.8230750718712807,\n",
       "   0.8034846503535906,\n",
       "   0.8351253633697827,\n",
       "   0.8324406227469444,\n",
       "   0.8271724064151446,\n",
       "   0.8011943422754606,\n",
       "   0.820343577961127,\n",
       "   0.8287694144248963,\n",
       "   0.7984485598405202,\n",
       "   0.8559292777379354,\n",
       "   0.793948367635409,\n",
       "   0.8197477628787359],\n",
       "  'val_loss_std': [0.08236909406322923,\n",
       "   0.09339231460382348,\n",
       "   0.10576904320190608,\n",
       "   0.11431139715609645,\n",
       "   0.11351660558696669,\n",
       "   0.12571157280370188,\n",
       "   0.12539333089662633,\n",
       "   0.127288416793855,\n",
       "   0.11893976692053894,\n",
       "   0.11857961665826383,\n",
       "   0.13314399695180065,\n",
       "   0.12917321367395096,\n",
       "   0.12964002486422432,\n",
       "   0.1323388614544805,\n",
       "   0.1319458229265092,\n",
       "   0.131731723530891,\n",
       "   0.13569759183266134,\n",
       "   0.1321178512556122,\n",
       "   0.13432013358022213,\n",
       "   0.1343943144822021,\n",
       "   0.13189308134964173,\n",
       "   0.13538646001422064,\n",
       "   0.13303069375142224,\n",
       "   0.1375880378766212,\n",
       "   0.13522909217678217,\n",
       "   0.13355226933155462,\n",
       "   0.14244751950307097,\n",
       "   0.12962129481772838,\n",
       "   0.13616444192178212,\n",
       "   0.13782690638170697,\n",
       "   0.14199393901353582,\n",
       "   0.14331784139852494,\n",
       "   0.1432414463856821,\n",
       "   0.1415365234495874,\n",
       "   0.14178111611636196,\n",
       "   0.14257672104815788,\n",
       "   0.14153814247257554,\n",
       "   0.14174017245671905,\n",
       "   0.14327588743851785,\n",
       "   0.14195059085377404,\n",
       "   0.14376561022471882,\n",
       "   0.1380180420565341,\n",
       "   0.13936455204086967,\n",
       "   0.14573254609172384,\n",
       "   0.139378807208103,\n",
       "   0.14579574274818122,\n",
       "   0.13982925913448055,\n",
       "   0.14016166349519368,\n",
       "   0.1493938500131544,\n",
       "   0.1363779496068937,\n",
       "   0.13659857422742805,\n",
       "   0.14062957669772663,\n",
       "   0.1420291517124603,\n",
       "   0.14148760305709215,\n",
       "   0.14572561920506052,\n",
       "   0.1393443262024157,\n",
       "   0.1404046252707321,\n",
       "   0.14072419163918912,\n",
       "   0.14675470717558212,\n",
       "   0.1412441140718948,\n",
       "   0.1473288008874422,\n",
       "   0.13945468382979231,\n",
       "   0.1379862777999113,\n",
       "   0.15062000754203939,\n",
       "   0.14283031594824244,\n",
       "   0.14174174685208152,\n",
       "   0.1462677835120186,\n",
       "   0.1394610514760964,\n",
       "   0.14467324634575754,\n",
       "   0.13985087632471652,\n",
       "   0.15255988236323015,\n",
       "   0.14718928974784984,\n",
       "   0.13910653438503626,\n",
       "   0.14473110653395208,\n",
       "   0.14731001076439354,\n",
       "   0.13828589263766342,\n",
       "   0.15409183651680858,\n",
       "   0.1373977026088596,\n",
       "   0.14025084153764072,\n",
       "   0.14580684047271839,\n",
       "   0.1482434116596452,\n",
       "   0.15002410160352764,\n",
       "   0.14232830995644644,\n",
       "   0.13887174819786086,\n",
       "   0.1475570224772372,\n",
       "   0.14158941384168394,\n",
       "   0.1461647600794514,\n",
       "   0.14965812771887074,\n",
       "   0.1347046485773977,\n",
       "   0.1415496626297977,\n",
       "   0.1495719782519324,\n",
       "   0.14595245677177754,\n",
       "   0.15651819403757206,\n",
       "   0.13784789217300691,\n",
       "   0.14647697719896777,\n",
       "   0.15114289540728854,\n",
       "   0.13482948534290776,\n",
       "   0.13906551192682287,\n",
       "   0.1332200517687758,\n",
       "   0.14500098324816738,\n",
       "   0.14618536303065943,\n",
       "   0.1332704896354343,\n",
       "   0.13991502736678565,\n",
       "   0.15040386391573649,\n",
       "   0.1367845603941481,\n",
       "   0.1428019310414054,\n",
       "   0.14147082178568196,\n",
       "   0.13837640861141653,\n",
       "   0.13921944585934626,\n",
       "   0.14362521889307336,\n",
       "   0.151231301048288,\n",
       "   0.13248412505898224,\n",
       "   0.13680061207526795,\n",
       "   0.137050365787563,\n",
       "   0.1504233077518279,\n",
       "   0.1401749899088686,\n",
       "   0.1403136861429386,\n",
       "   0.15080984534232253,\n",
       "   0.1454318323766392,\n",
       "   0.14515713977637398,\n",
       "   0.14470297651529837,\n",
       "   0.15472159742331495,\n",
       "   0.14624001718369453,\n",
       "   0.15105193593081204,\n",
       "   0.14212560677520733,\n",
       "   0.138434405199447,\n",
       "   0.14582485110819923,\n",
       "   0.13506629797082803,\n",
       "   0.14920560243596975,\n",
       "   0.14302452238484686,\n",
       "   0.15234457750460872,\n",
       "   0.14699706316114045,\n",
       "   0.1391490074960588,\n",
       "   0.14400596245504071,\n",
       "   0.1453951739511507,\n",
       "   0.14787961141793626,\n",
       "   0.14944045238275092,\n",
       "   0.14153461820971366,\n",
       "   0.14258738500888235,\n",
       "   0.1461595342461034,\n",
       "   0.14279468492704037,\n",
       "   0.13993213863440626,\n",
       "   0.1379659709315623,\n",
       "   0.14445085576529323,\n",
       "   0.1357528386767979,\n",
       "   0.14339580951731895,\n",
       "   0.13904326722940458,\n",
       "   0.1386134990960677],\n",
       "  'val_accuracy_mean': [0.3836444452901681,\n",
       "   0.4195333346227805,\n",
       "   0.48108888973792396,\n",
       "   0.5142222226659456,\n",
       "   0.5210666640599568,\n",
       "   0.506511111954848,\n",
       "   0.5494666653871536,\n",
       "   0.5427777776122094,\n",
       "   0.5647333319981893,\n",
       "   0.5685777761538824,\n",
       "   0.5992888867855072,\n",
       "   0.5890444430708885,\n",
       "   0.5993777774771054,\n",
       "   0.5996444437901179,\n",
       "   0.6045777771870295,\n",
       "   0.6079777792096138,\n",
       "   0.6131333324313164,\n",
       "   0.6319999978939692,\n",
       "   0.6391333321730296,\n",
       "   0.6313999992609024,\n",
       "   0.6426222235957781,\n",
       "   0.6507555555303891,\n",
       "   0.6549333327015241,\n",
       "   0.6575555570920308,\n",
       "   0.6424666666984558,\n",
       "   0.6600222206115722,\n",
       "   0.6596444447835287,\n",
       "   0.6613555554548899,\n",
       "   0.6752888879179955,\n",
       "   0.6589111113548278,\n",
       "   0.6714000008503596,\n",
       "   0.6827111115058263,\n",
       "   0.6751777780056,\n",
       "   0.6638444451491038,\n",
       "   0.680866667330265,\n",
       "   0.6737333329518637,\n",
       "   0.6665777790546418,\n",
       "   0.671644445459048,\n",
       "   0.6684222195545833,\n",
       "   0.6842444425821305,\n",
       "   0.6690666668613752,\n",
       "   0.6773555550972621,\n",
       "   0.6657555562257766,\n",
       "   0.6819777778784434,\n",
       "   0.674555554886659,\n",
       "   0.6725111108024915,\n",
       "   0.6812666684389115,\n",
       "   0.6763999990622203,\n",
       "   0.6825999989112218,\n",
       "   0.6827777782082558,\n",
       "   0.6748444454868635,\n",
       "   0.6762444444497426,\n",
       "   0.6760888886451721,\n",
       "   0.6822444444894791,\n",
       "   0.6663111114501953,\n",
       "   0.6735999980568885,\n",
       "   0.6799333341916403,\n",
       "   0.6803555555144946,\n",
       "   0.6827999995152155,\n",
       "   0.6795333329836527,\n",
       "   0.6708888906240463,\n",
       "   0.6895111115773519,\n",
       "   0.6807111102342606,\n",
       "   0.6835555559396744,\n",
       "   0.6809333319465319,\n",
       "   0.6820222208897273,\n",
       "   0.6811333339413007,\n",
       "   0.6837777776519457,\n",
       "   0.6881555551290512,\n",
       "   0.6788222213586171,\n",
       "   0.6753777778148651,\n",
       "   0.6831777758399645,\n",
       "   0.674488888780276,\n",
       "   0.6839111104607583,\n",
       "   0.6758888890345891,\n",
       "   0.6745777770876884,\n",
       "   0.6845777794718743,\n",
       "   0.6813555538654328,\n",
       "   0.6679333315292995,\n",
       "   0.6837555554509163,\n",
       "   0.685888888736566,\n",
       "   0.6838222215572993,\n",
       "   0.6756666652361552,\n",
       "   0.6877777792016665,\n",
       "   0.6814000003536542,\n",
       "   0.6811555551489195,\n",
       "   0.6853999981284141,\n",
       "   0.6777555547157923,\n",
       "   0.6875777765115102,\n",
       "   0.6716888892650604,\n",
       "   0.6759555550416311,\n",
       "   0.685622221827507,\n",
       "   0.6780222221215566,\n",
       "   0.6819777777791023,\n",
       "   0.6822222221891086,\n",
       "   0.6863333330551783,\n",
       "   0.687311111887296,\n",
       "   0.6897111092011133,\n",
       "   0.6754222209254901,\n",
       "   0.6910222228368124,\n",
       "   0.6789777769645056,\n",
       "   0.6763333332538605,\n",
       "   0.6804666675130526,\n",
       "   0.6768222216765086,\n",
       "   0.6813999993602434,\n",
       "   0.6813555554548899,\n",
       "   0.6856444452206294,\n",
       "   0.6886222219467163,\n",
       "   0.6893777797619501,\n",
       "   0.6845555541912715,\n",
       "   0.6813777772585551,\n",
       "   0.6817777784665425,\n",
       "   0.6923999998966853,\n",
       "   0.6888000015417735,\n",
       "   0.6760666657487552,\n",
       "   0.6814888885617256,\n",
       "   0.6846666661898295,\n",
       "   0.6837555545568467,\n",
       "   0.6873555561900139,\n",
       "   0.6867777768770854,\n",
       "   0.6910222222407659,\n",
       "   0.6808444446325302,\n",
       "   0.6903777778148651,\n",
       "   0.675955556333065,\n",
       "   0.6905555548270543,\n",
       "   0.687133332490921,\n",
       "   0.6912222222487132,\n",
       "   0.6843777771790822,\n",
       "   0.6870000018676122,\n",
       "   0.6819555560747782,\n",
       "   0.6979999989271164,\n",
       "   0.6777333329121272,\n",
       "   0.6880888893206915,\n",
       "   0.6846666673819224,\n",
       "   0.6894888880848885,\n",
       "   0.6841777769724527,\n",
       "   0.6905111115177472,\n",
       "   0.699288889169693,\n",
       "   0.6877111119031906,\n",
       "   0.683777777949969,\n",
       "   0.6857333348194758,\n",
       "   0.6946666653951009,\n",
       "   0.6868666664759318,\n",
       "   0.6870222212870916,\n",
       "   0.6936666677395503,\n",
       "   0.6769777781764666,\n",
       "   0.6988444442550341,\n",
       "   0.687422223687172],\n",
       "  'val_accuracy_std': [0.053624686199679084,\n",
       "   0.05543034371674429,\n",
       "   0.060321357239092196,\n",
       "   0.061300312244270065,\n",
       "   0.05992380292769722,\n",
       "   0.06224305635312529,\n",
       "   0.06347828952692203,\n",
       "   0.060888380948135166,\n",
       "   0.0600169976186404,\n",
       "   0.06184250590931793,\n",
       "   0.06327200131010428,\n",
       "   0.061433297291990145,\n",
       "   0.061044473319096924,\n",
       "   0.06136128184205862,\n",
       "   0.060875765610299865,\n",
       "   0.060432265856983754,\n",
       "   0.06135595000494595,\n",
       "   0.06382615377888752,\n",
       "   0.061086374734130686,\n",
       "   0.062429539556022345,\n",
       "   0.06121920153101314,\n",
       "   0.06016908026328471,\n",
       "   0.05879622267834406,\n",
       "   0.06016971994302225,\n",
       "   0.06251035972193497,\n",
       "   0.061136155031495366,\n",
       "   0.06052014912595406,\n",
       "   0.059255376868141495,\n",
       "   0.05959640564380807,\n",
       "   0.05680766417822001,\n",
       "   0.05859057003818465,\n",
       "   0.05742936096434106,\n",
       "   0.06004694050782062,\n",
       "   0.06051781227844796,\n",
       "   0.06104270620814574,\n",
       "   0.06110940004399978,\n",
       "   0.05942433441167555,\n",
       "   0.058896957208926,\n",
       "   0.06064859021669015,\n",
       "   0.06048127767139839,\n",
       "   0.05935096051664121,\n",
       "   0.059903375846808694,\n",
       "   0.05910116516554076,\n",
       "   0.05875069370059268,\n",
       "   0.060129388062192465,\n",
       "   0.05973824089879266,\n",
       "   0.05914222796549079,\n",
       "   0.05948236131580461,\n",
       "   0.06283354866542352,\n",
       "   0.06008255661281955,\n",
       "   0.05873554906202432,\n",
       "   0.05822026930561431,\n",
       "   0.05870042768492075,\n",
       "   0.06039253991784477,\n",
       "   0.06226960143199432,\n",
       "   0.060501019386624465,\n",
       "   0.059709133279819554,\n",
       "   0.06072542548717747,\n",
       "   0.061631016822538374,\n",
       "   0.06054264634921864,\n",
       "   0.06036636981307234,\n",
       "   0.05908861940416042,\n",
       "   0.05981525597494224,\n",
       "   0.059899505283112026,\n",
       "   0.05932724223657148,\n",
       "   0.061438793466180856,\n",
       "   0.060352459273915686,\n",
       "   0.06375841774354704,\n",
       "   0.061836147328892685,\n",
       "   0.05951860897238895,\n",
       "   0.060398412425589375,\n",
       "   0.06178677353650112,\n",
       "   0.05883182239305193,\n",
       "   0.059385429459650745,\n",
       "   0.058914775623975686,\n",
       "   0.057883083165149056,\n",
       "   0.06088063480951018,\n",
       "   0.05835345316066268,\n",
       "   0.06062901413321796,\n",
       "   0.061032834628283794,\n",
       "   0.05932574019305336,\n",
       "   0.06280291112610219,\n",
       "   0.0605997813834624,\n",
       "   0.059503708881904004,\n",
       "   0.06205585411202227,\n",
       "   0.05818869281222922,\n",
       "   0.06256039575926749,\n",
       "   0.05989496485758414,\n",
       "   0.05748223194910487,\n",
       "   0.06068711174167911,\n",
       "   0.059974792981494364,\n",
       "   0.05780123964615031,\n",
       "   0.062458991455803074,\n",
       "   0.058989763747721954,\n",
       "   0.0604681322166335,\n",
       "   0.06008913108386427,\n",
       "   0.058106157163663394,\n",
       "   0.05691636074351066,\n",
       "   0.05781009431649159,\n",
       "   0.05829928680099702,\n",
       "   0.06133930892062732,\n",
       "   0.059337393195103456,\n",
       "   0.06024337152172016,\n",
       "   0.06063471213698916,\n",
       "   0.059406659008223485,\n",
       "   0.05961928262322178,\n",
       "   0.05908054394713421,\n",
       "   0.05823094146949394,\n",
       "   0.056264438249579884,\n",
       "   0.060220479905379654,\n",
       "   0.05950809302982604,\n",
       "   0.05672371801373561,\n",
       "   0.0570381539378409,\n",
       "   0.057536178197561086,\n",
       "   0.05894953228572979,\n",
       "   0.05745470135434099,\n",
       "   0.058594651125162156,\n",
       "   0.06161745294056894,\n",
       "   0.06224657921537886,\n",
       "   0.0583157672895285,\n",
       "   0.05942682663302859,\n",
       "   0.06047122076578073,\n",
       "   0.059950643121916516,\n",
       "   0.058862787716163684,\n",
       "   0.05862488039711161,\n",
       "   0.06013014276724436,\n",
       "   0.057769551036318104,\n",
       "   0.05790366952605313,\n",
       "   0.05939229409444336,\n",
       "   0.05838592919223286,\n",
       "   0.05786446634260626,\n",
       "   0.060147128838677545,\n",
       "   0.0615507576606847,\n",
       "   0.05904047643682041,\n",
       "   0.06030445605981702,\n",
       "   0.05810614504986827,\n",
       "   0.06266103468491274,\n",
       "   0.059626724231758146,\n",
       "   0.05684239040668668,\n",
       "   0.06317485044365248,\n",
       "   0.05934721477932003,\n",
       "   0.05871967423809152,\n",
       "   0.0608394736870983,\n",
       "   0.05889494569321081,\n",
       "   0.058538053716414816,\n",
       "   0.05758902459516361,\n",
       "   0.05798210099579776,\n",
       "   0.05952920001052301],\n",
       "  'val_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'val_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0]}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델을 구성한다\n",
    "model = MAMLFewShotClassifier(args=args, device=device,\n",
    "                              im_shape=(2, 3,\n",
    "                                        args.image_height, args.image_width))\n",
    "\n",
    "data = MetaLearningSystemDataLoader\n",
    "\n",
    "maml_system = ExperimentBuilder(model=model, data=data, args=args, device=device)\n",
    "maml_system.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9236398a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\envs\\maml\\lib\\site-packages\\PIL\\ImageFile.py\u001b[0m in \u001b[0;36m_save\u001b[1;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[0;32m    517\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 518\u001b[1;33m         \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfileno\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    519\u001b[0m         \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_idat' object has no attribute 'fileno'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24392\\3085486066.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    130\u001b[0m                     \u001b[0msave_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m                     \u001b[0mtask_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_idx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m                     title_prefix=\"Support/Query Adaptation\")\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\prompt_maml\\utils\\basic.py\u001b[0m in \u001b[0;36mplot_support_query_before_after_fixed_axes\u001b[1;34m(support_before, query_before, support_after, query_after, y_support, y_query, save_dir, task_index, title_prefix, perplexity)\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[0msave_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"task{task_index:03d}_tsne_fixed_axes.png\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\maml\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36msavefig\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    975\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msavefig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m     \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 977\u001b[1;33m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    978\u001b[0m     \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# need this if 'transparent=True' to reset colors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\maml\\lib\\site-packages\\matplotlib\\figure.py\u001b[0m in \u001b[0;36msavefig\u001b[1;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[0;32m   3056\u001b[0m                         ax.patch._cm_set(facecolor='none', edgecolor='none'))\n\u001b[0;32m   3057\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3058\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3059\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3060\u001b[0m     def ginput(self, n=1, timeout=30, show_clicks=True,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\maml\\lib\\site-packages\\matplotlib\\backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[1;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[0;32m   2323\u001b[0m                         \u001b[0morientation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2324\u001b[0m                         \u001b[0mbbox_inches_restore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_bbox_inches_restore\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2325\u001b[1;33m                         **kwargs)\n\u001b[0m\u001b[0;32m   2326\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2327\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mrestore_bbox\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\maml\\lib\\site-packages\\matplotlib\\backend_bases.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1646\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1648\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1650\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\maml\\lib\\site-packages\\matplotlib\\_api\\deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[0;32m    413\u001b[0m                          \u001b[1;32melse\u001b[0m \u001b[0mdeprecation_addendum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m                 **kwargs)\n\u001b[1;32m--> 415\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0minner_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[0mDECORATORS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\maml\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[1;34m(self, filename_or_obj, metadata, pil_kwargs, *args)\u001b[0m\n\u001b[0;32m    541\u001b[0m         mpl.image.imsave(\n\u001b[0;32m    542\u001b[0m             \u001b[0mfilename_or_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer_rgba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"png\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morigin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"upper\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 543\u001b[1;33m             dpi=self.figure.dpi, metadata=metadata, pil_kwargs=pil_kwargs)\n\u001b[0m\u001b[0;32m    544\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    545\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mprint_to_buffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\maml\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36mimsave\u001b[1;34m(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs)\u001b[0m\n\u001b[0;32m   1673\u001b[0m         \u001b[0mpil_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"format\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1674\u001b[0m         \u001b[0mpil_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dpi\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdpi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1675\u001b[1;33m         \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpil_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\maml\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, fp, format, **params)\u001b[0m\n\u001b[0;32m   2429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2430\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2431\u001b[1;33m             \u001b[0msave_handler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2432\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2433\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mopen_fp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\maml\\lib\\site-packages\\PIL\\PngImagePlugin.py\u001b[0m in \u001b[0;36m_save\u001b[1;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[0;32m   1418\u001b[0m         \u001b[0m_write_multiple_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrawmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mappend_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1419\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m         \u001b[0mImageFile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_idat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"zip\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrawmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1422\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\maml\\lib\\site-packages\\PIL\\ImageFile.py\u001b[0m in \u001b[0;36m_save\u001b[1;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[0;32m    520\u001b[0m         \u001b[0m_encode_tile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbufsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnsupportedOperation\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 522\u001b[1;33m         \u001b[0m_encode_tile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbufsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    523\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"flush\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m         \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\maml\\lib\\site-packages\\PIL\\ImageFile.py\u001b[0m in \u001b[0;36m_encode_tile\u001b[1;34m(im, fp, tile, bufsize, fh, exc)\u001b[0m\n\u001b[0;32m    539\u001b[0m                     \u001b[1;31m# compress to Python file-compatible object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m                     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m                         \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbufsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m                         \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1400x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_path = f\"tsne_images/adaptation_effect/{datasets}\"\n",
    "\n",
    "train_data = maml_system.data.get_test_batches(total_batches=int(300/1), augment_images=False)\n",
    "\n",
    "for sample_idx, train_sample in enumerate(train_data):\n",
    "    \n",
    "    x_support_set, x_target_set, y_support_set, y_target_set, seed = train_sample\n",
    "    \n",
    "    x_support_set = torch.Tensor(x_support_set).float().to(device=maml_system.model.device)\n",
    "    x_target_set = torch.Tensor(x_target_set).float().to(device=maml_system.model.device)\n",
    "    y_support_set = torch.Tensor(y_support_set).long().to(device=maml_system.model.device)\n",
    "    y_target_set = torch.Tensor(y_target_set).long().to(device=maml_system.model.device)\n",
    "    \n",
    "    for task_id, (x_support_set_task, y_support_set_task, x_target_set_task, y_target_set_task) in enumerate(zip(x_support_set,\n",
    "                              y_support_set,\n",
    "                              x_target_set,\n",
    "                              y_target_set)):\n",
    "        \n",
    "        \n",
    "        names_weights_copy = maml_system.model.get_inner_loop_parameter_dict(maml_system.model.classifier.named_parameters())\n",
    "\n",
    "        num_devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "\n",
    "        names_weights_copy = {\n",
    "                name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
    "                    [num_devices] + [1 for i in range(len(value.shape))]) for\n",
    "                name, value in names_weights_copy.items()}\n",
    "\n",
    "        prompted_weights_copy = {}\n",
    "        if args.prompter:\n",
    "            prompted_weights_copy = {key: value for key, value in names_weights_copy.items() if 'prompt' in key}\n",
    "\n",
    "        names_weights_copy = {key: value for key, value in names_weights_copy.items() if 'layer_dict' in key}\n",
    "        \n",
    "        \n",
    "        n, s, c, h, w = x_target_set_task.shape\n",
    "\n",
    "        x_support_set_task = x_support_set_task.view(-1, c, h, w)\n",
    "        y_support_set_task = y_support_set_task.view(-1)\n",
    "        x_target_set_task = x_target_set_task.view(-1, c, h, w)\n",
    "        y_target_set_task = y_target_set_task.view(-1)\n",
    "        \n",
    "            \n",
    "        for num_step in range(5):\n",
    "\n",
    "            support_loss, support_preds, support_feature_map_list = maml_system.model.net_forward_feature_extractor(\n",
    "              x=x_support_set_task,\n",
    "              y=y_support_set_task,\n",
    "              weights=names_weights_copy,\n",
    "              prompted_weights=prompted_weights_copy,\n",
    "              backup_running_statistics=num_step == 0,\n",
    "              training=True,\n",
    "              num_step=num_step,\n",
    "              training_phase=False,\n",
    "              epoch=0)\n",
    "            \n",
    "            if num_step == 0:\n",
    "                ## Adatpation 전  support set에 대한 feature map을 구한다\n",
    "                feature_map_list_before =  support_feature_map_list[3]\n",
    "                \n",
    "                ## Adatpation 전  query set에 대한 feature map을 구한다\n",
    "                _, _, target_feature_map_list_before = maml_system.model.net_forward_feature_extractor(\n",
    "                                                            x=x_target_set_task,\n",
    "                                                            y=y_target_set_task,\n",
    "                                                            weights=names_weights_copy,\n",
    "                                                            prompted_weights=prompted_weights_copy,\n",
    "                                                            backup_running_statistics=False,\n",
    "                                                            training=True,\n",
    "                                                            num_step=num_step,\n",
    "                                                            training_phase=False,\n",
    "                                                            epoch=0)\n",
    "    \n",
    "                # query_before_np = gap(target_feature_map_list_before[3]).detach().cpu().numpy()\n",
    "                query_before_np = flatten_feature_map(target_feature_map_list_before[3]).detach().cpu().numpy()\n",
    "            \n",
    "            \n",
    "            names_weights_copy, prompted_weights_copy = maml_system.model.apply_inner_loop_update(\n",
    "                                                             loss=support_loss,\n",
    "                                                             names_weights_copy=names_weights_copy,\n",
    "                                                             prompted_weights_copy=prompted_weights_copy,\n",
    "                                                             use_second_order=True,\n",
    "                                                             current_step_idx=num_step,\n",
    "                                                             current_iter='test',\n",
    "                                                             training_phase=False)\n",
    "\n",
    "            if num_step == 4:\n",
    "                \n",
    "                ## Adatpation 후  support set에 대한 feature map을 구한다\n",
    "                feature_map_list_after =  support_feature_map_list[3]\n",
    "                \n",
    "                target_loss, target_preds, target_feature_map_list = maml_system.model.net_forward_feature_extractor(\n",
    "                    x=x_target_set_task,\n",
    "                    y=y_target_set_task,\n",
    "                    weights=names_weights_copy,\n",
    "                    prompted_weights=prompted_weights_copy,\n",
    "                    backup_running_statistics=False, training=True,\n",
    "                    num_step=num_step,\n",
    "                    training_phase=False,\n",
    "                    epoch=0)        \n",
    "                \n",
    "                \n",
    "                ## Adaptation 후에 query set에 대한 feature map을 구한다\n",
    "                query_after_np = target_feature_map_list[3]\n",
    "                \n",
    "                query_after_np = flatten_feature_map(target_feature_map_list[3]).detach().cpu().numpy()\n",
    "                support_before_np = flatten_feature_map(feature_map_list_before).detach().cpu().numpy()\n",
    "                support_after_np = flatten_feature_map(feature_map_list_after).detach().cpu().numpy()\n",
    "                \n",
    "\n",
    "                y_supp_np = y_support_set_task.cpu().numpy()\n",
    "                y_query_np = y_target_set_task.cpu().numpy()\n",
    "                \n",
    "                \n",
    "#                 plot_query_before_after_2d_fixed_axes(\n",
    "#                     query_before=query_before_np,\n",
    "#                     query_after=query_after_np,\n",
    "#                     y_query=y_query_np,\n",
    "#                     save_dir=save_path,\n",
    "#                     task_index=sample_idx,\n",
    "#                     title_prefix=\"Query Feature Map\"\n",
    "#                 )\n",
    "                \n",
    "                plot_support_query_before_after_fixed_axes(\n",
    "                    support_before=support_before_np,\n",
    "                    query_before=query_before_np,\n",
    "                    support_after=support_after_np,\n",
    "                    query_after=query_after_np,\n",
    "                    y_support=y_supp_np,\n",
    "                    y_query=y_query_np,\n",
    "                    save_dir=save_path,\n",
    "                    task_index=sample_idx,\n",
    "                    title_prefix=\"Support/Query Adaptation\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caca899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdfcb0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
