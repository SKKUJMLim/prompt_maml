{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16ca38f2",
   "metadata": {},
   "source": [
    "# With Prompt vs Without Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "912c279b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pytorchcv.model_provider import get_model as ptcv_get_model # model\n",
    "import sys, os\n",
    "import easydict\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline \n",
    "\n",
    "# enable cuda devices\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from meta_neural_network_architectures import VGGReLUNormNetwork, ResNet12\n",
    "from utils.parser_utils import get_args\n",
    "from data import MetaLearningSystemDataLoader\n",
    "from experiment_builder import ExperimentBuilder\n",
    "from few_shot_learning_system import MAMLFewShotClassifier\n",
    "import prompters\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from data_augmentation import mixup_data, cutmix_data, random_flip, random_flip_like_torchvision\n",
    "\n",
    "from utils.basic import gap, flatten_feature_map, plot_query_before_after_separate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f032c4",
   "metadata": {},
   "source": [
    "# 1. Dataset 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b5591885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choices=['padding', 'random_patch', 'fixed_patch'],\n",
    "method = 'padding'\n",
    "\n",
    "datasets = \"mini_imagenet\"\n",
    "# datasets = \"tiered_imagenet\"\n",
    "# datasets = \"CIFAR_FS\"\n",
    "# datasets = \"CUB\"\n",
    "\n",
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "# os.environ['TEST_DATASET'] = \"tiered_imagenet\" # https://mtl.yyliu.net/download/Lmzjm9tX.html\n",
    "# os.environ['TEST_DATASET'] = \"CIFAR_FS\" # https://drive.google.com/file/d/1pTsCCMDj45kzFYgrnO67BWVbKs48Q3NI/view\n",
    "# os.environ['TEST_DATASET'] = \"CUB\" # https://data.caltech.edu/records/65de6-vp158"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e33976d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.environ['DATASET_DIR'] ===  C:/Users/JM/PycharmProjects/MAML/datasets\n"
     ]
    }
   ],
   "source": [
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "print(\"os.environ['DATASET_DIR'] === \", os.environ['DATASET_DIR'])\n",
    "\n",
    "args = easydict.EasyDict(\n",
    "{\n",
    "  \"batch_size\":2,\n",
    "  \"image_height\":84,\n",
    "  \"image_width\":84,\n",
    "  \"image_channels\":3,\n",
    "  \"gpu_to_use\":0,\n",
    "  \"num_dataprovider_workers\":4,\n",
    "  \"max_models_to_save\":5,\n",
    "  \"dataset_name\":\"mini_imagenet_full_size\",\n",
    "  \"dataset_path\":\"mini_imagenet_full_size\",\n",
    "  \"reset_stored_paths\":False,\n",
    "  \"experiment_name\":\"../MAML_Prompt_padding_5way_5shot_filter128_miniImagenet\",\n",
    "  \"train_seed\": 0, \"val_seed\": 0,\n",
    "  \"indexes_of_folders_indicating_class\": [-3, -2],\n",
    "  \"sets_are_pre_split\": True,\n",
    "  \"train_val_test_split\": [0.64, 0.16, 0.20],\n",
    "  \"evaluate_on_test_set_only\": False,\n",
    "\n",
    "  \"total_epochs\": 100,\n",
    "  \"total_iter_per_epoch\":500, \"continue_from_epoch\": -2,\n",
    "  \"num_evaluation_tasks\":600,\n",
    "  \"multi_step_loss_num_epochs\": 15,\n",
    "  \"minimum_per_task_contribution\": 0.01,\n",
    "  \"learnable_per_layer_per_step_inner_loop_learning_rate\": True,\n",
    "  \"enable_inner_loop_optimizable_bn_params\": False,\n",
    "  \"evalute_on_test_set_only\": False,\n",
    "\n",
    "  \"max_pooling\": True,\n",
    "  \"per_step_bn_statistics\": False,\n",
    "  \"learnable_batch_norm_momentum\": False,\n",
    "  \"load_into_memory\": False,\n",
    "  \"init_inner_loop_learning_rate\": 0.01,\n",
    "  \"init_inner_loop_weight_decay\": 0.0005,\n",
    "  \"learnable_bn_gamma\": True,\n",
    "  \"learnable_bn_beta\": True,\n",
    "\n",
    "  \"dropout_rate_value\":0.0,\n",
    "  \"min_learning_rate\":0.001,\n",
    "  \"meta_learning_rate\":0.001,   \"total_epochs_before_pause\": 100,\n",
    "  \"first_order_to_second_order_epoch\":-1,\n",
    "  \"weight_decay\": 0.0,\n",
    "\n",
    "  \"norm_layer\":\"batch_norm\",\n",
    "  \"cnn_num_filters\":128,\n",
    "  \"num_stages\":4,\n",
    "  \"conv_padding\": True,\n",
    "  \"number_of_training_steps_per_iter\":5,\n",
    "  \"number_of_evaluation_steps_per_iter\":5,\n",
    "  \"cnn_blocks_per_stage\":1,\n",
    "  \"num_classes_per_set\":5,\n",
    "  \"num_samples_per_class\":5,\n",
    "  \"num_target_samples\": 15,\n",
    "  \"samples_per_iter\" : 1,\n",
    "\n",
    "  \"second_order\": True,\n",
    "  \"use_multi_step_loss_optimization\":False,\n",
    "  \"backbone\": \"4-CONV\",\n",
    "  \"arbiter\": False,\n",
    "  \"use_bias\": True,\n",
    "  \"prompter\": True,\n",
    "  \"prompt_engineering\": method,\n",
    "  \"prompt_size\" : 5,\n",
    "  \"image_size\" : 84,\n",
    "  \"prompt_random_init\": False,\n",
    "  \"outer_prompt_learning_rate\": 0.001,\n",
    "  \"inner_prompt_learning_rate\": 0.01,\n",
    "  \"data_aug\" : \"random\"\n",
    "}\n",
    ")\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "args.im_shape = (2, 3, args.image_height, args.image_width)\n",
    "\n",
    "args.use_cuda = torch.cuda.is_available()\n",
    "args.seed = 104\n",
    "args.reverse_channels=False\n",
    "args.labels_as_int=False\n",
    "args.reset_stored_filepaths=False\n",
    "args.num_of_gpus=1\n",
    "\n",
    "args.continue_from_epoch='latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9b3c2f44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using max pooling\n",
      "meta network params\n",
      "0.01\n",
      "Inner Loop parameters\n",
      "prompt_learning_rates_dict.prompt_weight_learning_rate torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv0-conv-weight torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv0-conv-bias torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv1-conv-weight torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv1-conv-bias torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv2-conv-weight torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv2-conv-bias torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv3-conv-weight torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv3-conv-bias torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-linear-weights torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-linear-bias torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv0-conv-weight torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv0-conv-bias torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv1-conv-weight torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv1-conv-bias torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv2-conv-weight torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv2-conv-bias torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv3-conv-weight torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv3-conv-bias torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-linear-weights torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-linear-bias torch.Size([6])\n",
      "Outer Loop parameters\n",
      "classifier.layer_dict.conv0.conv.weight torch.Size([128, 3, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv0.conv.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv0.norm_layer.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv0.norm_layer.weight torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv1.conv.weight torch.Size([128, 128, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv1.conv.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv1.norm_layer.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv1.norm_layer.weight torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv2.conv.weight torch.Size([128, 128, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv2.conv.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv2.norm_layer.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv2.norm_layer.weight torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv3.conv.weight torch.Size([128, 128, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv3.conv.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv3.norm_layer.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv3.norm_layer.weight torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.linear.weights torch.Size([5, 3200]) cuda:0 True\n",
      "classifier.layer_dict.linear.bias torch.Size([5]) cuda:0 True\n",
      "classifier.prompt.prompt_dict.pad_up torch.Size([3, 5, 84]) cuda:0 True\n",
      "classifier.prompt.prompt_dict.pad_down torch.Size([3, 5, 84]) cuda:0 True\n",
      "classifier.prompt.prompt_dict.pad_left torch.Size([3, 74, 5]) cuda:0 True\n",
      "classifier.prompt.prompt_dict.pad_right torch.Size([3, 74, 5]) cuda:0 True\n",
      "inner_loop_optimizer.prompt_learning_rates_dict.prompt_weight_learning_rate torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv0-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv0-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv1-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv1-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv2-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv2-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv3-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv3-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-weights torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv0-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv0-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv1-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv1-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv2-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv2-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv3-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv3-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-linear-weights torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-linear-bias torch.Size([6]) cuda:0 True\n",
      "1\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\MAML_Prompt_padding_5way_5shot_filter128_miniImagenet\n",
      "attempting to find existing checkpoint\n",
      "data {'test': 12000, 'train': 38400, 'val': 9600}\n",
      "train_seed 985773, val_seed: 985773, at start time\n",
      "75000 50000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_val_acc': 0.699288889169693,\n",
       " 'best_val_iter': 69500,\n",
       " 'current_iter': 75000,\n",
       " 'best_epoch': 139,\n",
       " 'train_loss_mean': 0.44109563875198365,\n",
       " 'train_loss_std': 0.1291735863213422,\n",
       " 'train_accuracy_mean': 0.8391066664457321,\n",
       " 'train_accuracy_std': 0.053464857991797364,\n",
       " 'train_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_0_std': 0.0,\n",
       " 'train_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_1_std': 0.0,\n",
       " 'train_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_2_std': 0.0,\n",
       " 'train_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_3_std': 0.0,\n",
       " 'train_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'train_loss_importance_vector_4_std': 0.0,\n",
       " 'train_learning_rate_mean': 0.0010000000000000005,\n",
       " 'train_learning_rate_std': 4.336808689942018e-19,\n",
       " 'val_loss_mean': 0.8291508863369624,\n",
       " 'val_loss_std': 0.14279745482250403,\n",
       " 'val_accuracy_mean': 0.6856222219268481,\n",
       " 'val_accuracy_std': 0.05946382204087874,\n",
       " 'val_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_0_std': 0.0,\n",
       " 'val_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_1_std': 0.0,\n",
       " 'val_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_2_std': 0.0,\n",
       " 'val_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_3_std': 0.0,\n",
       " 'val_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'val_loss_importance_vector_4_std': 0.0,\n",
       " 'network': OrderedDict([('classifier.layer_dict.conv0.conv.weight',\n",
       "               tensor([[[[ 1.9228e-01, -3.8295e-01,  2.1642e-01],\n",
       "                         [-5.5411e-04, -4.2862e-02,  1.0049e-02],\n",
       "                         [-1.9220e-01,  4.4129e-01, -2.6217e-01]],\n",
       "               \n",
       "                        [[ 2.0767e-01, -4.2443e-01,  2.3391e-01],\n",
       "                         [-1.8607e-03, -2.2443e-02,  5.8218e-02],\n",
       "                         [-2.1587e-01,  3.7522e-01, -1.9190e-01]],\n",
       "               \n",
       "                        [[ 1.6017e-01, -2.9681e-01,  1.1097e-01],\n",
       "                         [ 2.8712e-02,  7.0504e-03, -2.6183e-02],\n",
       "                         [-1.5920e-01,  3.3434e-01, -1.4752e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.3566e-06, -1.4712e-06, -1.6298e-06],\n",
       "                         [-1.3851e-06, -1.5410e-06, -1.6120e-06],\n",
       "                         [-1.3997e-06, -1.5809e-06, -1.5675e-06]],\n",
       "               \n",
       "                        [[-1.5547e-06, -1.6984e-06, -1.8430e-06],\n",
       "                         [-1.5758e-06, -1.7545e-06, -1.7947e-06],\n",
       "                         [-1.5013e-06, -1.7067e-06, -1.7273e-06]],\n",
       "               \n",
       "                        [[-1.2922e-06, -1.4896e-06, -1.6343e-06],\n",
       "                         [-1.3808e-06, -1.6499e-06, -1.6773e-06],\n",
       "                         [-1.3512e-06, -1.6254e-06, -1.6297e-06]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.0596e-05,  1.4122e-05,  2.1456e-05],\n",
       "                         [ 2.6158e-06,  1.3649e-06,  1.3392e-05],\n",
       "                         [ 5.6518e-06,  4.9864e-06,  1.2944e-05]],\n",
       "               \n",
       "                        [[ 1.3616e-06,  2.8715e-06,  6.9109e-06],\n",
       "                         [-5.8312e-06, -8.3366e-06, -5.3908e-09],\n",
       "                         [-2.9931e-06, -2.8658e-06,  1.9354e-06]],\n",
       "               \n",
       "                        [[-1.0209e-06, -6.1541e-06, -2.1187e-06],\n",
       "                         [-6.7038e-06, -1.5934e-05, -1.2052e-05],\n",
       "                         [-6.8605e-06, -1.0175e-05, -9.9732e-06]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-5.6669e-05, -4.3113e-05, -3.2973e-05],\n",
       "                         [-4.5658e-05, -3.8774e-05, -2.2090e-05],\n",
       "                         [-8.6215e-05, -7.2827e-05, -5.3452e-05]],\n",
       "               \n",
       "                        [[ 1.1640e-04,  1.3112e-04,  1.3284e-04],\n",
       "                         [ 1.2972e-04,  1.3209e-04,  1.4257e-04],\n",
       "                         [ 8.1345e-05,  8.7237e-05,  9.9537e-05]],\n",
       "               \n",
       "                        [[ 5.5699e-04,  5.8572e-04,  6.6027e-04],\n",
       "                         [ 6.2147e-04,  6.3539e-04,  7.0788e-04],\n",
       "                         [ 4.7796e-04,  5.0228e-04,  5.4027e-04]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 9.2844e-05,  9.9655e-05,  1.0707e-04],\n",
       "                         [ 1.0091e-04,  1.1212e-04,  1.2581e-04],\n",
       "                         [ 1.0796e-04,  1.1651e-04,  1.2718e-04]],\n",
       "               \n",
       "                        [[ 1.1589e-04,  1.3381e-04,  1.4426e-04],\n",
       "                         [ 1.2781e-04,  1.4793e-04,  1.5086e-04],\n",
       "                         [ 1.4000e-04,  1.4373e-04,  1.4288e-04]],\n",
       "               \n",
       "                        [[ 4.6410e-05,  5.2902e-05,  5.5448e-05],\n",
       "                         [ 5.8372e-05,  6.7719e-05,  6.9776e-05],\n",
       "                         [ 6.7931e-05,  7.0582e-05,  7.1054e-05]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.9477e-08, -1.9241e-08, -1.8357e-08],\n",
       "                         [-2.0274e-08, -2.0286e-08, -1.9949e-08],\n",
       "                         [-1.9327e-08, -1.9144e-08, -1.8706e-08]],\n",
       "               \n",
       "                        [[-1.2993e-08, -1.0423e-08, -9.2905e-09],\n",
       "                         [-1.1519e-08, -1.0288e-08, -8.8197e-09],\n",
       "                         [-1.1433e-08, -9.9111e-09, -9.7060e-09]],\n",
       "               \n",
       "                        [[-2.2416e-08, -2.1352e-08, -1.9316e-08],\n",
       "                         [-2.2470e-08, -2.1539e-08, -2.0338e-08],\n",
       "                         [-2.1251e-08, -2.0551e-08, -1.9282e-08]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.conv.bias',\n",
       "               tensor([-1.0766e-04, -1.8208e-06,  1.7318e-09,  1.8022e-09, -6.0413e-13,\n",
       "                       -1.7608e-11, -2.4008e-07, -2.0747e-06, -5.4608e-14,  3.0420e-05,\n",
       "                        2.2418e-08, -1.2807e-04, -4.8821e-05,  7.3929e-06,  1.6470e-06,\n",
       "                        5.5269e-08,  5.2318e-08,  8.3448e-08, -1.9499e-07,  1.8654e-14,\n",
       "                        2.8759e-09, -1.4524e-11,  2.0112e-09,  8.9400e-07, -1.2196e-13,\n",
       "                        1.3235e-08,  1.6770e-06,  5.5523e-09,  6.5927e-08,  3.5236e-05,\n",
       "                        1.2349e-08, -2.5207e-05,  3.8266e-05,  1.2119e-05,  8.8170e-07,\n",
       "                       -5.8973e-06, -3.4360e-05,  2.8971e-06,  5.7751e-08,  1.6981e-05,\n",
       "                       -6.3020e-06,  6.4398e-07,  4.9946e-09,  7.9443e-06, -1.0413e-04,\n",
       "                        2.7680e-05,  3.4991e-06,  3.1208e-08,  6.2563e-06, -5.0337e-06,\n",
       "                        2.2825e-13, -1.4672e-05, -5.3406e-10,  1.0271e-08,  5.3552e-09,\n",
       "                       -6.2026e-06,  7.4721e-07, -1.1659e-08, -3.4967e-08,  1.5412e-07,\n",
       "                       -2.6710e-10, -3.6212e-09, -8.7714e-06, -1.0849e-05, -4.3477e-12,\n",
       "                        3.1768e-08,  2.3553e-06, -3.2610e-13, -5.6527e-08, -6.3316e-05,\n",
       "                        1.1863e-06,  5.1931e-08, -2.5802e-07, -5.8583e-08,  1.4091e-06,\n",
       "                        3.8682e-12,  1.0926e-05, -1.6802e-12,  7.9895e-08,  2.9296e-06,\n",
       "                       -9.4223e-13,  1.1736e-09,  9.6172e-08,  2.8990e-08,  6.0627e-13,\n",
       "                        4.9836e-08,  8.6083e-12, -5.9371e-07, -1.6258e-06,  1.0432e-11,\n",
       "                       -2.4476e-05, -1.3638e-08,  3.2943e-04,  1.2705e-07, -1.2575e-06,\n",
       "                       -1.8062e-05,  9.7118e-09, -2.5870e-05,  3.8594e-14,  7.8743e-06,\n",
       "                       -2.7276e-04, -1.2596e-08, -1.1878e-08, -4.4890e-14, -3.3862e-06,\n",
       "                        1.5649e-08,  5.7829e-06,  2.6338e-07,  4.0213e-07,  3.3605e-05,\n",
       "                       -3.6644e-07,  2.0759e-06, -8.1007e-09, -8.7374e-05, -9.9383e-07,\n",
       "                        5.5006e-05,  6.2239e-15,  1.0627e-05, -9.5537e-14, -9.8418e-05,\n",
       "                       -5.9476e-05,  8.2735e-09, -5.9381e-06, -5.5690e-05, -1.0448e-05,\n",
       "                        1.9987e-06,  1.1278e-08, -7.9950e-11], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.bias',\n",
       "               tensor([-2.5809e-02, -5.1914e-04, -1.8808e-03, -2.3333e-03, -2.3490e-08,\n",
       "                       -9.3809e-08, -1.1433e-03, -4.1157e-09, -1.8247e-10, -2.9051e-01,\n",
       "                       -4.2983e-02, -2.9330e-02, -1.7057e-01, -7.6430e-01,  6.6548e-01,\n",
       "                       -1.9083e-08, -3.5998e-02, -4.9337e-02, -3.2940e-03, -8.5166e-10,\n",
       "                       -9.6635e-08, -7.7134e-05, -4.5418e-10, -4.1506e-10, -8.6609e-08,\n",
       "                       -2.8743e-02, -5.0643e-10, -6.2151e-06, -5.3311e-06,  1.2237e-01,\n",
       "                       -1.1510e-02, -2.4103e-01,  4.5868e-02, -7.2895e-02, -4.2280e-02,\n",
       "                       -2.8928e-02, -7.9263e-01,  7.6610e-02, -3.7418e-02, -3.8003e-01,\n",
       "                       -1.5165e-02, -1.8458e-02, -1.6674e-10, -4.3972e-02, -6.6921e-02,\n",
       "                       -4.8274e-02, -3.9454e-04, -4.3398e-02, -4.9393e-02, -4.2917e-02,\n",
       "                       -5.5716e-09,  1.2862e-01, -2.2174e-06, -6.0387e-03, -1.0611e-02,\n",
       "                       -3.3598e-02, -8.0976e-02, -1.3177e-08, -1.5636e-03, -4.8358e-04,\n",
       "                       -8.5369e-05, -3.6528e-02, -9.1975e-02, -4.6839e-02, -2.4339e-09,\n",
       "                       -3.2042e-02, -4.6208e-02, -3.9834e-07, -8.4504e-09, -1.8178e-02,\n",
       "                       -8.6329e-03, -8.8436e-11, -1.9233e-02, -2.4675e-02, -4.5663e-07,\n",
       "                       -1.3715e-09,  9.3442e-02, -4.4416e-09, -2.4440e-02, -5.0634e-02,\n",
       "                       -4.7305e-09, -1.8038e-10, -3.5714e-02, -9.7317e-06, -7.6755e-10,\n",
       "                       -3.1391e-02, -5.0279e-09, -4.1558e-02, -2.7369e-09, -3.5630e-07,\n",
       "                       -4.7416e-03, -2.3511e-07, -2.5465e-02, -1.7810e-02, -5.8317e-09,\n",
       "                       -5.7122e-02, -3.1276e-02, -2.7462e-01, -7.6875e-10, -2.3602e-01,\n",
       "                       -2.5703e-02, -2.2502e-02, -7.5949e-10,  3.6854e-08,  1.4450e-01,\n",
       "                       -1.0464e-04, -2.3220e-02, -2.7830e-02, -1.8998e-02, -3.7490e-05,\n",
       "                       -5.6322e-03,  2.9649e-01, -6.5495e-02, -4.2253e-02, -1.3606e-02,\n",
       "                       -3.1794e-02, -3.1002e-09,  1.4254e-02, -1.7801e-09,  6.8282e-02,\n",
       "                       -2.5634e-02, -4.2781e-02,  2.6376e-01, -6.9117e-02, -2.1164e-10,\n",
       "                       -1.2218e-02, -1.5941e-02, -4.4482e-06], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.weight',\n",
       "               tensor([ 3.3526e-01,  1.9167e-04, -6.8391e-04, -2.1747e-04, -4.9496e-07,\n",
       "                       -4.9566e-11,  2.7613e-04,  2.4229e-19, -9.6174e-20,  1.8506e-01,\n",
       "                       -4.1493e-03,  4.6442e-01,  2.2548e-01,  3.4757e-01,  1.4051e-03,\n",
       "                        2.9909e-06,  1.9145e-03,  1.8588e-02,  7.4195e-04, -8.3158e-24,\n",
       "                        6.3285e-07,  3.1460e-05,  6.7130e-21,  4.8129e-08,  6.7764e-08,\n",
       "                       -3.7241e-03,  1.1095e-10,  2.8907e-19,  1.3859e-06,  1.9952e-01,\n",
       "                        3.4571e-03,  3.2557e-01,  2.8908e-01,  3.1386e-01,  1.3068e-02,\n",
       "                        5.2425e-01,  4.9968e-01,  1.8781e-01,  8.5281e-03,  3.8761e-01,\n",
       "                        4.9566e-03,  4.0077e-03,  8.2336e-11,  1.9664e-03,  3.7786e-01,\n",
       "                        4.0160e-01,  1.1259e-04, -3.9127e-03,  2.2199e-02,  1.3936e-02,\n",
       "                        1.5354e-07,  4.5942e-01,  3.7167e-05,  4.4951e-04, -1.1669e-03,\n",
       "                        5.1375e-03,  3.1180e-01, -2.7176e-15, -5.3439e-04, -5.6643e-05,\n",
       "                       -1.0769e-04,  3.6468e-03,  2.6200e-01,  1.8098e-02, -1.1121e-05,\n",
       "                        5.5202e-03, -2.2146e-02,  1.5499e-06,  9.7797e-14,  3.4626e-01,\n",
       "                        5.6925e-04,  3.1853e-14,  1.1209e-03, -1.3584e-03, -3.7818e-04,\n",
       "                        2.3882e-10,  1.7380e-01,  4.6401e-13, -1.0196e-02, -1.4447e-02,\n",
       "                        5.7228e-17, -1.4056e-09, -9.1493e-03, -7.7048e-05, -1.1837e-07,\n",
       "                        8.8579e-03,  4.8816e-15, -5.7310e-03,  3.4566e-08,  4.5127e-04,\n",
       "                        8.4108e-04,  5.6366e-06,  3.7016e-01, -6.6159e-03,  2.5826e-07,\n",
       "                        1.3502e-02, -6.2362e-03,  3.9604e-01, -3.4727e-08,  2.5340e-01,\n",
       "                        5.0110e-01,  1.5501e-03,  3.3106e-09,  1.3406e-16,  1.0688e-03,\n",
       "                        1.2580e-04, -2.7252e-03,  3.6551e-04,  1.0066e-03,  3.5513e-06,\n",
       "                        8.6921e-04, -2.7740e-03, -1.4297e-02,  3.5214e-01,  4.1803e-03,\n",
       "                        5.4773e-01,  1.6884e-23, -4.2999e-02,  4.7924e-08,  4.5025e-01,\n",
       "                        4.7415e-01, -2.8217e-03,  4.0074e-01,  3.5637e-01,  2.1544e-09,\n",
       "                        1.6793e-03, -6.8909e-04,  1.9759e-05], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.weight',\n",
       "               tensor([[[[ 1.5695e-03, -5.7032e-02, -2.9868e-03],\n",
       "                         [-2.7996e-02, -8.3531e-02,  3.1111e-02],\n",
       "                         [ 1.0478e-01, -1.0303e-01,  3.3634e-02]],\n",
       "               \n",
       "                        [[ 4.3103e-07, -2.0037e-07,  1.2237e-06],\n",
       "                         [ 5.7432e-07, -1.1839e-07,  1.1313e-06],\n",
       "                         [-7.8870e-07, -1.6104e-06, -8.5676e-07]],\n",
       "               \n",
       "                        [[ 3.8306e-06,  4.1996e-06,  4.3505e-06],\n",
       "                         [-2.1818e-07,  6.6459e-08, -1.0587e-08],\n",
       "                         [-3.4418e-06, -3.3818e-06, -3.8774e-06]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 8.3234e-05,  6.4253e-05,  7.0436e-05],\n",
       "                         [ 3.5203e-05,  2.2897e-05,  3.4650e-05],\n",
       "                         [-4.8081e-05, -4.6398e-05, -2.7905e-05]],\n",
       "               \n",
       "                        [[ 2.7242e-05,  1.3766e-05,  3.8717e-05],\n",
       "                         [ 8.5378e-07, -1.7300e-05,  1.1540e-05],\n",
       "                         [-7.9276e-05, -9.7125e-05, -7.1933e-05]],\n",
       "               \n",
       "                        [[ 2.8196e-08,  1.2342e-08,  5.7852e-08],\n",
       "                         [ 1.3742e-08, -4.5810e-10,  3.9569e-08],\n",
       "                         [ 1.4135e-07,  1.2498e-07,  1.6029e-07]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.6531e-03, -7.5278e-02, -6.4949e-02],\n",
       "                         [ 3.6331e-02, -8.7467e-02, -9.6507e-02],\n",
       "                         [ 2.4241e-02, -3.9710e-02,  5.6739e-03]],\n",
       "               \n",
       "                        [[ 5.1084e-06,  5.4070e-06,  7.3495e-06],\n",
       "                         [-3.8266e-07,  2.6003e-07,  2.5452e-06],\n",
       "                         [-1.3988e-06, -9.1484e-07,  1.6825e-06]],\n",
       "               \n",
       "                        [[ 6.1651e-06,  7.1286e-06,  1.0248e-05],\n",
       "                         [-1.9111e-06, -7.7084e-07,  3.0698e-06],\n",
       "                         [-1.1638e-06,  5.4033e-07,  5.6840e-06]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.2410e-04,  2.1415e-04,  2.3240e-04],\n",
       "                         [ 1.0465e-04,  1.2986e-04,  1.6661e-04],\n",
       "                         [ 8.4319e-05,  1.1937e-04,  1.5559e-04]],\n",
       "               \n",
       "                        [[ 2.1665e-04,  2.2382e-04,  2.8856e-04],\n",
       "                         [ 4.1147e-05,  6.3914e-05,  1.2887e-04],\n",
       "                         [ 2.8632e-06,  3.2031e-05,  1.0273e-04]],\n",
       "               \n",
       "                        [[-1.5103e-08, -6.7282e-09, -2.5531e-08],\n",
       "                         [-4.8396e-09, -5.8739e-11, -1.8241e-08],\n",
       "                         [-2.1287e-08, -1.1913e-08, -2.4971e-08]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 4.0683e-03,  5.2381e-02, -3.0850e-03],\n",
       "                         [ 6.8705e-02, -8.1159e-02,  5.3797e-02],\n",
       "                         [-2.1517e-03,  2.6284e-02, -6.2020e-02]],\n",
       "               \n",
       "                        [[ 2.8706e-06,  1.2291e-06,  2.2474e-06],\n",
       "                         [ 1.4600e-06, -1.0326e-07,  9.4203e-07],\n",
       "                         [ 2.3155e-06,  5.9170e-07,  1.5742e-06]],\n",
       "               \n",
       "                        [[ 3.0357e-06,  1.1254e-06,  4.9354e-06],\n",
       "                         [ 1.5113e-06, -5.5967e-07,  3.1499e-06],\n",
       "                         [ 2.4232e-06,  1.0744e-07,  3.2646e-06]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-6.8424e-05, -1.0420e-04, -3.7301e-05],\n",
       "                         [-3.8869e-05, -9.7091e-05, -7.5316e-05],\n",
       "                         [ 1.4194e-05, -4.0148e-05, -4.1150e-05]],\n",
       "               \n",
       "                        [[ 4.1896e-05,  8.8409e-06,  5.2717e-05],\n",
       "                         [ 2.4159e-05, -1.3305e-05,  2.0680e-05],\n",
       "                         [ 3.7933e-05,  1.2007e-06,  2.8198e-05]],\n",
       "               \n",
       "                        [[ 6.2969e-08,  4.6771e-08,  8.1204e-08],\n",
       "                         [ 1.3417e-08,  1.9803e-11,  4.0851e-08],\n",
       "                         [ 2.6059e-08,  1.0058e-08,  5.2158e-08]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-8.8614e-14,  6.8446e-13, -5.4504e-13],\n",
       "                         [-2.7504e-13,  8.0959e-13, -7.1600e-13],\n",
       "                         [ 1.8672e-12, -7.7381e-13, -1.0437e-12]],\n",
       "               \n",
       "                        [[-1.7152e-09,  1.0296e-09,  1.6819e-08],\n",
       "                         [ 2.1325e-09, -2.4045e-11, -8.2644e-11],\n",
       "                         [ 1.4497e-12, -1.7644e-15,  8.1426e-08]],\n",
       "               \n",
       "                        [[-7.5469e-17, -1.1966e-17,  1.4466e-17],\n",
       "                         [-5.1931e-17, -7.0395e-18, -6.6036e-17],\n",
       "                         [-1.6349e-16, -1.2963e-16, -6.2481e-17]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.4536e-15, -2.2394e-15, -1.0331e-15],\n",
       "                         [-2.7920e-15, -2.9148e-15, -1.0163e-15],\n",
       "                         [-3.5155e-15, -2.8757e-15, -1.6031e-15]],\n",
       "               \n",
       "                        [[-1.2591e-15,  9.4592e-16,  3.3485e-15],\n",
       "                         [-6.5137e-16,  1.1505e-15,  3.5876e-15],\n",
       "                         [ 4.7514e-16,  3.1834e-15,  5.5867e-15]],\n",
       "               \n",
       "                        [[ 6.0253e-08,  1.3847e-12,  4.8175e-14],\n",
       "                         [-1.1157e-14,  1.1654e-16,  7.5787e-09],\n",
       "                         [ 7.6728e-15,  8.4598e-15,  1.4966e-11]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 6.0716e-02, -5.4222e-02, -6.5542e-02],\n",
       "                         [ 7.6453e-02,  7.7619e-02,  5.3585e-02],\n",
       "                         [-3.6023e-03,  1.2287e-01,  7.2876e-02]],\n",
       "               \n",
       "                        [[ 1.9270e-06,  3.1721e-08,  3.7762e-09],\n",
       "                         [ 2.1262e-06, -3.0081e-08, -7.6382e-08],\n",
       "                         [-9.4317e-07, -2.8849e-06, -2.8278e-06]],\n",
       "               \n",
       "                        [[-1.1461e-06, -4.9580e-07, -3.5916e-07],\n",
       "                         [-1.1484e-07,  1.3600e-08,  5.5160e-07],\n",
       "                         [-2.9553e-06, -2.3556e-06, -1.3710e-06]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.2907e-04, -1.3657e-04, -1.6962e-04],\n",
       "                         [-1.1038e-04, -1.0456e-04, -1.1740e-04],\n",
       "                         [-1.1941e-04, -1.1952e-04, -1.2450e-04]],\n",
       "               \n",
       "                        [[-1.7785e-05, -3.4181e-05, -5.0757e-05],\n",
       "                         [-2.6008e-05, -4.2766e-05, -4.4512e-05],\n",
       "                         [-4.8817e-05, -6.0582e-05, -5.3144e-05]],\n",
       "               \n",
       "                        [[ 4.1328e-08,  3.3762e-08,  2.2801e-08],\n",
       "                         [ 3.2353e-09,  7.2394e-10, -1.4439e-08],\n",
       "                         [-1.7319e-08, -6.1286e-09, -3.0143e-08]]],\n",
       "               \n",
       "               \n",
       "                       [[[-5.5801e-02, -3.9283e-03, -2.1383e-02],\n",
       "                         [ 1.5353e-02, -4.3544e-03,  1.8595e-02],\n",
       "                         [-6.8868e-03,  5.3788e-02,  3.0877e-03]],\n",
       "               \n",
       "                        [[ 6.4186e-06,  8.9440e-06,  1.4311e-06],\n",
       "                         [-3.0039e-06, -1.3032e-07, -8.4991e-06],\n",
       "                         [-8.7553e-06, -6.5315e-06, -1.5881e-05]],\n",
       "               \n",
       "                        [[ 2.6454e-05,  2.1426e-05,  1.8478e-05],\n",
       "                         [ 7.2922e-06,  6.1884e-07, -1.3302e-06],\n",
       "                         [ 1.5738e-05,  4.3340e-06,  3.2809e-06]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 4.0983e-05,  8.7041e-05, -5.7032e-05],\n",
       "                         [-1.8424e-04, -1.2311e-04, -2.8319e-04],\n",
       "                         [-2.2099e-04, -1.6492e-04, -2.9923e-04]],\n",
       "               \n",
       "                        [[ 1.8022e-04,  2.3264e-04,  6.1339e-06],\n",
       "                         [-1.0308e-04, -3.2971e-05, -2.7344e-04],\n",
       "                         [-1.2316e-04, -1.4013e-04, -2.9492e-04]],\n",
       "               \n",
       "                        [[-6.5505e-09,  6.0620e-08, -2.5865e-08],\n",
       "                         [-6.2518e-08,  1.4481e-10, -8.5896e-08],\n",
       "                         [-6.0947e-08, -5.6647e-08, -1.3193e-07]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.bias',\n",
       "               tensor([ 4.5168e-05, -3.0117e-05,  3.6113e-05, -2.1736e-06,  1.6835e-04,\n",
       "                        3.5484e-05, -2.1631e-09, -4.2750e-08, -1.2322e-13, -8.8280e-06,\n",
       "                       -6.3897e-07,  1.5514e-08, -8.1350e-08, -8.0250e-06, -2.0894e-09,\n",
       "                        9.3035e-06,  1.4602e-09,  1.1298e-05,  1.0218e-05, -5.4976e-08,\n",
       "                        7.9948e-05,  1.0569e-08,  9.7000e-08, -8.1266e-06,  2.6625e-05,\n",
       "                       -1.4714e-06, -1.8307e-05, -1.2225e-04, -1.4232e-04, -6.8579e-05,\n",
       "                        9.3456e-06, -3.2339e-05, -7.2536e-14, -8.0666e-06,  3.7663e-05,\n",
       "                       -2.3811e-05,  9.0170e-08, -6.1606e-05, -1.4049e-05, -4.0197e-05,\n",
       "                       -1.3548e-05, -7.0012e-05,  1.6610e-15,  2.3690e-06, -1.7877e-08,\n",
       "                        3.3214e-05, -2.0823e-05, -1.1988e-06, -6.3415e-05,  1.4345e-07,\n",
       "                       -7.0419e-09, -2.8471e-05, -4.2456e-05,  3.5109e-06,  1.7452e-06,\n",
       "                       -4.3338e-05, -3.4289e-06,  5.4746e-08, -2.9869e-06,  6.5749e-05,\n",
       "                       -1.6871e-08,  2.6363e-05, -8.5679e-11, -1.1450e-05,  1.9462e-05,\n",
       "                        1.6616e-10, -5.2157e-15, -4.8548e-05, -3.9929e-06,  2.0022e-05,\n",
       "                       -2.3773e-05,  6.1655e-07, -7.5857e-09,  9.8063e-06, -8.6595e-06,\n",
       "                       -2.4165e-05, -1.8007e-08,  4.4242e-05, -3.1743e-05, -1.0179e-05,\n",
       "                        6.2616e-07, -2.5623e-05, -4.8078e-08, -5.2821e-08, -1.3029e-05,\n",
       "                       -8.6953e-06, -5.9220e-05, -1.2131e-04,  2.0129e-06,  4.8198e-08,\n",
       "                        3.1398e-05,  1.1218e-05, -1.1826e-05, -7.7615e-08,  1.2837e-04,\n",
       "                       -1.5474e-08,  3.1306e-05,  1.5030e-06, -2.2974e-07, -5.1837e-06,\n",
       "                       -1.3173e-05, -3.7323e-06,  2.8273e-06,  9.2429e-16, -4.1643e-05,\n",
       "                       -7.9699e-06,  2.6537e-05, -1.0435e-05, -5.1954e-07,  2.3600e-05,\n",
       "                       -1.1240e-08, -3.6531e-05,  1.6629e-09, -1.4557e-05, -3.3385e-05,\n",
       "                       -2.9059e-05, -8.6512e-07,  1.2803e-04,  2.6218e-05,  2.8815e-05,\n",
       "                       -1.5188e-07,  1.7123e-06, -1.3313e-05,  4.6257e-05,  3.2781e-07,\n",
       "                        3.9636e-07, -2.7265e-05,  7.2250e-06], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.bias',\n",
       "               tensor([-1.5115e-01, -3.1402e-01, -1.8303e-01, -1.2133e-02, -2.1663e-01,\n",
       "                       -2.4162e-01, -2.2323e-07, -4.9754e-02, -4.7388e-11, -3.0910e-01,\n",
       "                       -6.8840e-06, -4.8328e-02, -5.1842e-02, -6.3425e-06, -6.8398e-10,\n",
       "                       -1.5040e-07, -2.7792e-05, -9.4731e-02, -2.7108e-01, -4.8928e-08,\n",
       "                       -1.2728e-01, -5.2919e-03, -2.3063e-02, -1.8212e-02, -1.5119e-01,\n",
       "                       -3.9287e-02, -2.3425e-01, -1.8593e-01, -1.5186e-02, -1.2504e-01,\n",
       "                       -1.2405e-01,  6.3092e-02, -4.3638e-08, -1.9626e-01, -1.7277e-01,\n",
       "                       -2.4427e-01, -4.6361e-02, -1.9265e-01, -2.4029e-01, -2.0945e-01,\n",
       "                       -1.7426e-01, -5.4928e-02, -3.3216e-08, -5.7906e-02, -1.8073e-02,\n",
       "                       -2.8147e-01, -2.1982e-01, -2.2724e-01, -2.0162e-01, -6.2892e-02,\n",
       "                       -1.4753e-03, -2.2835e-01, -1.6745e-01, -9.9206e-10, -6.7113e-02,\n",
       "                       -1.4650e-01, -1.1744e-04, -1.7754e-04, -1.4878e-01, -1.8586e-01,\n",
       "                       -3.8858e-04, -2.4775e-01, -4.7108e-06, -1.5611e-01, -1.2541e-01,\n",
       "                       -1.9238e-06, -4.8290e-11, -4.4985e-01, -8.4414e-02, -3.5834e-02,\n",
       "                       -1.1117e-01, -5.0948e-02, -7.1629e-03, -2.3098e-01, -1.8589e-01,\n",
       "                       -2.3255e-01, -8.5299e-03, -1.6975e-01, -1.9599e-01, -1.4893e-01,\n",
       "                       -4.0870e-02, -5.5576e-03, -7.6101e-03, -4.4184e-02, -1.5317e-01,\n",
       "                       -2.5087e-01, -3.1078e-01, -2.0881e-01, -1.5448e-01, -1.7251e-02,\n",
       "                       -2.1197e-01, -5.7133e-02, -1.3835e-01, -1.2130e-02, -2.4563e-01,\n",
       "                       -3.7382e-03, -2.8209e-01, -1.4660e-02, -5.3973e-02, -3.4547e-01,\n",
       "                       -1.9406e-01, -1.2694e-02, -6.6265e-05, -7.5035e-06, -1.8469e-01,\n",
       "                       -1.4090e-02, -1.3891e-01, -1.3155e-01, -4.4367e-02, -2.2380e-01,\n",
       "                       -3.6748e-02, -2.0704e-01, -1.1590e-02, -1.5942e-09, -2.1537e-01,\n",
       "                       -2.3270e-01, -3.5155e-02, -2.8855e-01, -1.6806e-01, -2.1776e-01,\n",
       "                       -5.2874e-02, -1.6782e-01, -2.1668e-01, -2.0899e-01, -4.4701e-02,\n",
       "                       -2.6233e-09, -2.3584e-01, -2.8385e-01], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.weight',\n",
       "               tensor([ 2.6532e-01,  2.0715e-01,  3.8209e-01, -1.8945e-03,  4.9843e-01,\n",
       "                        2.1708e-01, -5.8063e-05, -1.1586e-03, -4.9505e-08,  4.8084e-01,\n",
       "                       -7.6735e-11,  2.9084e-03, -2.4962e-02,  1.5918e-04, -2.5288e-10,\n",
       "                       -9.1117e-13, -3.2566e-04, -1.4554e-02,  1.8416e-01,  2.0516e-08,\n",
       "                        2.7979e-01, -8.6358e-04,  3.5524e-03,  2.2911e-03,  2.8209e-01,\n",
       "                       -6.7828e-03,  4.1100e-01,  5.7101e-01,  2.3564e-01,  2.3557e-01,\n",
       "                        1.8358e-01,  1.9113e-01,  1.4182e-12,  3.8140e-01,  2.8341e-01,\n",
       "                        1.7351e-01,  4.9462e-03,  3.1468e-01,  3.3420e-01,  3.5592e-01,\n",
       "                        2.0780e-01,  2.9089e-01,  6.7969e-10, -5.0611e-03, -2.5277e-03,\n",
       "                        3.5446e-01,  3.4840e-01,  3.6084e-01,  2.9067e-01,  4.0440e-03,\n",
       "                       -7.0053e-04,  1.4181e-01,  4.2094e-01, -2.2558e-06,  2.3988e-01,\n",
       "                        3.4501e-01,  1.3630e-04,  2.5436e-04,  8.7545e-02,  3.4499e-01,\n",
       "                       -2.2140e-04,  3.0961e-01,  3.6507e-05,  2.5331e-01,  2.8411e-01,\n",
       "                        1.4401e-04,  2.5930e-14,  3.6041e-01,  2.0267e-01,  4.7530e-03,\n",
       "                        3.8362e-01,  4.7990e-03,  1.3029e-03,  3.8613e-01,  3.8848e-01,\n",
       "                        2.9537e-01, -1.6289e-03,  3.0565e-01,  3.0061e-01,  2.0382e-01,\n",
       "                       -1.6124e-02, -1.1573e-03,  1.0887e-03,  6.8003e-03,  2.3247e-01,\n",
       "                        3.5745e-01,  4.6195e-01,  6.7543e-01,  3.1324e-01,  1.6888e-03,\n",
       "                        2.1650e-01,  2.1976e-02,  1.5640e-01, -3.0253e-04,  5.1016e-01,\n",
       "                        8.7720e-04,  2.2294e-01, -1.7185e-03, -1.6013e-02,  1.3848e-01,\n",
       "                        1.8158e-01, -2.0348e-03, -3.3019e-04, -5.6111e-10,  3.9194e-01,\n",
       "                       -1.7497e-03,  2.9699e-01,  2.6754e-01, -6.6517e-03,  2.5667e-01,\n",
       "                       -3.3140e-04,  4.9930e-01, -1.3358e-03,  7.7385e-11,  2.9451e-01,\n",
       "                        2.9847e-01, -2.9918e-03,  4.4894e-01,  2.6036e-01,  3.2710e-01,\n",
       "                       -6.9202e-03,  3.0018e-01,  3.7325e-01,  2.9502e-01,  2.5506e-02,\n",
       "                        1.6260e-06,  2.9978e-01,  3.3959e-01], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.weight',\n",
       "               tensor([[[[-1.7331e-02, -1.0295e-02,  6.0006e-02],\n",
       "                         [ 1.4751e-02,  1.1840e-02,  4.4171e-02],\n",
       "                         [-1.8700e-02,  9.9152e-03,  9.8307e-03]],\n",
       "               \n",
       "                        [[ 3.2548e-02, -3.1620e-03,  4.7042e-02],\n",
       "                         [ 8.7354e-03, -4.6184e-02, -2.6271e-02],\n",
       "                         [ 3.9488e-02, -1.0486e-02, -4.3416e-02]],\n",
       "               \n",
       "                        [[-3.5336e-03, -4.2004e-02,  9.4704e-02],\n",
       "                         [ 2.4011e-02,  6.0114e-02,  1.0110e-01],\n",
       "                         [-4.9922e-02,  1.9818e-02, -3.7397e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 8.4008e-12,  1.3610e-11,  1.9948e-11],\n",
       "                         [-2.4882e-10,  2.6633e-11,  2.2842e-11],\n",
       "                         [-1.2628e-10, -1.2588e-10, -1.1820e-10]],\n",
       "               \n",
       "                        [[-2.6030e-02, -2.2646e-02, -4.5673e-02],\n",
       "                         [-3.6530e-02,  4.4861e-02, -3.2543e-02],\n",
       "                         [-9.9535e-02, -7.1312e-02, -3.1628e-02]],\n",
       "               \n",
       "                        [[-5.0294e-02, -2.0907e-03, -5.2632e-02],\n",
       "                         [-5.4901e-02,  8.1871e-03, -2.9578e-02],\n",
       "                         [-2.3333e-02,  2.3127e-02,  5.3094e-03]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.3915e-08, -1.3539e-08, -1.0208e-08],\n",
       "                         [-7.6919e-09, -1.0022e-08, -1.1036e-08],\n",
       "                         [-1.6319e-08, -1.6095e-08, -1.8324e-08]],\n",
       "               \n",
       "                        [[-7.1355e-09, -1.8958e-09, -5.7762e-09],\n",
       "                         [-8.8858e-09, -1.0907e-08, -7.5267e-09],\n",
       "                         [-1.3050e-08, -4.8257e-09, -6.3485e-09]],\n",
       "               \n",
       "                        [[ 1.1903e-09, -9.5016e-10, -4.6731e-09],\n",
       "                         [ 3.1978e-09, -2.5720e-09, -5.3921e-09],\n",
       "                         [-1.1078e-09, -4.2107e-09,  1.2604e-09]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 3.5160e-17,  1.1911e-17,  2.7220e-17],\n",
       "                         [ 1.1128e-07, -1.7800e-20,  6.4114e-18],\n",
       "                         [ 1.8461e-07, -1.7758e-13, -2.3470e-07]],\n",
       "               \n",
       "                        [[-4.5536e-09,  3.8789e-09,  1.4197e-09],\n",
       "                         [-8.1445e-09, -7.2252e-09, -3.6486e-09],\n",
       "                         [-1.3546e-08, -8.2122e-10, -5.5370e-09]],\n",
       "               \n",
       "                        [[ 1.2165e-08,  1.5477e-08,  1.8301e-08],\n",
       "                         [ 1.1084e-08,  1.2940e-08,  1.1914e-08],\n",
       "                         [ 6.5109e-09,  1.2531e-08,  8.0403e-09]]],\n",
       "               \n",
       "               \n",
       "                       [[[-2.0525e-02,  1.5007e-04, -6.8799e-04],\n",
       "                         [ 1.3207e-02, -2.5649e-02, -4.2961e-02],\n",
       "                         [-4.4341e-03, -4.7235e-02, -5.5492e-02]],\n",
       "               \n",
       "                        [[-1.4054e-02,  5.3095e-02,  1.9296e-02],\n",
       "                         [ 7.4229e-03, -4.8933e-02,  7.1470e-03],\n",
       "                         [-1.2933e-02, -7.3876e-02, -9.2843e-02]],\n",
       "               \n",
       "                        [[-1.6037e-02,  3.1129e-02,  1.1733e-01],\n",
       "                         [-3.2431e-02,  5.7320e-02,  5.8305e-02],\n",
       "                         [-6.3027e-02, -1.6542e-02,  1.3459e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-3.2700e-10, -3.7454e-12, -1.6353e-10],\n",
       "                         [-3.0087e-10,  1.8615e-11, -1.4256e-10],\n",
       "                         [-4.9974e-10, -1.6924e-10, -3.0955e-10]],\n",
       "               \n",
       "                        [[-1.0961e-02,  8.5780e-02,  6.3631e-02],\n",
       "                         [ 2.8655e-02,  1.5317e-02,  5.1010e-03],\n",
       "                         [ 6.3905e-02,  7.3681e-02,  4.9896e-02]],\n",
       "               \n",
       "                        [[-6.6658e-02,  4.9188e-02, -6.8371e-05],\n",
       "                         [-2.6394e-02,  2.4509e-02, -6.3541e-02],\n",
       "                         [ 2.9110e-02, -8.6412e-03, -8.7618e-03]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-2.4340e-03, -7.6446e-02, -4.8478e-02],\n",
       "                         [-1.8862e-02, -5.6813e-02, -2.8551e-02],\n",
       "                         [-2.7547e-02,  9.6841e-03, -4.1929e-02]],\n",
       "               \n",
       "                        [[-9.8509e-02, -5.6164e-02, -1.8223e-02],\n",
       "                         [-9.2969e-02, -8.3309e-02, -4.7287e-02],\n",
       "                         [-6.3075e-02, -8.8742e-02, -5.0179e-02]],\n",
       "               \n",
       "                        [[-4.3223e-02, -9.3727e-02, -4.2817e-02],\n",
       "                         [ 9.1428e-03,  4.6011e-02, -9.3698e-03],\n",
       "                         [ 2.0409e-02,  6.4687e-02,  2.5330e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 4.8324e-10,  1.1324e-10,  3.6723e-10],\n",
       "                         [ 4.0323e-10, -3.5206e-11,  1.7468e-10],\n",
       "                         [ 6.3891e-10,  3.5280e-10,  8.3946e-10]],\n",
       "               \n",
       "                        [[ 3.2267e-02,  2.4582e-02,  8.6782e-02],\n",
       "                         [ 1.1874e-02, -5.1519e-02, -6.6258e-03],\n",
       "                         [ 9.2948e-02, -3.0583e-03, -4.3539e-02]],\n",
       "               \n",
       "                        [[-7.7873e-02,  4.6624e-02,  6.2669e-02],\n",
       "                         [-8.6713e-02,  1.7010e-03,  6.5482e-02],\n",
       "                         [-1.0964e-01, -7.4254e-03,  3.4417e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 3.8917e-02,  4.7500e-02,  5.0847e-02],\n",
       "                         [ 2.4168e-02, -1.4516e-02, -3.8104e-02],\n",
       "                         [-6.3570e-05, -1.7624e-02, -5.0674e-03]],\n",
       "               \n",
       "                        [[-4.4582e-02, -5.3816e-02, -5.7317e-02],\n",
       "                         [-8.4361e-02, -5.4290e-02, -4.6439e-02],\n",
       "                         [-1.1457e-01,  5.1760e-03, -2.5250e-02]],\n",
       "               \n",
       "                        [[-1.0224e-01, -6.3555e-02,  3.9176e-02],\n",
       "                         [-1.0838e-01, -6.0592e-02,  1.5014e-02],\n",
       "                         [ 8.9480e-02,  3.7615e-02, -1.7611e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.1091e-11,  8.6899e-11,  1.9585e-10],\n",
       "                         [-1.0885e-10, -2.4445e-11, -1.5381e-10],\n",
       "                         [-1.3987e-10, -2.5605e-11,  3.7611e-11]],\n",
       "               \n",
       "                        [[-2.4591e-02,  9.2847e-02,  7.3792e-02],\n",
       "                         [ 6.3681e-03, -3.6026e-02, -4.5709e-02],\n",
       "                         [-4.1956e-02, -5.0628e-03,  2.1480e-02]],\n",
       "               \n",
       "                        [[-4.2018e-02, -2.1273e-02, -2.1377e-02],\n",
       "                         [-1.0082e-02, -9.3999e-03,  4.4530e-03],\n",
       "                         [ 3.0255e-02,  4.7723e-02,  3.0611e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-4.0684e-09, -2.1106e-09, -2.1904e-09],\n",
       "                         [-2.0228e-09, -1.9443e-09, -2.7149e-09],\n",
       "                         [-4.5859e-09, -3.6660e-09, -3.6155e-09]],\n",
       "               \n",
       "                        [[-1.0328e-09, -2.3321e-09, -1.5114e-09],\n",
       "                         [-1.7891e-09, -2.2804e-09, -1.6883e-09],\n",
       "                         [-2.4500e-09, -2.4208e-09, -1.7160e-09]],\n",
       "               \n",
       "                        [[-4.3234e-10, -2.2020e-09, -1.0053e-10],\n",
       "                         [ 2.5660e-09,  4.9719e-10, -1.2553e-09],\n",
       "                         [-3.1708e-10, -1.1552e-09,  2.4696e-09]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.5557e-08,  1.5582e-10,  1.2349e-16],\n",
       "                         [-1.4798e-11, -1.9435e-13,  2.0571e-14],\n",
       "                         [-1.5178e-05,  1.5439e-06,  1.7371e-10]],\n",
       "               \n",
       "                        [[ 8.8018e-11,  1.8227e-09,  2.7502e-09],\n",
       "                         [-1.8966e-09, -4.7839e-09, -2.7987e-09],\n",
       "                         [-3.8434e-09, -3.6197e-09, -5.6636e-10]],\n",
       "               \n",
       "                        [[ 2.9312e-09,  4.4875e-09,  4.3292e-09],\n",
       "                         [ 3.1608e-09,  3.6188e-09,  3.8885e-09],\n",
       "                         [ 1.8508e-09,  2.5927e-09,  1.9351e-09]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.bias',\n",
       "               tensor([-5.3384e-05,  5.5572e-07,  1.3152e-05,  6.0222e-05,  4.7515e-06,\n",
       "                        2.8423e-05, -5.4325e-05,  3.5956e-05,  4.4897e-05, -3.2208e-05,\n",
       "                        5.3095e-08, -4.4170e-05, -2.3967e-05, -5.7456e-06,  1.9538e-05,\n",
       "                       -2.3510e-05, -8.4308e-06,  5.2713e-06,  3.5566e-05,  1.1082e-05,\n",
       "                        4.6182e-06,  1.6678e-05,  5.6850e-05,  1.0873e-05, -9.0663e-06,\n",
       "                        1.1822e-05,  4.7681e-05,  6.2435e-05, -1.2264e-05,  6.4167e-05,\n",
       "                       -2.3430e-05, -2.4236e-05, -2.1450e-06,  5.0586e-05, -5.4256e-06,\n",
       "                       -1.6500e-05, -2.5635e-05, -7.3973e-05, -2.7957e-05, -2.5788e-05,\n",
       "                       -2.3111e-05,  1.0915e-06, -4.1539e-05,  9.3469e-06,  6.3956e-07,\n",
       "                        1.0264e-05,  9.2556e-06,  2.4093e-05,  3.9909e-05, -7.1425e-05,\n",
       "                        1.1722e-05, -6.6994e-06,  1.2229e-05, -1.0588e-04, -3.7606e-05,\n",
       "                       -2.8187e-05,  9.3681e-05,  2.1290e-05,  2.6317e-05, -3.7702e-05,\n",
       "                        5.4658e-06, -8.9180e-06, -8.5006e-08, -2.4837e-05,  1.4544e-05,\n",
       "                       -5.0152e-06,  5.9595e-06,  1.0830e-05, -4.6397e-05,  8.7463e-05,\n",
       "                       -2.2675e-06,  4.3275e-05,  1.1086e-05,  1.6485e-05, -3.6259e-05,\n",
       "                        1.1657e-05, -2.8464e-05, -1.4426e-05, -3.0550e-07,  5.8997e-05,\n",
       "                       -1.2027e-04,  5.7587e-05, -6.8657e-05, -2.7564e-05,  2.2999e-05,\n",
       "                       -2.8612e-05, -1.4204e-04,  3.2883e-11,  9.0800e-06, -8.7231e-05,\n",
       "                        4.3347e-05,  6.1474e-05,  3.1957e-08, -1.7775e-05,  1.3384e-05,\n",
       "                        8.8116e-05, -3.2787e-05, -1.6790e-06,  3.0065e-05,  1.1317e-05,\n",
       "                       -6.2813e-05, -4.9765e-05,  4.6875e-06,  4.0287e-05,  8.7538e-06,\n",
       "                        1.2088e-05, -8.4215e-06,  2.6617e-05, -1.8096e-05, -8.3361e-06,\n",
       "                        6.2211e-06,  3.5478e-05, -1.2395e-07,  2.8924e-05,  8.4677e-05,\n",
       "                        2.6372e-05,  5.6342e-06, -1.9853e-05,  7.4075e-06, -1.1090e-05,\n",
       "                        3.6349e-05,  3.5301e-05,  1.7749e-05,  6.1878e-05, -9.1290e-06,\n",
       "                        3.6832e-06, -4.2189e-05, -2.8849e-08], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.bias',\n",
       "               tensor([-2.8877e-01, -2.5261e-06, -2.3193e-01, -3.2576e-01, -3.3149e-01,\n",
       "                       -2.6402e-01, -3.9454e-01, -3.4977e-01, -1.5933e-01, -2.4774e-01,\n",
       "                       -7.4630e-02, -2.6290e-01, -3.8616e-01, -3.6579e-01, -2.9344e-01,\n",
       "                       -2.4617e-01, -2.1937e-01, -4.2036e-01, -3.9470e-01, -4.7105e-01,\n",
       "                       -3.6036e-01, -2.9724e-01, -4.5556e-01, -2.4364e-01, -1.9501e-01,\n",
       "                       -3.0994e-01, -2.0532e-01, -3.4752e-01, -2.5188e-01, -2.1788e-01,\n",
       "                       -4.9082e-01, -4.9991e-01, -2.9614e-01, -2.6678e-01, -2.2864e-01,\n",
       "                       -2.9664e-01, -2.5535e-01, -3.1973e-01, -3.5434e-01, -2.7234e-01,\n",
       "                       -2.4243e-01, -3.8635e-01, -2.6127e-01, -2.3128e-01, -3.1675e-01,\n",
       "                       -3.2966e-01, -2.5050e-01, -3.6137e-01, -2.7179e-01, -3.7908e-01,\n",
       "                       -6.3067e-01, -2.7521e-01, -4.2928e-01, -2.6467e-01, -3.9084e-01,\n",
       "                       -3.4539e-01, -3.0123e-01, -3.4827e-01, -3.1838e-01, -2.7537e-01,\n",
       "                       -3.3725e-01, -2.7659e-01, -3.7621e-02, -4.2970e-01, -3.1315e-01,\n",
       "                       -1.9243e-01, -3.1630e-01, -3.1210e-01, -2.2573e-01, -3.5831e-01,\n",
       "                       -2.9017e-01, -3.8468e-01, -3.2735e-01, -2.7492e-01, -1.4545e-01,\n",
       "                       -2.6132e-01, -3.3288e-01, -2.8789e-01, -9.5447e-03, -2.3727e-01,\n",
       "                       -3.6506e-01, -3.1933e-01, -3.4813e-01, -2.3968e-01, -2.3048e-01,\n",
       "                       -3.2235e-01, -4.5712e-01, -8.2832e-11, -3.7387e-02, -4.2282e-01,\n",
       "                       -2.6802e-01, -4.1776e-01, -3.5843e-02, -2.3747e-01, -3.4734e-01,\n",
       "                       -3.4931e-01, -3.9029e-01, -2.8163e-01, -3.4240e-01, -6.5988e-01,\n",
       "                       -4.1117e-01, -3.7355e-01, -2.6374e-01, -2.5283e-01, -2.4225e-01,\n",
       "                       -3.8587e-01, -3.4695e-01, -1.7012e-01, -3.5898e-01, -2.2599e-01,\n",
       "                       -3.6073e-01, -3.1948e-01, -2.3852e-01, -3.0432e-01, -2.8615e-01,\n",
       "                       -4.8673e-01, -2.4185e-01, -2.8005e-01, -2.2292e-01, -3.3806e-01,\n",
       "                       -3.4269e-01, -2.2614e-01, -2.9892e-01, -2.8446e-01, -2.6000e-01,\n",
       "                       -2.8056e-01, -3.4189e-01, -6.5306e-07], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.weight',\n",
       "               tensor([ 2.0678e-01, -9.5074e-05,  2.4242e-01,  2.2641e-01,  3.3979e-01,\n",
       "                        2.6554e-01,  2.6870e-01,  3.4303e-01,  2.5907e-01,  2.5934e-01,\n",
       "                       -4.6513e-03,  2.5524e-01,  2.8665e-01,  3.4962e-01,  3.4227e-01,\n",
       "                        2.6426e-01,  2.6422e-01,  3.8013e-01,  2.7958e-01,  2.6097e-01,\n",
       "                        2.3944e-01,  2.8393e-01,  3.0921e-01,  1.5783e-01,  2.8303e-01,\n",
       "                        2.8895e-01,  2.6113e-01,  3.4879e-01,  1.7416e-01,  2.5073e-01,\n",
       "                        2.9264e-01,  5.9240e-01,  2.2539e-01,  2.4922e-01,  2.7103e-01,\n",
       "                        3.2336e-01,  2.5207e-01,  2.5241e-01,  2.6298e-01,  2.5556e-01,\n",
       "                        2.8258e-01,  2.3481e-01,  2.4579e-01,  2.1036e-01,  2.8397e-01,\n",
       "                        1.4527e-01,  2.1243e-01,  3.3618e-01,  2.9245e-01,  2.8892e-01,\n",
       "                        3.2394e-01,  3.4894e-01,  3.8505e-01,  2.3979e-01,  3.7395e-01,\n",
       "                        1.9660e-01,  2.6565e-01,  3.2480e-01,  2.3506e-01,  2.9744e-01,\n",
       "                        1.9628e-01,  2.0955e-01,  4.4421e-03,  2.8274e-01,  1.9789e-01,\n",
       "                        2.1098e-01,  2.7745e-01,  1.3352e-01,  2.7085e-01,  2.2034e-01,\n",
       "                        2.9111e-01,  3.1565e-01,  2.2906e-01,  1.7416e-01,  1.3961e-01,\n",
       "                        2.5378e-01,  3.2304e-01,  2.6205e-01, -9.1787e-04,  2.0702e-01,\n",
       "                        2.3118e-01,  3.5330e-01,  3.2838e-01,  2.5715e-01,  1.6050e-01,\n",
       "                        2.2387e-01,  2.7300e-01, -5.7141e-09, -3.0046e-03,  3.7713e-01,\n",
       "                        2.4274e-01,  3.8799e-01, -5.8702e-03,  2.5189e-01,  3.2774e-01,\n",
       "                        3.2763e-01,  2.8939e-01,  1.0957e-01,  3.5117e-01,  3.8153e-01,\n",
       "                        4.3353e-01,  4.1552e-01,  2.5881e-01,  1.9501e-01,  2.3573e-01,\n",
       "                        2.8571e-01,  2.6685e-01,  2.8701e-01,  3.0291e-01,  2.8970e-01,\n",
       "                        2.1864e-01,  3.0988e-01,  2.4018e-01,  3.0244e-01,  2.7472e-01,\n",
       "                        3.7712e-01,  2.5809e-01,  2.9308e-01,  3.1725e-01,  3.5354e-01,\n",
       "                        2.9664e-01,  1.9477e-01,  1.8307e-01,  3.1260e-01,  2.4936e-01,\n",
       "                        2.5207e-01,  3.4603e-01, -1.0371e-04], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.weight',\n",
       "               tensor([[[[ 7.0982e-02,  7.9410e-02,  3.0355e-03],\n",
       "                         [ 1.7107e-02,  6.1006e-02,  7.4279e-02],\n",
       "                         [ 1.7022e-02, -6.4248e-03,  5.5946e-02]],\n",
       "               \n",
       "                        [[ 9.8332e-09,  1.8531e-08, -8.8301e-09],\n",
       "                         [-1.0304e-08,  3.0064e-10, -2.3638e-08],\n",
       "                         [-5.9089e-09, -9.8482e-09,  1.6421e-08]],\n",
       "               \n",
       "                        [[ 5.0339e-02,  2.7086e-02, -5.3247e-02],\n",
       "                         [ 4.2114e-02,  1.0880e-02, -7.0717e-02],\n",
       "                         [-2.9786e-03,  2.6186e-03,  7.5392e-04]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.0683e-02,  2.1060e-03,  4.0872e-02],\n",
       "                         [-9.6171e-02, -6.6975e-02, -1.0878e-02],\n",
       "                         [-6.9846e-02, -3.2795e-02,  4.5723e-03]],\n",
       "               \n",
       "                        [[-4.9733e-02, -3.4573e-02, -4.4569e-02],\n",
       "                         [-6.5452e-02, -6.2136e-02, -4.1287e-02],\n",
       "                         [-7.9759e-02, -4.9359e-02, -9.6032e-02]],\n",
       "               \n",
       "                        [[ 2.7246e-09,  4.5237e-09, -2.5491e-09],\n",
       "                         [-2.8867e-09,  6.8706e-12, -5.1977e-09],\n",
       "                         [ 4.0512e-09,  5.1350e-09,  6.8290e-09]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 6.5778e-02,  5.8882e-02, -3.1478e-02],\n",
       "                         [ 1.6190e-02, -4.6832e-02, -3.2797e-02],\n",
       "                         [ 5.3279e-02, -2.5976e-03, -3.3902e-02]],\n",
       "               \n",
       "                        [[-4.1491e-08, -1.6706e-08, -7.9568e-08],\n",
       "                         [-4.4218e-08,  6.9352e-10, -9.3721e-08],\n",
       "                         [-1.6869e-07, -1.3960e-07, -2.2876e-07]],\n",
       "               \n",
       "                        [[ 5.7947e-02, -2.1033e-02,  7.8558e-02],\n",
       "                         [-7.4864e-02, -5.4443e-02, -8.8665e-03],\n",
       "                         [ 6.9208e-03,  4.1272e-02,  9.1094e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-5.9438e-02, -1.4834e-04, -1.8204e-02],\n",
       "                         [ 8.5289e-03, -3.5314e-02,  4.3399e-02],\n",
       "                         [-9.7253e-03, -6.2594e-02,  1.6519e-02]],\n",
       "               \n",
       "                        [[-5.3520e-02,  8.3235e-02,  5.9917e-02],\n",
       "                         [ 3.3629e-02,  4.4292e-02,  3.9087e-02],\n",
       "                         [-3.4302e-03,  8.4357e-02, -2.4349e-02]],\n",
       "               \n",
       "                        [[-2.3189e-08, -4.6250e-09, -2.7872e-08],\n",
       "                         [-1.4601e-08,  1.7770e-10, -2.8017e-08],\n",
       "                         [-4.6100e-08, -3.8590e-08, -6.2809e-08]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 6.8090e-03,  8.3769e-03,  2.6661e-03],\n",
       "                         [-1.6320e-02, -3.2914e-02, -5.5196e-03],\n",
       "                         [ 8.1040e-03, -2.2148e-02, -1.6673e-02]],\n",
       "               \n",
       "                        [[-1.2260e-07,  3.7368e-08, -4.7756e-08],\n",
       "                         [-2.0057e-07,  2.4082e-09, -1.2699e-07],\n",
       "                         [ 1.5508e-07,  1.7639e-08,  1.7988e-08]],\n",
       "               \n",
       "                        [[ 8.2727e-02,  6.7308e-02, -4.5194e-02],\n",
       "                         [ 5.2855e-02,  8.8404e-03, -1.0772e-01],\n",
       "                         [ 4.8017e-02,  5.9041e-02, -3.1666e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-5.0887e-02,  1.8263e-02, -5.6861e-02],\n",
       "                         [-1.1412e-02,  6.1381e-02,  1.9727e-02],\n",
       "                         [ 1.7353e-02,  8.5976e-02,  2.1617e-02]],\n",
       "               \n",
       "                        [[ 2.4995e-02,  8.3277e-03, -5.3684e-02],\n",
       "                         [ 1.2380e-01,  5.6386e-02,  2.0702e-02],\n",
       "                         [ 7.4589e-02,  4.5642e-02,  1.6249e-02]],\n",
       "               \n",
       "                        [[ 3.6409e-08,  1.4030e-08,  2.9305e-08],\n",
       "                         [ 3.6743e-08,  7.1305e-10,  1.5890e-08],\n",
       "                         [ 8.7513e-08,  4.9962e-08,  5.8368e-08]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-3.3791e-02, -4.3370e-03, -1.8219e-02],\n",
       "                         [-4.4120e-03,  2.7360e-02, -4.1297e-02],\n",
       "                         [ 3.9245e-02,  3.7069e-02, -4.2522e-02]],\n",
       "               \n",
       "                        [[-1.3034e-07, -1.2226e-07, -1.6549e-07],\n",
       "                         [-7.8189e-08,  2.8045e-09, -3.9921e-08],\n",
       "                         [-1.5518e-07, -1.4035e-07, -1.7120e-07]],\n",
       "               \n",
       "                        [[ 1.8813e-02,  1.0196e-01,  9.2164e-02],\n",
       "                         [ 1.0857e-01,  1.9857e-02,  8.9373e-02],\n",
       "                         [-2.3130e-02, -1.0688e-02, -8.9704e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-6.7933e-02, -7.3628e-02, -9.5557e-02],\n",
       "                         [-6.8063e-02, -4.6807e-02, -4.3664e-02],\n",
       "                         [-5.2275e-02, -7.1555e-02, -5.7333e-02]],\n",
       "               \n",
       "                        [[-2.4382e-02, -6.2317e-03,  6.2511e-03],\n",
       "                         [ 9.1717e-03, -5.0477e-02,  8.7841e-03],\n",
       "                         [-4.1638e-02, -7.3323e-02, -6.7270e-02]],\n",
       "               \n",
       "                        [[-5.5694e-08, -2.9984e-08, -4.0238e-08],\n",
       "                         [-1.9129e-08,  9.3745e-10, -1.0803e-08],\n",
       "                         [-7.9667e-08, -6.3053e-08, -5.6711e-08]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 5.2338e-02,  6.0799e-02,  3.1711e-02],\n",
       "                         [ 1.2327e-02,  1.6936e-02,  5.8571e-03],\n",
       "                         [ 1.7760e-02,  2.9862e-02,  5.8698e-03]],\n",
       "               \n",
       "                        [[ 3.5637e-08,  6.9852e-08,  3.5410e-08],\n",
       "                         [-2.6170e-08,  2.1785e-09, -4.0555e-08],\n",
       "                         [-1.0676e-07, -8.9558e-08, -1.2063e-07]],\n",
       "               \n",
       "                        [[ 1.5593e-02,  3.3697e-02, -1.8942e-02],\n",
       "                         [ 1.3374e-02,  2.6558e-03,  2.1688e-02],\n",
       "                         [ 1.6505e-02, -8.2735e-03,  2.9050e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 9.2746e-03,  6.8915e-02,  5.2035e-02],\n",
       "                         [ 3.4178e-03,  7.4335e-02,  6.6590e-02],\n",
       "                         [ 1.3377e-02,  5.1529e-02,  8.9151e-02]],\n",
       "               \n",
       "                        [[-1.8626e-03,  1.7149e-03, -1.9535e-02],\n",
       "                         [-6.0669e-02, -2.7301e-02, -5.9660e-03],\n",
       "                         [-5.5495e-03, -1.2876e-04, -2.1121e-02]],\n",
       "               \n",
       "                        [[ 1.1242e-08,  1.7249e-08,  9.2259e-09],\n",
       "                         [-3.0355e-09,  6.2973e-10, -1.0758e-08],\n",
       "                         [-2.2272e-08, -2.2997e-08, -3.0779e-08]]],\n",
       "               \n",
       "               \n",
       "                       [[[-8.2447e-02, -8.5969e-02, -4.0496e-02],\n",
       "                         [ 1.4534e-04,  4.7732e-03, -1.2706e-02],\n",
       "                         [ 7.4253e-03,  4.7436e-02, -1.9959e-02]],\n",
       "               \n",
       "                        [[-2.5602e-08, -1.0256e-07, -3.6268e-07],\n",
       "                         [ 1.0315e-07,  1.8022e-09, -2.0729e-07],\n",
       "                         [ 1.2862e-07,  9.3515e-08, -1.1672e-07]],\n",
       "               \n",
       "                        [[ 2.4610e-02,  3.8200e-02,  9.2711e-03],\n",
       "                         [ 4.5607e-02,  6.4954e-03, -1.6531e-02],\n",
       "                         [-3.7691e-02, -8.3063e-02, -5.9664e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 7.9180e-02,  1.4805e-01,  1.7302e-01],\n",
       "                         [ 2.1721e-02,  7.2818e-02,  9.3465e-02],\n",
       "                         [ 7.9417e-03, -1.2020e-02, -3.9898e-02]],\n",
       "               \n",
       "                        [[-9.3138e-03, -3.0494e-02, -4.3417e-02],\n",
       "                         [-1.4944e-02, -8.8690e-02, -1.0930e-02],\n",
       "                         [-1.4744e-02, -5.5430e-02, -2.9767e-02]],\n",
       "               \n",
       "                        [[ 8.2815e-08,  3.1840e-09, -6.1195e-08],\n",
       "                         [ 6.1240e-08,  1.0097e-09, -6.4833e-08],\n",
       "                         [ 9.6884e-09, -5.7379e-08, -3.8444e-08]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.bias',\n",
       "               tensor([-4.9818e-05, -4.9128e-05, -4.9026e-06,  6.0460e-05, -1.2960e-06,\n",
       "                       -2.6684e-05, -2.4007e-05, -8.2047e-06,  4.2040e-10,  1.1959e-05,\n",
       "                       -2.8109e-12, -1.9603e-09, -6.5203e-05,  3.6199e-05, -2.9295e-07,\n",
       "                        4.0757e-05,  5.3992e-06, -6.6394e-06, -3.2356e-05,  8.4213e-06,\n",
       "                        4.1439e-10, -5.6632e-05, -4.2017e-06,  4.5048e-09, -4.3890e-05,\n",
       "                       -1.8752e-05, -1.1911e-04, -8.0684e-05,  4.0105e-06,  2.6982e-05,\n",
       "                        5.7867e-06, -4.7395e-06,  4.6435e-05,  7.5639e-05,  4.8089e-08,\n",
       "                        3.0158e-05, -5.1838e-05,  4.7928e-05, -4.9714e-05, -1.5330e-07,\n",
       "                       -1.7552e-05,  4.0345e-09,  4.6622e-05, -3.5057e-05, -4.4895e-05,\n",
       "                       -3.8652e-12, -1.7395e-05, -1.3697e-05, -3.8390e-08,  1.0753e-11,\n",
       "                        5.2107e-05,  3.0900e-15,  2.4130e-05,  4.3603e-05, -1.3968e-05,\n",
       "                       -1.0691e-05,  1.7734e-05, -3.9420e-06, -4.3011e-05, -5.0145e-05,\n",
       "                       -5.9573e-07,  6.1723e-05,  8.4465e-05, -2.3809e-05, -1.2263e-05,\n",
       "                        4.9214e-05,  9.1012e-11,  2.8544e-05,  1.0471e-04, -4.6739e-06,\n",
       "                       -5.4063e-05, -1.4323e-05, -5.6934e-05, -3.9008e-06,  2.7981e-05,\n",
       "                        2.1995e-05, -1.0173e-15, -1.1939e-05,  7.1817e-13,  2.2785e-05,\n",
       "                        3.8789e-06, -1.9016e-05,  1.8728e-06,  4.1041e-05, -1.0123e-08,\n",
       "                       -7.0457e-05, -6.7310e-05, -3.2320e-05,  4.9395e-05, -7.2372e-05,\n",
       "                        2.2037e-05, -2.7530e-05, -6.8682e-05, -2.7555e-05, -7.6898e-05,\n",
       "                       -1.1269e-04, -2.7107e-06,  4.7947e-08,  2.1195e-05, -1.3600e-04,\n",
       "                       -9.9001e-07, -1.1858e-05, -2.6857e-05, -8.7576e-05,  8.7753e-14,\n",
       "                        5.7057e-05, -4.0086e-05,  6.4778e-05, -6.2133e-05, -4.0435e-05,\n",
       "                       -2.9529e-05,  5.2916e-07, -2.2138e-05,  4.9253e-05, -1.1146e-04,\n",
       "                       -1.4121e-05, -4.2397e-06, -4.1352e-05, -1.3730e-05,  3.7409e-06,\n",
       "                        2.3817e-05,  1.8746e-08,  1.2473e-12, -6.3390e-06, -3.4651e-05,\n",
       "                        6.8277e-05,  3.2506e-05,  1.6163e-04], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.bias',\n",
       "               tensor([-3.1727e-01, -2.1232e-01, -3.7658e-01, -6.2612e-01, -8.1702e-06,\n",
       "                       -4.9993e-01, -2.9261e-01, -1.4359e-01, -4.8696e-13, -1.2529e-04,\n",
       "                       -1.2306e-06, -1.9392e-03, -2.4268e-01, -3.7255e-01, -3.3178e-03,\n",
       "                       -2.7396e-01, -2.2390e-01, -2.3781e-01, -2.1436e-01, -2.9339e-01,\n",
       "                       -1.6199e-09, -1.1930e-01, -1.0736e-03, -1.2399e-04, -2.2485e-01,\n",
       "                       -3.9703e-01, -2.8769e-01, -2.7503e-01, -2.7306e-01, -2.5825e-01,\n",
       "                       -4.6269e-01, -5.9403e-01, -9.9626e-02, -4.3996e-01, -2.6369e-01,\n",
       "                       -4.0215e-01, -5.7431e-01, -2.6101e-01, -4.6382e-01, -3.6287e-14,\n",
       "                       -5.6752e-04, -5.9543e-03, -2.6546e-01, -1.7982e-01, -2.6815e-01,\n",
       "                       -3.3883e-08, -2.7374e-01, -4.2947e-01, -8.3639e-07, -6.9047e-05,\n",
       "                       -6.1529e-01, -2.4836e-09, -3.3099e-01, -4.2239e-01, -2.2350e-01,\n",
       "                       -1.6356e-01, -2.7682e-01, -2.5305e-01, -2.1428e-01, -3.6047e-01,\n",
       "                       -6.4510e-04, -5.7068e-02, -5.3824e-01,  1.0521e-01, -3.4511e-01,\n",
       "                       -2.1750e-01, -5.0276e-32, -1.9979e-01, -2.0614e-01, -2.0909e-01,\n",
       "                       -3.2428e-01, -4.5414e-01, -5.3133e-01, -1.3205e-28, -3.0260e-01,\n",
       "                       -4.0488e-01, -1.7121e-04, -1.4692e-01, -1.4711e-17, -1.3676e-01,\n",
       "                       -3.2487e-01, -2.0687e-01, -3.1969e-06, -2.5972e-01, -1.7248e-19,\n",
       "                       -4.9652e-02, -2.5657e-01, -2.9610e-01, -3.3691e-01, -3.6858e-01,\n",
       "                       -3.4705e-01, -2.2554e-01, -2.5759e-01, -3.2570e-01, -4.2677e-01,\n",
       "                       -3.5633e-01, -5.1244e-01, -2.7078e-11, -3.3285e-01, -3.3559e-01,\n",
       "                       -6.1424e-03, -1.8606e-01, -2.7248e-01, -3.6253e-01, -2.3002e-10,\n",
       "                       -2.0330e-01, -3.1376e-01, -3.8102e-01, -4.0893e-01, -3.1467e-01,\n",
       "                       -2.5545e-01, -5.6661e-01, -3.0435e-01, -4.0062e-01, -4.6427e-01,\n",
       "                       -6.8853e-02, -4.1148e-01, -3.3666e-01, -6.7905e-08, -2.3305e-01,\n",
       "                       -1.6369e-01, -9.9178e-23, -2.0268e-22, -3.3787e-14, -2.4897e-01,\n",
       "                       -3.3798e-01, -2.8520e-01, -4.5763e-01], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.weight',\n",
       "               tensor([ 2.7527e-01,  2.4636e-01,  2.6848e-01,  4.5465e-01,  1.0100e-08,\n",
       "                        4.1025e-01,  2.3395e-01,  2.1365e-01,  3.7873e-20, -1.3704e-03,\n",
       "                        9.1009e-04, -2.8100e-03,  1.7035e-01,  2.8924e-01, -2.9825e-03,\n",
       "                        2.5310e-01,  2.5050e-01,  1.8335e-01,  2.1503e-01,  2.7216e-01,\n",
       "                        2.5592e-08,  2.0786e-01,  1.1159e-07,  6.0587e-04,  1.6532e-01,\n",
       "                        3.6026e-01,  2.8183e-01,  2.8791e-01,  2.8258e-01,  2.2325e-01,\n",
       "                        3.9285e-01,  4.4712e-01,  1.6823e-01,  4.4531e-01,  2.3329e-01,\n",
       "                        3.6514e-01,  3.7981e-01,  2.5525e-01,  4.0061e-01,  3.0705e-21,\n",
       "                        1.7587e-03, -2.6958e-03,  2.9358e-01,  1.7412e-01,  2.8113e-01,\n",
       "                       -2.1205e-06,  3.1851e-01,  3.9157e-01, -1.6880e-04,  2.1628e-04,\n",
       "                        5.2849e-01, -3.3207e-07,  3.2059e-01,  3.9118e-01,  2.4038e-01,\n",
       "                        2.3104e-01,  2.5519e-01,  1.7289e-01,  1.9927e-01,  3.3068e-01,\n",
       "                        3.2032e-04,  1.4664e-01,  4.5794e-01,  2.1653e-01,  3.1212e-01,\n",
       "                        2.4167e-01,  1.0193e-37,  2.4424e-01,  2.1541e-01,  2.1907e-01,\n",
       "                        2.7524e-01,  3.5053e-01,  4.4657e-01,  9.8704e-31,  2.7408e-01,\n",
       "                        3.7310e-01, -1.2008e-10,  1.3076e-01, -1.2253e-12,  1.8733e-01,\n",
       "                        3.3040e-01,  1.8623e-01,  9.5487e-07,  2.0028e-01, -3.3073e-26,\n",
       "                        1.8495e-01,  2.6064e-01,  2.6635e-01,  3.0643e-01,  3.2801e-01,\n",
       "                        3.1806e-01,  2.1947e-01,  2.6013e-01,  2.4732e-01,  3.6779e-01,\n",
       "                        3.4801e-01,  3.9539e-01, -1.8914e-06,  3.0592e-01,  3.1476e-01,\n",
       "                       -2.8300e-03,  2.0191e-01,  2.2849e-01,  2.8560e-01, -6.5300e-16,\n",
       "                        2.5745e-01,  2.4548e-01,  2.9317e-01,  3.4889e-01,  2.5692e-01,\n",
       "                        2.7356e-01,  5.0862e-01,  2.1172e-01,  3.4361e-01,  3.8585e-01,\n",
       "                        1.8967e-01,  3.8773e-01,  3.6571e-01, -1.2569e-06,  2.3266e-01,\n",
       "                        2.1316e-01,  4.6101e-17,  5.5050e-17, -1.0791e-20,  2.1346e-01,\n",
       "                        2.9702e-01,  2.2017e-01,  4.3030e-01], device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.weights',\n",
       "               tensor([[-0.0041,  0.0071,  0.0054,  ...,  0.0027, -0.0167, -0.0264],\n",
       "                       [-0.0084, -0.0043, -0.0170,  ...,  0.0094,  0.0111,  0.0355],\n",
       "                       [ 0.0060, -0.0006,  0.0064,  ...,  0.0039,  0.0240,  0.0125],\n",
       "                       [-0.0038,  0.0021,  0.0006,  ...,  0.0204, -0.0059, -0.0324],\n",
       "                       [ 0.0109, -0.0025,  0.0067,  ..., -0.0347, -0.0119,  0.0107]],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.bias',\n",
       "               tensor([ 0.0234,  0.0147,  0.0096,  0.0109, -0.0048], device='cuda:0')),\n",
       "              ('classifier.prompt.prompt_dict.pad_up',\n",
       "               tensor([[[ 0.0211, -0.0003,  0.0142,  ...,  0.0298,  0.0178, -0.0118],\n",
       "                        [ 0.0163, -0.0321,  0.0165,  ...,  0.0308,  0.0045, -0.0201],\n",
       "                        [-0.0089, -0.0224, -0.0356,  ..., -0.0141, -0.0025, -0.0224],\n",
       "                        [ 0.0038, -0.0367,  0.0503,  ...,  0.0184,  0.0320,  0.0073],\n",
       "                        [ 0.0135, -0.0361,  0.0317,  ...,  0.0371,  0.0104, -0.0169]],\n",
       "               \n",
       "                       [[ 0.0166,  0.0089,  0.0254,  ..., -0.0175, -0.0008,  0.0017],\n",
       "                        [ 0.0201, -0.0236,  0.0310,  ..., -0.0140, -0.0252, -0.0071],\n",
       "                        [-0.0153,  0.0114, -0.0301,  ...,  0.0003,  0.0054, -0.0027],\n",
       "                        [ 0.0092,  0.0151,  0.0174,  ..., -0.0264, -0.0394,  0.0003],\n",
       "                        [ 0.0417,  0.0184, -0.0012,  ..., -0.0327, -0.0337, -0.0114]],\n",
       "               \n",
       "                       [[ 0.0013, -0.0131, -0.0136,  ...,  0.0044,  0.0124,  0.0199],\n",
       "                        [ 0.0162, -0.0046,  0.0227,  ...,  0.0100,  0.0043,  0.0168],\n",
       "                        [-0.0069,  0.0192, -0.0164,  ...,  0.0120,  0.0186, -0.0220],\n",
       "                        [ 0.0116,  0.0023,  0.0191,  ...,  0.0029,  0.0151, -0.0084],\n",
       "                        [ 0.0139,  0.0179,  0.0226,  ..., -0.0025,  0.0208, -0.0295]]],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.prompt.prompt_dict.pad_down',\n",
       "               tensor([[[-0.0021, -0.0166, -0.0048,  ..., -0.0183,  0.0040, -0.0075],\n",
       "                        [-0.0062, -0.0431,  0.0394,  ...,  0.0165, -0.0243, -0.0322],\n",
       "                        [-0.0035,  0.0125,  0.0212,  ...,  0.0195, -0.0094, -0.0054],\n",
       "                        [-0.0110, -0.0078, -0.0032,  ...,  0.0228,  0.0218,  0.0151],\n",
       "                        [ 0.0023, -0.0058,  0.0238,  ..., -0.0066, -0.0245, -0.0361]],\n",
       "               \n",
       "                       [[-0.0109,  0.0114, -0.0407,  ..., -0.0319,  0.0378,  0.0172],\n",
       "                        [ 0.0055, -0.0345,  0.0244,  ...,  0.0098, -0.0128,  0.0109],\n",
       "                        [ 0.0028,  0.0244,  0.0051,  ...,  0.0171, -0.0291,  0.0015],\n",
       "                        [ 0.0048,  0.0040, -0.0212,  ...,  0.0098,  0.0048, -0.0024],\n",
       "                        [ 0.0133,  0.0026, -0.0244,  ...,  0.0443,  0.0241, -0.0326]],\n",
       "               \n",
       "                       [[ 0.0263,  0.0506, -0.0234,  ..., -0.0246,  0.0141, -0.0027],\n",
       "                        [ 0.0105, -0.0207,  0.0207,  ...,  0.0034,  0.0051, -0.0077],\n",
       "                        [ 0.0102,  0.0172, -0.0223,  ...,  0.0109,  0.0097, -0.0096],\n",
       "                        [ 0.0043,  0.0120, -0.0279,  ...,  0.0250,  0.0142, -0.0224],\n",
       "                        [-0.0059,  0.0121,  0.0088,  ...,  0.0052,  0.0109, -0.0288]]],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.prompt.prompt_dict.pad_left',\n",
       "               tensor([[[-0.0361,  0.0181,  0.0015,  0.0414, -0.0141],\n",
       "                        [ 0.0207, -0.0074,  0.0291, -0.0509, -0.0065],\n",
       "                        [-0.0296,  0.0392,  0.0126, -0.0215, -0.0064],\n",
       "                        ...,\n",
       "                        [ 0.0038, -0.0160,  0.0226, -0.0025, -0.0211],\n",
       "                        [-0.0221, -0.0242, -0.0216, -0.0005, -0.0028],\n",
       "                        [-0.0528,  0.0071,  0.0391,  0.0320, -0.0060]],\n",
       "               \n",
       "                       [[-0.0190,  0.0345, -0.0503,  0.0181, -0.0090],\n",
       "                        [ 0.0334, -0.0115, -0.0136, -0.0334,  0.0404],\n",
       "                        [-0.0287, -0.0378, -0.0499,  0.0153,  0.0276],\n",
       "                        ...,\n",
       "                        [ 0.0309,  0.0224,  0.0627,  0.0258, -0.0048],\n",
       "                        [ 0.0132, -0.0021, -0.0401, -0.0358, -0.0237],\n",
       "                        [-0.0207,  0.0401, -0.0444, -0.0585, -0.0076]],\n",
       "               \n",
       "                       [[-0.0392,  0.0214, -0.0197,  0.0056,  0.0014],\n",
       "                        [ 0.0247,  0.0004,  0.0198, -0.0185,  0.0238],\n",
       "                        [ 0.0099,  0.0355,  0.0057, -0.0061, -0.0209],\n",
       "                        ...,\n",
       "                        [ 0.0073, -0.0171,  0.0015,  0.0137, -0.0128],\n",
       "                        [-0.0094,  0.0088, -0.0004,  0.0176,  0.0181],\n",
       "                        [-0.0053,  0.0537,  0.0093,  0.0063,  0.0025]]], device='cuda:0')),\n",
       "              ('classifier.prompt.prompt_dict.pad_right',\n",
       "               tensor([[[ 0.0407,  0.0253, -0.0056, -0.0073, -0.0219],\n",
       "                        [-0.0675, -0.0263, -0.0154,  0.0511, -0.0253],\n",
       "                        [-0.0079,  0.0215,  0.0188,  0.0133, -0.0017],\n",
       "                        ...,\n",
       "                        [ 0.0204, -0.0219, -0.0286,  0.0491, -0.0422],\n",
       "                        [-0.0405,  0.0354, -0.0159,  0.0053, -0.0128],\n",
       "                        [ 0.0209,  0.0060, -0.0118,  0.0188,  0.0153]],\n",
       "               \n",
       "                       [[ 0.0175, -0.0550, -0.0023,  0.0306,  0.0410],\n",
       "                        [-0.0162, -0.0179, -0.0118,  0.0253, -0.0090],\n",
       "                        [ 0.0116,  0.0313,  0.0170, -0.0135, -0.0171],\n",
       "                        ...,\n",
       "                        [ 0.0131, -0.0344,  0.0073,  0.0704, -0.0151],\n",
       "                        [-0.0130,  0.0210, -0.0191,  0.0039,  0.0313],\n",
       "                        [ 0.0195, -0.0412, -0.0130,  0.0064, -0.0097]],\n",
       "               \n",
       "                       [[ 0.0241,  0.0059, -0.0169, -0.0118, -0.0412],\n",
       "                        [-0.0177,  0.0270, -0.0232,  0.0192, -0.0059],\n",
       "                        [ 0.0152,  0.0016, -0.0157,  0.0317,  0.0526],\n",
       "                        ...,\n",
       "                        [-0.0072,  0.0053,  0.0063,  0.0082, -0.0206],\n",
       "                        [-0.0608,  0.0606, -0.0013, -0.0226,  0.0287],\n",
       "                        [ 0.0149, -0.0083, -0.0204, -0.0085, -0.0053]]], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.prompt_learning_rates_dict.prompt_weight_learning_rate',\n",
       "               tensor([ 6.7735e-03, -3.5076e-04,  1.3716e-03,  8.3016e-03, -7.6450e-04,\n",
       "                        9.6921e-12], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv0-conv-weight',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv0-conv-bias',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv1-conv-weight',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv1-conv-bias',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv2-conv-weight',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv2-conv-bias',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv3-conv-weight',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv3-conv-bias',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-weights',\n",
       "               tensor([3.9216e-01, 5.4648e-01, 9.2367e-01, 1.1267e+00, 1.0611e+00, 9.6921e-12],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-bias',\n",
       "               tensor([ 1.7182e-01,  1.7953e-01,  3.6042e-02, -1.0476e-01,  9.4527e-02,\n",
       "                        9.6921e-12], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv0-conv-weight',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv0-conv-bias',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv1-conv-weight',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv1-conv-bias',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv2-conv-weight',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv2-conv-bias',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv3-conv-weight',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv3-conv-bias',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-linear-weights',\n",
       "               tensor([ 9.9289e-01, -2.0374e-01, -8.4855e-01, -1.0434e+00, -1.7972e+00,\n",
       "                        5.5578e-10], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-linear-bias',\n",
       "               tensor([ 2.2165e-01,  1.8181e-01,  1.1143e-01,  9.3132e-03, -2.5020e-02,\n",
       "                        5.5578e-10], device='cuda:0'))]),\n",
       " 'per_epoch_statistics': {'train_loss_mean': [1.6207496290206909,\n",
       "   1.342897901058197,\n",
       "   1.2236537531614304,\n",
       "   1.1348049457073213,\n",
       "   1.0828544614315032,\n",
       "   1.0395694247484206,\n",
       "   1.0173706741333008,\n",
       "   0.9757553768157959,\n",
       "   0.9500979747772217,\n",
       "   0.942631136059761,\n",
       "   0.8980446484088898,\n",
       "   0.8778141911029815,\n",
       "   0.8616352691650391,\n",
       "   0.8257936012744903,\n",
       "   0.8036870549321175,\n",
       "   0.7834847896695137,\n",
       "   0.7570608896613121,\n",
       "   0.7447124595046043,\n",
       "   0.7367639369368553,\n",
       "   0.7155400152802467,\n",
       "   0.6868605269789696,\n",
       "   0.6682883580327034,\n",
       "   0.6575315053462982,\n",
       "   0.6474029502272606,\n",
       "   0.6359921804070473,\n",
       "   0.6178860854506493,\n",
       "   0.6051021536588669,\n",
       "   0.6006010538339615,\n",
       "   0.5974101521372795,\n",
       "   0.6038241744041443,\n",
       "   0.5774149975180626,\n",
       "   0.5732839403152465,\n",
       "   0.5677141354382038,\n",
       "   0.5605494692325592,\n",
       "   0.5702561058998108,\n",
       "   0.5429059826731681,\n",
       "   0.5425041063427926,\n",
       "   0.5426579396724701,\n",
       "   0.545066031217575,\n",
       "   0.5318128537237644,\n",
       "   0.542967883437872,\n",
       "   0.5228942370414734,\n",
       "   0.5206523779928685,\n",
       "   0.5279693941175938,\n",
       "   0.5169165155887604,\n",
       "   0.5126333491206169,\n",
       "   0.5150745753347874,\n",
       "   0.50938992780447,\n",
       "   0.5055655215382576,\n",
       "   0.5101301383376121,\n",
       "   0.5067368417978286,\n",
       "   0.49460212591290476,\n",
       "   0.5119811763763428,\n",
       "   0.4939535201489925,\n",
       "   0.5019407892227172,\n",
       "   0.49968585413694383,\n",
       "   0.4969970774650574,\n",
       "   0.491454145014286,\n",
       "   0.48856047829985616,\n",
       "   0.4957058700621128,\n",
       "   0.4985882677733898,\n",
       "   0.4857126070857048,\n",
       "   0.47704756781458857,\n",
       "   0.47438314494490624,\n",
       "   0.48193382316827776,\n",
       "   0.4861598019897938,\n",
       "   0.4924478581547737,\n",
       "   0.4772479425370693,\n",
       "   0.479242424249649,\n",
       "   0.4835649347305298,\n",
       "   0.47209443771839144,\n",
       "   0.47544128581881523,\n",
       "   0.47873051118850707,\n",
       "   0.4695740088224411,\n",
       "   0.4737606158554554,\n",
       "   0.46875581809878347,\n",
       "   0.47342330414056777,\n",
       "   0.4708072311580181,\n",
       "   0.46047626322507856,\n",
       "   0.47161584442853927,\n",
       "   0.45594624188542365,\n",
       "   0.46462134128808974,\n",
       "   0.47184071227908136,\n",
       "   0.47357726147770884,\n",
       "   0.45588243955373764,\n",
       "   0.4646154716908932,\n",
       "   0.4604516882300377,\n",
       "   0.46534774380922317,\n",
       "   0.4551590910255909,\n",
       "   0.4500463908016682,\n",
       "   0.45917791962623594,\n",
       "   0.459131173402071,\n",
       "   0.44881688073277476,\n",
       "   0.4561172934472561,\n",
       "   0.45814833241701125,\n",
       "   0.4632396776378155,\n",
       "   0.4641384927034378,\n",
       "   0.4592018865644932,\n",
       "   0.4424883471429348,\n",
       "   0.4613002304434776,\n",
       "   0.44902970623970034,\n",
       "   0.44778686910867693,\n",
       "   0.451509437084198,\n",
       "   0.45878856068849566,\n",
       "   0.4579270626604557,\n",
       "   0.45507100349664686,\n",
       "   0.4498344167768955,\n",
       "   0.4561372907757759,\n",
       "   0.4372186378240585,\n",
       "   0.44489673498272897,\n",
       "   0.4528974254131317,\n",
       "   0.45654128047823905,\n",
       "   0.4438097078204155,\n",
       "   0.4511192974150181,\n",
       "   0.45328653666377067,\n",
       "   0.4539843057394028,\n",
       "   0.4459153451323509,\n",
       "   0.45172879257798193,\n",
       "   0.44730655324459073,\n",
       "   0.44623310589790344,\n",
       "   0.4495633260756731,\n",
       "   0.4389980199337006,\n",
       "   0.43544453498721125,\n",
       "   0.4446828138232231,\n",
       "   0.4471737766265869,\n",
       "   0.44192572477459907,\n",
       "   0.43888657763600347,\n",
       "   0.44132227611541747,\n",
       "   0.449049838244915,\n",
       "   0.4389493317604065,\n",
       "   0.4434012931883335,\n",
       "   0.43817851746082304,\n",
       "   0.4529563322067261,\n",
       "   0.42974509581923487,\n",
       "   0.4370175579190254,\n",
       "   0.4377596211135387,\n",
       "   0.43504974791407586,\n",
       "   0.4370754491090775,\n",
       "   0.4292725393176079,\n",
       "   0.43267275288701057,\n",
       "   0.4363987884670496,\n",
       "   0.4385294496119022,\n",
       "   0.4368993066847324,\n",
       "   0.4347342394590378,\n",
       "   0.43551832109689714,\n",
       "   0.437297875225544,\n",
       "   0.44219688612222674,\n",
       "   0.4326752769649029],\n",
       "  'train_loss_std': [0.5701254183200988,\n",
       "   0.12204418895870366,\n",
       "   0.13654178128566352,\n",
       "   0.1394452266483158,\n",
       "   0.14787013789240025,\n",
       "   0.14352702334361342,\n",
       "   0.1484441045627974,\n",
       "   0.1411010248003142,\n",
       "   0.15023887267729344,\n",
       "   0.14550382871616763,\n",
       "   0.14543687451459517,\n",
       "   0.1533186619104157,\n",
       "   0.14503134537057402,\n",
       "   0.15272660449559586,\n",
       "   0.14370392298366733,\n",
       "   0.14452262102728078,\n",
       "   0.13459980500043883,\n",
       "   0.14842590944590012,\n",
       "   0.14847778699538403,\n",
       "   0.1422503432780089,\n",
       "   0.1457361564797551,\n",
       "   0.1356326876368041,\n",
       "   0.14790061711979013,\n",
       "   0.14672857239756942,\n",
       "   0.13790628125241755,\n",
       "   0.13893919572314395,\n",
       "   0.1356942288849327,\n",
       "   0.1433650498074844,\n",
       "   0.14029708338306462,\n",
       "   0.14516633660659176,\n",
       "   0.14148226968704816,\n",
       "   0.14199564265487985,\n",
       "   0.13954572530040107,\n",
       "   0.1443576075116581,\n",
       "   0.1325772000601414,\n",
       "   0.13710205022987237,\n",
       "   0.13343932875813497,\n",
       "   0.14218156967366177,\n",
       "   0.14509140935841988,\n",
       "   0.13884201570289215,\n",
       "   0.1351642432662727,\n",
       "   0.1324296468172866,\n",
       "   0.13487797075971694,\n",
       "   0.13961403656866092,\n",
       "   0.13500739149627528,\n",
       "   0.13026662193399802,\n",
       "   0.1346854347931988,\n",
       "   0.13430363328828093,\n",
       "   0.13297564137535733,\n",
       "   0.13281899434801708,\n",
       "   0.1300347310398624,\n",
       "   0.1285025379663272,\n",
       "   0.13188600132362968,\n",
       "   0.13872244343137616,\n",
       "   0.1402513253000202,\n",
       "   0.13764885295876433,\n",
       "   0.1282814757161406,\n",
       "   0.1368383172644055,\n",
       "   0.1349302926110465,\n",
       "   0.12513697363443496,\n",
       "   0.1341942470559963,\n",
       "   0.13328847898929827,\n",
       "   0.13291781760841875,\n",
       "   0.13250915293971743,\n",
       "   0.13216485612737144,\n",
       "   0.12718338366989676,\n",
       "   0.13344520652459296,\n",
       "   0.12563239944275117,\n",
       "   0.13156584074475075,\n",
       "   0.1323348927462172,\n",
       "   0.12805550861187437,\n",
       "   0.12995734120029864,\n",
       "   0.13299024982574817,\n",
       "   0.13163904226180906,\n",
       "   0.13295422377890548,\n",
       "   0.12675829087654908,\n",
       "   0.13513858886441532,\n",
       "   0.12590118695546917,\n",
       "   0.12836102730202964,\n",
       "   0.1353924073321222,\n",
       "   0.12863425040269832,\n",
       "   0.12908041914720247,\n",
       "   0.13131768644598865,\n",
       "   0.13704373762645836,\n",
       "   0.13030337971576283,\n",
       "   0.12393749938320081,\n",
       "   0.1334796990076111,\n",
       "   0.13049857121780112,\n",
       "   0.12854013784589202,\n",
       "   0.1201523726904664,\n",
       "   0.12363445052060348,\n",
       "   0.1276398798246761,\n",
       "   0.12194959397191421,\n",
       "   0.1317483326776292,\n",
       "   0.1274694094966332,\n",
       "   0.13576485502559868,\n",
       "   0.12230545359418532,\n",
       "   0.12869998890598253,\n",
       "   0.12331387418604615,\n",
       "   0.13564906005986924,\n",
       "   0.12207328912676288,\n",
       "   0.1253247660627108,\n",
       "   0.12592473123232742,\n",
       "   0.1283814534563065,\n",
       "   0.12494338166925605,\n",
       "   0.12209753479104135,\n",
       "   0.12937258143279895,\n",
       "   0.12676454523203956,\n",
       "   0.12337921290202537,\n",
       "   0.13055563762571498,\n",
       "   0.12912062605628952,\n",
       "   0.12363725571621653,\n",
       "   0.1268686687411449,\n",
       "   0.12848764933989457,\n",
       "   0.13816004826701317,\n",
       "   0.12713648000820718,\n",
       "   0.1251445863130046,\n",
       "   0.12244531196223317,\n",
       "   0.13033385312955148,\n",
       "   0.13438971736079341,\n",
       "   0.1251080528407045,\n",
       "   0.11887901056962903,\n",
       "   0.12880950877700797,\n",
       "   0.1256116553071109,\n",
       "   0.1234636277813903,\n",
       "   0.12561706390131513,\n",
       "   0.12255022233890762,\n",
       "   0.12864124751194267,\n",
       "   0.12823032946697369,\n",
       "   0.11920847120512101,\n",
       "   0.12594644869594754,\n",
       "   0.12363017848855123,\n",
       "   0.1337680445848671,\n",
       "   0.12287802379283538,\n",
       "   0.13042965297618891,\n",
       "   0.13061527374957646,\n",
       "   0.12412405369365882,\n",
       "   0.12754218351426516,\n",
       "   0.13043165788763725,\n",
       "   0.11964840289787076,\n",
       "   0.1324266893213183,\n",
       "   0.12278431974551458,\n",
       "   0.12525726822910374,\n",
       "   0.1282588288573014,\n",
       "   0.12811982907459965,\n",
       "   0.12457849275292185,\n",
       "   0.13230363488567773,\n",
       "   0.12522783357079997],\n",
       "  'train_accuracy_mean': [0.3587600005865097,\n",
       "   0.4357333332300186,\n",
       "   0.5032133337855339,\n",
       "   0.5472533318400383,\n",
       "   0.571399999320507,\n",
       "   0.593893332362175,\n",
       "   0.6042266663908958,\n",
       "   0.6191999998688698,\n",
       "   0.6327066656351089,\n",
       "   0.6334799995422363,\n",
       "   0.6535066657066345,\n",
       "   0.6624933336377143,\n",
       "   0.6679733315706253,\n",
       "   0.6855599996447563,\n",
       "   0.6945066661834717,\n",
       "   0.7024666675329209,\n",
       "   0.7150266677141189,\n",
       "   0.7215733343362808,\n",
       "   0.7229733327627182,\n",
       "   0.7317333317995072,\n",
       "   0.7422933328151703,\n",
       "   0.7495466660261154,\n",
       "   0.7542399997711182,\n",
       "   0.758426666021347,\n",
       "   0.7614533331394195,\n",
       "   0.7683200001716614,\n",
       "   0.7751066666841507,\n",
       "   0.7755333337783813,\n",
       "   0.7765733320713043,\n",
       "   0.7736399972438812,\n",
       "   0.7858133326768875,\n",
       "   0.7869466661214829,\n",
       "   0.7884266664981842,\n",
       "   0.791453332901001,\n",
       "   0.7866666649580002,\n",
       "   0.7965066654682159,\n",
       "   0.7974533327817916,\n",
       "   0.7982000002861023,\n",
       "   0.7988000000715256,\n",
       "   0.8027999988794327,\n",
       "   0.7995599987506866,\n",
       "   0.8055199995040894,\n",
       "   0.8085599981546402,\n",
       "   0.8037200003862381,\n",
       "   0.808186667561531,\n",
       "   0.8096400002241134,\n",
       "   0.8087866654396058,\n",
       "   0.8107866662740707,\n",
       "   0.8126666661500931,\n",
       "   0.8109199995994568,\n",
       "   0.811146666765213,\n",
       "   0.8176266663074493,\n",
       "   0.8105199999809265,\n",
       "   0.818413333773613,\n",
       "   0.8138933327794075,\n",
       "   0.814626669049263,\n",
       "   0.817880000114441,\n",
       "   0.8183066660165786,\n",
       "   0.819506667137146,\n",
       "   0.8162133350372315,\n",
       "   0.8150933333635331,\n",
       "   0.8209333342313766,\n",
       "   0.8227333347797394,\n",
       "   0.82548000061512,\n",
       "   0.8220533343553543,\n",
       "   0.8196800009012223,\n",
       "   0.8174000000953674,\n",
       "   0.8237999992370606,\n",
       "   0.8242533326148986,\n",
       "   0.8194266667366028,\n",
       "   0.8264266659021378,\n",
       "   0.8245466675758362,\n",
       "   0.8257600005865097,\n",
       "   0.8263866684436798,\n",
       "   0.8249999984502793,\n",
       "   0.8262133328914643,\n",
       "   0.824599999666214,\n",
       "   0.8249466671943665,\n",
       "   0.8299866679906845,\n",
       "   0.8254933342933655,\n",
       "   0.8314533327817917,\n",
       "   0.8282533339262008,\n",
       "   0.8262266675233841,\n",
       "   0.8241866672039032,\n",
       "   0.832653333067894,\n",
       "   0.8280266671180725,\n",
       "   0.8292800015211106,\n",
       "   0.8286666665077209,\n",
       "   0.8327599998712539,\n",
       "   0.8347866673469544,\n",
       "   0.8303600004911422,\n",
       "   0.8302266664505005,\n",
       "   0.8348666681051254,\n",
       "   0.8310533353090286,\n",
       "   0.8307466675043106,\n",
       "   0.8285333335399627,\n",
       "   0.8281466666460037,\n",
       "   0.8315466672182084,\n",
       "   0.8377999999523162,\n",
       "   0.8303333330154419,\n",
       "   0.8332400012016297,\n",
       "   0.8358000000715256,\n",
       "   0.8339866666793824,\n",
       "   0.8308933335542679,\n",
       "   0.8305600000619888,\n",
       "   0.831053334236145,\n",
       "   0.8346533335447311,\n",
       "   0.831373334288597,\n",
       "   0.8402933336496353,\n",
       "   0.83721333360672,\n",
       "   0.8329200013875961,\n",
       "   0.8313733339309692,\n",
       "   0.8363866653442383,\n",
       "   0.8330266667604447,\n",
       "   0.8343466680049896,\n",
       "   0.8319733335971832,\n",
       "   0.8361333329677582,\n",
       "   0.8343600002527237,\n",
       "   0.8350533336400986,\n",
       "   0.8376400002241134,\n",
       "   0.8349733330011367,\n",
       "   0.838133334159851,\n",
       "   0.8399200019836426,\n",
       "   0.8350400011539459,\n",
       "   0.8360400000810623,\n",
       "   0.8379333338737488,\n",
       "   0.8384533344507218,\n",
       "   0.8381333335638046,\n",
       "   0.8354533343315125,\n",
       "   0.8386400017738342,\n",
       "   0.8366400010585785,\n",
       "   0.8374133343696595,\n",
       "   0.8326533340215683,\n",
       "   0.8401866685152054,\n",
       "   0.8395599998235702,\n",
       "   0.8385733330249786,\n",
       "   0.8391200014352799,\n",
       "   0.8394266667366028,\n",
       "   0.842733335018158,\n",
       "   0.8404400013685226,\n",
       "   0.8389200006723404,\n",
       "   0.8380933347940445,\n",
       "   0.8400800006389618,\n",
       "   0.8404666664600372,\n",
       "   0.8406133335828782,\n",
       "   0.840453333735466,\n",
       "   0.8380666667222977,\n",
       "   0.8412666673660278],\n",
       "  'train_accuracy_std': [0.07886962537844523,\n",
       "   0.07026881631922058,\n",
       "   0.07330232009775471,\n",
       "   0.07297374009062686,\n",
       "   0.07374125775426174,\n",
       "   0.07013667415318214,\n",
       "   0.07360194555771413,\n",
       "   0.0674492726844266,\n",
       "   0.07194231125979224,\n",
       "   0.07217925747774732,\n",
       "   0.06875829708255225,\n",
       "   0.07468634485948317,\n",
       "   0.06871311909258222,\n",
       "   0.07036364279680934,\n",
       "   0.06571352669097906,\n",
       "   0.06772135695184836,\n",
       "   0.0625180800333156,\n",
       "   0.06668942994247003,\n",
       "   0.06823328765854518,\n",
       "   0.06638018416321761,\n",
       "   0.06521951413795153,\n",
       "   0.061826054684348916,\n",
       "   0.06356851262778161,\n",
       "   0.06519860386072111,\n",
       "   0.06123995502771815,\n",
       "   0.0619298860180819,\n",
       "   0.0595312213898618,\n",
       "   0.06356837048170232,\n",
       "   0.06057861904919525,\n",
       "   0.0626496020383926,\n",
       "   0.061153219922533135,\n",
       "   0.061382673998520175,\n",
       "   0.0589049337871019,\n",
       "   0.06310149832790016,\n",
       "   0.05626761562408467,\n",
       "   0.06064007499233582,\n",
       "   0.05950614495660266,\n",
       "   0.06046527049290608,\n",
       "   0.06058954762159572,\n",
       "   0.057523559102717225,\n",
       "   0.0576617115894456,\n",
       "   0.055564344323803215,\n",
       "   0.05635575279725951,\n",
       "   0.05870401838739842,\n",
       "   0.0565606911593917,\n",
       "   0.056610593554967316,\n",
       "   0.05693695369507426,\n",
       "   0.056966296151998726,\n",
       "   0.05755384057447954,\n",
       "   0.05718700659795122,\n",
       "   0.05654827602657838,\n",
       "   0.05487167498010173,\n",
       "   0.05597615178459887,\n",
       "   0.05861412797011499,\n",
       "   0.05948256361450368,\n",
       "   0.05878184514393767,\n",
       "   0.05338201121694022,\n",
       "   0.059189145998295,\n",
       "   0.05666922498080967,\n",
       "   0.05175041882320637,\n",
       "   0.057144769593400216,\n",
       "   0.055863484860378634,\n",
       "   0.05722854871472283,\n",
       "   0.05535755356703228,\n",
       "   0.055795315046277834,\n",
       "   0.05438369738595403,\n",
       "   0.05695452484391348,\n",
       "   0.05175888684473089,\n",
       "   0.054930647533018184,\n",
       "   0.05648150382326039,\n",
       "   0.05281359415915145,\n",
       "   0.056190501377405694,\n",
       "   0.055682235954727884,\n",
       "   0.05518463466485158,\n",
       "   0.05494866432011799,\n",
       "   0.054783363088130664,\n",
       "   0.05564885985549246,\n",
       "   0.05528891334796946,\n",
       "   0.05487359151122599,\n",
       "   0.05797203518515245,\n",
       "   0.053133993598267405,\n",
       "   0.05464119716156778,\n",
       "   0.05624140273046057,\n",
       "   0.05778797343368325,\n",
       "   0.054676259451465294,\n",
       "   0.0530535946961993,\n",
       "   0.05678119266468469,\n",
       "   0.056170377603075125,\n",
       "   0.055032357707590886,\n",
       "   0.05116008862968624,\n",
       "   0.053167696176842624,\n",
       "   0.05363450184135096,\n",
       "   0.050336249849537974,\n",
       "   0.05649976953003563,\n",
       "   0.053312269959653824,\n",
       "   0.056076773381767295,\n",
       "   0.054001120022498864,\n",
       "   0.053819523618640855,\n",
       "   0.050512530741718895,\n",
       "   0.054666464212285576,\n",
       "   0.05141111172816182,\n",
       "   0.05248538617963892,\n",
       "   0.05194715228238676,\n",
       "   0.05460486299774985,\n",
       "   0.052636677623179876,\n",
       "   0.052143834965649095,\n",
       "   0.05478474987677001,\n",
       "   0.05398582789440922,\n",
       "   0.05095273044046717,\n",
       "   0.053321364428849646,\n",
       "   0.054643148990610194,\n",
       "   0.05308925955818002,\n",
       "   0.05268448521384365,\n",
       "   0.05403204302644353,\n",
       "   0.05639735968503305,\n",
       "   0.05396228603702185,\n",
       "   0.052616686181192446,\n",
       "   0.05088823695804636,\n",
       "   0.05621365673258605,\n",
       "   0.055566851955246126,\n",
       "   0.05252956317102217,\n",
       "   0.050313507918074984,\n",
       "   0.053218150432856076,\n",
       "   0.05330101817545723,\n",
       "   0.05142466622917108,\n",
       "   0.05139040152399645,\n",
       "   0.05256707105863284,\n",
       "   0.053891084251328734,\n",
       "   0.05479532700711681,\n",
       "   0.0502165918443237,\n",
       "   0.05126575011649125,\n",
       "   0.05138848048703028,\n",
       "   0.055826556963001706,\n",
       "   0.05353097575109976,\n",
       "   0.054589232794169065,\n",
       "   0.05443863218677491,\n",
       "   0.05252156297252018,\n",
       "   0.05353072255669786,\n",
       "   0.05616242486653854,\n",
       "   0.04992868069300265,\n",
       "   0.05585368112060202,\n",
       "   0.05137928979311616,\n",
       "   0.05503286166889159,\n",
       "   0.053451994439947435,\n",
       "   0.05379778648410453,\n",
       "   0.05106134828647597,\n",
       "   0.05534775132884393,\n",
       "   0.0514958053792535],\n",
       "  'train_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'train_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_learning_rate_mean': [0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005],\n",
       "  'train_learning_rate_std': [4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19],\n",
       "  'val_loss_mean': [1.441416900952657,\n",
       "   1.3846008117993673,\n",
       "   1.2682440356413522,\n",
       "   1.2167465635140737,\n",
       "   1.1956471415360768,\n",
       "   1.2200581177075704,\n",
       "   1.123721994360288,\n",
       "   1.1467954057455063,\n",
       "   1.0952849153677622,\n",
       "   1.0884990980227789,\n",
       "   1.0175517588853835,\n",
       "   1.0389101243019103,\n",
       "   1.0180739078919092,\n",
       "   1.0213970828056336,\n",
       "   1.0133472857872645,\n",
       "   1.0012899972995122,\n",
       "   0.993230233391126,\n",
       "   0.9507336417833964,\n",
       "   0.9401851975917817,\n",
       "   0.9469722998142243,\n",
       "   0.9226946310202281,\n",
       "   0.8991351246833801,\n",
       "   0.8960789090394974,\n",
       "   0.8867152986923853,\n",
       "   0.9207024757067362,\n",
       "   0.8809591494003932,\n",
       "   0.8881822508573533,\n",
       "   0.8737424963712692,\n",
       "   0.8496209345261256,\n",
       "   0.8854538373152415,\n",
       "   0.8618759785095851,\n",
       "   0.8372607083121936,\n",
       "   0.8495802170038224,\n",
       "   0.8708902553717295,\n",
       "   0.8311816070477168,\n",
       "   0.8509714325269063,\n",
       "   0.8673336320122083,\n",
       "   0.864456162750721,\n",
       "   0.8687086872259776,\n",
       "   0.8249712866544724,\n",
       "   0.8644219722350438,\n",
       "   0.8492595298091571,\n",
       "   0.8691458024581273,\n",
       "   0.8481071758270263,\n",
       "   0.851428176065286,\n",
       "   0.8603093306223552,\n",
       "   0.826475417514642,\n",
       "   0.8466011018554369,\n",
       "   0.8335600717862447,\n",
       "   0.8274499620000522,\n",
       "   0.8514898576339086,\n",
       "   0.8453600164254507,\n",
       "   0.8455632005135219,\n",
       "   0.8338497508565584,\n",
       "   0.8739532963434855,\n",
       "   0.8486619993050893,\n",
       "   0.8377566576004029,\n",
       "   0.8347696493069331,\n",
       "   0.8405166812737783,\n",
       "   0.8327777141332626,\n",
       "   0.8658423807223637,\n",
       "   0.8161090305447578,\n",
       "   0.8334618045886357,\n",
       "   0.839156512717406,\n",
       "   0.8425728618105253,\n",
       "   0.8348103048404057,\n",
       "   0.8459403287371,\n",
       "   0.8230433669686318,\n",
       "   0.8238097157080968,\n",
       "   0.8433031636476517,\n",
       "   0.8589755072196325,\n",
       "   0.8365624426802,\n",
       "   0.8499731681744258,\n",
       "   0.835593406756719,\n",
       "   0.862349781692028,\n",
       "   0.8515472497542699,\n",
       "   0.8485699105262756,\n",
       "   0.8320024336377779,\n",
       "   0.8758211203416189,\n",
       "   0.8291878257195154,\n",
       "   0.8285033581654231,\n",
       "   0.820870799223582,\n",
       "   0.8549568528930346,\n",
       "   0.823812338411808,\n",
       "   0.8413575561841329,\n",
       "   0.8462733939290047,\n",
       "   0.8324114596843719,\n",
       "   0.854802497625351,\n",
       "   0.8304598619540532,\n",
       "   0.8593304971853892,\n",
       "   0.863206976254781,\n",
       "   0.8332255019744237,\n",
       "   0.8533081621925036,\n",
       "   0.8367525950074196,\n",
       "   0.8284652309616407,\n",
       "   0.8285233368476231,\n",
       "   0.8216201958060264,\n",
       "   0.8145164253314336,\n",
       "   0.8474739395578702,\n",
       "   0.8183140490452449,\n",
       "   0.854152193069458,\n",
       "   0.8536674376328787,\n",
       "   0.8347023249665896,\n",
       "   0.856138942639033,\n",
       "   0.8343078074852626,\n",
       "   0.8427321048577626,\n",
       "   0.8257475736737251,\n",
       "   0.820791670580705,\n",
       "   0.8235936591029167,\n",
       "   0.8187837612628937,\n",
       "   0.8498469489812851,\n",
       "   0.8366892975568772,\n",
       "   0.8144606240590413,\n",
       "   0.8152683785557747,\n",
       "   0.8499923638502757,\n",
       "   0.8362324607372283,\n",
       "   0.8268182643254598,\n",
       "   0.8390438640117646,\n",
       "   0.8169289497534434,\n",
       "   0.8296001875400543,\n",
       "   0.8160243553916613,\n",
       "   0.8450383215149244,\n",
       "   0.8173968028028806,\n",
       "   0.8636752602458,\n",
       "   0.8138732849558195,\n",
       "   0.8166079483429591,\n",
       "   0.8214769736925761,\n",
       "   0.8210546944538752,\n",
       "   0.8190888904531797,\n",
       "   0.8405789794524511,\n",
       "   0.8176964631676674,\n",
       "   0.843300825258096,\n",
       "   0.8165814751386642,\n",
       "   0.8334662779172262,\n",
       "   0.8223350204030673,\n",
       "   0.8276028515895207,\n",
       "   0.8230750718712807,\n",
       "   0.8034846503535906,\n",
       "   0.8351253633697827,\n",
       "   0.8324406227469444,\n",
       "   0.8271724064151446,\n",
       "   0.8011943422754606,\n",
       "   0.820343577961127,\n",
       "   0.8287694144248963,\n",
       "   0.7984485598405202,\n",
       "   0.8559292777379354,\n",
       "   0.793948367635409,\n",
       "   0.8197477628787359],\n",
       "  'val_loss_std': [0.08236909406322923,\n",
       "   0.09339231460382348,\n",
       "   0.10576904320190608,\n",
       "   0.11431139715609645,\n",
       "   0.11351660558696669,\n",
       "   0.12571157280370188,\n",
       "   0.12539333089662633,\n",
       "   0.127288416793855,\n",
       "   0.11893976692053894,\n",
       "   0.11857961665826383,\n",
       "   0.13314399695180065,\n",
       "   0.12917321367395096,\n",
       "   0.12964002486422432,\n",
       "   0.1323388614544805,\n",
       "   0.1319458229265092,\n",
       "   0.131731723530891,\n",
       "   0.13569759183266134,\n",
       "   0.1321178512556122,\n",
       "   0.13432013358022213,\n",
       "   0.1343943144822021,\n",
       "   0.13189308134964173,\n",
       "   0.13538646001422064,\n",
       "   0.13303069375142224,\n",
       "   0.1375880378766212,\n",
       "   0.13522909217678217,\n",
       "   0.13355226933155462,\n",
       "   0.14244751950307097,\n",
       "   0.12962129481772838,\n",
       "   0.13616444192178212,\n",
       "   0.13782690638170697,\n",
       "   0.14199393901353582,\n",
       "   0.14331784139852494,\n",
       "   0.1432414463856821,\n",
       "   0.1415365234495874,\n",
       "   0.14178111611636196,\n",
       "   0.14257672104815788,\n",
       "   0.14153814247257554,\n",
       "   0.14174017245671905,\n",
       "   0.14327588743851785,\n",
       "   0.14195059085377404,\n",
       "   0.14376561022471882,\n",
       "   0.1380180420565341,\n",
       "   0.13936455204086967,\n",
       "   0.14573254609172384,\n",
       "   0.139378807208103,\n",
       "   0.14579574274818122,\n",
       "   0.13982925913448055,\n",
       "   0.14016166349519368,\n",
       "   0.1493938500131544,\n",
       "   0.1363779496068937,\n",
       "   0.13659857422742805,\n",
       "   0.14062957669772663,\n",
       "   0.1420291517124603,\n",
       "   0.14148760305709215,\n",
       "   0.14572561920506052,\n",
       "   0.1393443262024157,\n",
       "   0.1404046252707321,\n",
       "   0.14072419163918912,\n",
       "   0.14675470717558212,\n",
       "   0.1412441140718948,\n",
       "   0.1473288008874422,\n",
       "   0.13945468382979231,\n",
       "   0.1379862777999113,\n",
       "   0.15062000754203939,\n",
       "   0.14283031594824244,\n",
       "   0.14174174685208152,\n",
       "   0.1462677835120186,\n",
       "   0.1394610514760964,\n",
       "   0.14467324634575754,\n",
       "   0.13985087632471652,\n",
       "   0.15255988236323015,\n",
       "   0.14718928974784984,\n",
       "   0.13910653438503626,\n",
       "   0.14473110653395208,\n",
       "   0.14731001076439354,\n",
       "   0.13828589263766342,\n",
       "   0.15409183651680858,\n",
       "   0.1373977026088596,\n",
       "   0.14025084153764072,\n",
       "   0.14580684047271839,\n",
       "   0.1482434116596452,\n",
       "   0.15002410160352764,\n",
       "   0.14232830995644644,\n",
       "   0.13887174819786086,\n",
       "   0.1475570224772372,\n",
       "   0.14158941384168394,\n",
       "   0.1461647600794514,\n",
       "   0.14965812771887074,\n",
       "   0.1347046485773977,\n",
       "   0.1415496626297977,\n",
       "   0.1495719782519324,\n",
       "   0.14595245677177754,\n",
       "   0.15651819403757206,\n",
       "   0.13784789217300691,\n",
       "   0.14647697719896777,\n",
       "   0.15114289540728854,\n",
       "   0.13482948534290776,\n",
       "   0.13906551192682287,\n",
       "   0.1332200517687758,\n",
       "   0.14500098324816738,\n",
       "   0.14618536303065943,\n",
       "   0.1332704896354343,\n",
       "   0.13991502736678565,\n",
       "   0.15040386391573649,\n",
       "   0.1367845603941481,\n",
       "   0.1428019310414054,\n",
       "   0.14147082178568196,\n",
       "   0.13837640861141653,\n",
       "   0.13921944585934626,\n",
       "   0.14362521889307336,\n",
       "   0.151231301048288,\n",
       "   0.13248412505898224,\n",
       "   0.13680061207526795,\n",
       "   0.137050365787563,\n",
       "   0.1504233077518279,\n",
       "   0.1401749899088686,\n",
       "   0.1403136861429386,\n",
       "   0.15080984534232253,\n",
       "   0.1454318323766392,\n",
       "   0.14515713977637398,\n",
       "   0.14470297651529837,\n",
       "   0.15472159742331495,\n",
       "   0.14624001718369453,\n",
       "   0.15105193593081204,\n",
       "   0.14212560677520733,\n",
       "   0.138434405199447,\n",
       "   0.14582485110819923,\n",
       "   0.13506629797082803,\n",
       "   0.14920560243596975,\n",
       "   0.14302452238484686,\n",
       "   0.15234457750460872,\n",
       "   0.14699706316114045,\n",
       "   0.1391490074960588,\n",
       "   0.14400596245504071,\n",
       "   0.1453951739511507,\n",
       "   0.14787961141793626,\n",
       "   0.14944045238275092,\n",
       "   0.14153461820971366,\n",
       "   0.14258738500888235,\n",
       "   0.1461595342461034,\n",
       "   0.14279468492704037,\n",
       "   0.13993213863440626,\n",
       "   0.1379659709315623,\n",
       "   0.14445085576529323,\n",
       "   0.1357528386767979,\n",
       "   0.14339580951731895,\n",
       "   0.13904326722940458,\n",
       "   0.1386134990960677],\n",
       "  'val_accuracy_mean': [0.3836444452901681,\n",
       "   0.4195333346227805,\n",
       "   0.48108888973792396,\n",
       "   0.5142222226659456,\n",
       "   0.5210666640599568,\n",
       "   0.506511111954848,\n",
       "   0.5494666653871536,\n",
       "   0.5427777776122094,\n",
       "   0.5647333319981893,\n",
       "   0.5685777761538824,\n",
       "   0.5992888867855072,\n",
       "   0.5890444430708885,\n",
       "   0.5993777774771054,\n",
       "   0.5996444437901179,\n",
       "   0.6045777771870295,\n",
       "   0.6079777792096138,\n",
       "   0.6131333324313164,\n",
       "   0.6319999978939692,\n",
       "   0.6391333321730296,\n",
       "   0.6313999992609024,\n",
       "   0.6426222235957781,\n",
       "   0.6507555555303891,\n",
       "   0.6549333327015241,\n",
       "   0.6575555570920308,\n",
       "   0.6424666666984558,\n",
       "   0.6600222206115722,\n",
       "   0.6596444447835287,\n",
       "   0.6613555554548899,\n",
       "   0.6752888879179955,\n",
       "   0.6589111113548278,\n",
       "   0.6714000008503596,\n",
       "   0.6827111115058263,\n",
       "   0.6751777780056,\n",
       "   0.6638444451491038,\n",
       "   0.680866667330265,\n",
       "   0.6737333329518637,\n",
       "   0.6665777790546418,\n",
       "   0.671644445459048,\n",
       "   0.6684222195545833,\n",
       "   0.6842444425821305,\n",
       "   0.6690666668613752,\n",
       "   0.6773555550972621,\n",
       "   0.6657555562257766,\n",
       "   0.6819777778784434,\n",
       "   0.674555554886659,\n",
       "   0.6725111108024915,\n",
       "   0.6812666684389115,\n",
       "   0.6763999990622203,\n",
       "   0.6825999989112218,\n",
       "   0.6827777782082558,\n",
       "   0.6748444454868635,\n",
       "   0.6762444444497426,\n",
       "   0.6760888886451721,\n",
       "   0.6822444444894791,\n",
       "   0.6663111114501953,\n",
       "   0.6735999980568885,\n",
       "   0.6799333341916403,\n",
       "   0.6803555555144946,\n",
       "   0.6827999995152155,\n",
       "   0.6795333329836527,\n",
       "   0.6708888906240463,\n",
       "   0.6895111115773519,\n",
       "   0.6807111102342606,\n",
       "   0.6835555559396744,\n",
       "   0.6809333319465319,\n",
       "   0.6820222208897273,\n",
       "   0.6811333339413007,\n",
       "   0.6837777776519457,\n",
       "   0.6881555551290512,\n",
       "   0.6788222213586171,\n",
       "   0.6753777778148651,\n",
       "   0.6831777758399645,\n",
       "   0.674488888780276,\n",
       "   0.6839111104607583,\n",
       "   0.6758888890345891,\n",
       "   0.6745777770876884,\n",
       "   0.6845777794718743,\n",
       "   0.6813555538654328,\n",
       "   0.6679333315292995,\n",
       "   0.6837555554509163,\n",
       "   0.685888888736566,\n",
       "   0.6838222215572993,\n",
       "   0.6756666652361552,\n",
       "   0.6877777792016665,\n",
       "   0.6814000003536542,\n",
       "   0.6811555551489195,\n",
       "   0.6853999981284141,\n",
       "   0.6777555547157923,\n",
       "   0.6875777765115102,\n",
       "   0.6716888892650604,\n",
       "   0.6759555550416311,\n",
       "   0.685622221827507,\n",
       "   0.6780222221215566,\n",
       "   0.6819777777791023,\n",
       "   0.6822222221891086,\n",
       "   0.6863333330551783,\n",
       "   0.687311111887296,\n",
       "   0.6897111092011133,\n",
       "   0.6754222209254901,\n",
       "   0.6910222228368124,\n",
       "   0.6789777769645056,\n",
       "   0.6763333332538605,\n",
       "   0.6804666675130526,\n",
       "   0.6768222216765086,\n",
       "   0.6813999993602434,\n",
       "   0.6813555554548899,\n",
       "   0.6856444452206294,\n",
       "   0.6886222219467163,\n",
       "   0.6893777797619501,\n",
       "   0.6845555541912715,\n",
       "   0.6813777772585551,\n",
       "   0.6817777784665425,\n",
       "   0.6923999998966853,\n",
       "   0.6888000015417735,\n",
       "   0.6760666657487552,\n",
       "   0.6814888885617256,\n",
       "   0.6846666661898295,\n",
       "   0.6837555545568467,\n",
       "   0.6873555561900139,\n",
       "   0.6867777768770854,\n",
       "   0.6910222222407659,\n",
       "   0.6808444446325302,\n",
       "   0.6903777778148651,\n",
       "   0.675955556333065,\n",
       "   0.6905555548270543,\n",
       "   0.687133332490921,\n",
       "   0.6912222222487132,\n",
       "   0.6843777771790822,\n",
       "   0.6870000018676122,\n",
       "   0.6819555560747782,\n",
       "   0.6979999989271164,\n",
       "   0.6777333329121272,\n",
       "   0.6880888893206915,\n",
       "   0.6846666673819224,\n",
       "   0.6894888880848885,\n",
       "   0.6841777769724527,\n",
       "   0.6905111115177472,\n",
       "   0.699288889169693,\n",
       "   0.6877111119031906,\n",
       "   0.683777777949969,\n",
       "   0.6857333348194758,\n",
       "   0.6946666653951009,\n",
       "   0.6868666664759318,\n",
       "   0.6870222212870916,\n",
       "   0.6936666677395503,\n",
       "   0.6769777781764666,\n",
       "   0.6988444442550341,\n",
       "   0.687422223687172],\n",
       "  'val_accuracy_std': [0.053624686199679084,\n",
       "   0.05543034371674429,\n",
       "   0.060321357239092196,\n",
       "   0.061300312244270065,\n",
       "   0.05992380292769722,\n",
       "   0.06224305635312529,\n",
       "   0.06347828952692203,\n",
       "   0.060888380948135166,\n",
       "   0.0600169976186404,\n",
       "   0.06184250590931793,\n",
       "   0.06327200131010428,\n",
       "   0.061433297291990145,\n",
       "   0.061044473319096924,\n",
       "   0.06136128184205862,\n",
       "   0.060875765610299865,\n",
       "   0.060432265856983754,\n",
       "   0.06135595000494595,\n",
       "   0.06382615377888752,\n",
       "   0.061086374734130686,\n",
       "   0.062429539556022345,\n",
       "   0.06121920153101314,\n",
       "   0.06016908026328471,\n",
       "   0.05879622267834406,\n",
       "   0.06016971994302225,\n",
       "   0.06251035972193497,\n",
       "   0.061136155031495366,\n",
       "   0.06052014912595406,\n",
       "   0.059255376868141495,\n",
       "   0.05959640564380807,\n",
       "   0.05680766417822001,\n",
       "   0.05859057003818465,\n",
       "   0.05742936096434106,\n",
       "   0.06004694050782062,\n",
       "   0.06051781227844796,\n",
       "   0.06104270620814574,\n",
       "   0.06110940004399978,\n",
       "   0.05942433441167555,\n",
       "   0.058896957208926,\n",
       "   0.06064859021669015,\n",
       "   0.06048127767139839,\n",
       "   0.05935096051664121,\n",
       "   0.059903375846808694,\n",
       "   0.05910116516554076,\n",
       "   0.05875069370059268,\n",
       "   0.060129388062192465,\n",
       "   0.05973824089879266,\n",
       "   0.05914222796549079,\n",
       "   0.05948236131580461,\n",
       "   0.06283354866542352,\n",
       "   0.06008255661281955,\n",
       "   0.05873554906202432,\n",
       "   0.05822026930561431,\n",
       "   0.05870042768492075,\n",
       "   0.06039253991784477,\n",
       "   0.06226960143199432,\n",
       "   0.060501019386624465,\n",
       "   0.059709133279819554,\n",
       "   0.06072542548717747,\n",
       "   0.061631016822538374,\n",
       "   0.06054264634921864,\n",
       "   0.06036636981307234,\n",
       "   0.05908861940416042,\n",
       "   0.05981525597494224,\n",
       "   0.059899505283112026,\n",
       "   0.05932724223657148,\n",
       "   0.061438793466180856,\n",
       "   0.060352459273915686,\n",
       "   0.06375841774354704,\n",
       "   0.061836147328892685,\n",
       "   0.05951860897238895,\n",
       "   0.060398412425589375,\n",
       "   0.06178677353650112,\n",
       "   0.05883182239305193,\n",
       "   0.059385429459650745,\n",
       "   0.058914775623975686,\n",
       "   0.057883083165149056,\n",
       "   0.06088063480951018,\n",
       "   0.05835345316066268,\n",
       "   0.06062901413321796,\n",
       "   0.061032834628283794,\n",
       "   0.05932574019305336,\n",
       "   0.06280291112610219,\n",
       "   0.0605997813834624,\n",
       "   0.059503708881904004,\n",
       "   0.06205585411202227,\n",
       "   0.05818869281222922,\n",
       "   0.06256039575926749,\n",
       "   0.05989496485758414,\n",
       "   0.05748223194910487,\n",
       "   0.06068711174167911,\n",
       "   0.059974792981494364,\n",
       "   0.05780123964615031,\n",
       "   0.062458991455803074,\n",
       "   0.058989763747721954,\n",
       "   0.0604681322166335,\n",
       "   0.06008913108386427,\n",
       "   0.058106157163663394,\n",
       "   0.05691636074351066,\n",
       "   0.05781009431649159,\n",
       "   0.05829928680099702,\n",
       "   0.06133930892062732,\n",
       "   0.059337393195103456,\n",
       "   0.06024337152172016,\n",
       "   0.06063471213698916,\n",
       "   0.059406659008223485,\n",
       "   0.05961928262322178,\n",
       "   0.05908054394713421,\n",
       "   0.05823094146949394,\n",
       "   0.056264438249579884,\n",
       "   0.060220479905379654,\n",
       "   0.05950809302982604,\n",
       "   0.05672371801373561,\n",
       "   0.0570381539378409,\n",
       "   0.057536178197561086,\n",
       "   0.05894953228572979,\n",
       "   0.05745470135434099,\n",
       "   0.058594651125162156,\n",
       "   0.06161745294056894,\n",
       "   0.06224657921537886,\n",
       "   0.0583157672895285,\n",
       "   0.05942682663302859,\n",
       "   0.06047122076578073,\n",
       "   0.059950643121916516,\n",
       "   0.058862787716163684,\n",
       "   0.05862488039711161,\n",
       "   0.06013014276724436,\n",
       "   0.057769551036318104,\n",
       "   0.05790366952605313,\n",
       "   0.05939229409444336,\n",
       "   0.05838592919223286,\n",
       "   0.05786446634260626,\n",
       "   0.060147128838677545,\n",
       "   0.0615507576606847,\n",
       "   0.05904047643682041,\n",
       "   0.06030445605981702,\n",
       "   0.05810614504986827,\n",
       "   0.06266103468491274,\n",
       "   0.059626724231758146,\n",
       "   0.05684239040668668,\n",
       "   0.06317485044365248,\n",
       "   0.05934721477932003,\n",
       "   0.05871967423809152,\n",
       "   0.0608394736870983,\n",
       "   0.05889494569321081,\n",
       "   0.058538053716414816,\n",
       "   0.05758902459516361,\n",
       "   0.05798210099579776,\n",
       "   0.05952920001052301],\n",
       "  'val_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'val_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0]}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델을 구성한다\n",
    "model = MAMLFewShotClassifier(args=args, device=device,\n",
    "                              im_shape=(2, 3,\n",
    "                                        args.image_height, args.image_width))\n",
    "\n",
    "data = MetaLearningSystemDataLoader\n",
    "\n",
    "maml_system = ExperimentBuilder(model=model, data=data, args=args, device=device)\n",
    "maml_system.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9236398a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_target_set_task ==  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_target_set_task ==  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_target_set_task ==  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_target_set_task ==  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_target_set_task ==  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_target_set_task ==  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_target_set_task ==  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_target_set_task ==  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_target_set_task ==  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_target_set_task ==  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_target_set_task ==  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_target_set_task ==  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_target_set_task ==  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_target_set_task ==  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x000001CA95B59288>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1466, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1424, in _shutdown_workers\n",
      "    if self._persistent_workers or self._workers_status[worker_id]:\n",
      "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_target_set_task ==  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_target_set_task ==  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_target_set_task ==  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_target_set_task ==  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_target_set_task ==  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_target_set_task ==  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_target_set_task ==  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_target_set_task ==  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_target_set_task ==  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_target_set_task ==  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_target_set_task ==  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_target_set_task ==  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_target_set_task ==  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_target_set_task ==  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_target_set_task ==  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_target_set_task ==  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_target_set_task ==  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_target_set_task ==  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_target_set_task ==  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_target_set_task ==  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_target_set_task ==  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_target_set_task ==  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_target_set_task ==  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_target_set_task ==  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_target_set_task ==  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_target_set_task ==  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_target_set_task ==  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_target_set_task ==  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_target_set_task ==  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py:1010: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  if param.grad is not None:\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\JM\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11772\\2412368615.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    111\u001b[0m                     \u001b[0mtask_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_idx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m                     \u001b[0mtitle_prefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Effect of Prompt on Query Features\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m                     \u001b[0mmarker_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m120\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m                 )\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\prompt_maml\\utils\\basic.py\u001b[0m in \u001b[0;36mplot_query_before_after_separate\u001b[1;34m(query_before, query_after, y_query, save_dir, task_index, title_prefix, marker_size, perplexity, random_state)\u001b[0m\n\u001b[0;32m     38\u001b[0m                     \u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_before\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mtsne_after\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperplexity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mperplexity\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pca'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m                     \u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_after\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;31m# 공통 x/y 범위 고정\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   1106\u001b[0m             \u001b[0mEmbedding\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlow\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mdimensional\u001b[0m \u001b[0mspace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1107\u001b[0m         \"\"\"\n\u001b[1;32m-> 1108\u001b[1;33m         \u001b[0membedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1109\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1110\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, skip_num_points)\u001b[0m\n\u001b[0;32m   1008\u001b[0m             \u001b[0mX_embedded\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_embedded\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1009\u001b[0m             \u001b[0mneighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mneighbors_nn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1010\u001b[1;33m             \u001b[0mskip_num_points\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mskip_num_points\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1011\u001b[0m         )\n\u001b[0;32m   1012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py\u001b[0m in \u001b[0;36m_tsne\u001b[1;34m(self, P, degrees_of_freedom, n_samples, X_embedded, neighbors, skip_num_points)\u001b[0m\n\u001b[0;32m   1054\u001b[0m         \u001b[1;31m# higher learning rate controlled via the early exaggeration parameter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1055\u001b[0m         \u001b[0mP\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mearly_exaggeration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1056\u001b[1;33m         \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkl_divergence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_gradient_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mopt_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1057\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1058\u001b[0m             print(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py\u001b[0m in \u001b[0;36m_gradient_descent\u001b[1;34m(objective, p0, it, n_iter, n_iter_check, n_iter_without_progress, momentum, learning_rate, min_gain, min_grad_norm, verbose, args, kwargs)\u001b[0m\n\u001b[0;32m    396\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"compute_error\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_convergence\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 398\u001b[1;33m         \u001b[0merror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobjective\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    399\u001b[0m         \u001b[0mgrad_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\maml\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py\u001b[0m in \u001b[0;36m_kl_divergence_bh\u001b[1;34m(params, P, degrees_of_freedom, n_samples, n_components, angle, skip_num_points, verbose, compute_error, num_threads)\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[0mdof\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdegrees_of_freedom\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m         \u001b[0mcompute_error\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_error\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 290\u001b[1;33m         \u001b[0mnum_threads\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_threads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    291\u001b[0m     )\n\u001b[0;32m    292\u001b[0m     \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2.0\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdegrees_of_freedom\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mdegrees_of_freedom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "save_path = f\"tsne_images/prompt_effect/{datasets}\"\n",
    "\n",
    "train_data = maml_system.data.get_test_batches(total_batches=int(300/1), augment_images=False)\n",
    "\n",
    "for sample_idx, train_sample in enumerate(train_data):\n",
    "    \n",
    "    x_support_set, x_target_set, y_support_set, y_target_set, seed = train_sample\n",
    "    \n",
    "    x_support_set = torch.Tensor(x_support_set).float().to(device=maml_system.model.device)\n",
    "    x_target_set = torch.Tensor(x_target_set).float().to(device=maml_system.model.device)\n",
    "    y_support_set = torch.Tensor(y_support_set).long().to(device=maml_system.model.device)\n",
    "    y_target_set = torch.Tensor(y_target_set).long().to(device=maml_system.model.device)\n",
    "    \n",
    "    for task_id, (x_support_set_task, y_support_set_task, x_target_set_task, y_target_set_task) in enumerate(zip(x_support_set,\n",
    "                              y_support_set,\n",
    "                              x_target_set,\n",
    "                              y_target_set)):\n",
    "        \n",
    "        \n",
    "        print(\"y_target_set_task == \", len(y_target_set_task))\n",
    "        \n",
    "        names_weights_copy = maml_system.model.get_inner_loop_parameter_dict(maml_system.model.classifier.named_parameters())\n",
    "\n",
    "        num_devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "\n",
    "        names_weights_copy = {\n",
    "                name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
    "                    [num_devices] + [1 for i in range(len(value.shape))]) for\n",
    "                name, value in names_weights_copy.items()}\n",
    "\n",
    "        prompted_weights_copy = {}\n",
    "        if args.prompter:\n",
    "            prompted_weights_copy = {key: value for key, value in names_weights_copy.items() if 'prompt' in key}\n",
    "\n",
    "        names_weights_copy = {key: value for key, value in names_weights_copy.items() if 'layer_dict' in key}\n",
    "        \n",
    "        \n",
    "        n, s, c, h, w = x_target_set_task.shape\n",
    "\n",
    "        x_support_set_task = x_support_set_task.view(-1, c, h, w)\n",
    "        y_support_set_task = y_support_set_task.view(-1)\n",
    "        x_target_set_task = x_target_set_task.view(-1, c, h, w)\n",
    "        y_target_set_task = y_target_set_task.view(-1)\n",
    "        \n",
    "            \n",
    "        for num_step in range(5):\n",
    "\n",
    "            support_loss, support_preds, support_feature_map_list_withprompt = maml_system.model.net_forward_feature_extractor(\n",
    "              x=x_support_set_task,\n",
    "              y=y_support_set_task,\n",
    "              weights=names_weights_copy,\n",
    "              prompted_weights=prompted_weights_copy,\n",
    "              backup_running_statistics=num_step == 0,\n",
    "              training=True,\n",
    "              num_step=num_step,\n",
    "              prepend_prompt=True,  \n",
    "              training_phase=False,\n",
    "              epoch=0)            \n",
    "            \n",
    "            names_weights_copy, prompted_weights_copy = maml_system.model.apply_inner_loop_update(\n",
    "                                                             loss=support_loss,\n",
    "                                                             names_weights_copy=names_weights_copy,\n",
    "                                                             prompted_weights_copy=prompted_weights_copy,\n",
    "                                                             use_second_order=True,\n",
    "                                                             current_step_idx=num_step,\n",
    "                                                             current_iter='test',\n",
    "                                                             training_phase=False)\n",
    "\n",
    "            if num_step == 4:\n",
    "                \n",
    "                target_loss, target_preds, target_feature_map_list_without_prompt = maml_system.model.net_forward_feature_extractor(\n",
    "                    x=x_target_set_task,\n",
    "                    y=y_target_set_task,\n",
    "                    weights=names_weights_copy,\n",
    "                    prompted_weights=prompted_weights_copy,\n",
    "                    backup_running_statistics=False, training=True,\n",
    "                    prepend_prompt=False,  \n",
    "                    num_step=num_step,\n",
    "                    training_phase=False,\n",
    "                    epoch=0)\n",
    "   \n",
    "                target_loss, target_preds, target_feature_map_list_with_prompt = maml_system.model.net_forward_feature_extractor(\n",
    "                    x=x_target_set_task,\n",
    "                    y=y_target_set_task,\n",
    "                    weights=names_weights_copy,\n",
    "                    prompted_weights=prompted_weights_copy,\n",
    "                    backup_running_statistics=False, training=True,\n",
    "                    prepend_prompt=True,  \n",
    "                    num_step=num_step,\n",
    "                    training_phase=False,\n",
    "                    epoch=0)\n",
    "                        \n",
    "                ## Adaptation 후에 query set에 대한 feature map을 구한다\n",
    "                query_without_prompt = target_feature_map_list_without_prompt[3]\n",
    "                query_with_prompt    = target_feature_map_list_with_prompt[3]\n",
    "                                \n",
    "#                 query_with_prompt_np     = gap(query_with_prompt).detach().cpu().numpy()\n",
    "#                 query_without_prompt_np  = gap(query_without_prompt).detach().cpu().numpy()\n",
    "                \n",
    "                query_with_prompt_np = flatten_feature_map(query_with_prompt).detach().cpu().numpy()\n",
    "                query_without_prompt_np = flatten_feature_map(query_without_prompt).detach().cpu().numpy()\n",
    "                \n",
    "                \n",
    "                y_query_np = y_target_set_task.cpu().numpy()\n",
    "                \n",
    "                plot_query_before_after_separate(\n",
    "                    query_before=query_without_prompt_np,\n",
    "                    query_after=query_with_prompt_np,\n",
    "                    y_query=y_query_np,\n",
    "                    save_dir=save_path,\n",
    "                    task_index=sample_idx,\n",
    "                    title_prefix=\"Effect of Prompt on Query Features\",\n",
    "                    marker_size=120\n",
    "                )\n",
    "\n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caca899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdfcb0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
