{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae6278ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pytorchcv.model_provider import get_model as ptcv_get_model # model\n",
    "import sys, os\n",
    "import easydict\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline \n",
    "\n",
    "# enable cuda devices\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from meta_neural_network_architectures import VGGReLUNormNetwork, ResNet12\n",
    "from utils.parser_utils import get_args\n",
    "from data import MetaLearningSystemDataLoader\n",
    "from experiment_builder import ExperimentBuilder\n",
    "from utils.basic import show_batch\n",
    "from few_shot_learning_system import MAMLFewShotClassifier\n",
    "import prompters\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from data_augmentation import mixup_data, cutmix_data, random_flip, random_flip_like_torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdefd73",
   "metadata": {},
   "source": [
    "# 1. Dataset 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7d81d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choices=['padding', 'random_patch', 'fixed_patch'],\n",
    "method = 'padding'\n",
    "\n",
    "datasets = \"mini_imagenet\"\n",
    "# datasets = \"tiered_imagenet\"\n",
    "# datasets = \"CIFAR_FS\"\n",
    "# datasets = \"CUB\"\n",
    "\n",
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "# os.environ['TEST_DATASET'] = \"tiered_imagenet\" # https://mtl.yyliu.net/download/Lmzjm9tX.html\n",
    "# os.environ['TEST_DATASET'] = \"CIFAR_FS\" # https://drive.google.com/file/d/1pTsCCMDj45kzFYgrnO67BWVbKs48Q3NI/view\n",
    "# os.environ['TEST_DATASET'] = \"CUB\" # https://data.caltech.edu/records/65de6-vp158"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb0744f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.environ['DATASET_DIR'] ===  C:/Users/JM/PycharmProjects/MAML/datasets\n"
     ]
    }
   ],
   "source": [
    "os.environ['DATASET_DIR'] = 'C:/Users/JM/PycharmProjects/MAML/datasets'\n",
    "print(\"os.environ['DATASET_DIR'] === \", os.environ['DATASET_DIR'])\n",
    "\n",
    "args = easydict.EasyDict(\n",
    "{\n",
    "  \"batch_size\":2,\n",
    "  \"image_height\":84,\n",
    "  \"image_width\":84,\n",
    "  \"image_channels\":3,\n",
    "  \"gpu_to_use\":0,\n",
    "  \"num_dataprovider_workers\":4,\n",
    "  \"max_models_to_save\":5,\n",
    "  \"dataset_name\":\"mini_imagenet_full_size\",\n",
    "  \"dataset_path\":\"mini_imagenet_full_size\",\n",
    "  \"reset_stored_paths\":False,\n",
    "  \"experiment_name\":\"../MAML_Prompt_padding_5way_5shot_filter128_miniImagenet\",\n",
    "  \"train_seed\": 0, \"val_seed\": 0,\n",
    "  \"indexes_of_folders_indicating_class\": [-3, -2],\n",
    "  \"sets_are_pre_split\": True,\n",
    "  \"train_val_test_split\": [0.64, 0.16, 0.20],\n",
    "  \"evaluate_on_test_set_only\": False,\n",
    "\n",
    "  \"total_epochs\": 100,\n",
    "  \"total_iter_per_epoch\":500, \"continue_from_epoch\": -2,\n",
    "  \"num_evaluation_tasks\":600,\n",
    "  \"multi_step_loss_num_epochs\": 15,\n",
    "  \"minimum_per_task_contribution\": 0.01,\n",
    "  \"learnable_per_layer_per_step_inner_loop_learning_rate\": True,\n",
    "  \"enable_inner_loop_optimizable_bn_params\": False,\n",
    "  \"evalute_on_test_set_only\": False,\n",
    "\n",
    "  \"max_pooling\": True,\n",
    "  \"per_step_bn_statistics\": False,\n",
    "  \"learnable_batch_norm_momentum\": False,\n",
    "  \"load_into_memory\": False,\n",
    "  \"init_inner_loop_learning_rate\": 0.01,\n",
    "  \"init_inner_loop_weight_decay\": 0.0005,\n",
    "  \"learnable_bn_gamma\": True,\n",
    "  \"learnable_bn_beta\": True,\n",
    "\n",
    "  \"dropout_rate_value\":0.0,\n",
    "  \"min_learning_rate\":0.001,\n",
    "  \"meta_learning_rate\":0.001,   \"total_epochs_before_pause\": 100,\n",
    "  \"first_order_to_second_order_epoch\":-1,\n",
    "  \"weight_decay\": 0.0,\n",
    "\n",
    "  \"norm_layer\":\"batch_norm\",\n",
    "  \"cnn_num_filters\":128,\n",
    "  \"num_stages\":4,\n",
    "  \"conv_padding\": True,\n",
    "  \"number_of_training_steps_per_iter\":5,\n",
    "  \"number_of_evaluation_steps_per_iter\":5,\n",
    "  \"cnn_blocks_per_stage\":1,\n",
    "  \"num_classes_per_set\":5,\n",
    "  \"num_samples_per_class\":5,\n",
    "  \"num_target_samples\": 15,\n",
    "  \"samples_per_iter\" : 1,\n",
    "\n",
    "  \"second_order\": True,\n",
    "  \"use_multi_step_loss_optimization\":False,\n",
    "  \"backbone\": \"4-CONV\",\n",
    "  \"arbiter\": False,\n",
    "  \"use_bias\": True,\n",
    "  \"prompter\": True,\n",
    "  \"prompt_engineering\": method,\n",
    "  \"prompt_size\" : 5,\n",
    "  \"image_size\" : 84,\n",
    "  \"prompt_random_init\": False,\n",
    "  \"outer_prompt_learning_rate\": 0.001,\n",
    "  \"inner_prompt_learning_rate\": 0.01,\n",
    "  \"data_aug\" : \"random\"\n",
    "}\n",
    ")\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "args.im_shape = (2, 3, args.image_height, args.image_width)\n",
    "\n",
    "args.use_cuda = torch.cuda.is_available()\n",
    "args.seed = 104\n",
    "args.reverse_channels=False\n",
    "args.labels_as_int=False\n",
    "args.reset_stored_filepaths=False\n",
    "args.num_of_gpus=1\n",
    "\n",
    "args.continue_from_epoch='latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a347cd16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using max pooling\n",
      "meta network params\n",
      "0.01\n",
      "Inner Loop parameters\n",
      "prompt_learning_rates_dict.prompt_weight_learning_rate torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv0-conv-weight torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv0-conv-bias torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv1-conv-weight torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv1-conv-bias torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv2-conv-weight torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv2-conv-bias torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv3-conv-weight torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-conv3-conv-bias torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-linear-weights torch.Size([6])\n",
      "names_learning_rates_dict.layer_dict-linear-bias torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv0-conv-weight torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv0-conv-bias torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv1-conv-weight torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv1-conv-bias torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv2-conv-weight torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv2-conv-bias torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv3-conv-weight torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-conv3-conv-bias torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-linear-weights torch.Size([6])\n",
      "names_weight_decay_dict.layer_dict-linear-bias torch.Size([6])\n",
      "Outer Loop parameters\n",
      "classifier.layer_dict.conv0.conv.weight torch.Size([128, 3, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv0.conv.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv0.norm_layer.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv0.norm_layer.weight torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv1.conv.weight torch.Size([128, 128, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv1.conv.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv1.norm_layer.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv1.norm_layer.weight torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv2.conv.weight torch.Size([128, 128, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv2.conv.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv2.norm_layer.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv2.norm_layer.weight torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv3.conv.weight torch.Size([128, 128, 3, 3]) cuda:0 True\n",
      "classifier.layer_dict.conv3.conv.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv3.norm_layer.bias torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.conv3.norm_layer.weight torch.Size([128]) cuda:0 True\n",
      "classifier.layer_dict.linear.weights torch.Size([5, 3200]) cuda:0 True\n",
      "classifier.layer_dict.linear.bias torch.Size([5]) cuda:0 True\n",
      "classifier.prompt.prompt_dict.pad_up torch.Size([3, 5, 84]) cuda:0 True\n",
      "classifier.prompt.prompt_dict.pad_down torch.Size([3, 5, 84]) cuda:0 True\n",
      "classifier.prompt.prompt_dict.pad_left torch.Size([3, 74, 5]) cuda:0 True\n",
      "classifier.prompt.prompt_dict.pad_right torch.Size([3, 74, 5]) cuda:0 True\n",
      "inner_loop_optimizer.prompt_learning_rates_dict.prompt_weight_learning_rate torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv0-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv0-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv1-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv1-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv2-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv2-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv3-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv3-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-weights torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv0-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv0-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv1-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv1-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv2-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv2-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv3-conv-weight torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv3-conv-bias torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-linear-weights torch.Size([6]) cuda:0 True\n",
      "inner_loop_optimizer.names_weight_decay_dict.layer_dict-linear-bias torch.Size([6]) cuda:0 True\n",
      "1\n",
      "C:\\Users\\JM\\PycharmProjects\\prompt_maml\\MAML_Prompt_padding_5way_5shot_filter128_miniImagenet\n",
      "attempting to find existing checkpoint\n",
      "data {'test': 12000, 'train': 38400, 'val': 9600}\n",
      "train_seed 985773, val_seed: 985773, at start time\n",
      "75000 50000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_val_acc': 0.699288889169693,\n",
       " 'best_val_iter': 69500,\n",
       " 'current_iter': 75000,\n",
       " 'best_epoch': 139,\n",
       " 'train_loss_mean': 0.44109563875198365,\n",
       " 'train_loss_std': 0.1291735863213422,\n",
       " 'train_accuracy_mean': 0.8391066664457321,\n",
       " 'train_accuracy_std': 0.053464857991797364,\n",
       " 'train_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_0_std': 0.0,\n",
       " 'train_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_1_std': 0.0,\n",
       " 'train_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_2_std': 0.0,\n",
       " 'train_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'train_loss_importance_vector_3_std': 0.0,\n",
       " 'train_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'train_loss_importance_vector_4_std': 0.0,\n",
       " 'train_learning_rate_mean': 0.0010000000000000005,\n",
       " 'train_learning_rate_std': 4.336808689942018e-19,\n",
       " 'val_loss_mean': 0.8291508863369624,\n",
       " 'val_loss_std': 0.14279745482250403,\n",
       " 'val_accuracy_mean': 0.6856222219268481,\n",
       " 'val_accuracy_std': 0.05946382204087874,\n",
       " 'val_loss_importance_vector_0_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_0_std': 0.0,\n",
       " 'val_loss_importance_vector_1_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_1_std': 0.0,\n",
       " 'val_loss_importance_vector_2_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_2_std': 0.0,\n",
       " 'val_loss_importance_vector_3_mean': 0.006000000052154064,\n",
       " 'val_loss_importance_vector_3_std': 0.0,\n",
       " 'val_loss_importance_vector_4_mean': 0.9760000109672546,\n",
       " 'val_loss_importance_vector_4_std': 0.0,\n",
       " 'network': OrderedDict([('classifier.layer_dict.conv0.conv.weight',\n",
       "               tensor([[[[ 1.9228e-01, -3.8295e-01,  2.1642e-01],\n",
       "                         [-5.5411e-04, -4.2862e-02,  1.0049e-02],\n",
       "                         [-1.9220e-01,  4.4129e-01, -2.6217e-01]],\n",
       "               \n",
       "                        [[ 2.0767e-01, -4.2443e-01,  2.3391e-01],\n",
       "                         [-1.8607e-03, -2.2443e-02,  5.8218e-02],\n",
       "                         [-2.1587e-01,  3.7522e-01, -1.9190e-01]],\n",
       "               \n",
       "                        [[ 1.6017e-01, -2.9681e-01,  1.1097e-01],\n",
       "                         [ 2.8712e-02,  7.0504e-03, -2.6183e-02],\n",
       "                         [-1.5920e-01,  3.3434e-01, -1.4752e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.3566e-06, -1.4712e-06, -1.6298e-06],\n",
       "                         [-1.3851e-06, -1.5410e-06, -1.6120e-06],\n",
       "                         [-1.3997e-06, -1.5809e-06, -1.5675e-06]],\n",
       "               \n",
       "                        [[-1.5547e-06, -1.6984e-06, -1.8430e-06],\n",
       "                         [-1.5758e-06, -1.7545e-06, -1.7947e-06],\n",
       "                         [-1.5013e-06, -1.7067e-06, -1.7273e-06]],\n",
       "               \n",
       "                        [[-1.2922e-06, -1.4896e-06, -1.6343e-06],\n",
       "                         [-1.3808e-06, -1.6499e-06, -1.6773e-06],\n",
       "                         [-1.3512e-06, -1.6254e-06, -1.6297e-06]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 1.0596e-05,  1.4122e-05,  2.1456e-05],\n",
       "                         [ 2.6158e-06,  1.3649e-06,  1.3392e-05],\n",
       "                         [ 5.6518e-06,  4.9864e-06,  1.2944e-05]],\n",
       "               \n",
       "                        [[ 1.3616e-06,  2.8715e-06,  6.9109e-06],\n",
       "                         [-5.8312e-06, -8.3366e-06, -5.3908e-09],\n",
       "                         [-2.9931e-06, -2.8658e-06,  1.9354e-06]],\n",
       "               \n",
       "                        [[-1.0209e-06, -6.1541e-06, -2.1187e-06],\n",
       "                         [-6.7038e-06, -1.5934e-05, -1.2052e-05],\n",
       "                         [-6.8605e-06, -1.0175e-05, -9.9732e-06]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-5.6669e-05, -4.3113e-05, -3.2973e-05],\n",
       "                         [-4.5658e-05, -3.8774e-05, -2.2090e-05],\n",
       "                         [-8.6215e-05, -7.2827e-05, -5.3452e-05]],\n",
       "               \n",
       "                        [[ 1.1640e-04,  1.3112e-04,  1.3284e-04],\n",
       "                         [ 1.2972e-04,  1.3209e-04,  1.4257e-04],\n",
       "                         [ 8.1345e-05,  8.7237e-05,  9.9537e-05]],\n",
       "               \n",
       "                        [[ 5.5699e-04,  5.8572e-04,  6.6027e-04],\n",
       "                         [ 6.2147e-04,  6.3539e-04,  7.0788e-04],\n",
       "                         [ 4.7796e-04,  5.0228e-04,  5.4027e-04]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 9.2844e-05,  9.9655e-05,  1.0707e-04],\n",
       "                         [ 1.0091e-04,  1.1212e-04,  1.2581e-04],\n",
       "                         [ 1.0796e-04,  1.1651e-04,  1.2718e-04]],\n",
       "               \n",
       "                        [[ 1.1589e-04,  1.3381e-04,  1.4426e-04],\n",
       "                         [ 1.2781e-04,  1.4793e-04,  1.5086e-04],\n",
       "                         [ 1.4000e-04,  1.4373e-04,  1.4288e-04]],\n",
       "               \n",
       "                        [[ 4.6410e-05,  5.2902e-05,  5.5448e-05],\n",
       "                         [ 5.8372e-05,  6.7719e-05,  6.9776e-05],\n",
       "                         [ 6.7931e-05,  7.0582e-05,  7.1054e-05]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.9477e-08, -1.9241e-08, -1.8357e-08],\n",
       "                         [-2.0274e-08, -2.0286e-08, -1.9949e-08],\n",
       "                         [-1.9327e-08, -1.9144e-08, -1.8706e-08]],\n",
       "               \n",
       "                        [[-1.2993e-08, -1.0423e-08, -9.2905e-09],\n",
       "                         [-1.1519e-08, -1.0288e-08, -8.8197e-09],\n",
       "                         [-1.1433e-08, -9.9111e-09, -9.7060e-09]],\n",
       "               \n",
       "                        [[-2.2416e-08, -2.1352e-08, -1.9316e-08],\n",
       "                         [-2.2470e-08, -2.1539e-08, -2.0338e-08],\n",
       "                         [-2.1251e-08, -2.0551e-08, -1.9282e-08]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.conv.bias',\n",
       "               tensor([-1.0766e-04, -1.8208e-06,  1.7318e-09,  1.8022e-09, -6.0413e-13,\n",
       "                       -1.7608e-11, -2.4008e-07, -2.0747e-06, -5.4608e-14,  3.0420e-05,\n",
       "                        2.2418e-08, -1.2807e-04, -4.8821e-05,  7.3929e-06,  1.6470e-06,\n",
       "                        5.5269e-08,  5.2318e-08,  8.3448e-08, -1.9499e-07,  1.8654e-14,\n",
       "                        2.8759e-09, -1.4524e-11,  2.0112e-09,  8.9400e-07, -1.2196e-13,\n",
       "                        1.3235e-08,  1.6770e-06,  5.5523e-09,  6.5927e-08,  3.5236e-05,\n",
       "                        1.2349e-08, -2.5207e-05,  3.8266e-05,  1.2119e-05,  8.8170e-07,\n",
       "                       -5.8973e-06, -3.4360e-05,  2.8971e-06,  5.7751e-08,  1.6981e-05,\n",
       "                       -6.3020e-06,  6.4398e-07,  4.9946e-09,  7.9443e-06, -1.0413e-04,\n",
       "                        2.7680e-05,  3.4991e-06,  3.1208e-08,  6.2563e-06, -5.0337e-06,\n",
       "                        2.2825e-13, -1.4672e-05, -5.3406e-10,  1.0271e-08,  5.3552e-09,\n",
       "                       -6.2026e-06,  7.4721e-07, -1.1659e-08, -3.4967e-08,  1.5412e-07,\n",
       "                       -2.6710e-10, -3.6212e-09, -8.7714e-06, -1.0849e-05, -4.3477e-12,\n",
       "                        3.1768e-08,  2.3553e-06, -3.2610e-13, -5.6527e-08, -6.3316e-05,\n",
       "                        1.1863e-06,  5.1931e-08, -2.5802e-07, -5.8583e-08,  1.4091e-06,\n",
       "                        3.8682e-12,  1.0926e-05, -1.6802e-12,  7.9895e-08,  2.9296e-06,\n",
       "                       -9.4223e-13,  1.1736e-09,  9.6172e-08,  2.8990e-08,  6.0627e-13,\n",
       "                        4.9836e-08,  8.6083e-12, -5.9371e-07, -1.6258e-06,  1.0432e-11,\n",
       "                       -2.4476e-05, -1.3638e-08,  3.2943e-04,  1.2705e-07, -1.2575e-06,\n",
       "                       -1.8062e-05,  9.7118e-09, -2.5870e-05,  3.8594e-14,  7.8743e-06,\n",
       "                       -2.7276e-04, -1.2596e-08, -1.1878e-08, -4.4890e-14, -3.3862e-06,\n",
       "                        1.5649e-08,  5.7829e-06,  2.6338e-07,  4.0213e-07,  3.3605e-05,\n",
       "                       -3.6644e-07,  2.0759e-06, -8.1007e-09, -8.7374e-05, -9.9383e-07,\n",
       "                        5.5006e-05,  6.2239e-15,  1.0627e-05, -9.5537e-14, -9.8418e-05,\n",
       "                       -5.9476e-05,  8.2735e-09, -5.9381e-06, -5.5690e-05, -1.0448e-05,\n",
       "                        1.9987e-06,  1.1278e-08, -7.9950e-11], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.bias',\n",
       "               tensor([-2.5809e-02, -5.1914e-04, -1.8808e-03, -2.3333e-03, -2.3490e-08,\n",
       "                       -9.3809e-08, -1.1433e-03, -4.1157e-09, -1.8247e-10, -2.9051e-01,\n",
       "                       -4.2983e-02, -2.9330e-02, -1.7057e-01, -7.6430e-01,  6.6548e-01,\n",
       "                       -1.9083e-08, -3.5998e-02, -4.9337e-02, -3.2940e-03, -8.5166e-10,\n",
       "                       -9.6635e-08, -7.7134e-05, -4.5418e-10, -4.1506e-10, -8.6609e-08,\n",
       "                       -2.8743e-02, -5.0643e-10, -6.2151e-06, -5.3311e-06,  1.2237e-01,\n",
       "                       -1.1510e-02, -2.4103e-01,  4.5868e-02, -7.2895e-02, -4.2280e-02,\n",
       "                       -2.8928e-02, -7.9263e-01,  7.6610e-02, -3.7418e-02, -3.8003e-01,\n",
       "                       -1.5165e-02, -1.8458e-02, -1.6674e-10, -4.3972e-02, -6.6921e-02,\n",
       "                       -4.8274e-02, -3.9454e-04, -4.3398e-02, -4.9393e-02, -4.2917e-02,\n",
       "                       -5.5716e-09,  1.2862e-01, -2.2174e-06, -6.0387e-03, -1.0611e-02,\n",
       "                       -3.3598e-02, -8.0976e-02, -1.3177e-08, -1.5636e-03, -4.8358e-04,\n",
       "                       -8.5369e-05, -3.6528e-02, -9.1975e-02, -4.6839e-02, -2.4339e-09,\n",
       "                       -3.2042e-02, -4.6208e-02, -3.9834e-07, -8.4504e-09, -1.8178e-02,\n",
       "                       -8.6329e-03, -8.8436e-11, -1.9233e-02, -2.4675e-02, -4.5663e-07,\n",
       "                       -1.3715e-09,  9.3442e-02, -4.4416e-09, -2.4440e-02, -5.0634e-02,\n",
       "                       -4.7305e-09, -1.8038e-10, -3.5714e-02, -9.7317e-06, -7.6755e-10,\n",
       "                       -3.1391e-02, -5.0279e-09, -4.1558e-02, -2.7369e-09, -3.5630e-07,\n",
       "                       -4.7416e-03, -2.3511e-07, -2.5465e-02, -1.7810e-02, -5.8317e-09,\n",
       "                       -5.7122e-02, -3.1276e-02, -2.7462e-01, -7.6875e-10, -2.3602e-01,\n",
       "                       -2.5703e-02, -2.2502e-02, -7.5949e-10,  3.6854e-08,  1.4450e-01,\n",
       "                       -1.0464e-04, -2.3220e-02, -2.7830e-02, -1.8998e-02, -3.7490e-05,\n",
       "                       -5.6322e-03,  2.9649e-01, -6.5495e-02, -4.2253e-02, -1.3606e-02,\n",
       "                       -3.1794e-02, -3.1002e-09,  1.4254e-02, -1.7801e-09,  6.8282e-02,\n",
       "                       -2.5634e-02, -4.2781e-02,  2.6376e-01, -6.9117e-02, -2.1164e-10,\n",
       "                       -1.2218e-02, -1.5941e-02, -4.4482e-06], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv0.norm_layer.weight',\n",
       "               tensor([ 3.3526e-01,  1.9167e-04, -6.8391e-04, -2.1747e-04, -4.9496e-07,\n",
       "                       -4.9566e-11,  2.7613e-04,  2.4229e-19, -9.6174e-20,  1.8506e-01,\n",
       "                       -4.1493e-03,  4.6442e-01,  2.2548e-01,  3.4757e-01,  1.4051e-03,\n",
       "                        2.9909e-06,  1.9145e-03,  1.8588e-02,  7.4195e-04, -8.3158e-24,\n",
       "                        6.3285e-07,  3.1460e-05,  6.7130e-21,  4.8129e-08,  6.7764e-08,\n",
       "                       -3.7241e-03,  1.1095e-10,  2.8907e-19,  1.3859e-06,  1.9952e-01,\n",
       "                        3.4571e-03,  3.2557e-01,  2.8908e-01,  3.1386e-01,  1.3068e-02,\n",
       "                        5.2425e-01,  4.9968e-01,  1.8781e-01,  8.5281e-03,  3.8761e-01,\n",
       "                        4.9566e-03,  4.0077e-03,  8.2336e-11,  1.9664e-03,  3.7786e-01,\n",
       "                        4.0160e-01,  1.1259e-04, -3.9127e-03,  2.2199e-02,  1.3936e-02,\n",
       "                        1.5354e-07,  4.5942e-01,  3.7167e-05,  4.4951e-04, -1.1669e-03,\n",
       "                        5.1375e-03,  3.1180e-01, -2.7176e-15, -5.3439e-04, -5.6643e-05,\n",
       "                       -1.0769e-04,  3.6468e-03,  2.6200e-01,  1.8098e-02, -1.1121e-05,\n",
       "                        5.5202e-03, -2.2146e-02,  1.5499e-06,  9.7797e-14,  3.4626e-01,\n",
       "                        5.6925e-04,  3.1853e-14,  1.1209e-03, -1.3584e-03, -3.7818e-04,\n",
       "                        2.3882e-10,  1.7380e-01,  4.6401e-13, -1.0196e-02, -1.4447e-02,\n",
       "                        5.7228e-17, -1.4056e-09, -9.1493e-03, -7.7048e-05, -1.1837e-07,\n",
       "                        8.8579e-03,  4.8816e-15, -5.7310e-03,  3.4566e-08,  4.5127e-04,\n",
       "                        8.4108e-04,  5.6366e-06,  3.7016e-01, -6.6159e-03,  2.5826e-07,\n",
       "                        1.3502e-02, -6.2362e-03,  3.9604e-01, -3.4727e-08,  2.5340e-01,\n",
       "                        5.0110e-01,  1.5501e-03,  3.3106e-09,  1.3406e-16,  1.0688e-03,\n",
       "                        1.2580e-04, -2.7252e-03,  3.6551e-04,  1.0066e-03,  3.5513e-06,\n",
       "                        8.6921e-04, -2.7740e-03, -1.4297e-02,  3.5214e-01,  4.1803e-03,\n",
       "                        5.4773e-01,  1.6884e-23, -4.2999e-02,  4.7924e-08,  4.5025e-01,\n",
       "                        4.7415e-01, -2.8217e-03,  4.0074e-01,  3.5637e-01,  2.1544e-09,\n",
       "                        1.6793e-03, -6.8909e-04,  1.9759e-05], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.weight',\n",
       "               tensor([[[[ 1.5695e-03, -5.7032e-02, -2.9868e-03],\n",
       "                         [-2.7996e-02, -8.3531e-02,  3.1111e-02],\n",
       "                         [ 1.0478e-01, -1.0303e-01,  3.3634e-02]],\n",
       "               \n",
       "                        [[ 4.3103e-07, -2.0037e-07,  1.2237e-06],\n",
       "                         [ 5.7432e-07, -1.1839e-07,  1.1313e-06],\n",
       "                         [-7.8870e-07, -1.6104e-06, -8.5676e-07]],\n",
       "               \n",
       "                        [[ 3.8306e-06,  4.1996e-06,  4.3505e-06],\n",
       "                         [-2.1818e-07,  6.6459e-08, -1.0587e-08],\n",
       "                         [-3.4418e-06, -3.3818e-06, -3.8774e-06]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 8.3234e-05,  6.4253e-05,  7.0436e-05],\n",
       "                         [ 3.5203e-05,  2.2897e-05,  3.4650e-05],\n",
       "                         [-4.8081e-05, -4.6398e-05, -2.7905e-05]],\n",
       "               \n",
       "                        [[ 2.7242e-05,  1.3766e-05,  3.8717e-05],\n",
       "                         [ 8.5378e-07, -1.7300e-05,  1.1540e-05],\n",
       "                         [-7.9276e-05, -9.7125e-05, -7.1933e-05]],\n",
       "               \n",
       "                        [[ 2.8196e-08,  1.2342e-08,  5.7852e-08],\n",
       "                         [ 1.3742e-08, -4.5810e-10,  3.9569e-08],\n",
       "                         [ 1.4135e-07,  1.2498e-07,  1.6029e-07]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.6531e-03, -7.5278e-02, -6.4949e-02],\n",
       "                         [ 3.6331e-02, -8.7467e-02, -9.6507e-02],\n",
       "                         [ 2.4241e-02, -3.9710e-02,  5.6739e-03]],\n",
       "               \n",
       "                        [[ 5.1084e-06,  5.4070e-06,  7.3495e-06],\n",
       "                         [-3.8266e-07,  2.6003e-07,  2.5452e-06],\n",
       "                         [-1.3988e-06, -9.1484e-07,  1.6825e-06]],\n",
       "               \n",
       "                        [[ 6.1651e-06,  7.1286e-06,  1.0248e-05],\n",
       "                         [-1.9111e-06, -7.7084e-07,  3.0698e-06],\n",
       "                         [-1.1638e-06,  5.4033e-07,  5.6840e-06]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 2.2410e-04,  2.1415e-04,  2.3240e-04],\n",
       "                         [ 1.0465e-04,  1.2986e-04,  1.6661e-04],\n",
       "                         [ 8.4319e-05,  1.1937e-04,  1.5559e-04]],\n",
       "               \n",
       "                        [[ 2.1665e-04,  2.2382e-04,  2.8856e-04],\n",
       "                         [ 4.1147e-05,  6.3914e-05,  1.2887e-04],\n",
       "                         [ 2.8632e-06,  3.2031e-05,  1.0273e-04]],\n",
       "               \n",
       "                        [[-1.5103e-08, -6.7282e-09, -2.5531e-08],\n",
       "                         [-4.8396e-09, -5.8739e-11, -1.8241e-08],\n",
       "                         [-2.1287e-08, -1.1913e-08, -2.4971e-08]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 4.0683e-03,  5.2381e-02, -3.0850e-03],\n",
       "                         [ 6.8705e-02, -8.1159e-02,  5.3797e-02],\n",
       "                         [-2.1517e-03,  2.6284e-02, -6.2020e-02]],\n",
       "               \n",
       "                        [[ 2.8706e-06,  1.2291e-06,  2.2474e-06],\n",
       "                         [ 1.4600e-06, -1.0326e-07,  9.4203e-07],\n",
       "                         [ 2.3155e-06,  5.9170e-07,  1.5742e-06]],\n",
       "               \n",
       "                        [[ 3.0357e-06,  1.1254e-06,  4.9354e-06],\n",
       "                         [ 1.5113e-06, -5.5967e-07,  3.1499e-06],\n",
       "                         [ 2.4232e-06,  1.0744e-07,  3.2646e-06]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-6.8424e-05, -1.0420e-04, -3.7301e-05],\n",
       "                         [-3.8869e-05, -9.7091e-05, -7.5316e-05],\n",
       "                         [ 1.4194e-05, -4.0148e-05, -4.1150e-05]],\n",
       "               \n",
       "                        [[ 4.1896e-05,  8.8409e-06,  5.2717e-05],\n",
       "                         [ 2.4159e-05, -1.3305e-05,  2.0680e-05],\n",
       "                         [ 3.7933e-05,  1.2007e-06,  2.8198e-05]],\n",
       "               \n",
       "                        [[ 6.2969e-08,  4.6771e-08,  8.1204e-08],\n",
       "                         [ 1.3417e-08,  1.9803e-11,  4.0851e-08],\n",
       "                         [ 2.6059e-08,  1.0058e-08,  5.2158e-08]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-8.8614e-14,  6.8446e-13, -5.4504e-13],\n",
       "                         [-2.7504e-13,  8.0959e-13, -7.1600e-13],\n",
       "                         [ 1.8672e-12, -7.7381e-13, -1.0437e-12]],\n",
       "               \n",
       "                        [[-1.7152e-09,  1.0296e-09,  1.6819e-08],\n",
       "                         [ 2.1325e-09, -2.4045e-11, -8.2644e-11],\n",
       "                         [ 1.4497e-12, -1.7644e-15,  8.1426e-08]],\n",
       "               \n",
       "                        [[-7.5469e-17, -1.1966e-17,  1.4466e-17],\n",
       "                         [-5.1931e-17, -7.0395e-18, -6.6036e-17],\n",
       "                         [-1.6349e-16, -1.2963e-16, -6.2481e-17]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.4536e-15, -2.2394e-15, -1.0331e-15],\n",
       "                         [-2.7920e-15, -2.9148e-15, -1.0163e-15],\n",
       "                         [-3.5155e-15, -2.8757e-15, -1.6031e-15]],\n",
       "               \n",
       "                        [[-1.2591e-15,  9.4592e-16,  3.3485e-15],\n",
       "                         [-6.5137e-16,  1.1505e-15,  3.5876e-15],\n",
       "                         [ 4.7514e-16,  3.1834e-15,  5.5867e-15]],\n",
       "               \n",
       "                        [[ 6.0253e-08,  1.3847e-12,  4.8175e-14],\n",
       "                         [-1.1157e-14,  1.1654e-16,  7.5787e-09],\n",
       "                         [ 7.6728e-15,  8.4598e-15,  1.4966e-11]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 6.0716e-02, -5.4222e-02, -6.5542e-02],\n",
       "                         [ 7.6453e-02,  7.7619e-02,  5.3585e-02],\n",
       "                         [-3.6023e-03,  1.2287e-01,  7.2876e-02]],\n",
       "               \n",
       "                        [[ 1.9270e-06,  3.1721e-08,  3.7762e-09],\n",
       "                         [ 2.1262e-06, -3.0081e-08, -7.6382e-08],\n",
       "                         [-9.4317e-07, -2.8849e-06, -2.8278e-06]],\n",
       "               \n",
       "                        [[-1.1461e-06, -4.9580e-07, -3.5916e-07],\n",
       "                         [-1.1484e-07,  1.3600e-08,  5.5160e-07],\n",
       "                         [-2.9553e-06, -2.3556e-06, -1.3710e-06]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.2907e-04, -1.3657e-04, -1.6962e-04],\n",
       "                         [-1.1038e-04, -1.0456e-04, -1.1740e-04],\n",
       "                         [-1.1941e-04, -1.1952e-04, -1.2450e-04]],\n",
       "               \n",
       "                        [[-1.7785e-05, -3.4181e-05, -5.0757e-05],\n",
       "                         [-2.6008e-05, -4.2766e-05, -4.4512e-05],\n",
       "                         [-4.8817e-05, -6.0582e-05, -5.3144e-05]],\n",
       "               \n",
       "                        [[ 4.1328e-08,  3.3762e-08,  2.2801e-08],\n",
       "                         [ 3.2353e-09,  7.2394e-10, -1.4439e-08],\n",
       "                         [-1.7319e-08, -6.1286e-09, -3.0143e-08]]],\n",
       "               \n",
       "               \n",
       "                       [[[-5.5801e-02, -3.9283e-03, -2.1383e-02],\n",
       "                         [ 1.5353e-02, -4.3544e-03,  1.8595e-02],\n",
       "                         [-6.8868e-03,  5.3788e-02,  3.0877e-03]],\n",
       "               \n",
       "                        [[ 6.4186e-06,  8.9440e-06,  1.4311e-06],\n",
       "                         [-3.0039e-06, -1.3032e-07, -8.4991e-06],\n",
       "                         [-8.7553e-06, -6.5315e-06, -1.5881e-05]],\n",
       "               \n",
       "                        [[ 2.6454e-05,  2.1426e-05,  1.8478e-05],\n",
       "                         [ 7.2922e-06,  6.1884e-07, -1.3302e-06],\n",
       "                         [ 1.5738e-05,  4.3340e-06,  3.2809e-06]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 4.0983e-05,  8.7041e-05, -5.7032e-05],\n",
       "                         [-1.8424e-04, -1.2311e-04, -2.8319e-04],\n",
       "                         [-2.2099e-04, -1.6492e-04, -2.9923e-04]],\n",
       "               \n",
       "                        [[ 1.8022e-04,  2.3264e-04,  6.1339e-06],\n",
       "                         [-1.0308e-04, -3.2971e-05, -2.7344e-04],\n",
       "                         [-1.2316e-04, -1.4013e-04, -2.9492e-04]],\n",
       "               \n",
       "                        [[-6.5505e-09,  6.0620e-08, -2.5865e-08],\n",
       "                         [-6.2518e-08,  1.4481e-10, -8.5896e-08],\n",
       "                         [-6.0947e-08, -5.6647e-08, -1.3193e-07]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.conv.bias',\n",
       "               tensor([ 4.5168e-05, -3.0117e-05,  3.6113e-05, -2.1736e-06,  1.6835e-04,\n",
       "                        3.5484e-05, -2.1631e-09, -4.2750e-08, -1.2322e-13, -8.8280e-06,\n",
       "                       -6.3897e-07,  1.5514e-08, -8.1350e-08, -8.0250e-06, -2.0894e-09,\n",
       "                        9.3035e-06,  1.4602e-09,  1.1298e-05,  1.0218e-05, -5.4976e-08,\n",
       "                        7.9948e-05,  1.0569e-08,  9.7000e-08, -8.1266e-06,  2.6625e-05,\n",
       "                       -1.4714e-06, -1.8307e-05, -1.2225e-04, -1.4232e-04, -6.8579e-05,\n",
       "                        9.3456e-06, -3.2339e-05, -7.2536e-14, -8.0666e-06,  3.7663e-05,\n",
       "                       -2.3811e-05,  9.0170e-08, -6.1606e-05, -1.4049e-05, -4.0197e-05,\n",
       "                       -1.3548e-05, -7.0012e-05,  1.6610e-15,  2.3690e-06, -1.7877e-08,\n",
       "                        3.3214e-05, -2.0823e-05, -1.1988e-06, -6.3415e-05,  1.4345e-07,\n",
       "                       -7.0419e-09, -2.8471e-05, -4.2456e-05,  3.5109e-06,  1.7452e-06,\n",
       "                       -4.3338e-05, -3.4289e-06,  5.4746e-08, -2.9869e-06,  6.5749e-05,\n",
       "                       -1.6871e-08,  2.6363e-05, -8.5679e-11, -1.1450e-05,  1.9462e-05,\n",
       "                        1.6616e-10, -5.2157e-15, -4.8548e-05, -3.9929e-06,  2.0022e-05,\n",
       "                       -2.3773e-05,  6.1655e-07, -7.5857e-09,  9.8063e-06, -8.6595e-06,\n",
       "                       -2.4165e-05, -1.8007e-08,  4.4242e-05, -3.1743e-05, -1.0179e-05,\n",
       "                        6.2616e-07, -2.5623e-05, -4.8078e-08, -5.2821e-08, -1.3029e-05,\n",
       "                       -8.6953e-06, -5.9220e-05, -1.2131e-04,  2.0129e-06,  4.8198e-08,\n",
       "                        3.1398e-05,  1.1218e-05, -1.1826e-05, -7.7615e-08,  1.2837e-04,\n",
       "                       -1.5474e-08,  3.1306e-05,  1.5030e-06, -2.2974e-07, -5.1837e-06,\n",
       "                       -1.3173e-05, -3.7323e-06,  2.8273e-06,  9.2429e-16, -4.1643e-05,\n",
       "                       -7.9699e-06,  2.6537e-05, -1.0435e-05, -5.1954e-07,  2.3600e-05,\n",
       "                       -1.1240e-08, -3.6531e-05,  1.6629e-09, -1.4557e-05, -3.3385e-05,\n",
       "                       -2.9059e-05, -8.6512e-07,  1.2803e-04,  2.6218e-05,  2.8815e-05,\n",
       "                       -1.5188e-07,  1.7123e-06, -1.3313e-05,  4.6257e-05,  3.2781e-07,\n",
       "                        3.9636e-07, -2.7265e-05,  7.2250e-06], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.bias',\n",
       "               tensor([-1.5115e-01, -3.1402e-01, -1.8303e-01, -1.2133e-02, -2.1663e-01,\n",
       "                       -2.4162e-01, -2.2323e-07, -4.9754e-02, -4.7388e-11, -3.0910e-01,\n",
       "                       -6.8840e-06, -4.8328e-02, -5.1842e-02, -6.3425e-06, -6.8398e-10,\n",
       "                       -1.5040e-07, -2.7792e-05, -9.4731e-02, -2.7108e-01, -4.8928e-08,\n",
       "                       -1.2728e-01, -5.2919e-03, -2.3063e-02, -1.8212e-02, -1.5119e-01,\n",
       "                       -3.9287e-02, -2.3425e-01, -1.8593e-01, -1.5186e-02, -1.2504e-01,\n",
       "                       -1.2405e-01,  6.3092e-02, -4.3638e-08, -1.9626e-01, -1.7277e-01,\n",
       "                       -2.4427e-01, -4.6361e-02, -1.9265e-01, -2.4029e-01, -2.0945e-01,\n",
       "                       -1.7426e-01, -5.4928e-02, -3.3216e-08, -5.7906e-02, -1.8073e-02,\n",
       "                       -2.8147e-01, -2.1982e-01, -2.2724e-01, -2.0162e-01, -6.2892e-02,\n",
       "                       -1.4753e-03, -2.2835e-01, -1.6745e-01, -9.9206e-10, -6.7113e-02,\n",
       "                       -1.4650e-01, -1.1744e-04, -1.7754e-04, -1.4878e-01, -1.8586e-01,\n",
       "                       -3.8858e-04, -2.4775e-01, -4.7108e-06, -1.5611e-01, -1.2541e-01,\n",
       "                       -1.9238e-06, -4.8290e-11, -4.4985e-01, -8.4414e-02, -3.5834e-02,\n",
       "                       -1.1117e-01, -5.0948e-02, -7.1629e-03, -2.3098e-01, -1.8589e-01,\n",
       "                       -2.3255e-01, -8.5299e-03, -1.6975e-01, -1.9599e-01, -1.4893e-01,\n",
       "                       -4.0870e-02, -5.5576e-03, -7.6101e-03, -4.4184e-02, -1.5317e-01,\n",
       "                       -2.5087e-01, -3.1078e-01, -2.0881e-01, -1.5448e-01, -1.7251e-02,\n",
       "                       -2.1197e-01, -5.7133e-02, -1.3835e-01, -1.2130e-02, -2.4563e-01,\n",
       "                       -3.7382e-03, -2.8209e-01, -1.4660e-02, -5.3973e-02, -3.4547e-01,\n",
       "                       -1.9406e-01, -1.2694e-02, -6.6265e-05, -7.5035e-06, -1.8469e-01,\n",
       "                       -1.4090e-02, -1.3891e-01, -1.3155e-01, -4.4367e-02, -2.2380e-01,\n",
       "                       -3.6748e-02, -2.0704e-01, -1.1590e-02, -1.5942e-09, -2.1537e-01,\n",
       "                       -2.3270e-01, -3.5155e-02, -2.8855e-01, -1.6806e-01, -2.1776e-01,\n",
       "                       -5.2874e-02, -1.6782e-01, -2.1668e-01, -2.0899e-01, -4.4701e-02,\n",
       "                       -2.6233e-09, -2.3584e-01, -2.8385e-01], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv1.norm_layer.weight',\n",
       "               tensor([ 2.6532e-01,  2.0715e-01,  3.8209e-01, -1.8945e-03,  4.9843e-01,\n",
       "                        2.1708e-01, -5.8063e-05, -1.1586e-03, -4.9505e-08,  4.8084e-01,\n",
       "                       -7.6735e-11,  2.9084e-03, -2.4962e-02,  1.5918e-04, -2.5288e-10,\n",
       "                       -9.1117e-13, -3.2566e-04, -1.4554e-02,  1.8416e-01,  2.0516e-08,\n",
       "                        2.7979e-01, -8.6358e-04,  3.5524e-03,  2.2911e-03,  2.8209e-01,\n",
       "                       -6.7828e-03,  4.1100e-01,  5.7101e-01,  2.3564e-01,  2.3557e-01,\n",
       "                        1.8358e-01,  1.9113e-01,  1.4182e-12,  3.8140e-01,  2.8341e-01,\n",
       "                        1.7351e-01,  4.9462e-03,  3.1468e-01,  3.3420e-01,  3.5592e-01,\n",
       "                        2.0780e-01,  2.9089e-01,  6.7969e-10, -5.0611e-03, -2.5277e-03,\n",
       "                        3.5446e-01,  3.4840e-01,  3.6084e-01,  2.9067e-01,  4.0440e-03,\n",
       "                       -7.0053e-04,  1.4181e-01,  4.2094e-01, -2.2558e-06,  2.3988e-01,\n",
       "                        3.4501e-01,  1.3630e-04,  2.5436e-04,  8.7545e-02,  3.4499e-01,\n",
       "                       -2.2140e-04,  3.0961e-01,  3.6507e-05,  2.5331e-01,  2.8411e-01,\n",
       "                        1.4401e-04,  2.5930e-14,  3.6041e-01,  2.0267e-01,  4.7530e-03,\n",
       "                        3.8362e-01,  4.7990e-03,  1.3029e-03,  3.8613e-01,  3.8848e-01,\n",
       "                        2.9537e-01, -1.6289e-03,  3.0565e-01,  3.0061e-01,  2.0382e-01,\n",
       "                       -1.6124e-02, -1.1573e-03,  1.0887e-03,  6.8003e-03,  2.3247e-01,\n",
       "                        3.5745e-01,  4.6195e-01,  6.7543e-01,  3.1324e-01,  1.6888e-03,\n",
       "                        2.1650e-01,  2.1976e-02,  1.5640e-01, -3.0253e-04,  5.1016e-01,\n",
       "                        8.7720e-04,  2.2294e-01, -1.7185e-03, -1.6013e-02,  1.3848e-01,\n",
       "                        1.8158e-01, -2.0348e-03, -3.3019e-04, -5.6111e-10,  3.9194e-01,\n",
       "                       -1.7497e-03,  2.9699e-01,  2.6754e-01, -6.6517e-03,  2.5667e-01,\n",
       "                       -3.3140e-04,  4.9930e-01, -1.3358e-03,  7.7385e-11,  2.9451e-01,\n",
       "                        2.9847e-01, -2.9918e-03,  4.4894e-01,  2.6036e-01,  3.2710e-01,\n",
       "                       -6.9202e-03,  3.0018e-01,  3.7325e-01,  2.9502e-01,  2.5506e-02,\n",
       "                        1.6260e-06,  2.9978e-01,  3.3959e-01], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.weight',\n",
       "               tensor([[[[-1.7331e-02, -1.0295e-02,  6.0006e-02],\n",
       "                         [ 1.4751e-02,  1.1840e-02,  4.4171e-02],\n",
       "                         [-1.8700e-02,  9.9152e-03,  9.8307e-03]],\n",
       "               \n",
       "                        [[ 3.2548e-02, -3.1620e-03,  4.7042e-02],\n",
       "                         [ 8.7354e-03, -4.6184e-02, -2.6271e-02],\n",
       "                         [ 3.9488e-02, -1.0486e-02, -4.3416e-02]],\n",
       "               \n",
       "                        [[-3.5336e-03, -4.2004e-02,  9.4704e-02],\n",
       "                         [ 2.4011e-02,  6.0114e-02,  1.0110e-01],\n",
       "                         [-4.9922e-02,  1.9818e-02, -3.7397e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 8.4008e-12,  1.3610e-11,  1.9948e-11],\n",
       "                         [-2.4882e-10,  2.6633e-11,  2.2842e-11],\n",
       "                         [-1.2628e-10, -1.2588e-10, -1.1820e-10]],\n",
       "               \n",
       "                        [[-2.6030e-02, -2.2646e-02, -4.5673e-02],\n",
       "                         [-3.6530e-02,  4.4861e-02, -3.2543e-02],\n",
       "                         [-9.9535e-02, -7.1312e-02, -3.1628e-02]],\n",
       "               \n",
       "                        [[-5.0294e-02, -2.0907e-03, -5.2632e-02],\n",
       "                         [-5.4901e-02,  8.1871e-03, -2.9578e-02],\n",
       "                         [-2.3333e-02,  2.3127e-02,  5.3094e-03]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.3915e-08, -1.3539e-08, -1.0208e-08],\n",
       "                         [-7.6919e-09, -1.0022e-08, -1.1036e-08],\n",
       "                         [-1.6319e-08, -1.6095e-08, -1.8324e-08]],\n",
       "               \n",
       "                        [[-7.1355e-09, -1.8958e-09, -5.7762e-09],\n",
       "                         [-8.8858e-09, -1.0907e-08, -7.5267e-09],\n",
       "                         [-1.3050e-08, -4.8257e-09, -6.3485e-09]],\n",
       "               \n",
       "                        [[ 1.1903e-09, -9.5016e-10, -4.6731e-09],\n",
       "                         [ 3.1978e-09, -2.5720e-09, -5.3921e-09],\n",
       "                         [-1.1078e-09, -4.2107e-09,  1.2604e-09]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 3.5160e-17,  1.1911e-17,  2.7220e-17],\n",
       "                         [ 1.1128e-07, -1.7800e-20,  6.4114e-18],\n",
       "                         [ 1.8461e-07, -1.7758e-13, -2.3470e-07]],\n",
       "               \n",
       "                        [[-4.5536e-09,  3.8789e-09,  1.4197e-09],\n",
       "                         [-8.1445e-09, -7.2252e-09, -3.6486e-09],\n",
       "                         [-1.3546e-08, -8.2122e-10, -5.5370e-09]],\n",
       "               \n",
       "                        [[ 1.2165e-08,  1.5477e-08,  1.8301e-08],\n",
       "                         [ 1.1084e-08,  1.2940e-08,  1.1914e-08],\n",
       "                         [ 6.5109e-09,  1.2531e-08,  8.0403e-09]]],\n",
       "               \n",
       "               \n",
       "                       [[[-2.0525e-02,  1.5007e-04, -6.8799e-04],\n",
       "                         [ 1.3207e-02, -2.5649e-02, -4.2961e-02],\n",
       "                         [-4.4341e-03, -4.7235e-02, -5.5492e-02]],\n",
       "               \n",
       "                        [[-1.4054e-02,  5.3095e-02,  1.9296e-02],\n",
       "                         [ 7.4229e-03, -4.8933e-02,  7.1470e-03],\n",
       "                         [-1.2933e-02, -7.3876e-02, -9.2843e-02]],\n",
       "               \n",
       "                        [[-1.6037e-02,  3.1129e-02,  1.1733e-01],\n",
       "                         [-3.2431e-02,  5.7320e-02,  5.8305e-02],\n",
       "                         [-6.3027e-02, -1.6542e-02,  1.3459e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-3.2700e-10, -3.7454e-12, -1.6353e-10],\n",
       "                         [-3.0087e-10,  1.8615e-11, -1.4256e-10],\n",
       "                         [-4.9974e-10, -1.6924e-10, -3.0955e-10]],\n",
       "               \n",
       "                        [[-1.0961e-02,  8.5780e-02,  6.3631e-02],\n",
       "                         [ 2.8655e-02,  1.5317e-02,  5.1010e-03],\n",
       "                         [ 6.3905e-02,  7.3681e-02,  4.9896e-02]],\n",
       "               \n",
       "                        [[-6.6658e-02,  4.9188e-02, -6.8371e-05],\n",
       "                         [-2.6394e-02,  2.4509e-02, -6.3541e-02],\n",
       "                         [ 2.9110e-02, -8.6412e-03, -8.7618e-03]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-2.4340e-03, -7.6446e-02, -4.8478e-02],\n",
       "                         [-1.8862e-02, -5.6813e-02, -2.8551e-02],\n",
       "                         [-2.7547e-02,  9.6841e-03, -4.1929e-02]],\n",
       "               \n",
       "                        [[-9.8509e-02, -5.6164e-02, -1.8223e-02],\n",
       "                         [-9.2969e-02, -8.3309e-02, -4.7287e-02],\n",
       "                         [-6.3075e-02, -8.8742e-02, -5.0179e-02]],\n",
       "               \n",
       "                        [[-4.3223e-02, -9.3727e-02, -4.2817e-02],\n",
       "                         [ 9.1428e-03,  4.6011e-02, -9.3698e-03],\n",
       "                         [ 2.0409e-02,  6.4687e-02,  2.5330e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 4.8324e-10,  1.1324e-10,  3.6723e-10],\n",
       "                         [ 4.0323e-10, -3.5206e-11,  1.7468e-10],\n",
       "                         [ 6.3891e-10,  3.5280e-10,  8.3946e-10]],\n",
       "               \n",
       "                        [[ 3.2267e-02,  2.4582e-02,  8.6782e-02],\n",
       "                         [ 1.1874e-02, -5.1519e-02, -6.6258e-03],\n",
       "                         [ 9.2948e-02, -3.0583e-03, -4.3539e-02]],\n",
       "               \n",
       "                        [[-7.7873e-02,  4.6624e-02,  6.2669e-02],\n",
       "                         [-8.6713e-02,  1.7010e-03,  6.5482e-02],\n",
       "                         [-1.0964e-01, -7.4254e-03,  3.4417e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 3.8917e-02,  4.7500e-02,  5.0847e-02],\n",
       "                         [ 2.4168e-02, -1.4516e-02, -3.8104e-02],\n",
       "                         [-6.3570e-05, -1.7624e-02, -5.0674e-03]],\n",
       "               \n",
       "                        [[-4.4582e-02, -5.3816e-02, -5.7317e-02],\n",
       "                         [-8.4361e-02, -5.4290e-02, -4.6439e-02],\n",
       "                         [-1.1457e-01,  5.1760e-03, -2.5250e-02]],\n",
       "               \n",
       "                        [[-1.0224e-01, -6.3555e-02,  3.9176e-02],\n",
       "                         [-1.0838e-01, -6.0592e-02,  1.5014e-02],\n",
       "                         [ 8.9480e-02,  3.7615e-02, -1.7611e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-2.1091e-11,  8.6899e-11,  1.9585e-10],\n",
       "                         [-1.0885e-10, -2.4445e-11, -1.5381e-10],\n",
       "                         [-1.3987e-10, -2.5605e-11,  3.7611e-11]],\n",
       "               \n",
       "                        [[-2.4591e-02,  9.2847e-02,  7.3792e-02],\n",
       "                         [ 6.3681e-03, -3.6026e-02, -4.5709e-02],\n",
       "                         [-4.1956e-02, -5.0628e-03,  2.1480e-02]],\n",
       "               \n",
       "                        [[-4.2018e-02, -2.1273e-02, -2.1377e-02],\n",
       "                         [-1.0082e-02, -9.3999e-03,  4.4530e-03],\n",
       "                         [ 3.0255e-02,  4.7723e-02,  3.0611e-02]]],\n",
       "               \n",
       "               \n",
       "                       [[[-4.0684e-09, -2.1106e-09, -2.1904e-09],\n",
       "                         [-2.0228e-09, -1.9443e-09, -2.7149e-09],\n",
       "                         [-4.5859e-09, -3.6660e-09, -3.6155e-09]],\n",
       "               \n",
       "                        [[-1.0328e-09, -2.3321e-09, -1.5114e-09],\n",
       "                         [-1.7891e-09, -2.2804e-09, -1.6883e-09],\n",
       "                         [-2.4500e-09, -2.4208e-09, -1.7160e-09]],\n",
       "               \n",
       "                        [[-4.3234e-10, -2.2020e-09, -1.0053e-10],\n",
       "                         [ 2.5660e-09,  4.9719e-10, -1.2553e-09],\n",
       "                         [-3.1708e-10, -1.1552e-09,  2.4696e-09]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.5557e-08,  1.5582e-10,  1.2349e-16],\n",
       "                         [-1.4798e-11, -1.9435e-13,  2.0571e-14],\n",
       "                         [-1.5178e-05,  1.5439e-06,  1.7371e-10]],\n",
       "               \n",
       "                        [[ 8.8018e-11,  1.8227e-09,  2.7502e-09],\n",
       "                         [-1.8966e-09, -4.7839e-09, -2.7987e-09],\n",
       "                         [-3.8434e-09, -3.6197e-09, -5.6636e-10]],\n",
       "               \n",
       "                        [[ 2.9312e-09,  4.4875e-09,  4.3292e-09],\n",
       "                         [ 3.1608e-09,  3.6188e-09,  3.8885e-09],\n",
       "                         [ 1.8508e-09,  2.5927e-09,  1.9351e-09]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.conv.bias',\n",
       "               tensor([-5.3384e-05,  5.5572e-07,  1.3152e-05,  6.0222e-05,  4.7515e-06,\n",
       "                        2.8423e-05, -5.4325e-05,  3.5956e-05,  4.4897e-05, -3.2208e-05,\n",
       "                        5.3095e-08, -4.4170e-05, -2.3967e-05, -5.7456e-06,  1.9538e-05,\n",
       "                       -2.3510e-05, -8.4308e-06,  5.2713e-06,  3.5566e-05,  1.1082e-05,\n",
       "                        4.6182e-06,  1.6678e-05,  5.6850e-05,  1.0873e-05, -9.0663e-06,\n",
       "                        1.1822e-05,  4.7681e-05,  6.2435e-05, -1.2264e-05,  6.4167e-05,\n",
       "                       -2.3430e-05, -2.4236e-05, -2.1450e-06,  5.0586e-05, -5.4256e-06,\n",
       "                       -1.6500e-05, -2.5635e-05, -7.3973e-05, -2.7957e-05, -2.5788e-05,\n",
       "                       -2.3111e-05,  1.0915e-06, -4.1539e-05,  9.3469e-06,  6.3956e-07,\n",
       "                        1.0264e-05,  9.2556e-06,  2.4093e-05,  3.9909e-05, -7.1425e-05,\n",
       "                        1.1722e-05, -6.6994e-06,  1.2229e-05, -1.0588e-04, -3.7606e-05,\n",
       "                       -2.8187e-05,  9.3681e-05,  2.1290e-05,  2.6317e-05, -3.7702e-05,\n",
       "                        5.4658e-06, -8.9180e-06, -8.5006e-08, -2.4837e-05,  1.4544e-05,\n",
       "                       -5.0152e-06,  5.9595e-06,  1.0830e-05, -4.6397e-05,  8.7463e-05,\n",
       "                       -2.2675e-06,  4.3275e-05,  1.1086e-05,  1.6485e-05, -3.6259e-05,\n",
       "                        1.1657e-05, -2.8464e-05, -1.4426e-05, -3.0550e-07,  5.8997e-05,\n",
       "                       -1.2027e-04,  5.7587e-05, -6.8657e-05, -2.7564e-05,  2.2999e-05,\n",
       "                       -2.8612e-05, -1.4204e-04,  3.2883e-11,  9.0800e-06, -8.7231e-05,\n",
       "                        4.3347e-05,  6.1474e-05,  3.1957e-08, -1.7775e-05,  1.3384e-05,\n",
       "                        8.8116e-05, -3.2787e-05, -1.6790e-06,  3.0065e-05,  1.1317e-05,\n",
       "                       -6.2813e-05, -4.9765e-05,  4.6875e-06,  4.0287e-05,  8.7538e-06,\n",
       "                        1.2088e-05, -8.4215e-06,  2.6617e-05, -1.8096e-05, -8.3361e-06,\n",
       "                        6.2211e-06,  3.5478e-05, -1.2395e-07,  2.8924e-05,  8.4677e-05,\n",
       "                        2.6372e-05,  5.6342e-06, -1.9853e-05,  7.4075e-06, -1.1090e-05,\n",
       "                        3.6349e-05,  3.5301e-05,  1.7749e-05,  6.1878e-05, -9.1290e-06,\n",
       "                        3.6832e-06, -4.2189e-05, -2.8849e-08], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.bias',\n",
       "               tensor([-2.8877e-01, -2.5261e-06, -2.3193e-01, -3.2576e-01, -3.3149e-01,\n",
       "                       -2.6402e-01, -3.9454e-01, -3.4977e-01, -1.5933e-01, -2.4774e-01,\n",
       "                       -7.4630e-02, -2.6290e-01, -3.8616e-01, -3.6579e-01, -2.9344e-01,\n",
       "                       -2.4617e-01, -2.1937e-01, -4.2036e-01, -3.9470e-01, -4.7105e-01,\n",
       "                       -3.6036e-01, -2.9724e-01, -4.5556e-01, -2.4364e-01, -1.9501e-01,\n",
       "                       -3.0994e-01, -2.0532e-01, -3.4752e-01, -2.5188e-01, -2.1788e-01,\n",
       "                       -4.9082e-01, -4.9991e-01, -2.9614e-01, -2.6678e-01, -2.2864e-01,\n",
       "                       -2.9664e-01, -2.5535e-01, -3.1973e-01, -3.5434e-01, -2.7234e-01,\n",
       "                       -2.4243e-01, -3.8635e-01, -2.6127e-01, -2.3128e-01, -3.1675e-01,\n",
       "                       -3.2966e-01, -2.5050e-01, -3.6137e-01, -2.7179e-01, -3.7908e-01,\n",
       "                       -6.3067e-01, -2.7521e-01, -4.2928e-01, -2.6467e-01, -3.9084e-01,\n",
       "                       -3.4539e-01, -3.0123e-01, -3.4827e-01, -3.1838e-01, -2.7537e-01,\n",
       "                       -3.3725e-01, -2.7659e-01, -3.7621e-02, -4.2970e-01, -3.1315e-01,\n",
       "                       -1.9243e-01, -3.1630e-01, -3.1210e-01, -2.2573e-01, -3.5831e-01,\n",
       "                       -2.9017e-01, -3.8468e-01, -3.2735e-01, -2.7492e-01, -1.4545e-01,\n",
       "                       -2.6132e-01, -3.3288e-01, -2.8789e-01, -9.5447e-03, -2.3727e-01,\n",
       "                       -3.6506e-01, -3.1933e-01, -3.4813e-01, -2.3968e-01, -2.3048e-01,\n",
       "                       -3.2235e-01, -4.5712e-01, -8.2832e-11, -3.7387e-02, -4.2282e-01,\n",
       "                       -2.6802e-01, -4.1776e-01, -3.5843e-02, -2.3747e-01, -3.4734e-01,\n",
       "                       -3.4931e-01, -3.9029e-01, -2.8163e-01, -3.4240e-01, -6.5988e-01,\n",
       "                       -4.1117e-01, -3.7355e-01, -2.6374e-01, -2.5283e-01, -2.4225e-01,\n",
       "                       -3.8587e-01, -3.4695e-01, -1.7012e-01, -3.5898e-01, -2.2599e-01,\n",
       "                       -3.6073e-01, -3.1948e-01, -2.3852e-01, -3.0432e-01, -2.8615e-01,\n",
       "                       -4.8673e-01, -2.4185e-01, -2.8005e-01, -2.2292e-01, -3.3806e-01,\n",
       "                       -3.4269e-01, -2.2614e-01, -2.9892e-01, -2.8446e-01, -2.6000e-01,\n",
       "                       -2.8056e-01, -3.4189e-01, -6.5306e-07], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv2.norm_layer.weight',\n",
       "               tensor([ 2.0678e-01, -9.5074e-05,  2.4242e-01,  2.2641e-01,  3.3979e-01,\n",
       "                        2.6554e-01,  2.6870e-01,  3.4303e-01,  2.5907e-01,  2.5934e-01,\n",
       "                       -4.6513e-03,  2.5524e-01,  2.8665e-01,  3.4962e-01,  3.4227e-01,\n",
       "                        2.6426e-01,  2.6422e-01,  3.8013e-01,  2.7958e-01,  2.6097e-01,\n",
       "                        2.3944e-01,  2.8393e-01,  3.0921e-01,  1.5783e-01,  2.8303e-01,\n",
       "                        2.8895e-01,  2.6113e-01,  3.4879e-01,  1.7416e-01,  2.5073e-01,\n",
       "                        2.9264e-01,  5.9240e-01,  2.2539e-01,  2.4922e-01,  2.7103e-01,\n",
       "                        3.2336e-01,  2.5207e-01,  2.5241e-01,  2.6298e-01,  2.5556e-01,\n",
       "                        2.8258e-01,  2.3481e-01,  2.4579e-01,  2.1036e-01,  2.8397e-01,\n",
       "                        1.4527e-01,  2.1243e-01,  3.3618e-01,  2.9245e-01,  2.8892e-01,\n",
       "                        3.2394e-01,  3.4894e-01,  3.8505e-01,  2.3979e-01,  3.7395e-01,\n",
       "                        1.9660e-01,  2.6565e-01,  3.2480e-01,  2.3506e-01,  2.9744e-01,\n",
       "                        1.9628e-01,  2.0955e-01,  4.4421e-03,  2.8274e-01,  1.9789e-01,\n",
       "                        2.1098e-01,  2.7745e-01,  1.3352e-01,  2.7085e-01,  2.2034e-01,\n",
       "                        2.9111e-01,  3.1565e-01,  2.2906e-01,  1.7416e-01,  1.3961e-01,\n",
       "                        2.5378e-01,  3.2304e-01,  2.6205e-01, -9.1787e-04,  2.0702e-01,\n",
       "                        2.3118e-01,  3.5330e-01,  3.2838e-01,  2.5715e-01,  1.6050e-01,\n",
       "                        2.2387e-01,  2.7300e-01, -5.7141e-09, -3.0046e-03,  3.7713e-01,\n",
       "                        2.4274e-01,  3.8799e-01, -5.8702e-03,  2.5189e-01,  3.2774e-01,\n",
       "                        3.2763e-01,  2.8939e-01,  1.0957e-01,  3.5117e-01,  3.8153e-01,\n",
       "                        4.3353e-01,  4.1552e-01,  2.5881e-01,  1.9501e-01,  2.3573e-01,\n",
       "                        2.8571e-01,  2.6685e-01,  2.8701e-01,  3.0291e-01,  2.8970e-01,\n",
       "                        2.1864e-01,  3.0988e-01,  2.4018e-01,  3.0244e-01,  2.7472e-01,\n",
       "                        3.7712e-01,  2.5809e-01,  2.9308e-01,  3.1725e-01,  3.5354e-01,\n",
       "                        2.9664e-01,  1.9477e-01,  1.8307e-01,  3.1260e-01,  2.4936e-01,\n",
       "                        2.5207e-01,  3.4603e-01, -1.0371e-04], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.weight',\n",
       "               tensor([[[[ 7.0982e-02,  7.9410e-02,  3.0355e-03],\n",
       "                         [ 1.7107e-02,  6.1006e-02,  7.4279e-02],\n",
       "                         [ 1.7022e-02, -6.4248e-03,  5.5946e-02]],\n",
       "               \n",
       "                        [[ 9.8332e-09,  1.8531e-08, -8.8301e-09],\n",
       "                         [-1.0304e-08,  3.0064e-10, -2.3638e-08],\n",
       "                         [-5.9089e-09, -9.8482e-09,  1.6421e-08]],\n",
       "               \n",
       "                        [[ 5.0339e-02,  2.7086e-02, -5.3247e-02],\n",
       "                         [ 4.2114e-02,  1.0880e-02, -7.0717e-02],\n",
       "                         [-2.9786e-03,  2.6186e-03,  7.5392e-04]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-1.0683e-02,  2.1060e-03,  4.0872e-02],\n",
       "                         [-9.6171e-02, -6.6975e-02, -1.0878e-02],\n",
       "                         [-6.9846e-02, -3.2795e-02,  4.5723e-03]],\n",
       "               \n",
       "                        [[-4.9733e-02, -3.4573e-02, -4.4569e-02],\n",
       "                         [-6.5452e-02, -6.2136e-02, -4.1287e-02],\n",
       "                         [-7.9759e-02, -4.9359e-02, -9.6032e-02]],\n",
       "               \n",
       "                        [[ 2.7246e-09,  4.5237e-09, -2.5491e-09],\n",
       "                         [-2.8867e-09,  6.8706e-12, -5.1977e-09],\n",
       "                         [ 4.0512e-09,  5.1350e-09,  6.8290e-09]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 6.5778e-02,  5.8882e-02, -3.1478e-02],\n",
       "                         [ 1.6190e-02, -4.6832e-02, -3.2797e-02],\n",
       "                         [ 5.3279e-02, -2.5976e-03, -3.3902e-02]],\n",
       "               \n",
       "                        [[-4.1491e-08, -1.6706e-08, -7.9568e-08],\n",
       "                         [-4.4218e-08,  6.9352e-10, -9.3721e-08],\n",
       "                         [-1.6869e-07, -1.3960e-07, -2.2876e-07]],\n",
       "               \n",
       "                        [[ 5.7947e-02, -2.1033e-02,  7.8558e-02],\n",
       "                         [-7.4864e-02, -5.4443e-02, -8.8665e-03],\n",
       "                         [ 6.9208e-03,  4.1272e-02,  9.1094e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-5.9438e-02, -1.4834e-04, -1.8204e-02],\n",
       "                         [ 8.5289e-03, -3.5314e-02,  4.3399e-02],\n",
       "                         [-9.7253e-03, -6.2594e-02,  1.6519e-02]],\n",
       "               \n",
       "                        [[-5.3520e-02,  8.3235e-02,  5.9917e-02],\n",
       "                         [ 3.3629e-02,  4.4292e-02,  3.9087e-02],\n",
       "                         [-3.4302e-03,  8.4357e-02, -2.4349e-02]],\n",
       "               \n",
       "                        [[-2.3189e-08, -4.6250e-09, -2.7872e-08],\n",
       "                         [-1.4601e-08,  1.7770e-10, -2.8017e-08],\n",
       "                         [-4.6100e-08, -3.8590e-08, -6.2809e-08]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 6.8090e-03,  8.3769e-03,  2.6661e-03],\n",
       "                         [-1.6320e-02, -3.2914e-02, -5.5196e-03],\n",
       "                         [ 8.1040e-03, -2.2148e-02, -1.6673e-02]],\n",
       "               \n",
       "                        [[-1.2260e-07,  3.7368e-08, -4.7756e-08],\n",
       "                         [-2.0057e-07,  2.4082e-09, -1.2699e-07],\n",
       "                         [ 1.5508e-07,  1.7639e-08,  1.7988e-08]],\n",
       "               \n",
       "                        [[ 8.2727e-02,  6.7308e-02, -4.5194e-02],\n",
       "                         [ 5.2855e-02,  8.8404e-03, -1.0772e-01],\n",
       "                         [ 4.8017e-02,  5.9041e-02, -3.1666e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-5.0887e-02,  1.8263e-02, -5.6861e-02],\n",
       "                         [-1.1412e-02,  6.1381e-02,  1.9727e-02],\n",
       "                         [ 1.7353e-02,  8.5976e-02,  2.1617e-02]],\n",
       "               \n",
       "                        [[ 2.4995e-02,  8.3277e-03, -5.3684e-02],\n",
       "                         [ 1.2380e-01,  5.6386e-02,  2.0702e-02],\n",
       "                         [ 7.4589e-02,  4.5642e-02,  1.6249e-02]],\n",
       "               \n",
       "                        [[ 3.6409e-08,  1.4030e-08,  2.9305e-08],\n",
       "                         [ 3.6743e-08,  7.1305e-10,  1.5890e-08],\n",
       "                         [ 8.7513e-08,  4.9962e-08,  5.8368e-08]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-3.3791e-02, -4.3370e-03, -1.8219e-02],\n",
       "                         [-4.4120e-03,  2.7360e-02, -4.1297e-02],\n",
       "                         [ 3.9245e-02,  3.7069e-02, -4.2522e-02]],\n",
       "               \n",
       "                        [[-1.3034e-07, -1.2226e-07, -1.6549e-07],\n",
       "                         [-7.8189e-08,  2.8045e-09, -3.9921e-08],\n",
       "                         [-1.5518e-07, -1.4035e-07, -1.7120e-07]],\n",
       "               \n",
       "                        [[ 1.8813e-02,  1.0196e-01,  9.2164e-02],\n",
       "                         [ 1.0857e-01,  1.9857e-02,  8.9373e-02],\n",
       "                         [-2.3130e-02, -1.0688e-02, -8.9704e-03]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[-6.7933e-02, -7.3628e-02, -9.5557e-02],\n",
       "                         [-6.8063e-02, -4.6807e-02, -4.3664e-02],\n",
       "                         [-5.2275e-02, -7.1555e-02, -5.7333e-02]],\n",
       "               \n",
       "                        [[-2.4382e-02, -6.2317e-03,  6.2511e-03],\n",
       "                         [ 9.1717e-03, -5.0477e-02,  8.7841e-03],\n",
       "                         [-4.1638e-02, -7.3323e-02, -6.7270e-02]],\n",
       "               \n",
       "                        [[-5.5694e-08, -2.9984e-08, -4.0238e-08],\n",
       "                         [-1.9129e-08,  9.3745e-10, -1.0803e-08],\n",
       "                         [-7.9667e-08, -6.3053e-08, -5.6711e-08]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 5.2338e-02,  6.0799e-02,  3.1711e-02],\n",
       "                         [ 1.2327e-02,  1.6936e-02,  5.8571e-03],\n",
       "                         [ 1.7760e-02,  2.9862e-02,  5.8698e-03]],\n",
       "               \n",
       "                        [[ 3.5637e-08,  6.9852e-08,  3.5410e-08],\n",
       "                         [-2.6170e-08,  2.1785e-09, -4.0555e-08],\n",
       "                         [-1.0676e-07, -8.9558e-08, -1.2063e-07]],\n",
       "               \n",
       "                        [[ 1.5593e-02,  3.3697e-02, -1.8942e-02],\n",
       "                         [ 1.3374e-02,  2.6558e-03,  2.1688e-02],\n",
       "                         [ 1.6505e-02, -8.2735e-03,  2.9050e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 9.2746e-03,  6.8915e-02,  5.2035e-02],\n",
       "                         [ 3.4178e-03,  7.4335e-02,  6.6590e-02],\n",
       "                         [ 1.3377e-02,  5.1529e-02,  8.9151e-02]],\n",
       "               \n",
       "                        [[-1.8626e-03,  1.7149e-03, -1.9535e-02],\n",
       "                         [-6.0669e-02, -2.7301e-02, -5.9660e-03],\n",
       "                         [-5.5495e-03, -1.2876e-04, -2.1121e-02]],\n",
       "               \n",
       "                        [[ 1.1242e-08,  1.7249e-08,  9.2259e-09],\n",
       "                         [-3.0355e-09,  6.2973e-10, -1.0758e-08],\n",
       "                         [-2.2272e-08, -2.2997e-08, -3.0779e-08]]],\n",
       "               \n",
       "               \n",
       "                       [[[-8.2447e-02, -8.5969e-02, -4.0496e-02],\n",
       "                         [ 1.4534e-04,  4.7732e-03, -1.2706e-02],\n",
       "                         [ 7.4253e-03,  4.7436e-02, -1.9959e-02]],\n",
       "               \n",
       "                        [[-2.5602e-08, -1.0256e-07, -3.6268e-07],\n",
       "                         [ 1.0315e-07,  1.8022e-09, -2.0729e-07],\n",
       "                         [ 1.2862e-07,  9.3515e-08, -1.1672e-07]],\n",
       "               \n",
       "                        [[ 2.4610e-02,  3.8200e-02,  9.2711e-03],\n",
       "                         [ 4.5607e-02,  6.4954e-03, -1.6531e-02],\n",
       "                         [-3.7691e-02, -8.3063e-02, -5.9664e-02]],\n",
       "               \n",
       "                        ...,\n",
       "               \n",
       "                        [[ 7.9180e-02,  1.4805e-01,  1.7302e-01],\n",
       "                         [ 2.1721e-02,  7.2818e-02,  9.3465e-02],\n",
       "                         [ 7.9417e-03, -1.2020e-02, -3.9898e-02]],\n",
       "               \n",
       "                        [[-9.3138e-03, -3.0494e-02, -4.3417e-02],\n",
       "                         [-1.4944e-02, -8.8690e-02, -1.0930e-02],\n",
       "                         [-1.4744e-02, -5.5430e-02, -2.9767e-02]],\n",
       "               \n",
       "                        [[ 8.2815e-08,  3.1840e-09, -6.1195e-08],\n",
       "                         [ 6.1240e-08,  1.0097e-09, -6.4833e-08],\n",
       "                         [ 9.6884e-09, -5.7379e-08, -3.8444e-08]]]], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.conv.bias',\n",
       "               tensor([-4.9818e-05, -4.9128e-05, -4.9026e-06,  6.0460e-05, -1.2960e-06,\n",
       "                       -2.6684e-05, -2.4007e-05, -8.2047e-06,  4.2040e-10,  1.1959e-05,\n",
       "                       -2.8109e-12, -1.9603e-09, -6.5203e-05,  3.6199e-05, -2.9295e-07,\n",
       "                        4.0757e-05,  5.3992e-06, -6.6394e-06, -3.2356e-05,  8.4213e-06,\n",
       "                        4.1439e-10, -5.6632e-05, -4.2017e-06,  4.5048e-09, -4.3890e-05,\n",
       "                       -1.8752e-05, -1.1911e-04, -8.0684e-05,  4.0105e-06,  2.6982e-05,\n",
       "                        5.7867e-06, -4.7395e-06,  4.6435e-05,  7.5639e-05,  4.8089e-08,\n",
       "                        3.0158e-05, -5.1838e-05,  4.7928e-05, -4.9714e-05, -1.5330e-07,\n",
       "                       -1.7552e-05,  4.0345e-09,  4.6622e-05, -3.5057e-05, -4.4895e-05,\n",
       "                       -3.8652e-12, -1.7395e-05, -1.3697e-05, -3.8390e-08,  1.0753e-11,\n",
       "                        5.2107e-05,  3.0900e-15,  2.4130e-05,  4.3603e-05, -1.3968e-05,\n",
       "                       -1.0691e-05,  1.7734e-05, -3.9420e-06, -4.3011e-05, -5.0145e-05,\n",
       "                       -5.9573e-07,  6.1723e-05,  8.4465e-05, -2.3809e-05, -1.2263e-05,\n",
       "                        4.9214e-05,  9.1012e-11,  2.8544e-05,  1.0471e-04, -4.6739e-06,\n",
       "                       -5.4063e-05, -1.4323e-05, -5.6934e-05, -3.9008e-06,  2.7981e-05,\n",
       "                        2.1995e-05, -1.0173e-15, -1.1939e-05,  7.1817e-13,  2.2785e-05,\n",
       "                        3.8789e-06, -1.9016e-05,  1.8728e-06,  4.1041e-05, -1.0123e-08,\n",
       "                       -7.0457e-05, -6.7310e-05, -3.2320e-05,  4.9395e-05, -7.2372e-05,\n",
       "                        2.2037e-05, -2.7530e-05, -6.8682e-05, -2.7555e-05, -7.6898e-05,\n",
       "                       -1.1269e-04, -2.7107e-06,  4.7947e-08,  2.1195e-05, -1.3600e-04,\n",
       "                       -9.9001e-07, -1.1858e-05, -2.6857e-05, -8.7576e-05,  8.7753e-14,\n",
       "                        5.7057e-05, -4.0086e-05,  6.4778e-05, -6.2133e-05, -4.0435e-05,\n",
       "                       -2.9529e-05,  5.2916e-07, -2.2138e-05,  4.9253e-05, -1.1146e-04,\n",
       "                       -1.4121e-05, -4.2397e-06, -4.1352e-05, -1.3730e-05,  3.7409e-06,\n",
       "                        2.3817e-05,  1.8746e-08,  1.2473e-12, -6.3390e-06, -3.4651e-05,\n",
       "                        6.8277e-05,  3.2506e-05,  1.6163e-04], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.running_var',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.bias',\n",
       "               tensor([-3.1727e-01, -2.1232e-01, -3.7658e-01, -6.2612e-01, -8.1702e-06,\n",
       "                       -4.9993e-01, -2.9261e-01, -1.4359e-01, -4.8696e-13, -1.2529e-04,\n",
       "                       -1.2306e-06, -1.9392e-03, -2.4268e-01, -3.7255e-01, -3.3178e-03,\n",
       "                       -2.7396e-01, -2.2390e-01, -2.3781e-01, -2.1436e-01, -2.9339e-01,\n",
       "                       -1.6199e-09, -1.1930e-01, -1.0736e-03, -1.2399e-04, -2.2485e-01,\n",
       "                       -3.9703e-01, -2.8769e-01, -2.7503e-01, -2.7306e-01, -2.5825e-01,\n",
       "                       -4.6269e-01, -5.9403e-01, -9.9626e-02, -4.3996e-01, -2.6369e-01,\n",
       "                       -4.0215e-01, -5.7431e-01, -2.6101e-01, -4.6382e-01, -3.6287e-14,\n",
       "                       -5.6752e-04, -5.9543e-03, -2.6546e-01, -1.7982e-01, -2.6815e-01,\n",
       "                       -3.3883e-08, -2.7374e-01, -4.2947e-01, -8.3639e-07, -6.9047e-05,\n",
       "                       -6.1529e-01, -2.4836e-09, -3.3099e-01, -4.2239e-01, -2.2350e-01,\n",
       "                       -1.6356e-01, -2.7682e-01, -2.5305e-01, -2.1428e-01, -3.6047e-01,\n",
       "                       -6.4510e-04, -5.7068e-02, -5.3824e-01,  1.0521e-01, -3.4511e-01,\n",
       "                       -2.1750e-01, -5.0276e-32, -1.9979e-01, -2.0614e-01, -2.0909e-01,\n",
       "                       -3.2428e-01, -4.5414e-01, -5.3133e-01, -1.3205e-28, -3.0260e-01,\n",
       "                       -4.0488e-01, -1.7121e-04, -1.4692e-01, -1.4711e-17, -1.3676e-01,\n",
       "                       -3.2487e-01, -2.0687e-01, -3.1969e-06, -2.5972e-01, -1.7248e-19,\n",
       "                       -4.9652e-02, -2.5657e-01, -2.9610e-01, -3.3691e-01, -3.6858e-01,\n",
       "                       -3.4705e-01, -2.2554e-01, -2.5759e-01, -3.2570e-01, -4.2677e-01,\n",
       "                       -3.5633e-01, -5.1244e-01, -2.7078e-11, -3.3285e-01, -3.3559e-01,\n",
       "                       -6.1424e-03, -1.8606e-01, -2.7248e-01, -3.6253e-01, -2.3002e-10,\n",
       "                       -2.0330e-01, -3.1376e-01, -3.8102e-01, -4.0893e-01, -3.1467e-01,\n",
       "                       -2.5545e-01, -5.6661e-01, -3.0435e-01, -4.0062e-01, -4.6427e-01,\n",
       "                       -6.8853e-02, -4.1148e-01, -3.3666e-01, -6.7905e-08, -2.3305e-01,\n",
       "                       -1.6369e-01, -9.9178e-23, -2.0268e-22, -3.3787e-14, -2.4897e-01,\n",
       "                       -3.3798e-01, -2.8520e-01, -4.5763e-01], device='cuda:0')),\n",
       "              ('classifier.layer_dict.conv3.norm_layer.weight',\n",
       "               tensor([ 2.7527e-01,  2.4636e-01,  2.6848e-01,  4.5465e-01,  1.0100e-08,\n",
       "                        4.1025e-01,  2.3395e-01,  2.1365e-01,  3.7873e-20, -1.3704e-03,\n",
       "                        9.1009e-04, -2.8100e-03,  1.7035e-01,  2.8924e-01, -2.9825e-03,\n",
       "                        2.5310e-01,  2.5050e-01,  1.8335e-01,  2.1503e-01,  2.7216e-01,\n",
       "                        2.5592e-08,  2.0786e-01,  1.1159e-07,  6.0587e-04,  1.6532e-01,\n",
       "                        3.6026e-01,  2.8183e-01,  2.8791e-01,  2.8258e-01,  2.2325e-01,\n",
       "                        3.9285e-01,  4.4712e-01,  1.6823e-01,  4.4531e-01,  2.3329e-01,\n",
       "                        3.6514e-01,  3.7981e-01,  2.5525e-01,  4.0061e-01,  3.0705e-21,\n",
       "                        1.7587e-03, -2.6958e-03,  2.9358e-01,  1.7412e-01,  2.8113e-01,\n",
       "                       -2.1205e-06,  3.1851e-01,  3.9157e-01, -1.6880e-04,  2.1628e-04,\n",
       "                        5.2849e-01, -3.3207e-07,  3.2059e-01,  3.9118e-01,  2.4038e-01,\n",
       "                        2.3104e-01,  2.5519e-01,  1.7289e-01,  1.9927e-01,  3.3068e-01,\n",
       "                        3.2032e-04,  1.4664e-01,  4.5794e-01,  2.1653e-01,  3.1212e-01,\n",
       "                        2.4167e-01,  1.0193e-37,  2.4424e-01,  2.1541e-01,  2.1907e-01,\n",
       "                        2.7524e-01,  3.5053e-01,  4.4657e-01,  9.8704e-31,  2.7408e-01,\n",
       "                        3.7310e-01, -1.2008e-10,  1.3076e-01, -1.2253e-12,  1.8733e-01,\n",
       "                        3.3040e-01,  1.8623e-01,  9.5487e-07,  2.0028e-01, -3.3073e-26,\n",
       "                        1.8495e-01,  2.6064e-01,  2.6635e-01,  3.0643e-01,  3.2801e-01,\n",
       "                        3.1806e-01,  2.1947e-01,  2.6013e-01,  2.4732e-01,  3.6779e-01,\n",
       "                        3.4801e-01,  3.9539e-01, -1.8914e-06,  3.0592e-01,  3.1476e-01,\n",
       "                       -2.8300e-03,  2.0191e-01,  2.2849e-01,  2.8560e-01, -6.5300e-16,\n",
       "                        2.5745e-01,  2.4548e-01,  2.9317e-01,  3.4889e-01,  2.5692e-01,\n",
       "                        2.7356e-01,  5.0862e-01,  2.1172e-01,  3.4361e-01,  3.8585e-01,\n",
       "                        1.8967e-01,  3.8773e-01,  3.6571e-01, -1.2569e-06,  2.3266e-01,\n",
       "                        2.1316e-01,  4.6101e-17,  5.5050e-17, -1.0791e-20,  2.1346e-01,\n",
       "                        2.9702e-01,  2.2017e-01,  4.3030e-01], device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.weights',\n",
       "               tensor([[-0.0041,  0.0071,  0.0054,  ...,  0.0027, -0.0167, -0.0264],\n",
       "                       [-0.0084, -0.0043, -0.0170,  ...,  0.0094,  0.0111,  0.0355],\n",
       "                       [ 0.0060, -0.0006,  0.0064,  ...,  0.0039,  0.0240,  0.0125],\n",
       "                       [-0.0038,  0.0021,  0.0006,  ...,  0.0204, -0.0059, -0.0324],\n",
       "                       [ 0.0109, -0.0025,  0.0067,  ..., -0.0347, -0.0119,  0.0107]],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.layer_dict.linear.bias',\n",
       "               tensor([ 0.0234,  0.0147,  0.0096,  0.0109, -0.0048], device='cuda:0')),\n",
       "              ('classifier.prompt.prompt_dict.pad_up',\n",
       "               tensor([[[ 0.0211, -0.0003,  0.0142,  ...,  0.0298,  0.0178, -0.0118],\n",
       "                        [ 0.0163, -0.0321,  0.0165,  ...,  0.0308,  0.0045, -0.0201],\n",
       "                        [-0.0089, -0.0224, -0.0356,  ..., -0.0141, -0.0025, -0.0224],\n",
       "                        [ 0.0038, -0.0367,  0.0503,  ...,  0.0184,  0.0320,  0.0073],\n",
       "                        [ 0.0135, -0.0361,  0.0317,  ...,  0.0371,  0.0104, -0.0169]],\n",
       "               \n",
       "                       [[ 0.0166,  0.0089,  0.0254,  ..., -0.0175, -0.0008,  0.0017],\n",
       "                        [ 0.0201, -0.0236,  0.0310,  ..., -0.0140, -0.0252, -0.0071],\n",
       "                        [-0.0153,  0.0114, -0.0301,  ...,  0.0003,  0.0054, -0.0027],\n",
       "                        [ 0.0092,  0.0151,  0.0174,  ..., -0.0264, -0.0394,  0.0003],\n",
       "                        [ 0.0417,  0.0184, -0.0012,  ..., -0.0327, -0.0337, -0.0114]],\n",
       "               \n",
       "                       [[ 0.0013, -0.0131, -0.0136,  ...,  0.0044,  0.0124,  0.0199],\n",
       "                        [ 0.0162, -0.0046,  0.0227,  ...,  0.0100,  0.0043,  0.0168],\n",
       "                        [-0.0069,  0.0192, -0.0164,  ...,  0.0120,  0.0186, -0.0220],\n",
       "                        [ 0.0116,  0.0023,  0.0191,  ...,  0.0029,  0.0151, -0.0084],\n",
       "                        [ 0.0139,  0.0179,  0.0226,  ..., -0.0025,  0.0208, -0.0295]]],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.prompt.prompt_dict.pad_down',\n",
       "               tensor([[[-0.0021, -0.0166, -0.0048,  ..., -0.0183,  0.0040, -0.0075],\n",
       "                        [-0.0062, -0.0431,  0.0394,  ...,  0.0165, -0.0243, -0.0322],\n",
       "                        [-0.0035,  0.0125,  0.0212,  ...,  0.0195, -0.0094, -0.0054],\n",
       "                        [-0.0110, -0.0078, -0.0032,  ...,  0.0228,  0.0218,  0.0151],\n",
       "                        [ 0.0023, -0.0058,  0.0238,  ..., -0.0066, -0.0245, -0.0361]],\n",
       "               \n",
       "                       [[-0.0109,  0.0114, -0.0407,  ..., -0.0319,  0.0378,  0.0172],\n",
       "                        [ 0.0055, -0.0345,  0.0244,  ...,  0.0098, -0.0128,  0.0109],\n",
       "                        [ 0.0028,  0.0244,  0.0051,  ...,  0.0171, -0.0291,  0.0015],\n",
       "                        [ 0.0048,  0.0040, -0.0212,  ...,  0.0098,  0.0048, -0.0024],\n",
       "                        [ 0.0133,  0.0026, -0.0244,  ...,  0.0443,  0.0241, -0.0326]],\n",
       "               \n",
       "                       [[ 0.0263,  0.0506, -0.0234,  ..., -0.0246,  0.0141, -0.0027],\n",
       "                        [ 0.0105, -0.0207,  0.0207,  ...,  0.0034,  0.0051, -0.0077],\n",
       "                        [ 0.0102,  0.0172, -0.0223,  ...,  0.0109,  0.0097, -0.0096],\n",
       "                        [ 0.0043,  0.0120, -0.0279,  ...,  0.0250,  0.0142, -0.0224],\n",
       "                        [-0.0059,  0.0121,  0.0088,  ...,  0.0052,  0.0109, -0.0288]]],\n",
       "                      device='cuda:0')),\n",
       "              ('classifier.prompt.prompt_dict.pad_left',\n",
       "               tensor([[[-0.0361,  0.0181,  0.0015,  0.0414, -0.0141],\n",
       "                        [ 0.0207, -0.0074,  0.0291, -0.0509, -0.0065],\n",
       "                        [-0.0296,  0.0392,  0.0126, -0.0215, -0.0064],\n",
       "                        ...,\n",
       "                        [ 0.0038, -0.0160,  0.0226, -0.0025, -0.0211],\n",
       "                        [-0.0221, -0.0242, -0.0216, -0.0005, -0.0028],\n",
       "                        [-0.0528,  0.0071,  0.0391,  0.0320, -0.0060]],\n",
       "               \n",
       "                       [[-0.0190,  0.0345, -0.0503,  0.0181, -0.0090],\n",
       "                        [ 0.0334, -0.0115, -0.0136, -0.0334,  0.0404],\n",
       "                        [-0.0287, -0.0378, -0.0499,  0.0153,  0.0276],\n",
       "                        ...,\n",
       "                        [ 0.0309,  0.0224,  0.0627,  0.0258, -0.0048],\n",
       "                        [ 0.0132, -0.0021, -0.0401, -0.0358, -0.0237],\n",
       "                        [-0.0207,  0.0401, -0.0444, -0.0585, -0.0076]],\n",
       "               \n",
       "                       [[-0.0392,  0.0214, -0.0197,  0.0056,  0.0014],\n",
       "                        [ 0.0247,  0.0004,  0.0198, -0.0185,  0.0238],\n",
       "                        [ 0.0099,  0.0355,  0.0057, -0.0061, -0.0209],\n",
       "                        ...,\n",
       "                        [ 0.0073, -0.0171,  0.0015,  0.0137, -0.0128],\n",
       "                        [-0.0094,  0.0088, -0.0004,  0.0176,  0.0181],\n",
       "                        [-0.0053,  0.0537,  0.0093,  0.0063,  0.0025]]], device='cuda:0')),\n",
       "              ('classifier.prompt.prompt_dict.pad_right',\n",
       "               tensor([[[ 0.0407,  0.0253, -0.0056, -0.0073, -0.0219],\n",
       "                        [-0.0675, -0.0263, -0.0154,  0.0511, -0.0253],\n",
       "                        [-0.0079,  0.0215,  0.0188,  0.0133, -0.0017],\n",
       "                        ...,\n",
       "                        [ 0.0204, -0.0219, -0.0286,  0.0491, -0.0422],\n",
       "                        [-0.0405,  0.0354, -0.0159,  0.0053, -0.0128],\n",
       "                        [ 0.0209,  0.0060, -0.0118,  0.0188,  0.0153]],\n",
       "               \n",
       "                       [[ 0.0175, -0.0550, -0.0023,  0.0306,  0.0410],\n",
       "                        [-0.0162, -0.0179, -0.0118,  0.0253, -0.0090],\n",
       "                        [ 0.0116,  0.0313,  0.0170, -0.0135, -0.0171],\n",
       "                        ...,\n",
       "                        [ 0.0131, -0.0344,  0.0073,  0.0704, -0.0151],\n",
       "                        [-0.0130,  0.0210, -0.0191,  0.0039,  0.0313],\n",
       "                        [ 0.0195, -0.0412, -0.0130,  0.0064, -0.0097]],\n",
       "               \n",
       "                       [[ 0.0241,  0.0059, -0.0169, -0.0118, -0.0412],\n",
       "                        [-0.0177,  0.0270, -0.0232,  0.0192, -0.0059],\n",
       "                        [ 0.0152,  0.0016, -0.0157,  0.0317,  0.0526],\n",
       "                        ...,\n",
       "                        [-0.0072,  0.0053,  0.0063,  0.0082, -0.0206],\n",
       "                        [-0.0608,  0.0606, -0.0013, -0.0226,  0.0287],\n",
       "                        [ 0.0149, -0.0083, -0.0204, -0.0085, -0.0053]]], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.prompt_learning_rates_dict.prompt_weight_learning_rate',\n",
       "               tensor([ 6.7735e-03, -3.5076e-04,  1.3716e-03,  8.3016e-03, -7.6450e-04,\n",
       "                        9.6921e-12], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv0-conv-weight',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv0-conv-bias',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv1-conv-weight',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv1-conv-bias',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv2-conv-weight',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv2-conv-bias',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv3-conv-weight',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-conv3-conv-bias',\n",
       "               tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-weights',\n",
       "               tensor([3.9216e-01, 5.4648e-01, 9.2367e-01, 1.1267e+00, 1.0611e+00, 9.6921e-12],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_learning_rates_dict.layer_dict-linear-bias',\n",
       "               tensor([ 1.7182e-01,  1.7953e-01,  3.6042e-02, -1.0476e-01,  9.4527e-02,\n",
       "                        9.6921e-12], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv0-conv-weight',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv0-conv-bias',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv1-conv-weight',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv1-conv-bias',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv2-conv-weight',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv2-conv-bias',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv3-conv-weight',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-conv3-conv-bias',\n",
       "               tensor([5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06, 5.0000e-06],\n",
       "                      device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-linear-weights',\n",
       "               tensor([ 9.9289e-01, -2.0374e-01, -8.4855e-01, -1.0434e+00, -1.7972e+00,\n",
       "                        5.5578e-10], device='cuda:0')),\n",
       "              ('inner_loop_optimizer.names_weight_decay_dict.layer_dict-linear-bias',\n",
       "               tensor([ 2.2165e-01,  1.8181e-01,  1.1143e-01,  9.3132e-03, -2.5020e-02,\n",
       "                        5.5578e-10], device='cuda:0'))]),\n",
       " 'per_epoch_statistics': {'train_loss_mean': [1.6207496290206909,\n",
       "   1.342897901058197,\n",
       "   1.2236537531614304,\n",
       "   1.1348049457073213,\n",
       "   1.0828544614315032,\n",
       "   1.0395694247484206,\n",
       "   1.0173706741333008,\n",
       "   0.9757553768157959,\n",
       "   0.9500979747772217,\n",
       "   0.942631136059761,\n",
       "   0.8980446484088898,\n",
       "   0.8778141911029815,\n",
       "   0.8616352691650391,\n",
       "   0.8257936012744903,\n",
       "   0.8036870549321175,\n",
       "   0.7834847896695137,\n",
       "   0.7570608896613121,\n",
       "   0.7447124595046043,\n",
       "   0.7367639369368553,\n",
       "   0.7155400152802467,\n",
       "   0.6868605269789696,\n",
       "   0.6682883580327034,\n",
       "   0.6575315053462982,\n",
       "   0.6474029502272606,\n",
       "   0.6359921804070473,\n",
       "   0.6178860854506493,\n",
       "   0.6051021536588669,\n",
       "   0.6006010538339615,\n",
       "   0.5974101521372795,\n",
       "   0.6038241744041443,\n",
       "   0.5774149975180626,\n",
       "   0.5732839403152465,\n",
       "   0.5677141354382038,\n",
       "   0.5605494692325592,\n",
       "   0.5702561058998108,\n",
       "   0.5429059826731681,\n",
       "   0.5425041063427926,\n",
       "   0.5426579396724701,\n",
       "   0.545066031217575,\n",
       "   0.5318128537237644,\n",
       "   0.542967883437872,\n",
       "   0.5228942370414734,\n",
       "   0.5206523779928685,\n",
       "   0.5279693941175938,\n",
       "   0.5169165155887604,\n",
       "   0.5126333491206169,\n",
       "   0.5150745753347874,\n",
       "   0.50938992780447,\n",
       "   0.5055655215382576,\n",
       "   0.5101301383376121,\n",
       "   0.5067368417978286,\n",
       "   0.49460212591290476,\n",
       "   0.5119811763763428,\n",
       "   0.4939535201489925,\n",
       "   0.5019407892227172,\n",
       "   0.49968585413694383,\n",
       "   0.4969970774650574,\n",
       "   0.491454145014286,\n",
       "   0.48856047829985616,\n",
       "   0.4957058700621128,\n",
       "   0.4985882677733898,\n",
       "   0.4857126070857048,\n",
       "   0.47704756781458857,\n",
       "   0.47438314494490624,\n",
       "   0.48193382316827776,\n",
       "   0.4861598019897938,\n",
       "   0.4924478581547737,\n",
       "   0.4772479425370693,\n",
       "   0.479242424249649,\n",
       "   0.4835649347305298,\n",
       "   0.47209443771839144,\n",
       "   0.47544128581881523,\n",
       "   0.47873051118850707,\n",
       "   0.4695740088224411,\n",
       "   0.4737606158554554,\n",
       "   0.46875581809878347,\n",
       "   0.47342330414056777,\n",
       "   0.4708072311580181,\n",
       "   0.46047626322507856,\n",
       "   0.47161584442853927,\n",
       "   0.45594624188542365,\n",
       "   0.46462134128808974,\n",
       "   0.47184071227908136,\n",
       "   0.47357726147770884,\n",
       "   0.45588243955373764,\n",
       "   0.4646154716908932,\n",
       "   0.4604516882300377,\n",
       "   0.46534774380922317,\n",
       "   0.4551590910255909,\n",
       "   0.4500463908016682,\n",
       "   0.45917791962623594,\n",
       "   0.459131173402071,\n",
       "   0.44881688073277476,\n",
       "   0.4561172934472561,\n",
       "   0.45814833241701125,\n",
       "   0.4632396776378155,\n",
       "   0.4641384927034378,\n",
       "   0.4592018865644932,\n",
       "   0.4424883471429348,\n",
       "   0.4613002304434776,\n",
       "   0.44902970623970034,\n",
       "   0.44778686910867693,\n",
       "   0.451509437084198,\n",
       "   0.45878856068849566,\n",
       "   0.4579270626604557,\n",
       "   0.45507100349664686,\n",
       "   0.4498344167768955,\n",
       "   0.4561372907757759,\n",
       "   0.4372186378240585,\n",
       "   0.44489673498272897,\n",
       "   0.4528974254131317,\n",
       "   0.45654128047823905,\n",
       "   0.4438097078204155,\n",
       "   0.4511192974150181,\n",
       "   0.45328653666377067,\n",
       "   0.4539843057394028,\n",
       "   0.4459153451323509,\n",
       "   0.45172879257798193,\n",
       "   0.44730655324459073,\n",
       "   0.44623310589790344,\n",
       "   0.4495633260756731,\n",
       "   0.4389980199337006,\n",
       "   0.43544453498721125,\n",
       "   0.4446828138232231,\n",
       "   0.4471737766265869,\n",
       "   0.44192572477459907,\n",
       "   0.43888657763600347,\n",
       "   0.44132227611541747,\n",
       "   0.449049838244915,\n",
       "   0.4389493317604065,\n",
       "   0.4434012931883335,\n",
       "   0.43817851746082304,\n",
       "   0.4529563322067261,\n",
       "   0.42974509581923487,\n",
       "   0.4370175579190254,\n",
       "   0.4377596211135387,\n",
       "   0.43504974791407586,\n",
       "   0.4370754491090775,\n",
       "   0.4292725393176079,\n",
       "   0.43267275288701057,\n",
       "   0.4363987884670496,\n",
       "   0.4385294496119022,\n",
       "   0.4368993066847324,\n",
       "   0.4347342394590378,\n",
       "   0.43551832109689714,\n",
       "   0.437297875225544,\n",
       "   0.44219688612222674,\n",
       "   0.4326752769649029],\n",
       "  'train_loss_std': [0.5701254183200988,\n",
       "   0.12204418895870366,\n",
       "   0.13654178128566352,\n",
       "   0.1394452266483158,\n",
       "   0.14787013789240025,\n",
       "   0.14352702334361342,\n",
       "   0.1484441045627974,\n",
       "   0.1411010248003142,\n",
       "   0.15023887267729344,\n",
       "   0.14550382871616763,\n",
       "   0.14543687451459517,\n",
       "   0.1533186619104157,\n",
       "   0.14503134537057402,\n",
       "   0.15272660449559586,\n",
       "   0.14370392298366733,\n",
       "   0.14452262102728078,\n",
       "   0.13459980500043883,\n",
       "   0.14842590944590012,\n",
       "   0.14847778699538403,\n",
       "   0.1422503432780089,\n",
       "   0.1457361564797551,\n",
       "   0.1356326876368041,\n",
       "   0.14790061711979013,\n",
       "   0.14672857239756942,\n",
       "   0.13790628125241755,\n",
       "   0.13893919572314395,\n",
       "   0.1356942288849327,\n",
       "   0.1433650498074844,\n",
       "   0.14029708338306462,\n",
       "   0.14516633660659176,\n",
       "   0.14148226968704816,\n",
       "   0.14199564265487985,\n",
       "   0.13954572530040107,\n",
       "   0.1443576075116581,\n",
       "   0.1325772000601414,\n",
       "   0.13710205022987237,\n",
       "   0.13343932875813497,\n",
       "   0.14218156967366177,\n",
       "   0.14509140935841988,\n",
       "   0.13884201570289215,\n",
       "   0.1351642432662727,\n",
       "   0.1324296468172866,\n",
       "   0.13487797075971694,\n",
       "   0.13961403656866092,\n",
       "   0.13500739149627528,\n",
       "   0.13026662193399802,\n",
       "   0.1346854347931988,\n",
       "   0.13430363328828093,\n",
       "   0.13297564137535733,\n",
       "   0.13281899434801708,\n",
       "   0.1300347310398624,\n",
       "   0.1285025379663272,\n",
       "   0.13188600132362968,\n",
       "   0.13872244343137616,\n",
       "   0.1402513253000202,\n",
       "   0.13764885295876433,\n",
       "   0.1282814757161406,\n",
       "   0.1368383172644055,\n",
       "   0.1349302926110465,\n",
       "   0.12513697363443496,\n",
       "   0.1341942470559963,\n",
       "   0.13328847898929827,\n",
       "   0.13291781760841875,\n",
       "   0.13250915293971743,\n",
       "   0.13216485612737144,\n",
       "   0.12718338366989676,\n",
       "   0.13344520652459296,\n",
       "   0.12563239944275117,\n",
       "   0.13156584074475075,\n",
       "   0.1323348927462172,\n",
       "   0.12805550861187437,\n",
       "   0.12995734120029864,\n",
       "   0.13299024982574817,\n",
       "   0.13163904226180906,\n",
       "   0.13295422377890548,\n",
       "   0.12675829087654908,\n",
       "   0.13513858886441532,\n",
       "   0.12590118695546917,\n",
       "   0.12836102730202964,\n",
       "   0.1353924073321222,\n",
       "   0.12863425040269832,\n",
       "   0.12908041914720247,\n",
       "   0.13131768644598865,\n",
       "   0.13704373762645836,\n",
       "   0.13030337971576283,\n",
       "   0.12393749938320081,\n",
       "   0.1334796990076111,\n",
       "   0.13049857121780112,\n",
       "   0.12854013784589202,\n",
       "   0.1201523726904664,\n",
       "   0.12363445052060348,\n",
       "   0.1276398798246761,\n",
       "   0.12194959397191421,\n",
       "   0.1317483326776292,\n",
       "   0.1274694094966332,\n",
       "   0.13576485502559868,\n",
       "   0.12230545359418532,\n",
       "   0.12869998890598253,\n",
       "   0.12331387418604615,\n",
       "   0.13564906005986924,\n",
       "   0.12207328912676288,\n",
       "   0.1253247660627108,\n",
       "   0.12592473123232742,\n",
       "   0.1283814534563065,\n",
       "   0.12494338166925605,\n",
       "   0.12209753479104135,\n",
       "   0.12937258143279895,\n",
       "   0.12676454523203956,\n",
       "   0.12337921290202537,\n",
       "   0.13055563762571498,\n",
       "   0.12912062605628952,\n",
       "   0.12363725571621653,\n",
       "   0.1268686687411449,\n",
       "   0.12848764933989457,\n",
       "   0.13816004826701317,\n",
       "   0.12713648000820718,\n",
       "   0.1251445863130046,\n",
       "   0.12244531196223317,\n",
       "   0.13033385312955148,\n",
       "   0.13438971736079341,\n",
       "   0.1251080528407045,\n",
       "   0.11887901056962903,\n",
       "   0.12880950877700797,\n",
       "   0.1256116553071109,\n",
       "   0.1234636277813903,\n",
       "   0.12561706390131513,\n",
       "   0.12255022233890762,\n",
       "   0.12864124751194267,\n",
       "   0.12823032946697369,\n",
       "   0.11920847120512101,\n",
       "   0.12594644869594754,\n",
       "   0.12363017848855123,\n",
       "   0.1337680445848671,\n",
       "   0.12287802379283538,\n",
       "   0.13042965297618891,\n",
       "   0.13061527374957646,\n",
       "   0.12412405369365882,\n",
       "   0.12754218351426516,\n",
       "   0.13043165788763725,\n",
       "   0.11964840289787076,\n",
       "   0.1324266893213183,\n",
       "   0.12278431974551458,\n",
       "   0.12525726822910374,\n",
       "   0.1282588288573014,\n",
       "   0.12811982907459965,\n",
       "   0.12457849275292185,\n",
       "   0.13230363488567773,\n",
       "   0.12522783357079997],\n",
       "  'train_accuracy_mean': [0.3587600005865097,\n",
       "   0.4357333332300186,\n",
       "   0.5032133337855339,\n",
       "   0.5472533318400383,\n",
       "   0.571399999320507,\n",
       "   0.593893332362175,\n",
       "   0.6042266663908958,\n",
       "   0.6191999998688698,\n",
       "   0.6327066656351089,\n",
       "   0.6334799995422363,\n",
       "   0.6535066657066345,\n",
       "   0.6624933336377143,\n",
       "   0.6679733315706253,\n",
       "   0.6855599996447563,\n",
       "   0.6945066661834717,\n",
       "   0.7024666675329209,\n",
       "   0.7150266677141189,\n",
       "   0.7215733343362808,\n",
       "   0.7229733327627182,\n",
       "   0.7317333317995072,\n",
       "   0.7422933328151703,\n",
       "   0.7495466660261154,\n",
       "   0.7542399997711182,\n",
       "   0.758426666021347,\n",
       "   0.7614533331394195,\n",
       "   0.7683200001716614,\n",
       "   0.7751066666841507,\n",
       "   0.7755333337783813,\n",
       "   0.7765733320713043,\n",
       "   0.7736399972438812,\n",
       "   0.7858133326768875,\n",
       "   0.7869466661214829,\n",
       "   0.7884266664981842,\n",
       "   0.791453332901001,\n",
       "   0.7866666649580002,\n",
       "   0.7965066654682159,\n",
       "   0.7974533327817916,\n",
       "   0.7982000002861023,\n",
       "   0.7988000000715256,\n",
       "   0.8027999988794327,\n",
       "   0.7995599987506866,\n",
       "   0.8055199995040894,\n",
       "   0.8085599981546402,\n",
       "   0.8037200003862381,\n",
       "   0.808186667561531,\n",
       "   0.8096400002241134,\n",
       "   0.8087866654396058,\n",
       "   0.8107866662740707,\n",
       "   0.8126666661500931,\n",
       "   0.8109199995994568,\n",
       "   0.811146666765213,\n",
       "   0.8176266663074493,\n",
       "   0.8105199999809265,\n",
       "   0.818413333773613,\n",
       "   0.8138933327794075,\n",
       "   0.814626669049263,\n",
       "   0.817880000114441,\n",
       "   0.8183066660165786,\n",
       "   0.819506667137146,\n",
       "   0.8162133350372315,\n",
       "   0.8150933333635331,\n",
       "   0.8209333342313766,\n",
       "   0.8227333347797394,\n",
       "   0.82548000061512,\n",
       "   0.8220533343553543,\n",
       "   0.8196800009012223,\n",
       "   0.8174000000953674,\n",
       "   0.8237999992370606,\n",
       "   0.8242533326148986,\n",
       "   0.8194266667366028,\n",
       "   0.8264266659021378,\n",
       "   0.8245466675758362,\n",
       "   0.8257600005865097,\n",
       "   0.8263866684436798,\n",
       "   0.8249999984502793,\n",
       "   0.8262133328914643,\n",
       "   0.824599999666214,\n",
       "   0.8249466671943665,\n",
       "   0.8299866679906845,\n",
       "   0.8254933342933655,\n",
       "   0.8314533327817917,\n",
       "   0.8282533339262008,\n",
       "   0.8262266675233841,\n",
       "   0.8241866672039032,\n",
       "   0.832653333067894,\n",
       "   0.8280266671180725,\n",
       "   0.8292800015211106,\n",
       "   0.8286666665077209,\n",
       "   0.8327599998712539,\n",
       "   0.8347866673469544,\n",
       "   0.8303600004911422,\n",
       "   0.8302266664505005,\n",
       "   0.8348666681051254,\n",
       "   0.8310533353090286,\n",
       "   0.8307466675043106,\n",
       "   0.8285333335399627,\n",
       "   0.8281466666460037,\n",
       "   0.8315466672182084,\n",
       "   0.8377999999523162,\n",
       "   0.8303333330154419,\n",
       "   0.8332400012016297,\n",
       "   0.8358000000715256,\n",
       "   0.8339866666793824,\n",
       "   0.8308933335542679,\n",
       "   0.8305600000619888,\n",
       "   0.831053334236145,\n",
       "   0.8346533335447311,\n",
       "   0.831373334288597,\n",
       "   0.8402933336496353,\n",
       "   0.83721333360672,\n",
       "   0.8329200013875961,\n",
       "   0.8313733339309692,\n",
       "   0.8363866653442383,\n",
       "   0.8330266667604447,\n",
       "   0.8343466680049896,\n",
       "   0.8319733335971832,\n",
       "   0.8361333329677582,\n",
       "   0.8343600002527237,\n",
       "   0.8350533336400986,\n",
       "   0.8376400002241134,\n",
       "   0.8349733330011367,\n",
       "   0.838133334159851,\n",
       "   0.8399200019836426,\n",
       "   0.8350400011539459,\n",
       "   0.8360400000810623,\n",
       "   0.8379333338737488,\n",
       "   0.8384533344507218,\n",
       "   0.8381333335638046,\n",
       "   0.8354533343315125,\n",
       "   0.8386400017738342,\n",
       "   0.8366400010585785,\n",
       "   0.8374133343696595,\n",
       "   0.8326533340215683,\n",
       "   0.8401866685152054,\n",
       "   0.8395599998235702,\n",
       "   0.8385733330249786,\n",
       "   0.8391200014352799,\n",
       "   0.8394266667366028,\n",
       "   0.842733335018158,\n",
       "   0.8404400013685226,\n",
       "   0.8389200006723404,\n",
       "   0.8380933347940445,\n",
       "   0.8400800006389618,\n",
       "   0.8404666664600372,\n",
       "   0.8406133335828782,\n",
       "   0.840453333735466,\n",
       "   0.8380666667222977,\n",
       "   0.8412666673660278],\n",
       "  'train_accuracy_std': [0.07886962537844523,\n",
       "   0.07026881631922058,\n",
       "   0.07330232009775471,\n",
       "   0.07297374009062686,\n",
       "   0.07374125775426174,\n",
       "   0.07013667415318214,\n",
       "   0.07360194555771413,\n",
       "   0.0674492726844266,\n",
       "   0.07194231125979224,\n",
       "   0.07217925747774732,\n",
       "   0.06875829708255225,\n",
       "   0.07468634485948317,\n",
       "   0.06871311909258222,\n",
       "   0.07036364279680934,\n",
       "   0.06571352669097906,\n",
       "   0.06772135695184836,\n",
       "   0.0625180800333156,\n",
       "   0.06668942994247003,\n",
       "   0.06823328765854518,\n",
       "   0.06638018416321761,\n",
       "   0.06521951413795153,\n",
       "   0.061826054684348916,\n",
       "   0.06356851262778161,\n",
       "   0.06519860386072111,\n",
       "   0.06123995502771815,\n",
       "   0.0619298860180819,\n",
       "   0.0595312213898618,\n",
       "   0.06356837048170232,\n",
       "   0.06057861904919525,\n",
       "   0.0626496020383926,\n",
       "   0.061153219922533135,\n",
       "   0.061382673998520175,\n",
       "   0.0589049337871019,\n",
       "   0.06310149832790016,\n",
       "   0.05626761562408467,\n",
       "   0.06064007499233582,\n",
       "   0.05950614495660266,\n",
       "   0.06046527049290608,\n",
       "   0.06058954762159572,\n",
       "   0.057523559102717225,\n",
       "   0.0576617115894456,\n",
       "   0.055564344323803215,\n",
       "   0.05635575279725951,\n",
       "   0.05870401838739842,\n",
       "   0.0565606911593917,\n",
       "   0.056610593554967316,\n",
       "   0.05693695369507426,\n",
       "   0.056966296151998726,\n",
       "   0.05755384057447954,\n",
       "   0.05718700659795122,\n",
       "   0.05654827602657838,\n",
       "   0.05487167498010173,\n",
       "   0.05597615178459887,\n",
       "   0.05861412797011499,\n",
       "   0.05948256361450368,\n",
       "   0.05878184514393767,\n",
       "   0.05338201121694022,\n",
       "   0.059189145998295,\n",
       "   0.05666922498080967,\n",
       "   0.05175041882320637,\n",
       "   0.057144769593400216,\n",
       "   0.055863484860378634,\n",
       "   0.05722854871472283,\n",
       "   0.05535755356703228,\n",
       "   0.055795315046277834,\n",
       "   0.05438369738595403,\n",
       "   0.05695452484391348,\n",
       "   0.05175888684473089,\n",
       "   0.054930647533018184,\n",
       "   0.05648150382326039,\n",
       "   0.05281359415915145,\n",
       "   0.056190501377405694,\n",
       "   0.055682235954727884,\n",
       "   0.05518463466485158,\n",
       "   0.05494866432011799,\n",
       "   0.054783363088130664,\n",
       "   0.05564885985549246,\n",
       "   0.05528891334796946,\n",
       "   0.05487359151122599,\n",
       "   0.05797203518515245,\n",
       "   0.053133993598267405,\n",
       "   0.05464119716156778,\n",
       "   0.05624140273046057,\n",
       "   0.05778797343368325,\n",
       "   0.054676259451465294,\n",
       "   0.0530535946961993,\n",
       "   0.05678119266468469,\n",
       "   0.056170377603075125,\n",
       "   0.055032357707590886,\n",
       "   0.05116008862968624,\n",
       "   0.053167696176842624,\n",
       "   0.05363450184135096,\n",
       "   0.050336249849537974,\n",
       "   0.05649976953003563,\n",
       "   0.053312269959653824,\n",
       "   0.056076773381767295,\n",
       "   0.054001120022498864,\n",
       "   0.053819523618640855,\n",
       "   0.050512530741718895,\n",
       "   0.054666464212285576,\n",
       "   0.05141111172816182,\n",
       "   0.05248538617963892,\n",
       "   0.05194715228238676,\n",
       "   0.05460486299774985,\n",
       "   0.052636677623179876,\n",
       "   0.052143834965649095,\n",
       "   0.05478474987677001,\n",
       "   0.05398582789440922,\n",
       "   0.05095273044046717,\n",
       "   0.053321364428849646,\n",
       "   0.054643148990610194,\n",
       "   0.05308925955818002,\n",
       "   0.05268448521384365,\n",
       "   0.05403204302644353,\n",
       "   0.05639735968503305,\n",
       "   0.05396228603702185,\n",
       "   0.052616686181192446,\n",
       "   0.05088823695804636,\n",
       "   0.05621365673258605,\n",
       "   0.055566851955246126,\n",
       "   0.05252956317102217,\n",
       "   0.050313507918074984,\n",
       "   0.053218150432856076,\n",
       "   0.05330101817545723,\n",
       "   0.05142466622917108,\n",
       "   0.05139040152399645,\n",
       "   0.05256707105863284,\n",
       "   0.053891084251328734,\n",
       "   0.05479532700711681,\n",
       "   0.0502165918443237,\n",
       "   0.05126575011649125,\n",
       "   0.05138848048703028,\n",
       "   0.055826556963001706,\n",
       "   0.05353097575109976,\n",
       "   0.054589232794169065,\n",
       "   0.05443863218677491,\n",
       "   0.05252156297252018,\n",
       "   0.05353072255669786,\n",
       "   0.05616242486653854,\n",
       "   0.04992868069300265,\n",
       "   0.05585368112060202,\n",
       "   0.05137928979311616,\n",
       "   0.05503286166889159,\n",
       "   0.053451994439947435,\n",
       "   0.05379778648410453,\n",
       "   0.05106134828647597,\n",
       "   0.05534775132884393,\n",
       "   0.0514958053792535],\n",
       "  'train_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'train_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'train_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'train_learning_rate_mean': [0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005,\n",
       "   0.0010000000000000005],\n",
       "  'train_learning_rate_std': [4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19,\n",
       "   4.336808689942018e-19],\n",
       "  'val_loss_mean': [1.441416900952657,\n",
       "   1.3846008117993673,\n",
       "   1.2682440356413522,\n",
       "   1.2167465635140737,\n",
       "   1.1956471415360768,\n",
       "   1.2200581177075704,\n",
       "   1.123721994360288,\n",
       "   1.1467954057455063,\n",
       "   1.0952849153677622,\n",
       "   1.0884990980227789,\n",
       "   1.0175517588853835,\n",
       "   1.0389101243019103,\n",
       "   1.0180739078919092,\n",
       "   1.0213970828056336,\n",
       "   1.0133472857872645,\n",
       "   1.0012899972995122,\n",
       "   0.993230233391126,\n",
       "   0.9507336417833964,\n",
       "   0.9401851975917817,\n",
       "   0.9469722998142243,\n",
       "   0.9226946310202281,\n",
       "   0.8991351246833801,\n",
       "   0.8960789090394974,\n",
       "   0.8867152986923853,\n",
       "   0.9207024757067362,\n",
       "   0.8809591494003932,\n",
       "   0.8881822508573533,\n",
       "   0.8737424963712692,\n",
       "   0.8496209345261256,\n",
       "   0.8854538373152415,\n",
       "   0.8618759785095851,\n",
       "   0.8372607083121936,\n",
       "   0.8495802170038224,\n",
       "   0.8708902553717295,\n",
       "   0.8311816070477168,\n",
       "   0.8509714325269063,\n",
       "   0.8673336320122083,\n",
       "   0.864456162750721,\n",
       "   0.8687086872259776,\n",
       "   0.8249712866544724,\n",
       "   0.8644219722350438,\n",
       "   0.8492595298091571,\n",
       "   0.8691458024581273,\n",
       "   0.8481071758270263,\n",
       "   0.851428176065286,\n",
       "   0.8603093306223552,\n",
       "   0.826475417514642,\n",
       "   0.8466011018554369,\n",
       "   0.8335600717862447,\n",
       "   0.8274499620000522,\n",
       "   0.8514898576339086,\n",
       "   0.8453600164254507,\n",
       "   0.8455632005135219,\n",
       "   0.8338497508565584,\n",
       "   0.8739532963434855,\n",
       "   0.8486619993050893,\n",
       "   0.8377566576004029,\n",
       "   0.8347696493069331,\n",
       "   0.8405166812737783,\n",
       "   0.8327777141332626,\n",
       "   0.8658423807223637,\n",
       "   0.8161090305447578,\n",
       "   0.8334618045886357,\n",
       "   0.839156512717406,\n",
       "   0.8425728618105253,\n",
       "   0.8348103048404057,\n",
       "   0.8459403287371,\n",
       "   0.8230433669686318,\n",
       "   0.8238097157080968,\n",
       "   0.8433031636476517,\n",
       "   0.8589755072196325,\n",
       "   0.8365624426802,\n",
       "   0.8499731681744258,\n",
       "   0.835593406756719,\n",
       "   0.862349781692028,\n",
       "   0.8515472497542699,\n",
       "   0.8485699105262756,\n",
       "   0.8320024336377779,\n",
       "   0.8758211203416189,\n",
       "   0.8291878257195154,\n",
       "   0.8285033581654231,\n",
       "   0.820870799223582,\n",
       "   0.8549568528930346,\n",
       "   0.823812338411808,\n",
       "   0.8413575561841329,\n",
       "   0.8462733939290047,\n",
       "   0.8324114596843719,\n",
       "   0.854802497625351,\n",
       "   0.8304598619540532,\n",
       "   0.8593304971853892,\n",
       "   0.863206976254781,\n",
       "   0.8332255019744237,\n",
       "   0.8533081621925036,\n",
       "   0.8367525950074196,\n",
       "   0.8284652309616407,\n",
       "   0.8285233368476231,\n",
       "   0.8216201958060264,\n",
       "   0.8145164253314336,\n",
       "   0.8474739395578702,\n",
       "   0.8183140490452449,\n",
       "   0.854152193069458,\n",
       "   0.8536674376328787,\n",
       "   0.8347023249665896,\n",
       "   0.856138942639033,\n",
       "   0.8343078074852626,\n",
       "   0.8427321048577626,\n",
       "   0.8257475736737251,\n",
       "   0.820791670580705,\n",
       "   0.8235936591029167,\n",
       "   0.8187837612628937,\n",
       "   0.8498469489812851,\n",
       "   0.8366892975568772,\n",
       "   0.8144606240590413,\n",
       "   0.8152683785557747,\n",
       "   0.8499923638502757,\n",
       "   0.8362324607372283,\n",
       "   0.8268182643254598,\n",
       "   0.8390438640117646,\n",
       "   0.8169289497534434,\n",
       "   0.8296001875400543,\n",
       "   0.8160243553916613,\n",
       "   0.8450383215149244,\n",
       "   0.8173968028028806,\n",
       "   0.8636752602458,\n",
       "   0.8138732849558195,\n",
       "   0.8166079483429591,\n",
       "   0.8214769736925761,\n",
       "   0.8210546944538752,\n",
       "   0.8190888904531797,\n",
       "   0.8405789794524511,\n",
       "   0.8176964631676674,\n",
       "   0.843300825258096,\n",
       "   0.8165814751386642,\n",
       "   0.8334662779172262,\n",
       "   0.8223350204030673,\n",
       "   0.8276028515895207,\n",
       "   0.8230750718712807,\n",
       "   0.8034846503535906,\n",
       "   0.8351253633697827,\n",
       "   0.8324406227469444,\n",
       "   0.8271724064151446,\n",
       "   0.8011943422754606,\n",
       "   0.820343577961127,\n",
       "   0.8287694144248963,\n",
       "   0.7984485598405202,\n",
       "   0.8559292777379354,\n",
       "   0.793948367635409,\n",
       "   0.8197477628787359],\n",
       "  'val_loss_std': [0.08236909406322923,\n",
       "   0.09339231460382348,\n",
       "   0.10576904320190608,\n",
       "   0.11431139715609645,\n",
       "   0.11351660558696669,\n",
       "   0.12571157280370188,\n",
       "   0.12539333089662633,\n",
       "   0.127288416793855,\n",
       "   0.11893976692053894,\n",
       "   0.11857961665826383,\n",
       "   0.13314399695180065,\n",
       "   0.12917321367395096,\n",
       "   0.12964002486422432,\n",
       "   0.1323388614544805,\n",
       "   0.1319458229265092,\n",
       "   0.131731723530891,\n",
       "   0.13569759183266134,\n",
       "   0.1321178512556122,\n",
       "   0.13432013358022213,\n",
       "   0.1343943144822021,\n",
       "   0.13189308134964173,\n",
       "   0.13538646001422064,\n",
       "   0.13303069375142224,\n",
       "   0.1375880378766212,\n",
       "   0.13522909217678217,\n",
       "   0.13355226933155462,\n",
       "   0.14244751950307097,\n",
       "   0.12962129481772838,\n",
       "   0.13616444192178212,\n",
       "   0.13782690638170697,\n",
       "   0.14199393901353582,\n",
       "   0.14331784139852494,\n",
       "   0.1432414463856821,\n",
       "   0.1415365234495874,\n",
       "   0.14178111611636196,\n",
       "   0.14257672104815788,\n",
       "   0.14153814247257554,\n",
       "   0.14174017245671905,\n",
       "   0.14327588743851785,\n",
       "   0.14195059085377404,\n",
       "   0.14376561022471882,\n",
       "   0.1380180420565341,\n",
       "   0.13936455204086967,\n",
       "   0.14573254609172384,\n",
       "   0.139378807208103,\n",
       "   0.14579574274818122,\n",
       "   0.13982925913448055,\n",
       "   0.14016166349519368,\n",
       "   0.1493938500131544,\n",
       "   0.1363779496068937,\n",
       "   0.13659857422742805,\n",
       "   0.14062957669772663,\n",
       "   0.1420291517124603,\n",
       "   0.14148760305709215,\n",
       "   0.14572561920506052,\n",
       "   0.1393443262024157,\n",
       "   0.1404046252707321,\n",
       "   0.14072419163918912,\n",
       "   0.14675470717558212,\n",
       "   0.1412441140718948,\n",
       "   0.1473288008874422,\n",
       "   0.13945468382979231,\n",
       "   0.1379862777999113,\n",
       "   0.15062000754203939,\n",
       "   0.14283031594824244,\n",
       "   0.14174174685208152,\n",
       "   0.1462677835120186,\n",
       "   0.1394610514760964,\n",
       "   0.14467324634575754,\n",
       "   0.13985087632471652,\n",
       "   0.15255988236323015,\n",
       "   0.14718928974784984,\n",
       "   0.13910653438503626,\n",
       "   0.14473110653395208,\n",
       "   0.14731001076439354,\n",
       "   0.13828589263766342,\n",
       "   0.15409183651680858,\n",
       "   0.1373977026088596,\n",
       "   0.14025084153764072,\n",
       "   0.14580684047271839,\n",
       "   0.1482434116596452,\n",
       "   0.15002410160352764,\n",
       "   0.14232830995644644,\n",
       "   0.13887174819786086,\n",
       "   0.1475570224772372,\n",
       "   0.14158941384168394,\n",
       "   0.1461647600794514,\n",
       "   0.14965812771887074,\n",
       "   0.1347046485773977,\n",
       "   0.1415496626297977,\n",
       "   0.1495719782519324,\n",
       "   0.14595245677177754,\n",
       "   0.15651819403757206,\n",
       "   0.13784789217300691,\n",
       "   0.14647697719896777,\n",
       "   0.15114289540728854,\n",
       "   0.13482948534290776,\n",
       "   0.13906551192682287,\n",
       "   0.1332200517687758,\n",
       "   0.14500098324816738,\n",
       "   0.14618536303065943,\n",
       "   0.1332704896354343,\n",
       "   0.13991502736678565,\n",
       "   0.15040386391573649,\n",
       "   0.1367845603941481,\n",
       "   0.1428019310414054,\n",
       "   0.14147082178568196,\n",
       "   0.13837640861141653,\n",
       "   0.13921944585934626,\n",
       "   0.14362521889307336,\n",
       "   0.151231301048288,\n",
       "   0.13248412505898224,\n",
       "   0.13680061207526795,\n",
       "   0.137050365787563,\n",
       "   0.1504233077518279,\n",
       "   0.1401749899088686,\n",
       "   0.1403136861429386,\n",
       "   0.15080984534232253,\n",
       "   0.1454318323766392,\n",
       "   0.14515713977637398,\n",
       "   0.14470297651529837,\n",
       "   0.15472159742331495,\n",
       "   0.14624001718369453,\n",
       "   0.15105193593081204,\n",
       "   0.14212560677520733,\n",
       "   0.138434405199447,\n",
       "   0.14582485110819923,\n",
       "   0.13506629797082803,\n",
       "   0.14920560243596975,\n",
       "   0.14302452238484686,\n",
       "   0.15234457750460872,\n",
       "   0.14699706316114045,\n",
       "   0.1391490074960588,\n",
       "   0.14400596245504071,\n",
       "   0.1453951739511507,\n",
       "   0.14787961141793626,\n",
       "   0.14944045238275092,\n",
       "   0.14153461820971366,\n",
       "   0.14258738500888235,\n",
       "   0.1461595342461034,\n",
       "   0.14279468492704037,\n",
       "   0.13993213863440626,\n",
       "   0.1379659709315623,\n",
       "   0.14445085576529323,\n",
       "   0.1357528386767979,\n",
       "   0.14339580951731895,\n",
       "   0.13904326722940458,\n",
       "   0.1386134990960677],\n",
       "  'val_accuracy_mean': [0.3836444452901681,\n",
       "   0.4195333346227805,\n",
       "   0.48108888973792396,\n",
       "   0.5142222226659456,\n",
       "   0.5210666640599568,\n",
       "   0.506511111954848,\n",
       "   0.5494666653871536,\n",
       "   0.5427777776122094,\n",
       "   0.5647333319981893,\n",
       "   0.5685777761538824,\n",
       "   0.5992888867855072,\n",
       "   0.5890444430708885,\n",
       "   0.5993777774771054,\n",
       "   0.5996444437901179,\n",
       "   0.6045777771870295,\n",
       "   0.6079777792096138,\n",
       "   0.6131333324313164,\n",
       "   0.6319999978939692,\n",
       "   0.6391333321730296,\n",
       "   0.6313999992609024,\n",
       "   0.6426222235957781,\n",
       "   0.6507555555303891,\n",
       "   0.6549333327015241,\n",
       "   0.6575555570920308,\n",
       "   0.6424666666984558,\n",
       "   0.6600222206115722,\n",
       "   0.6596444447835287,\n",
       "   0.6613555554548899,\n",
       "   0.6752888879179955,\n",
       "   0.6589111113548278,\n",
       "   0.6714000008503596,\n",
       "   0.6827111115058263,\n",
       "   0.6751777780056,\n",
       "   0.6638444451491038,\n",
       "   0.680866667330265,\n",
       "   0.6737333329518637,\n",
       "   0.6665777790546418,\n",
       "   0.671644445459048,\n",
       "   0.6684222195545833,\n",
       "   0.6842444425821305,\n",
       "   0.6690666668613752,\n",
       "   0.6773555550972621,\n",
       "   0.6657555562257766,\n",
       "   0.6819777778784434,\n",
       "   0.674555554886659,\n",
       "   0.6725111108024915,\n",
       "   0.6812666684389115,\n",
       "   0.6763999990622203,\n",
       "   0.6825999989112218,\n",
       "   0.6827777782082558,\n",
       "   0.6748444454868635,\n",
       "   0.6762444444497426,\n",
       "   0.6760888886451721,\n",
       "   0.6822444444894791,\n",
       "   0.6663111114501953,\n",
       "   0.6735999980568885,\n",
       "   0.6799333341916403,\n",
       "   0.6803555555144946,\n",
       "   0.6827999995152155,\n",
       "   0.6795333329836527,\n",
       "   0.6708888906240463,\n",
       "   0.6895111115773519,\n",
       "   0.6807111102342606,\n",
       "   0.6835555559396744,\n",
       "   0.6809333319465319,\n",
       "   0.6820222208897273,\n",
       "   0.6811333339413007,\n",
       "   0.6837777776519457,\n",
       "   0.6881555551290512,\n",
       "   0.6788222213586171,\n",
       "   0.6753777778148651,\n",
       "   0.6831777758399645,\n",
       "   0.674488888780276,\n",
       "   0.6839111104607583,\n",
       "   0.6758888890345891,\n",
       "   0.6745777770876884,\n",
       "   0.6845777794718743,\n",
       "   0.6813555538654328,\n",
       "   0.6679333315292995,\n",
       "   0.6837555554509163,\n",
       "   0.685888888736566,\n",
       "   0.6838222215572993,\n",
       "   0.6756666652361552,\n",
       "   0.6877777792016665,\n",
       "   0.6814000003536542,\n",
       "   0.6811555551489195,\n",
       "   0.6853999981284141,\n",
       "   0.6777555547157923,\n",
       "   0.6875777765115102,\n",
       "   0.6716888892650604,\n",
       "   0.6759555550416311,\n",
       "   0.685622221827507,\n",
       "   0.6780222221215566,\n",
       "   0.6819777777791023,\n",
       "   0.6822222221891086,\n",
       "   0.6863333330551783,\n",
       "   0.687311111887296,\n",
       "   0.6897111092011133,\n",
       "   0.6754222209254901,\n",
       "   0.6910222228368124,\n",
       "   0.6789777769645056,\n",
       "   0.6763333332538605,\n",
       "   0.6804666675130526,\n",
       "   0.6768222216765086,\n",
       "   0.6813999993602434,\n",
       "   0.6813555554548899,\n",
       "   0.6856444452206294,\n",
       "   0.6886222219467163,\n",
       "   0.6893777797619501,\n",
       "   0.6845555541912715,\n",
       "   0.6813777772585551,\n",
       "   0.6817777784665425,\n",
       "   0.6923999998966853,\n",
       "   0.6888000015417735,\n",
       "   0.6760666657487552,\n",
       "   0.6814888885617256,\n",
       "   0.6846666661898295,\n",
       "   0.6837555545568467,\n",
       "   0.6873555561900139,\n",
       "   0.6867777768770854,\n",
       "   0.6910222222407659,\n",
       "   0.6808444446325302,\n",
       "   0.6903777778148651,\n",
       "   0.675955556333065,\n",
       "   0.6905555548270543,\n",
       "   0.687133332490921,\n",
       "   0.6912222222487132,\n",
       "   0.6843777771790822,\n",
       "   0.6870000018676122,\n",
       "   0.6819555560747782,\n",
       "   0.6979999989271164,\n",
       "   0.6777333329121272,\n",
       "   0.6880888893206915,\n",
       "   0.6846666673819224,\n",
       "   0.6894888880848885,\n",
       "   0.6841777769724527,\n",
       "   0.6905111115177472,\n",
       "   0.699288889169693,\n",
       "   0.6877111119031906,\n",
       "   0.683777777949969,\n",
       "   0.6857333348194758,\n",
       "   0.6946666653951009,\n",
       "   0.6868666664759318,\n",
       "   0.6870222212870916,\n",
       "   0.6936666677395503,\n",
       "   0.6769777781764666,\n",
       "   0.6988444442550341,\n",
       "   0.687422223687172],\n",
       "  'val_accuracy_std': [0.053624686199679084,\n",
       "   0.05543034371674429,\n",
       "   0.060321357239092196,\n",
       "   0.061300312244270065,\n",
       "   0.05992380292769722,\n",
       "   0.06224305635312529,\n",
       "   0.06347828952692203,\n",
       "   0.060888380948135166,\n",
       "   0.0600169976186404,\n",
       "   0.06184250590931793,\n",
       "   0.06327200131010428,\n",
       "   0.061433297291990145,\n",
       "   0.061044473319096924,\n",
       "   0.06136128184205862,\n",
       "   0.060875765610299865,\n",
       "   0.060432265856983754,\n",
       "   0.06135595000494595,\n",
       "   0.06382615377888752,\n",
       "   0.061086374734130686,\n",
       "   0.062429539556022345,\n",
       "   0.06121920153101314,\n",
       "   0.06016908026328471,\n",
       "   0.05879622267834406,\n",
       "   0.06016971994302225,\n",
       "   0.06251035972193497,\n",
       "   0.061136155031495366,\n",
       "   0.06052014912595406,\n",
       "   0.059255376868141495,\n",
       "   0.05959640564380807,\n",
       "   0.05680766417822001,\n",
       "   0.05859057003818465,\n",
       "   0.05742936096434106,\n",
       "   0.06004694050782062,\n",
       "   0.06051781227844796,\n",
       "   0.06104270620814574,\n",
       "   0.06110940004399978,\n",
       "   0.05942433441167555,\n",
       "   0.058896957208926,\n",
       "   0.06064859021669015,\n",
       "   0.06048127767139839,\n",
       "   0.05935096051664121,\n",
       "   0.059903375846808694,\n",
       "   0.05910116516554076,\n",
       "   0.05875069370059268,\n",
       "   0.060129388062192465,\n",
       "   0.05973824089879266,\n",
       "   0.05914222796549079,\n",
       "   0.05948236131580461,\n",
       "   0.06283354866542352,\n",
       "   0.06008255661281955,\n",
       "   0.05873554906202432,\n",
       "   0.05822026930561431,\n",
       "   0.05870042768492075,\n",
       "   0.06039253991784477,\n",
       "   0.06226960143199432,\n",
       "   0.060501019386624465,\n",
       "   0.059709133279819554,\n",
       "   0.06072542548717747,\n",
       "   0.061631016822538374,\n",
       "   0.06054264634921864,\n",
       "   0.06036636981307234,\n",
       "   0.05908861940416042,\n",
       "   0.05981525597494224,\n",
       "   0.059899505283112026,\n",
       "   0.05932724223657148,\n",
       "   0.061438793466180856,\n",
       "   0.060352459273915686,\n",
       "   0.06375841774354704,\n",
       "   0.061836147328892685,\n",
       "   0.05951860897238895,\n",
       "   0.060398412425589375,\n",
       "   0.06178677353650112,\n",
       "   0.05883182239305193,\n",
       "   0.059385429459650745,\n",
       "   0.058914775623975686,\n",
       "   0.057883083165149056,\n",
       "   0.06088063480951018,\n",
       "   0.05835345316066268,\n",
       "   0.06062901413321796,\n",
       "   0.061032834628283794,\n",
       "   0.05932574019305336,\n",
       "   0.06280291112610219,\n",
       "   0.0605997813834624,\n",
       "   0.059503708881904004,\n",
       "   0.06205585411202227,\n",
       "   0.05818869281222922,\n",
       "   0.06256039575926749,\n",
       "   0.05989496485758414,\n",
       "   0.05748223194910487,\n",
       "   0.06068711174167911,\n",
       "   0.059974792981494364,\n",
       "   0.05780123964615031,\n",
       "   0.062458991455803074,\n",
       "   0.058989763747721954,\n",
       "   0.0604681322166335,\n",
       "   0.06008913108386427,\n",
       "   0.058106157163663394,\n",
       "   0.05691636074351066,\n",
       "   0.05781009431649159,\n",
       "   0.05829928680099702,\n",
       "   0.06133930892062732,\n",
       "   0.059337393195103456,\n",
       "   0.06024337152172016,\n",
       "   0.06063471213698916,\n",
       "   0.059406659008223485,\n",
       "   0.05961928262322178,\n",
       "   0.05908054394713421,\n",
       "   0.05823094146949394,\n",
       "   0.056264438249579884,\n",
       "   0.060220479905379654,\n",
       "   0.05950809302982604,\n",
       "   0.05672371801373561,\n",
       "   0.0570381539378409,\n",
       "   0.057536178197561086,\n",
       "   0.05894953228572979,\n",
       "   0.05745470135434099,\n",
       "   0.058594651125162156,\n",
       "   0.06161745294056894,\n",
       "   0.06224657921537886,\n",
       "   0.0583157672895285,\n",
       "   0.05942682663302859,\n",
       "   0.06047122076578073,\n",
       "   0.059950643121916516,\n",
       "   0.058862787716163684,\n",
       "   0.05862488039711161,\n",
       "   0.06013014276724436,\n",
       "   0.057769551036318104,\n",
       "   0.05790366952605313,\n",
       "   0.05939229409444336,\n",
       "   0.05838592919223286,\n",
       "   0.05786446634260626,\n",
       "   0.060147128838677545,\n",
       "   0.0615507576606847,\n",
       "   0.05904047643682041,\n",
       "   0.06030445605981702,\n",
       "   0.05810614504986827,\n",
       "   0.06266103468491274,\n",
       "   0.059626724231758146,\n",
       "   0.05684239040668668,\n",
       "   0.06317485044365248,\n",
       "   0.05934721477932003,\n",
       "   0.05871967423809152,\n",
       "   0.0608394736870983,\n",
       "   0.05889494569321081,\n",
       "   0.058538053716414816,\n",
       "   0.05758902459516361,\n",
       "   0.05798210099579776,\n",
       "   0.05952920001052301],\n",
       "  'val_loss_importance_vector_0_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_0_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_1_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_1_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_2_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_2_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_3_mean': [0.20000000298023224,\n",
       "   0.18666666746139526,\n",
       "   0.1733333319425583,\n",
       "   0.1599999964237213,\n",
       "   0.14666666090488434,\n",
       "   0.13333334028720856,\n",
       "   0.11999999731779099,\n",
       "   0.1066666692495346,\n",
       "   0.09333333373069763,\n",
       "   0.07999999821186066,\n",
       "   0.06666667014360428,\n",
       "   0.0533333346247673,\n",
       "   0.03999999910593033,\n",
       "   0.02666666731238365,\n",
       "   0.013333333656191826,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064,\n",
       "   0.006000000052154064],\n",
       "  'val_loss_importance_vector_3_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0],\n",
       "  'val_loss_importance_vector_4_mean': [0.20000000298023224,\n",
       "   0.25333333015441895,\n",
       "   0.30666667222976685,\n",
       "   0.36000001430511475,\n",
       "   0.41333332657814026,\n",
       "   0.46666666865348816,\n",
       "   0.5199999809265137,\n",
       "   0.5733333230018616,\n",
       "   0.6266666650772095,\n",
       "   0.6800000071525574,\n",
       "   0.7333333492279053,\n",
       "   0.7866666913032532,\n",
       "   0.8399999737739563,\n",
       "   0.8933333158493042,\n",
       "   0.9466666579246521,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546,\n",
       "   0.9760000109672546],\n",
       "  'val_loss_importance_vector_4_std': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0]}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델을 구성한다\n",
    "model = MAMLFewShotClassifier(args=args, device=device,\n",
    "                              im_shape=(2, 3,\n",
    "                                        args.image_height, args.image_width))\n",
    "\n",
    "data = MetaLearningSystemDataLoader\n",
    "\n",
    "maml_system = ExperimentBuilder(model=model, data=data, args=args, device=device)\n",
    "maml_system.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2e15f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "def normalize_tensor(tensor):\n",
    "    min_val = tensor.min()\n",
    "    max_val = tensor.max()\n",
    "    return (tensor - min_val) / (max_val - min_val + 1e-8)\n",
    "\n",
    "def plot_all_pad_prompts(prompted_weights, save_path=None):\n",
    "    direction_keys = {\n",
    "        'Pad Up': 'prompt.prompt_dict.pad_up',\n",
    "        'Pad Down': 'prompt.prompt_dict.pad_down',\n",
    "        'Pad Left': 'prompt.prompt_dict.pad_left',\n",
    "        'Pad Right': 'prompt.prompt_dict.pad_right'\n",
    "    }\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(8, 8))\n",
    "\n",
    "    for ax, (title, key) in zip(axs.flatten(), direction_keys.items()):\n",
    "        tensor = prompted_weights[key]  # [1, 3, H, W]\n",
    "        tensor = tensor[0].detach().cpu()  # [3, H, W]\n",
    "        tensor_norm = normalize_tensor(tensor)\n",
    "        img = T.ToPILImage()(tensor_norm)\n",
    "\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(title)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Saved prompt visualization to {save_path}\")\n",
    "    else:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d44d20",
   "metadata": {},
   "source": [
    "# 2. Prompt 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66d791e5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x800 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxYAAAJfCAYAAADinkC3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAABDCUlEQVR4nO3deZReVZ0v7l2pSlKZK6lURjLPEEIGCBAoIJEpDAEUEBAasGn0oiL2tdVWf41w2wGlEbEBW0XAoCKDwG1kUCBCmAMhkJCBzGSeSGWsTFXv749e1PVwNiTWRipVPM9aLNb51M579judU986e+9TVCgUCgEAACBBs4buAAAA0PgpLAAAgGQKCwAAIJnCAgAASKawAAAAkiksAACAZAoLAAAgmcICAABIprAAAACSKSzgPYqKisJ3vvOdhu4GAH9njvfw4VJY0GjccccdoaioqO6/0tLSMHjw4PDFL34xrFmz5iPty1/+8pdQVFQU7rvvvujPv/jFL4aioqKPtE8ATcX+dLxfsmRJpi/NmzcPnTt3DuPGjQvf/OY3w9tvv/2R9gf2ZyUN3QH4W1177bWhX79+YceOHeHZZ58Nt956a3jkkUfCrFmzQuvWrRu6ewB8SPan4/35558fTjnllFBbWxs2btwYpk2bFm688cbwk5/8JNx2223hvPPO+0j7A/sjhQWNzsSJE8Ohhx4aQgjhsssuC+Xl5eGGG24IDz30UDj//PMbuHcAfFj2p+P96NGjw4UXXpjJli5dGk488cRw8cUXh2HDhoVDDjnkI+0T7G8MhaLRmzBhQgghhMWLF4cQQrj++uvDuHHjQnl5eWjVqlUYM2ZMdMjSzp07w1e+8pVQUVER2rVrFyZNmhSWL1/+d+tnUVFR+OIXvxh+85vfhCFDhoTS0tIwZsyY8Mwzz/zd9gnQlOxvx/s+ffqEO+64I+zatSv88Ic/zPxs0aJF4ZxzzgmdOnUKrVu3DkcccUT44x//WPfzQqEQOnfuHP75n/+5LqutrQ1lZWWhuLg4VFVV1eXXXXddKCkpCVu3bg0hhHDJJZeEtm3bhhUrVoQzzzwztG3bNlRUVISvfvWroaamJvl5QX0pLGj0Fi5cGEIIoby8PIQQwk9+8pMwatSocO2114bvfe97oaSkJJxzzjmZA3oI//PXrxtvvDGceOKJ4Qc/+EFo3rx5OPXUU/+ufX366afDVVddFS688MJw7bXXhg0bNoSTTz45zJo16++6X4CmYH883h955JFhwIAB4c9//nNdtmbNmjBu3Ljw+OOPhyuuuCJ897vfDTt27AiTJk0KDzzwQAjhf/7YdNRRR2X+uPTGG2+ETZs2hRBCeO655+ryqVOnhlGjRoW2bdvWZTU1NeGkk04K5eXl4frrrw/HHnts+I//+I/w85///EN5XlAvBWgkbr/99kIIofDEE08U1q1bV1i2bFnh7rvvLpSXlxdatWpVWL58eaFQKBS2b9+e+Xe7du0qDB8+vDBhwoS6bMaMGYUQQuGKK67ItL3gggsKIYTC1Vdf/YF9mTJlSiGEULj33nujP//CF75QeO/XK4RQCCEUXnnllbps6dKlhdLS0sJZZ5211+cP8HGxPx3vFy9eXAghFH70ox+9b5szzjijEEIobNq0qVAoFApXXXVVIYRQmDp1al2bLVu2FPr161fo27dvoaamplAoFAo/+tGPCsXFxYXNmzcXCoVC4aabbir06dOnMHbs2MLXv/71QqFQKNTU1BTKysoKX/nKV+oe6+KLLy6EEArXXnttph+jRo0qjBkz5gOfD/w9uWJBo3P88ceHioqK0KtXr3DeeeeFtm3bhgceeCD07NkzhBBCq1at6tpu3LgxbNq0KVRWVobp06fX5Y888kgIIYQrr7wy89hXXXXV37XvRx55ZBgzZkzddu/evcMZZ5wRHn/8cZevAd6jsRzv372SsGXLlrp9jh07Nhx99NGZNpdffnlYsmRJmD17dgghhMrKylBTUxOef/75EML/XJmorKwMlZWVYerUqSGEEGbNmhWqqqpCZWVlbr+f//znM9uVlZVh0aJFH9rzgr+Vyds0OjfffHMYPHhwKCkpCV27dg1DhgwJzZr9vxr54YcfDv/+7/8eZsyYEXbu3FmX//Xyr0uXLg3NmjULAwYMyDz2kCFD/q59HzRoUC4bPHhw2L59e1i3bl3o1q3b33X/AI1JYznevzv3oV27dnX7PPzww3Pthg0bVvfz4cOHh9GjR4fWrVuHqVOnhpNOOilMnTo1XHPNNaFbt27hpz/9adixY0ddgfHXRUoIIZSWloaKiopM1rFjx7Bx48YP7XnB30phQaMzduzYulVC3mvq1Klh0qRJ4Zhjjgm33HJL6N69e2jevHm4/fbbw29/+9sPrQ+lpaUhhBCqq6ujP9++fXtdGwDqZ3843u+LWbNmhS5duoT27dv/Tf+uefPm4fDDDw/PPPNMWLBgQVi9enWorKwMXbt2Dbt37w4vvfRSmDp1ahg6dGiuiCguLv4wnwJ8KBQWNCn3339/KC0tDY8//nho2bJlXX777bdn2vXp0yfU1taGhQsXZv5qNW/evH3aT58+fT6w/bx58+ra/LX58+fnsrfeeiu0bt06d9IA4P19VMf7vXnhhRfCwoULM0vR9unTJ/r4c+fOrfv5uyorK8N1110XnnjiidC5c+cwdOjQUFRUFA466KAwderUMHXq1HDaaad9KH2FvzdzLGhSiouLQ1FRUWa+wpIlS8KDDz6YaTdx4sQQQgg33XRTJr/xxhv3aT/du3cPI0eODHfddVdmScAQQnj11VfDiy++WLePv/bCCy9kxv4uW7YsPPTQQ+HEE0/01yeAv8FHdbz/IEuXLg2XXHJJaNGiRfiXf/mXuvyUU04JL7/8cnjhhRfqsm3btoWf//znoW/fvuHAAw+syysrK8POnTvDjTfeGI4++ui6YVyVlZVh8uTJYeXKldH5FbA/csWCJuXUU08NN9xwQzj55JPDBRdcENauXRtuvvnmMHDgwPDGG2/UtRs5cmQ4//zzwy233BI2bdoUxo0bF5588smwYMGCfd7XDTfcEE466aQwcuTIcMkll4QePXqEOXPmhJ///Oehe/fu4V//9V9z/2b48OHhpJNOCldeeWVo2bJluOWWW0IIIVxzzTXpTx7gY+SjPN6HEML06dPDXXfdFWpra0NVVVWYNm1auP/++0NRUVGYPHlyGDFiRF3bb3zjG+F3v/tdmDhxYrjyyitDp06dwp133hkWL14c7r///sw8kSOPPDKUlJSEefPmhcsvv7wuP+aYY8Ktt94aQggKCxqPhl6WCvbVu8sPTps27QPb3XbbbYVBgwYVWrZsWRg6dGjh9ttvL1x99dW55V+rq6sLV155ZaG8vLzQpk2bwumnn15YtmzZPi0/+K4XX3yxcNpppxU6duxYKCkpKfTs2bNw2WWX1S2F+NdCCIUvfOELhbvuuquuf6NGjSpMmTJlX18CgI+F/el4/+5ys+/+V1JSUujUqVPh8MMPL/zrv/5rYenSpdF/t3DhwsLZZ59dKCsrK5SWlhbGjh1bePjhh6NtDzvssEIIofDSSy/VZcuXLy+EEAq9evXKtb/44osLbdq0yeWx5w4fpaJCoVBokIoGPmaKiorCF77whfCf//mfDd0VAIAPnTkWAABAMoUFAACQTGEBAAAksyoUfERMZwIAmjJXLAAAgGQKCwAAIJnCAgAASLbPcyx+cev3M9vrdmzOtWnfMTuGfPWTW3Jtyg4rz2zPr67ItelVVZt93NLSXJuSjjsz26Vb3s612dCpXzZYvSPXpqJ4e2Z7Xkm+1mpT0zqzvem1dbk2fY9ontluP7hDrs3zT87JbA/qcVCuzZ6t6zPbxe225tqUNBuY2S4tNM+12dyqJrPdZntVrk2XFS0z24sH1eTabArZ9/mArT1ybQrbV2W2OzYvyrVZUMi+9jv75l/nbWuz+69d2CvXpmeLXZntfv3yn7GtVQMy26V78p+fzd3fzGwveW5lrk2P/tnXuUfnVrk2LXdnX/tVHdrn2pS3mpfZXjNvT65Ns+VjM9vVvZ/JtWne5ZBsMC3/3PuM2J3ZnrfhnVybFoNGZLabvfVark1plzbZf1PROdem9dvZ7+Cmrq1zbYp2ZY8J1S1W59pUlWcfe8fSqlybDsXZz2GrPS1ybV5/z3Gj7+D8e7F4Q/b71XZ5/hDYrSL73Gva5d+vxauyfezTe2SuzdZ3sm0676nKtdmwOfv+dO99aK7NxqLs+7yt2fJcm4pVZZntrttyTcLWsuz34O3ilrk23/vG5blsX139L3dktjf2yH9/20/9fWb7oIPzdxOesjt7N+RO69fn2pxVekxme2u7N3Nt5jTPHqeXFg3Ptanpm91uXbMq12b38uzxpHNFu1ybdQ88nNnuc/jJuTbbnng5sz2mef9cm8ffyR4DN/6v/BvZevP8zHaP1Sfm2szY+Hpmu/tB23Nt2jTrltmurOqdazN79RvZoHf3XJtZ1R0z20NHtsm1mT/96cz2uo6dcm167s6eM3tvzH83e76TPbc8MPPlXJvzv3N0Zrv6dy/l2uw66YDMdrOO+df5oOezfXxxy85cmyXLs8eYjYctzbVp3z97TB7zcP51fq7dhsz24M7512doIfs9mDc/f/xvM+A9597ajrk2bVeckNl+sus9uTYdT1ib2e708qhcmx1bu2S2T2iVPz++uH5hZntNzZpcmw4HZP/d3OX571d57+zvSltm5V+f8cuzx5vtY57NtWnTLHu+fqlZ/jw7fMR5me3nbvttrs0/f/LszPbk+Q/n2hT17ZrZ7lRblmszfdDGzHZN+bxcm3MWHpXZ7vVa/vv1YIe2me2KTbtzbU7YlD3+3177RK5Nj08enNnesip/7P3+F67NZe/ligUAAJBMYQEAACRTWAAAAMn2eY7FnuLs+Mcli/NjuEZ16ZnZLm8xPdem6j1DYQ8aNCDXpuUb07KP0yU/dnvttvfO8ciPo9/eIztOcEn7qlybA7pk53hsqtqYa9NhW3b/A44em2uzsez5zPaKzbW5Nm2HDs7ua16+z122lWW2V5Xm5z1s6J6dT7KuXf7+CP3bZscSvjE1P+ejTYeqzPaiK17M92dndjz+zvsW59qsKcrOA/lEzz65Nhvmz8xsd2yTn1/Sb3B2DPTLb8/OtTlveHaM75PV+fHmK2tfyGwXjjog1+btbtmxsCd8KT+uc/XGWZntZbcNyrVpvT47PnRLzaJcm8Ku7Geq06H5ceVVB2THRK8alv9+tSzLjsNttTU/L+TtovfMe6goz7Xp0npFtk2L/Ge14+7s3xyqFubfi26tsnMjZq2am2szoG12DPKCLmtzbfa0yH5++q7Kjwsue888p5U1m3JtOjcry2x3WJofgzyiLDuuvKh9fq7Gxm3Z97B6VFWuTbs22bHMvTvmx7CvfTM7p+qAofkx9Ts2Z7+7a98zJjmEENqVZ+d4bCvNz6navi37PIqK88eELaurs/vu9OHexmh3eXa+yNKX82PbLz0tO4fkgceezLU5+4hPZLbv/0F+Xs6AO4/LbH/7R5NzbSq/mX2c1/6S/8zsXJR9LY/fnZ9Dtv3A7Hdhwar8d2HEpMMz21tm5j/nQ88Yk9meWl6da7Ppyexr2Kk6f1wqVJ2S2d44Oz/mutdxwzLb71Tkv5unVGXnFex4PX/uKx6anb+x7q3Hc22OHJY9t6x66/Vcmz0lZZntAT365tqcviD7OX9tfX6M/JYJ7xlv3iF/rlm6PntsXzOmZ67NgNrs7x2zX86f16qKs2P2h9fk51oNHJh9XR+Yl59/2aVn9veFhT3zc5v6rMseA4tOzH/Hn5uRPf4fXTYi1+aNt7Kf3y6t8vMdX1iTPc/3+MrAXJvmi7Jj/Zu3zf/O8/r0v2S2Pzf4qFybF6dkP1ODxx+Za7Ng8SOZ7YMO3JVrs/WU7Nyn6m3549sB7bL7uq3Lq7k2g1Zkz0efGHVcrs0dv34ss33WgfnfVVa3y84ZXbQo/zvGoSOy57HSGfnfDXp1zh5bqrvl39NWk5/LbB8dmRv33dOz8zlOap8/jtUuzj7OsPH5+Xwzns0eE6oPyL8X+8IVCwAAIJnCAgAASKawAAAAku3zINuVC7PrD7cemB/DNW1qdlzecb3z65jP3pAd89e9Oj8OumWz7DjFzX3zY/dmLcyORe7dJr8OdYut2afXsXl+HNzQ2uz40B1t83MaXliVfe7lffPrWe9ckV3vt0+PQ3JtVu7Jjp+t6Rm5z8eObB8Lh0bGG27IjiEd1Tq/jvm6+1/JbH+ux8hcm7vaZ8eV7pqRX6N8UIvsWNT+d+fvF/JQeXauyNbu+TWmew3Iji9+cWd+DHLR1uw8jCMPyY9/X/meNdzXHJifi/DOsuzYxqI9+f40eyk7NnfRf+fv07CrdXaO0FFz8vcUeX1w9rNQNC6/TveWiuxnvmT2lFybg448LLO9fVf+/hO7X8mOM222Jv/5KWqZ3VfHSV1ybXatzD7O7m35cbjrBmfHf6/unp9DsOKd7OsxMvK+v706Oxdqc9/8fI6K3dk1uMdU59/31S2zn7EdI/L76rArO9a8uig/9nz9+uw4117P5f+20vuE7Bj2leveyLV5Z3d2jO2r04pzbdp3z86pWrI5/37Vvue+J83b5L9fxb2zY2rXvZ6fg9KrVXZ88czS/OvTtnP2WFdbPSfXJkVxTXbc8/gTDs61WbsmO+65fEx+DsGzW7NzmwZcfXiuzTfmXp/ZPvxb/XJt/jAn+zmvHJ8/vs2+NXsPl4Gn5o+lf9yabdPrgPxr22VKdh37hVX540mLndk5Nmu35ceAj56UHYO+qTj/Hk3rlR1PPfo989dCCGHIhOx36o8v5eeibWiXPTa8tjW/pv+brbKvYcVF+XmBo1/PHgNf+dPzuTbN//fIzPbbm/LP6633nOqeKc8flw6ZmZ3TMGp3t1ybV9/MPtfSCfnz/strsr9jtIicj7ZsyH5fDliSP3Ytm5A9Vu0e3TXXpnsh+3mp6Zt/Lx5ekD1O9lyc/zwXtcg+jylrn8616dV2Ymb72c35ea5dz8/Ov9n8+/zx7RMHZO91sa5b5P4TJ2TPUS+/9qdcmyOOzZ5nl5Tl3/cum7JzBQdOuDjX5v98/b8y2xeNzd8LZPsR2d83D1y9IdemU1n2GHDns/m5NSMnZo9b8zrm75Hz0Lzs/J8jzjwm16Z2evZ3sD4D8sexl/+QnYc2qnP+vm5Htc3+Lvn61BdybY496fzMdvsX8nP11izMztspnpi/70jHDdljeJeO+c/8vnDFAgAASKawAAAAkiksAACAZAoLAAAg2T5P3m7XLXujreXF+ZvpnDcyO9F39dyXc20G98/eBG3BnvxNgnodMi6zvWZxfvLM2COzE1Hmr8j3p1P50Mz21rvzE5na7B6dbVOUv/FKRe/szbla7cpP9DpyaPbGJhveyU8m3zwj+1x7n5y/cc9TP8/eXGh0y/zk29Iu2Ru11TzxVq7N52dnX+dms0bn2mwelJ30V9lpSK7NmZOzr0fJknyfW3bKTiYdMSt/06LXWmRvTlVRna9pDxifnXD4xu/yN5Q58IizM9vrf/tgrs3p/UZmtlc/lZ+gVdYsOzGw8nf5r8KuQf+U2V5elH+duw/MTsTbWnt0/nGeyU7i6r48v0DA9MnZyXFHjsy/zrWvZSdOtqvNT6xq3TJ7E6c/r1iRa7N7U1Vm+/j2+cnAr76UnSQ/5qJP59rM/EL2xkYXfe3UXJt/ezR7c7NDP5mfdFfybPaGmH3zL3PYMDI76W7lmlm5Nv1aZz+rmxbk39NWG7OT/Zv3zi9C8dLc7OTTEdvzE+mXVGUnQG4t5G/i2btzduJrbZv8Qgzt385O/jxoR/69eGVodpJ825btc206rinLbJcUqnJt2m3OTjBvNTT/OCmqm2Un8a5enf+Ot3g7+xoc/an8zaduvjF7XPqnCfmJ0G/Pzu5r/YD8hP+W7bOfmVX35j8znS/O3kTv56vn59r07JR9nDfezk8479cje+zc1TV/Q9KWXbOTt/t2yPd5YchORn61Iv8e7Xw+ez48vnl+wuc3p2YnybYcnp/kvP3x7AIEA8fmJ6BOHZc9br+8Jz9Bd8jGkzPbnz42f6z47rLsuW/jrvwk515DssfAmmfKcm12d80eFxeuzt8sbHaL7GdsaPv8JPDwTvb1GH5I/lgxZ0H2QPT6jPyE2LY1Z2S2p6+cmmtzxIjs+/z4m/nfi8YcOCmzvfb+/AIxrYdnJ9t26DUy1+apB6sy2yd/eniuzcsvZice95if/91gVrvs93Lz6gW5Np07ZyeTL+mSXyyjrCz7O9jTM/OTwE87J/s7zl9+n7/Z5em9Ls1sL1uYv/nm6q7vWbhk1RG5NnNez054P/30M3NtdrTNnmd/NTu/CNEx3bI3FRyzLr+IzKytB2a250Zu4nxo/+zr060qvwjI79tnz+EHX3pWrk3LP2RvftevkP9dbls4PbNdsiN/w8dBQ7MLjOyozt9UdF+4YgEAACRTWAAAAMkUFgAAQLJ9nmPRul+2Bil6z5jeEELYsD77cNUt8jcS2lmbHdfV/Mlnc23ajs2O79tZnB+bvHZz9qZF6zvl50bsqMqOjSu/6LhcmxlrsvMlhuzJj/nd9GL2RkYHl+fHv++YlR0L++yzz+XadBj/3tejPNdm4MC+me1ei/Ljebe2es/Y3Lb5cfRTjsneAOiddvmb4LTcmR03edjG/I2N1h2UHfv5+tn5seQ9F2fHDv/Xpvx7OvGo7Pt+7L35mrbsPUNP2479ZK7Nt5/N9vEbp4/PtWn+ZPaGO122tM21eXFkdgzrI5/PfxVqHs0+j9YjynJtdi3LjpFc9ev8mMRBPQ/NbNfOj9zg5shjM9t33P9Ars1n/y37Xiz+ye9zbbqNzH5+V7yRv6lf3x7ZeQ5ttjXPtVm3Lvv6rH0gP0/ls0dk50LdN/nRXJtTJ5yW2X7+M/mxsYccnZ2XsrJ/fpLF5Ney8znGHZufi7BpffZ7sG1Hfj7H7hHZ8cRvR+Y9bNmaPSZ0Lss/99azlmS2+ww7N9dm6LzsmNrXe0Tei3XZccG9puXH6k57z5j+TS3z8wBGt8veWG1mVf5xVrbIHjdWb8vPFUtR1jr7/Kq35I8VfWqyn89lf8jfxOrTZ2W/0ytn5ufYdWmX/U4v254fk96vKNvmwIH5m479967sd7F3u/xchMLc7GPv6pb/7G1ckT3/VM3Lz/lbdGH2HDHjnfyY/W1bsueElW/n5xD8S8jOhdg2/Re5NmPOyd7k8fVX8u/14NnZm7Lt+kR+jHzx2uyY+JO2Tci1ufi67Byk3Zflz/vlg7Lnmn8o5Oeihf/OHhuOu+jQXJO5M7KfhbKSAbk23Xpnj+VVLdfl2py0ITu3av69+XlUHT9xXGa76Ij8nLa322Tfr8HF+c/PEUPLMttLX8vfOLJ2ffZY0WF0ZH7ShOz3a8Oe/DyV8n7Z33lqN6zOtek6LDuv6MCB+XPf7nbZfS1fk59jUVuUnb/xwvr8ze/mv+dv17N35l/nPh2Py2yvaf5Yrk3PAdk5OtVl+fe9fef3zEV7O38DuHEXfT2zPW/+vbk2LdqOzGwPaZ///aH7iuwchnXL8vNmdvfOzrNtfkh+vssLrbI3RB3aIr+vmXOy87U2DM5/Dlv0zb7Ov9uZvwljj1XZz8buofnPxro12ePN5ifzx9Uv5ZI8VywAAIBkCgsAACCZwgIAAEimsAAAAJIVFQqF/KxnAACAv4ErFgAAQDKFBQAAkExhAQAAJFNYAAAAyRQWAABAMoUFAACQTGEBAAAkU1gAAADJFBYAAEAyhQUAAJBMYQEAACRTWAAAAMkUFgAAQDKFBQAAkExhAQAAJFNYAAAAyRQWAABAMoUFAACQTGEBAAAkU1gAAADJFBYAAEAyhQUAAJBMYQEAACRTWAAAAMkUFgAAQDKFBQAAkExhAQAAJFNYAAAAyRQWAABAMoUFAACQTGEBAAAkU1gAAADJFBYAAEAyhQUAAJBMYQEAACRTWAAAAMkUFgAAQDKFBQAAkExhAQAAJFNYAAAAyRQWAABAMoUFAACQTGEBAAAkU1gAAADJFBYAAEAyhQUAAJBMYQEAACRTWAAAAMkUFgAAQDKFBQAAkExhAQAAJFNYAAAAyRQWAABAMoUFAACQTGEBAAAkU1gAAADJFBYAAEAyhQUAAJBMYQEAACRTWAAAAMkUFgAAQDKFBQAAkExhAQAAJFNYAAAAyRQWAABAMoUFAACQTGEBAAAkU1gAAADJFBYAAEAyhQUAAJBMYQEAACRTWAAAAMkUFgAAQDKFBQAAkExhAQAAJFNYAAAAyRQWAABAMoUFAACQTGEBAAAkU1gAAADJFBYAAEAyhQUAAJBMYQEAACRTWAAAAMkUFgAAQDKFBQAAkExhAQAAJFNYAAAAyRQWfKwVFRWF73znOx/pPvfs2RO+9rWvhV69eoVmzZqFM8888yPdPwBZf49zwXe+851QVFSU9G/Xr1//ofYJ/t4UFuyX7rjjjlBUVFT3X2lpaRg8eHD44he/GNasWfOR9mXJkiWhqKgoXH/99R/K4/3qV78KP/rRj8LZZ58d7rzzzvCVr3wlzJ49O3znO98JS5Ys+VD2AdAU7I/ngnf/a9asWejUqVOYOHFieOGFFz7Svvy1733ve+HBBx9ssP3DXytp6A7AB7n22mtDv379wo4dO8Kzzz4bbr311vDII4+EWbNmhdatWzd09+rlqaeeCj179gw//vGP67L77rsvXHPNNeG4444Lffv2bbjOAeyH9qdzwfnnnx9OOeWUUFNTE956661wyy23hPHjx4dp06aFgw8+uK7dt7/97fCNb3zj796f733ve+Hss8929Zv9gsKC/drEiRPDoYceGkII4bLLLgvl5eXhhhtuCA899FA4//zzG7h39bN27dpQVlbW0N0AaDT2p3PB6NGjw4UXXli3XVlZGSZOnBhuvfXWcMstt9TlJSUloaTEr1l8vBgKRaMyYcKEEEIIixcvDiGEcP3114dx48aF8vLy0KpVqzBmzJhw33335f7dzp07w1e+8pVQUVER2rVrFyZNmhSWL1/+ofZt586d4eqrrw4DBw4MLVu2DL169Qpf+9rXws6dO0MI/+8y+pQpU8Kbb75Zdzn9jjvuCOecc04IIYTx48fX5X/5y18+1P4BNBX707mgsrIyhBDCwoULM3lsjkV1dXW48sorQ+fOnev2v2LFived41FVVRUuueSSUFZWFjp06BAuvfTSsH379rqfFxUVhW3btoU777yz7txxySWXJD0fSKGUplF598BdXl4eQgjhJz/5SZg0aVL4zGc+E3bt2hXuvvvucM4554SHH344nHrqqXX/7rLLLgt33XVXuOCCC8K4cePCU089lfl5qtra2jBp0qTw7LPPhssvvzwMGzYszJw5M/z4xz8Ob731VnjwwQdDRUVFmDx5cvjud78btm7dGr7//e+HEEIYNGhQuPLKK8NNN90UvvnNb4Zhw4aFEELd/wHI2p/OBe/OjevYseNe215yySXhnnvuCRdddFE44ogjwtNPP/2B+z/33HNDv379wve///0wffr08Mtf/jJ06dIlXHfddSGEECZPnhwuu+yyMHbs2HD55ZeHEEIYMGBA0vOBJAXYD91+++2FEELhiSeeKKxbt66wbNmywt13310oLy8vtGrVqrB8+fJCoVAobN++PfPvdu3aVRg+fHhhwoQJddmMGTMKIYTCFVdckWl7wQUXFEIIhauvvvoD+7J48eJCCKHwox/96H3bTJ48udCsWbPC1KlTM/nPfvazQgih8Nxzz9Vlxx57bOGggw7KtLv33nsLIYTClClTPrAvAB8n++O54JprrimsW7eusHr16sLUqVMLhx12WCGEULj33nsz7a+++urCX/+a9eqrrxZCCIWrrroq0+6SSy7J7f/df/vZz3420/ass84qlJeXZ7I2bdoULr744g/sO3xUDIViv3b88ceHioqK0KtXr3DeeeeFtm3bhgceeCD07NkzhBBCq1at6tpu3LgxbNq0KVRWVobp06fX5Y888kgIIYQrr7wy89hXXXXVh9bPe++9NwwbNiwMHTo0rF+/vu6/dy/XT5ky5UPbF8DHzf50Lrj66qtDRUVF6NatW6isrAxz5swJ//Ef/xHOPvvsD/x3jz32WAghhCuuuCKTf+lLX3rff/P5z38+s11ZWRk2bNgQNm/e/Df1GT4qhkKxX7v55pvD4MGDQ0lJSejatWsYMmRIaNbs/9XDDz/8cPj3f//3MGPGjLq5DCGEzLjWpUuXhmbNmuUuDw8ZMuRD6+f8+fPDnDlzQkVFRfTna9eu/dD2BfBxsz+dCy6//PJwzjnnhB07doSnnnoq3HTTTaGmpmav/+7d/ffr1y+TDxw48H3/Te/evTPb7w632rhxY2jfvv3f1G/4KCgs2K+NHTu2biWQ95o6dWqYNGlSOOaYY8Itt9wSunfvHpo3bx5uv/328Nvf/vYj7WdtbW04+OCDww033BD9ea9evT7S/gA0JfvTuWDQoEHh+OOPDyGEcNppp4Xi4uLwjW98I4wfP/59+1hfxcXF0bxQKHyo+4EPi8KCRuv+++8PpaWl4fHHHw8tW7asy2+//fZMuz59+oTa2tqwcOHCzF+m5s2b96H1ZcCAAeH1118Pn/jEJ+p1p9X63p0V4OOuoc8F3/rWt8IvfvGL8O1vf7tuuFPMu/tfvHhxGDRoUF2+YMGCpP07f7A/MceCRqu4uDgUFRVlLkEvWbIkdwfSiRMnhhBCuOmmmzL5jTfe+KH15dxzzw0rVqwIv/jFL3I/q66uDtu2bfvAf9+mTZsQwv8sLQjAvmvoc0FZWVn43Oc+Fx5//PEwY8aM92130kknhRBC5l4XIYTw05/+NGn/bdq0ce5gv+GKBY3WqaeeGm644YZw8sknhwsuuCCsXbs23HzzzWHgwIHhjTfeqGs3cuTIcP7554dbbrklbNq0KYwbNy48+eSTf/NfiZ588smwY8eOXH7mmWeGiy66KNxzzz3h85//fJgyZUo46qijQk1NTZg7d2645557wuOPP/6Bl8hHjhwZiouLw3XXXRc2bdoUWrZsGSZMmBC6dOnyN/UR4OPmoz4XxHz5y18ON954Y/jBD34Q7r777mibMWPGhE996lPhxhtvDBs2bKhbbvatt94KIdT/ysOYMWPCE088EW644YbQo0eP0K9fv3D44YfX+7lACoUFjdaECRPCbbfdFn7wgx+Eq666KvTr1y9cd911YcmSJZmTSQgh/OpXvwoVFRXhN7/5TXjwwQfDhAkTwh//+Me/ae7DY489Fr3M3bdv3zB8+PDw4IMPhh//+Mfh17/+dXjggQdC69atQ//+/cOXv/zlMHjw4A987G7duoWf/exn4fvf/374x3/8x1BTUxOmTJmisADYi4/6XBDTo0ePcMEFF4TJkyeHhQsXvu+9JH7961+Hbt26hd/97nfhgQceCMcff3z4/e9/H4YMGRJKS0vrte8bbrghXH755eHb3/52qK6uDhdffLHCggZTVDADCACgQcyYMSOMGjUq3HXXXeEzn/lMQ3cHkphjAQDwEaiurs5lN954Y2jWrFk45phjGqBH8OEyFAoA4CPwwx/+MLz66qth/PjxoaSkJDz66KPh0UcfDZdffrllyWkSDIUCAPgI/PnPfw7XXHNNmD17dti6dWvo3bt3uOiii8K3vvWtUFLib700fgoLAAAgmTkWAABAMoUFAACQTGEBAAAkM1OIJu3Cr54XzU879exovua6F6L5EZfEb3b0f9/aFc2/+29X7b1zADSoR267K5pvfHB9NF80blU0LwxsEc2r2iyJ5jecMnnvnYNGyBULAAAgmcICAABIprAAAACSKSwAAIBkJm/TpJ058fxoPnfKomg+Zszh0fzfHpsWzY/53FH16xgADe6H89+I5gcPGxXN225fEs1P3jUkmt//ZnxSdzhlr12DRskVCwAAIJnCAgAASKawAAAAkiksAACAZAoLAAAgmVWhaNIe//26aH7p1viKH2+9MyeaD7uwYzQveufp99nzmXvrGgANbHzvgdG8b5fl0bymqjSaz5ndPpr32DOofh2DRsoVCwAAIJnCAgAASKawAAAAkiksAACAZAoLAAAgmVWhaNLKesY/4mVrVkbz6uZV0XxPizbR/K7nX4/m3zx1730DoGEdPndmNF+4u2U0b9uyRTRfsXtBfAcdN7/Pniv31jVolFyxAAAAkiksAACAZAoLAAAgmcICAABIZvI2TdpnPzUmmv/bwWdF82tn/1c0X/7QY9H8u8ebpQ3QWM3sWBHNO1V3i+Zb5z0ezXv0HR/N36iKLxQCTZUrFgAAQDKFBQAAkExhAQAAJFNYAAAAyRQWAABAMqtC0aTd/5t7o/nxX/56NL9v2uJoPqzZwdF89Qtvx3ccXyAEgP3IzjGFaL5+UddoXuh6WDQveTF+Lhj/T8fXr2PQSLliAQAAJFNYAAAAyRQWAABAMoUFAACQTGEBAAAksyoUTdrupZui+WEXDo3mt8xbEc07N+8bzR98fkc0/1977xoADaxqTXU07/vm3Gi+qm28/YDj+kXz5QseeJ89n7jXvkFj5IoFAACQTGEBAAAkU1gAAADJFBYAAEAyhQUAAJDMqlA0aV13HxTNFz8yJ5oP67Q7mlf16BLNzz29Z/06BkCD61tVFM0Hba2N5l26F0fzZXsqovma7X3r1S9orFyxAAAAkiksAACAZAoLAAAgmcICAABIprAAAACSWRWKJm3tcX2i+bzty6L5qNF9o/ltt8+N5qeN712vfgHQ8Favj/8atOnQttF8YOn6aN7mgXeieY9Pn16/jkEj5YoFAACQTGEBAAAkU1gAAADJFBYAAEAyhQUAAJDMqlA0ab1GLYrmw0ZviObDC0Oj+Z/+EK/Bt6/vXL+OAdDg+rfsH803zV4czRed1i+aDzypVzR/etnk99nz9/faN2iMXLEAAACSKSwAAIBkCgsAACCZwgIAAEimsAAAAJJZFYombdOLM6L5uum/j+ZrS9ZG805tT4zm3ct21qtfADS8qubx1Z86lhRH853V8fyZJ6dF8z7HdKhfx6CRcsUCAABIprAAAACSKSwAAIBkCgsAACCZyds0aYevPiaab357QTR/+8D4JO1TBo+M5rOXTK9XvwBoeCvbtIr/YERpNK6aPTeajzt1XDR/asmM+nQLGi1XLAAAgGQKCwAAIJnCAgAASKawAAAAkiksAACAZFaFokl7YVaXaL5i6PnRfF7FAdF8wMzfRvOKDWPq1zEAGlz/jW2ieXFp/NejAzcWRfPVu5+N5qUlLerXMWikXLEAAACSKSwAAIBkCgsAACCZwgIAAEimsAAAAJJZFYom7bjre0XzGxb8PJqf0Xd4NO/bpmM0L126s34dA6DBbV1QG8079oi331bRLpo3L9kdzU84eEi9+gWNlSsWAABAMoUFAACQTGEBAAAkU1gAAADJFBYAAEAyq0LRpN3z6jXxH3SaHY3bvVmI5qtfHRDNV1Q0j+bH7r1rADSw5gf0i+ZbO66O5ttb74rmnXbEzwVz35gWzSedce4+9A4aH1csAACAZAoLAAAgmcICAABIprAAAACSKSwAAIBkVoWiSTu217ei+YaZO6N5u//7fDRv3S++WtTCsiX16hcADa/94JXRvKaqOpov2rU+mm9c0CWaH3DYqPp1DBopVywAAIBkCgsAACCZwgIAAEimsAAAAJIpLAAAgGRWhaJJu/GSn0Xzfxn2vWjevVvraD53wB3RfObsTvXqFwANb8fmqmi+fGb3aD5gYG00Lzsnfu544c/T3mfPF+yta9AouWIBAAAkU1gAAADJFBYAAEAyhQUAAJDM5G2atK/fMTiab//LTdH89XVF0Xx2+0I07zRgXP06BkCDW78+ng/pvyuab9pSFc23L4q3L93Wpj7dgkbLFQsAACCZwgIAAEimsAAAAJIpLAAAgGQKCwAAIJlVoWjSZjy9Ipr3Lz8kmu/otjyat2ndOZqvmj/vffZcude+AdCwCv17RvPXSuKrPPV8pTiaV721J5qPPapP/ToGjZQrFgAAQDKFBQAAkExhAQAAJFNYAAAAyRQWAABAMqtC0aQdNbRvNK9+rjaar5vYKZp3mPZGND9pwZL6dAuA/UDp8J3RfPtTRdG8Xa+h0Xzl2ObR/LUVc6L5mXvvGjRKrlgAAADJFBYAAEAyhQUAAJBMYQEAACRTWAAAAMmsCkWTdu/kN6P5hlHjo/mTr8+O5n84a1w0f77qT/XrGAANb/GsaHzw+oOj+ZZdi6P5pk/FH37mXVPjP7horz2DRskVCwAAIJnCAgAASKawAAAAkiksAACAZAoLAAAgmVWhaNKGfel/RfM/7o6v/vTJAw6J5nf+8D+jefuzzqtfxwBocIuX9Yrmh3WrjuZd2+2K5r++5pVo/v1jL61fx6CRcsUCAABIprAAAACSKSwAAIBkCgsAACCZwgIAAEhmVSiatDUrn4zmJx3SJpoftHJFNO9YOzSaP/pKfOWQ8Mm99w2AhtWtVadovrrq9Wi+9n3aX/DJr0Xzeavfiuan70PfoDFyxQIAAEimsAAAAJIpLAAAgGQKCwAAIJnCAgAASGZVKJq0FvOnRPPtRQOj+Zr/UxXNhwweHc17/lPnevULgIbXuawmmm9u/k40f6dF/Jh/4KxF0Xzxutn16xg0Uq5YAAAAyRQWAABAMoUFAACQTGEBAAAkM3mbJq1jx17R/KhN8cnbi7v0iObbexeieevm6+rXMQAa3PKVm6N5u7LB0Xzdzt3RfPaeomjee0D/+nUMGilXLAAAgGQKCwAAIJnCAgAASKawAAAAkiksAACAZFaFokmbMr9VNG915Zho3vnN+6L58r/Mi+YdzzuuXv0CoOG1XBJf5anHqfFzxOLXl0fzd1rGH79Fh2716hc0Vq5YAAAAyRQWAABAMoUFAACQTGEBAAAkU1gAAADJrApFk3bUP3SJ5nf+1xPR/B8mjIjmUw/pF81LF+yJ5seN34fOAdCghhzZOZq/+urSaN5pW200H/vlI6L5lEuviu/40rP32jdojFyxAAAAkiksAACAZAoLAAAgmcICAABIprAAAACSWRWKJm1I9a5oPvp98kG1R0XzV1b9KppXdCjUr2MANLjFm5dF89ajD4nmHefNj+a/vuH/i+af+afP1K9j0Ei5YgEAACRTWAAAAMkUFgAAQDKFBQAAkExhAQAAJLMqFE3aQw/viObl/bpE8+kl66L5rhYronlFoah+HQOgwbXcMiSab962O5pPe2JeND/lcxdF82efiZ87zjt3HzoHjZArFgAAQDKFBQAAkExhAQAAJFNYAAAAyRQWAABAMqtC0aR16985mpevej6aV7/VPpr36XZgNN85d2P9OgZAgysvbR3N/7To7Wh+1aWfiuZ/fumlaD706NH16xg0Uq5YAAAAyRQWAABAMoUFAACQTGEBAAAkU1gAAADJrApFk1a054loPqHbkGj+avGqaN78T5uieZ+BA+rXMQAaXPs1d0fzMzudFs1ffOCZaD7s1Pg55Z1dU95nz2furWvQKLliAQAAJFNYAAAAyRQWAABAMoUFAACQzORtmrRBo8dH8+ceqo3mS1fOjOb9N58SzXev61K/jgHQ4Aq9Rkfzd+avj+ZdB3eM5rVvb47mzUr61qtf0Fi5YgEAACRTWAAAAMkUFgAAQDKFBQAAkExhAQAAJLMqFE3aiy+0iub9agdG8wvO6BTN//vmFtF8d4+iaD5uH/oGQMN6Y1HLaN5mc/zvrjsO6BzN1y5dHc1794k/PjRVrlgAAADJFBYAAEAyhQUAAJBMYQEAACRTWAAAAMmsCkWTtq317mhe2nZ9vP2K5dG8ef/4V+XluVOi+T+GY/ahdwA0pJLF8XNE75Fl0fy1DbXR/JhLD47mD7/+ZL36BY2VKxYAAEAyhQUAAJBMYQEAACRTWAAAAMkUFgAAQDKrQtGktV+4LJq3GloTzbv1PTKaN28Xf5ywcEC9+gVAwys7qnU0bz8o/nfX7rt7RPPrf/HLaN7vjIPq1zFopFyxAAAAkiksAACAZAoLAAAgmcICAABIprAAAACSWRWKJm3AQUuj+ZI+G6P5V29eHs0nnTgsmk9/c2v9OgZAg3ts0fxofsT25tF81/I50fy8Iy6L5ut37Kxfx6CRcsUCAABIprAAAACSKSwAAIBkCgsAACCZwgIAAEhmVSiatG5F8Y/4lpWro/lXP9Uzmj/95wej+Zc//6V69QuAhnf26FOj+dhXZ0bzWZviKwoufTWeb+1XXL+OQSPligUAAJBMYQEAACRTWAAAAMkUFgAAQDKTt2nSlq8dFM03dB4VzV+Y9XQ0P7xdRTQf/fzs+I7POGHvnQOgQW2c96doPmTlsGj+u3E10by05cpo3qvT1vp1DBopVywAAIBkCgsAACCZwgIAAEimsAAAAJIpLAAAgGRWhaJJ2z2gRzRf1j3+0e/XekQ0n1uxLZrPf2R+NP+HfegbAA2r84C+0fyOVfEV/7ZuKY7mvbvEV4sKHf39lo8Xn3gAACCZwgIAAEimsAAAAJIpLAAAgGQKCwAAIJlVoWjSHp+7PJoXaoZH89XNNkbzLdPWRPMxQ+KPA8D+b92GXdG8ZGifaH5WdfNovmFX/Byxo3Zw/ToGjZQrFgAAQDKFBQAAkExhAQAAJFNYAAAAyRQWAABAMqtC0aSd3qx1NC9+O76CR7vHb4/mc845IprPfGdGND9v710DoIFtLX89mlcUHxDNZ67sH82rV7WJ5s889EA0v/AMZwmaJlcsAACAZAoLAAAgmcICAABIprAAAACSKSwAAIBkVoWiSes7KZ4/Oze+Esh3P3tINP/Pw5dE8+XvvM8OANjv9T08vnLgvOc3R/Mu81dG814TyqL5+YceXa9+QWPligUAAJBMYQEAACRTWAAAAMkUFgAAQDKFBQAAkMyqUDRpW+b0jubVyxdF8+t2V0XztX1HRfMVD/wlvuOjzt1b1wBoYNvXV0fzbXsqonmLS/tG81+9+PNoftC2QdH8wr13DRolVywAAIBkCgsAACCZwgIAAEimsAAAAJIpLAAAgGRWhaJJW/bKvGj+qQM7RvM3/1QZzZutaRPNR7bfVb+OAdDglq8dEs0PaDMsmk+966FofsVF46P5rs0t6tcxaKRcsQAAAJIpLAAAgGQKCwAAIJnCAgAASGbyNk1aVcvl0XzD8HhNvW1eUTQfX9gdzWu2bK5fxwBocG2ar4rmraa1jOZnrT8zmi/51uJoftDxB8Z3HH8YaPRcsQAAAJIpLAAAgGQKCwAAIJnCAgAASKawAAAAklkViiatpmJENH+lYmM07zi2bTR//bnV0fygLe3r1zEAGlyLHaXRfFVJIZqv67clmn/ihIOj+bYNr77Pno/fa9+gMXLFAgAASKawAAAAkiksAACAZAoLAAAgmcICAABIZlUomrTeYWs0f+OOZdH8lEPaRPOtT74Vzece84lofuQ+9A2AhlUb4iv7dd/RMZpv6LU+mj89fXY0P3b4sPp1DBopVywAAIBkCgsAACCZwgIAAEimsAAAAJIpLAAAgGRWhaJJe2zz8mi+/fjx0fzlP9RE84kHXxbNl817rX4dA6DBdXhzRDTfvWJxNC8d2y6a939nYzR//uVXo/lx4TP70DtofFyxAAAAkiksAACAZAoLAAAgmcICAABIprAAAACSWRWKJu3oIV2j+dM7WkbzVSe0jeYP/p/no3nvdvEVRQDY/7V5ckU0r+q7KZ732xXNtx15aDQvuueR+nUMGilXLAAAgGQKCwAAIJnCAgAASKawAAAAkiksAACAZFaFokkbvfagaL5udXU071y+MZoP6Novmm/65Nr6dQyABvfp9vFzwW/HxY/tMxfPiebNppdG88N3x88d0FS5YgEAACRTWAAAAMkUFgAAQDKFBQAAkExhAQAAJLMqFE3avE2dovnru2ZE88qSndG8rMOYaF41fWp8x5futWsANLAVbQbF823xc8dpE+PngrmT/hTNh046un4dg0bKFQsAACCZwgIAAEimsAAAAJIpLAAAgGQmb9OkPbt4ejQ/9bpW0XzqNT+N5lf2+lo0bzbnjfp1DIAG92av2mjeY0mvaL76gYXRvPzaa6P5lPW/jOZD96Fv0Bi5YgEAACRTWAAAAMkUFgAAQDKFBQAAkExhAQAAJLMqFE1a55abo3nF8vJo3m/MxGj+1A8WR/OhVwysX8cAaHDbexSi+dyZv4/mpQcfFM03z7szmncu3V6/jkEj5YoFAACQTGEBAAAkU1gAAADJFBYAAEAyhQUAAJDMqlA0aUNGj4jmr/zno9H86H+YEM2XddsVzXsOnPs+ez56r30DoGG9ULE0/oMj1kfjFt2ejOZbl/SN5ovWxc8d0FS5YgEAACRTWAAAAMkUFgAAQDKFBQAAkExhAQAAJLMqFE3aG8viK370qiiP5tVzt0TzHQdOieZzu3eK5oftQ98AaFjD2xwQzd+8Z0A0P+iweF68J75CYK+2/n7Lx4tPPAAAkExhAQAAJFNYAAAAyRQWAABAMoUFAACQzKpQNGnH1VRH8/MqB0Xzs25+I5r3P/WKaL7w8Uei+UWH7kPnAGhQh963MZr3fCd+EJ+/bXs0b772wGi+rs3W+nUMGilXLAAAgGQKCwAAIJnCAgAASKawAAAAkiksAACAZFaFoklbs31bNH/mj9Oj+cjPnB7Npz8fX/3p1P6F+nUMgAa3vlVVNH/hvNJoPrP6j9G814ajovnQ2hb16hc0Vq5YAAAAyRQWAABAMoUFAACQTGEBAAAkU1gAAADJrApFk1ZbMSiaP7PglWhe1m9lNB+8aEs0H7ZlU/06BkCDWzK8Ov6D3rOi8a7m8XPEoIO7RPPq1VX16RY0Wq5YAAAAyRQWAABAMoUFAACQTGEBAAAkM3mbJq1dVXzS9fhmI6L5a6++E81H9O0WzbfWHFy/jgHQ4FpNOzCat++4Ppof3rVnNN+wcUY0H1hSVp9uQaPligUAAJBMYQEAACRTWAAAAMkUFgAAQDKFBQAAkMyqUDRp69rHa+eVI7pG81ue+HU0/9U1X47mT14/M5qf8tlP7UPvAGhIFzy0Npr/ttMh0fzZTbOjeZeSrdF87lsL6tcxaKRcsQAAAJIpLAAAgGQKCwAAIJnCAgAASKawAAAAklkViibt1WfjqzYNP3x8NB8/5rPxx/nvt6J5cU27+nUMgAY3tdfyaN6rcmw0r+yyKpo/+H/jj3Pul/+pfh2DRsoVCwAAIJnCAgAASKawAAAAkiksAACAZAoLAAAgmVWhaNI+8/9dEc3//PBvovmIrx4XzVs92jGaLy/Mr1e/AGh4z196SDTv0X5tNJ8+4/FoPmTExGi+8C/PxHd82Gl77xw0Qq5YAAAAyRQWAABAMoUFAACQTGEBAAAkU1gAAADJrApFk/Zfv70vmo/YODuaN3+pezT/7wdfjOZntOtXv44B0OC6Dd0ezddUxVf8a9s2/mtTy+7ro/nm52rr1zFopFyxAAAAkiksAACAZAoLAAAgmcICAABIprAAAACSWRWKJu3obiui+Wknjo7mv34hvvrTcaeeEM2L765fvwBoeJtbLYvmCxbHV3kaMeLsaL5+xyvRvKJth/p1DBopVywAAIBkCgsAACCZwgIAAEimsAAAAJKZvE2TNrJ8WDR/a2jvaL55Xqtovrjt2mje7KB4ewD2f0VbB0TzPrU10fyt1S2jebOOJ0Xzdp231a9j0Ei5YgEAACRTWAAAAMkUFgAAQDKFBQAAkExhAQAAJLMqFE3alDXxFTnWzNsdzYcWt4nmb0zdGM27tRtdv44B0ODmPBpf/WlC/z7RfOfGRdF8zY520Xzd5p316xg0Uq5YAAAAyRQWAABAMoUFAACQTGEBAAAkU1gAAADJrApFk3baQfGVPX714F+i+f8eOTaa/2JDUTTfWFZdr34B0PCG71oezfdsia8EeNyO7dH8iar4ioJtDutdv45BI+WKBQAAkExhAQAAJFNYAAAAyRQWAABAMoUFAACQzKpQNGl7mq2L5sMPiK/g8cq0R6P5qaPOjea/KfYVAmislg3fE80Xj3ojmh/ddkg03/3LJdG8c4cu9eoXNFauWAAAAMkUFgAAQDKFBQAAkExhAQAAJFNYAAAAySxpQ5M2a04hmm/t0DWa31PZOpp3bfdKNN/Ys9v77PmUvfYNgIY1eHn876ttWw+N5sXNKqJ5aXn8XDPn7Tn16xg0Uq5YAAAAyRQWAABAMoUFAACQTGEBAAAkU1gAAADJrApFkzZ+4OBovrFmTzSf3rlnNH/oV/dH8199tn/9OgZAg+u9om0079xseDSfHjZE8/bF66N5Rcfu9esYNFKuWAAAAMkUFgAAQDKFBQAAkExhAQAAJFNYAAAAyawKRZM2ecamaF7cP15Tv73y19H8H8adF81XvL4umg86ZR86B0CD2jBqfjRfuG1XNF8zqGM03752ZjTfucbKgXy8uGIBAAAkU1gAAADJFBYAAEAyhQUAAJDM5G2atP4nxCfgLd+6KJp/+swDo/nCV5ZF84fejE/8Oy6cuw+9A6Ahbe/VOZrPfSX+d9cdezZH83M+PSaa/3Ly0vp1DBopVywAAIBkCgsAACCZwgIAAEimsAAAAJIpLAAAgGRWhaJJm79qdTSvXTcwmj9z95Zo3rtF82jeo+NB9esYAA3uyWXPRfNPHf3daP7im09E8xmPrYnm/cYcUb+OQSPligUAAJBMYQEAACRTWAAAAMkUFgAAQDKFBQAAkMyqUDRpb85oEc3PPvfYaH7ws89E8+0rq6L5tKG769UvABreJwZ+KpovefLhaN68d3zlwA7NOkTzNjPnx3d89t77Bo2RKxYAAEAyhQUAAJBMYQEAACRTWAAAAMkUFgAAQLKiQqFQaOhOAAAAjZsrFgAAQDKFBQAAkExhAQAAJFNYAAAAyRQWAABAMoUFAACQTGEBAAAkU1gAAADJFBYAAECy/x9Fk78PLgBVQAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 800x800 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_37220\\301226605.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     51\u001b[0m               \u001b[0mnum_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_step\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m               \u001b[0mtraining_phase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m               epoch=0)\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             names_weights_copy, prompted_weights_copy = maml_system.model.apply_inner_loop_update(\n",
      "\u001b[1;32m~\\PycharmProjects\\prompt_maml\\few_shot_learning_system.py\u001b[0m in \u001b[0;36mnet_forward\u001b[1;34m(self, x, y, weights, backup_running_statistics, training, num_step, training_phase, epoch, prompted_weights, prepend_prompt, inner_loop)\u001b[0m\n\u001b[0;32m    461\u001b[0m                                                               \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m                                                               \u001b[0mbackup_running_statistics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbackup_running_statistics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m                                                               num_step=num_step, prepend_prompt=prepend_prompt)\n\u001b[0m\u001b[0;32m    464\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\prompt_maml\\meta_neural_network_architectures.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, num_step, params, prompted_params, training, backup_running_statistics, task_embedding, prepend_prompt)\u001b[0m\n\u001b[0;32m    963\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprompter\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mprepend_prompt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m             \u001b[1;31m# get_task_embeddings을 통해 호출될때는 prompt를 추가하지 않는다\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 965\u001b[1;33m             \u001b[0mprompted_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprompt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprompted_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprompted_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    966\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprompted_image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\maml\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\prompt_maml\\prompters.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, prompted_params)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;31m# print(\"pad_right == \", pad_right.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[0mbase\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m         \u001b[0mprompt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpad_left\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad_right\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[0mprompt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpad_up\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad_down\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_data = maml_system.data.get_train_batches(total_batches=int(600/1), augment_images=False)\n",
    "\n",
    "for sample_idx, train_sample in enumerate(train_data):\n",
    "    \n",
    "    x_support_set, x_target_set, y_support_set, y_target_set, seed = train_sample\n",
    "    \n",
    "    x_support_set = torch.Tensor(x_support_set).float().to(device=maml_system.model.device)\n",
    "    x_target_set = torch.Tensor(x_target_set).float().to(device=maml_system.model.device)\n",
    "    y_support_set = torch.Tensor(y_support_set).long().to(device=maml_system.model.device)\n",
    "    y_target_set = torch.Tensor(y_target_set).long().to(device=maml_system.model.device)\n",
    "    \n",
    "    for task_id, (x_support_set_task, y_support_set_task, x_target_set_task, y_target_set_task) in enumerate(zip(x_support_set,\n",
    "                              y_support_set,\n",
    "                              x_target_set,\n",
    "                              y_target_set)):\n",
    "        \n",
    "        \n",
    "        names_weights_copy = maml_system.model.get_inner_loop_parameter_dict(maml_system.model.classifier.named_parameters())\n",
    "\n",
    "        num_devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "\n",
    "        names_weights_copy = {\n",
    "                name.replace('module.', ''): value.unsqueeze(0).repeat(\n",
    "                    [num_devices] + [1 for i in range(len(value.shape))]) for\n",
    "                name, value in names_weights_copy.items()}\n",
    "\n",
    "        prompted_weights_copy = {}\n",
    "        if args.prompter:\n",
    "            prompted_weights_copy = {key: value for key, value in names_weights_copy.items() if 'prompt' in key}\n",
    "\n",
    "        names_weights_copy = {key: value for key, value in names_weights_copy.items() if 'layer_dict' in key}\n",
    "        \n",
    "        \n",
    "        n, s, c, h, w = x_target_set_task.shape\n",
    "\n",
    "        x_support_set_task = x_support_set_task.view(-1, c, h, w)\n",
    "        y_support_set_task = y_support_set_task.view(-1)\n",
    "        x_target_set_task = x_target_set_task.view(-1, c, h, w)\n",
    "        y_target_set_task = y_target_set_task.view(-1)\n",
    "        \n",
    "        \n",
    "        for num_step in range(5):\n",
    "\n",
    "            support_loss, support_preds = maml_system.model.net_forward(\n",
    "              x=x_support_set_task,\n",
    "              y=y_support_set_task,\n",
    "              weights=names_weights_copy,\n",
    "              prompted_weights=prompted_weights_copy,\n",
    "              backup_running_statistics=num_step == 0,\n",
    "              training=True,\n",
    "              num_step=num_step,\n",
    "              training_phase=False,\n",
    "              epoch=0)\n",
    "            \n",
    "            names_weights_copy, prompted_weights_copy = maml_system.model.apply_inner_loop_update(\n",
    "             loss=support_loss,\n",
    "             names_weights_copy=names_weights_copy,\n",
    "             prompted_weights_copy=prompted_weights_copy,\n",
    "             use_second_order=True,\n",
    "             current_step_idx=num_step,\n",
    "             current_iter='test',\n",
    "             training_phase=False)\n",
    "            \n",
    "            # print(\"num_step == \", num_step)\n",
    "            # prompted_images = maml_system.model.classifier.prompt(x=x_support_set_task, prompted_params=prompted_weights_copy)    \n",
    "            # show_batch(images=prompted_images, labels=y_support_set_task, datasets=datasets)\n",
    "            \n",
    "            if num_step == 4:\n",
    "                \n",
    "                plot_all_pad_prompts(prompted_weights_copy)\n",
    "                \n",
    "                target_loss, target_preds = maml_system.model.net_forward(\n",
    "                    x=x_target_set_task,\n",
    "                    y=y_target_set_task,\n",
    "                    weights=names_weights_copy,\n",
    "                    prompted_weights=prompted_weights_copy,\n",
    "                    backup_running_statistics=False, training=True,\n",
    "                    num_step=num_step,\n",
    "                    training_phase=False,\n",
    "                    epoch=0)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c281cd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
